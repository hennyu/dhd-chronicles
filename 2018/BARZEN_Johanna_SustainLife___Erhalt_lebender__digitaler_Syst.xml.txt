
            
Der digitale Wandel verändert die Wissenschaft grundlegend (Kramp 2013). Das exponentielle Wachstum, die steigende Komplexität sowie der zunehmende Gebrauch von digitalen Forschungsdaten beeinflussen den Forschungsprozess signifikant. Um das Potential der fortschreitenden Digitalisierung optimal nutzen zu können, müssen entsprechende Infrastrukturen geschaffen werden, die das Management von Forschungsdaten, die Möglichkeit ihrer Vernetzung, ihre dauerhafte Verfügbarkeit und einen freien Zugang gewährleisten.

            
Die Vielzahl von wissenschaftspolitischen Empfehlungen, Bestandsaufnahmen, Umfragen und institutionellen Richtlinien rund um Forschungsdaten, die in den vergangen Jahren veröffentlicht wurden, sind ein Zeichen zunehmenden Problembewusstseins, politischen Handlungswillens, aber eben auch anhaltenden Handlungsbedarfs (Pampel / Bertelmann 2011; Wissenschaftsrat 2012; DV-ISA 2016; Rat für Informationsinfrastrukturen 2016). Durch die großen europäischen und nationalen Infrastrukturprojekte in den Geisteswissenschaften (CLARIN, DARIAH), die Einrichtung von fachspezifischen Datenzentren und spezifisch geisteswissenschaftlichen Datenzentren, wie dem Data Center for the Humanities (DCH, s. http://dch.uni-koeln.de), hat sich die Versorgungslage für Forschungsdaten stetig verbessert. Doch längst werden nicht alle produzierten Daten und digitalen Forschungsergebnisse tatsächlich für die Nachnutzung verfügbar gemacht bzw. sind für eine dauerhafte Verfügbarkeit in einer hochdynamischen digitalen Welt gerüstet (Razum / Neumann 2014; Wissenschaftsrat 2012).

            
Die Diskussion fokussiert bisher stark auf Forschungsinformationssysteme zur standardisierten Vorhaltung von Forschungsprimärdaten. Dabei bleibt weitgehend unberücksichtigt, dass ein Großteil der digitalen Produkte in den Geisteswissenschaften in einer Form vorliegen, die sich einer normierten Versorgung bislang entzieht. Gemeint sind Forschungsanwendungen, oder “lebende Systeme” (Sahle/Kronenwett 2013), die einen wesentlichen Bestandteil digitaler Ergebnissicherung darstellen: Präsentationssysteme, interaktive Visualisierungen, Recherche-Datenbanken, digitale Editionen und digitale Arbeitsumgebungen – um nur einige Formen zu nennen – sind Arbeitswerkzeuge, Plattformen der Dissemination und Aggregation von Forschungsergebnissen und sind aus dem Arbeitsalltag in Geisteswissenschaften nicht mehr wegzudenken. Ihre dauerhafte Erhaltung, Betreuung und Bereitstellung über den Zeitraum der Projektförderung hinaus stellt eine große organisatorische und letztlich finanzielle Herausforderung dar.

            
Ein grundlegendes Problem ist die Sorge um softwaregestützte Präsentationen und Funktionalitäten, die in Forschungsprojekten entstehen (Sahle/Kronenwett 2013). Nicht selten sind diese die eigentlichen Träger von „Informationsgehalt beziehungsweise wissenschaftliche[m] Mehrwert“ der im Projekt erbrachten Forschungsleistung (Wuttke et al. 2016). Eine Nachhaltigkeitsstrategie, die auf eine Abtrennung und Archivierung allein der Datenbasis zurückfällt, führt unweigerlich zum Verlust von Information und reduziert im Extremfall den wissenschaftlichen Nutzen auf null (Blumtritt/Mathiak 2016).

            
Forschungsanwendungen sind keineswegs statische Objekte, sondern unterliegen einem kontinuierlichen Veränderungszyklus. Viele Anwendungen fungieren als Plattformen, die User-Input entgegennehmen und damit ihre Datenbasis laufend verändern. Browser-Updates und Veränderungen der Nutzungsgewohnheiten können bestimmte Komponenten unbrauchbar oder obsolet machen und damit eine Überarbeitung des Codes anstoßen. Notwendige Sicherheitsupdates erfordern regelmäßiges Eingreifen und können Kaskaden von weiteren Updates und Softwareanpassungen nach sich ziehen. Der Verzicht auf kontinuierliche Wartung spart kurzfristig Kosten, verschärft mittelfristig aber das Problem. Erfahrungen aus dem “LAZARUS-Projekt” zeigen, dass Anwendungen, die über längere Zeit brach liegen, nur unter großem Ressourceneinsatz wieder revitalisiert werden können (Bingert et al. 2016). Ein erster Ansatz zur Archivierung von Forschungssoftware auf Basis des TOSCA-Standards wurde vorgestellt, löst jedoch nur das Problem der initialen automatisierten Bereitstellung, nicht der Revitalisierung terminierter Systeme (Breitenbücher et al. 2017).

            
Darüber hinaus sehen sich Forschungseinrichtungen mit dem Dilemma konfrontiert, dass befristete Projektlaufzeiten einen dauerhaften Betrieb organisatorisch deutlich erschweren. Dies ist insbesondere in den Geisteswissenschaften fatal, da hier oftmals ein anderer Maßstab bezüglich der Nachhaltigkeit angelegt wird, als dies in den vergleichsweise schnelllebigen Naturwissenschaften üblich ist. Die institutionelle Vorhaltung von Forschungsdaten für einige Jahrzehnte erfüllt den Zweck der Überprüfbarkeit und Reproduzierbarkeit im Sinne der guten wissenschaftlichen Praxis, für die Vorhaltung von Gegenständen des kulturellen Erbes ist eine zeitliche Beschränkung nicht sinnvoll. Selbst dann, wenn sich eine Abschaltung aus Sicherheits- oder Kostengründen nicht vermeiden lässt, so ist zumindest zu gewährleisten, dass Anwendung und Datenbasis auf eine Art und Weise archiviert werden, dass sie sich jederzeit verlustfrei und benutzbar wiederherstellen lassen.

            
Das Projekt „SustainLife“, das in Zusammenarbeit zwischen des Data Center for the Humanities (DCH) aus Köln und des Instituts für Architektur von Anwendungssystemen (IAAS) der Universität Stuttgart durchgeführt wird, adressiert diese Probleme. In diesem Poster stellen wir das Projekt vor und präsentieren den ersten Teil einer Anforderungsanalyse an lebende Systeme der Digital Humanities. Als zweiten Teil soll vor Ort eine Umfrage unter den Teilnehmern der Konferenz durchgeführt werden, um die Anforderungen an lebende Systeme aus unterschiedlichsten Domänen möglichst zielgenau zu analysieren. Diese Anforderungsanalyse stellt die Basis für im Projekt SustainLife geplante Erweiterungen der open-source Software „OpenTOSCA“ (Binz et al. 2013) dar, welche auf dem OASIS-Standard TOSCA (OASIS 2013) basieren. Dieser Standard ermöglicht die Automatisierung des Deployments und Managements von Anwendungen und stellt damit eine vielversprechende Grundlage zur kostengünstigen Sicherung der Nachhaltigkeit lebender Systeme dar.

        
