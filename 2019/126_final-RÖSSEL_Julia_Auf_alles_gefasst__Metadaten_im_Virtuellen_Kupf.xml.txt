
        

            

                
Einleitung

                
In mehreren Kooperationsprojekten der Herzog August Bibliothek Wolfenbüttel und des Herzog Anton Ulrich-Museums wurden von 2007 bis heute rund 100.000 Datensätze zu Kulturobjekten aus den Graphischen Sammlungen beider Institutionen erzeugt. Das Poster stellt Konzepte und Strategien eines Metadaten Assessment vor, das die Daten des Virtuellen Kupferstichkabinetts (www.virtuelles-kupferstichkabinett.de) analysiert. Die Analyse ist Teil einer nachhaltigen Qualitätssicherung für Metadaten beider Kulturinstitutionen und erweist sich als vermittelnde Praxis zwischen medialen Kontexten und aktuellen wie zukünftigen NutzerInnen.

            

            

                
Problemstellung

                
Das Virtuelle Kupferstichkabinett wurde von Beginn an als öffentlich zugängliches Online-Portal konzipiert. Der Blick auf die erzeugten Informationsobjekte wird so von einer Auffassung der Objekte als Komposite von Bildern und zugehörigen Metadaten innerhalb eines im World Wide Web verfügbaren Informationssystems dominiert. Diese visuelle Präsentation kann als Remediation gedruckter Kataloge gesehen werden, an denen sich Erwartungen der NutzerInnen zu orientieren scheinen. 

                
Neben der Präsentation und Publikation von Informationen verpflichten sich Kulturinstitutionen dazu, eine langfristige Perspektive in den Blick zu nehmen. Diese erfordert es, die eigenen Informationsobjekte bzw. -produkte jenseits aktueller Trends der Präsentation im Internet und jenseits einer bestimmten maschinellen Verarbeitung zu denken und entsprechend mit ihnen umzugehen. Sie müssen so konzipiert werden, dass sie innerhalb eines Curation Lifecycle dauerhaft genutzt werden können.

                
Metadaten Assessment geht vom dynamischen Wesen dieser Informationsobjekte aus, die innerhalb eines Nutzungsprozesses verschiedene Formen annehmen, welche jeweils unterschiedliche Praktiken und Epistemologien mit sich bringen. Mangelndes Verständnis für diesen Formwandel ist als ein Hemmschuh für die Metadatenqualität beispielsweise von der Task Force Metdatenqualität der Europeana ausgemacht worden (Dangerfield/Kalshoven 2015: 42). Gerade deshalb und trotz der individuellen Voraussetzungen, die unsere Projektstruktur mit sich bringt, zeigen wir, dass unsere Herangehensweise im Bereich der Kulturdaten im Prinzip übertragbar ist, denn gerade in Übertrag- bzw. Wiederholbarkeit ist Ziel und Grundidee von Metadaten Assessments.

                
Nach nunmehr zehnjähriger Digitalisierungs- und Erschließungsarbeit wird an Informationsobjekten des Virtuellen Kupferstichkabinetts deutlich, dass sich die Praxis der Verzeichnung und Konzepte der Digitalisierung weiter entwickelt haben. Ähnlich wie sich eine stets wandelnde Nutzung auf die Struktur materieller Sammlungen niederschlägt, verändern sich unter anderem auch die Auffassungen darüber, welche Informationen auf welche Weise als Metadaten aufgenommen werden sollen, was ein eigenständiges Informationsobjekt konstituiert oder wie eine adäquate Datenhaltung und -präsentation aussehen sollte. Unterschiedliche Wissenskontexte in den Sammlungen (z.B. wissenschaftliche Herangehensweisen an Zeichnungen und Druckgraphiken) spielen bei ihrer Formung ebenso eine Rolle, wie Unterschiede in der Erfassungstiefe oder Weiterentwicklungen der zugrundeliegenden technischen Systeme. Hinzu kommt die Verschiedenartigkeit der digitalisierten Objekte, die neben Einzelblättern auch Serien, Sammelbände und Graphik in Büchern mit einschließt, deren Bestandteile allerdings nicht in hierarchischer Form verzeichnet werden konnten. All diese Komponenten beeinflussen die Struktur eines „Objektes“, dessen (Feld-)Struktur nur auszugsweise im Poster präsentiert werden wird.

            

            

                
Vorgehen

                
Im Hinblick auf ihre signifikanten Eigenschaften werden die Metadaten des VKK in ihren verschiedenen Dimensionen analysiert, die Literatur aus bibliothekarischen, aber auch ökonomisch orientierten Bereichen entnommen sind. Einzelnen Sektoren im Curation Lifecycle können im Poster beispielhaft Qualitätsdimensionen zugeordnet werden. Dabei gelten für die Nutzbarkeit u.a. die Qualitätsparameter der Korrektheit, Vollständigkeit, Relevanz und Aktualität (Olson 2011: 24-27). Zur Vollständigkeit etwa gehört die Anreicherung durch Normdaten, wo sie noch nicht vorliegen. Im Poster werden die diesbezüglichen Methoden, Fragestellungen und erste Ergebnisse präsentiert werden. Einblicke in den Grad der Interoperabilität und Vollständigkeit können etwa durch die Analyse der Nutzung von Normdaten gewonnen werden. Objekt-Objekt-Beziehungen, die zwischen einzelnen graphischen Blättern hergestellt werden und bislang unterschiedliche Formen des Bezugs abdecken, sind die größeren Problemfelder des Projektes und müssen v.a. unter dem Blickwinkel der Konsistenz analysiert werden. 

                
Ein Metadaten Assessment fragt darüber hinaus nach ökonomisch-institutionellen Kontexten der Datenerzeugung, -nutzung und -haltung. Entsprechend kann im Poster eine Workflowanalyse vorgestellt werden, die aufzeigt, worin bisher angewandten Maßnahmen zur Qualitätssicherung bei der Datenerzeugung bestehen und wo sie möglicherweise zu kurz greifen. Dabei können zudem NutzerInnengruppen ermittelt werden, die verschiedene Anforderungen an die interne Nutzung (z.B. bibliothekarische Auskunft) haben. Zu diesen gehört unter anderem eine fachspezifische Recherche, deren Parameter aus in den 1990er Jahren durch die International Federation of Library Associations and Institutions (IFLA) aufgestellten Anforderungen an bibliographische Informationsobjekte abgeleitet werden können (Arbeitsstelle für Standardisierung 2006: 8).

                
Das Poster wird an dieser fachspezifischen Recherche Theorie und Praxis des Metadaten Assessment erläutern, sowie erste Analyseergebnisse vor und mögliche Qualitätssicherungsmaßnahmen zur Diskussion stellen.

            

        

        

            

                

                    
Bibliographie

                    

                        
Arbeitsstelle für Standardisierung (Hrsg.) (2006)
: 
Funktionelle Anforderungen an Bibliographische Datensätze – Abschlussbericht der IFLA Study Group on the Functional Requirements for Bibliographic Records
. Frankfurt am Main: Deutsche Nationalbibliothek.
                    

                    

                        
Bolter, Jay David / Grusin, Richard (1999)
: 
Remediation – understanding new media
, Cambridge: MIT Press.
                    

                    

                        
Bruce, Thomas R. / Hillmann, Diane I. (2004)
: 
“The Continuum of METADATA Quality: Defining, Expressing, Exploiting”
, in: Metadata in Practice, ALA Editions 238-256.
                    

                    

                        
Dangerfield, Marie-Claire / Kalshoven, Lisette (2015)
: 
Report and Recommendations from the Task Force on Metadata Quality
, https://pro.europeana.eu/post/metadata-quality-task-force-report [letzter Zugriff: 24. September 2018].
                    

                    

                        
Digital Curation Center (2018)
: http://www.dcc.ac.uk/resources/curation-lifecycle-model [letzter Zugriff: 24. September 2018].
                    

                    

                        
Dushay, Naomi / Hillmann, Diane I. (2003)
: 
“Analyzing Metadata for effective Use and Reuse”
, in: Proceedings of DCMI International Conference on Dublin Core and Metadata Applications: 161-170.
                    

                    

                        
Foulonneau, Muriel / Riley, Jenn (2008)
: 
Metadata for digital resources: implementation, systems design and interoperability
. Oxford: Chandos. 
                    

                    

                        
Janert, Philipp K. (2011)
: 
Data Analysis with Open Source Tools
. Sebastopol: O’Reilly Media.
                    

                    

                        
McCallum, Q. Ethan (2012)
: 
Bad Data Handbook
. Sebastopol: O’Reilly Media.
                    

                    

                        
Neuroth, Heike / Oßwald, Achim / Scheffel, Regine u. a. (2010) Hrsg.
: 
nestor Handbuch. Eine kleine Enzyklopädie der digitalen Langzeitarchivierung
. Frankfurt am Main: Nestor.
                    

                    

                        
Olson, Jack E. (2011)
: 
Data Quality – The Accuracy Dimension
. San Francisco: Morgan Kaufmann Publishers.
                    

                    

                        
Schöch, Christof (2013)
: 
“Big? Smart? Clean? Messy? Data in the Humanities”
, in: Journal of Digital Humanities 2: 2-13.
                    

                

            

        

    
