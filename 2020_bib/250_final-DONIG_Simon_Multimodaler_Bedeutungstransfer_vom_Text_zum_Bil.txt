
  
    
      Das DSM wurde mit
    einer Vektorgröße von 50, einer Wortfenstergröße von 10 und einer
    minimalen Wortzahl von fünf erstellt. Als Word2Vec-Modell kam
    Skipgram (Mikolov, Chen, Corrado, Dean 2013) mit Negative Sampling
    zum Einsatz.
    
      Das Korpus wurde
    aus den Sammlungen des Metropolitan Museum, New York, des Victoria
    & Albert Museum, London, der Wallace Collection, London sowie
    mehreren zeitgenössischen Musterbüchern zusammengestellt.
    
    
      Das Netzwerk
    besteht aus drei Convolutional-Blöcken mit jeweils zwei
    Convolutional-Layers mit 32/64/64 Filter der Größe 3x3. Nach jedem
    Block folgt ein Maximum-Pooling-Layer der Größe 2x2 sowie ein
    Dropout-Layer mit einer Dropoutwahrscheinlichkeit von 0.25. Ein
    Fully-Connected-Block, bestehend aus zwei Fully-Connected-Layers
    mit jeweils 256 Knoten, steht im Anschluss sowie nochmals ein
    Dropout Layer mit 0.5 Dropoutwahrscheinlichkeit. Jeder
    Convolutional- und Fully-Connected-Layer bis dahin wurde zufällig
    initialisiert und benutzt ReLU als Aktivierungsfunktion. Der
    letzte Layer ist ein Fully-Connected-Layer mit 50 Ausgabeknoten
    und benutzt eine lineare Aktivierungsfunktion. Es ist in den
    beiden von uns genutzten Frameworks Keras (https://keras.io) und
    TensorFlow (https://tensorflow.org) implementiert. Beim Training
    wird der durchschnittliche absolute Fehler durch die
    Optimierungsfunktion RMSprop minimiert.
    
      Das verwendete CNN
    unterscheidet sich von dem im Vektor-Experiment verwendeten Netz
    lediglich durch den letzte Layer, der ein Fully-Connected-Layer
    mit 28 Ausgabeknoten ist, korrespondierend mit den 28 flachen
    Labeln, und die Nutzung von softmax als
    Aktivierungsfunktion.
  
  

Bibliographie

Donig, Simon / Christoforaki, Maria / Handschuh, Siegfried (2016): “Neoclassica – A Multilingual Domain Ontology. Representing Material Culture from the Era of Classicism in the Semantic Web”, in: Bozic, Bojan/Mendel-Gleason, Gavin/Debruyne, Christophe / O’Sullivan, Declan  (eds.): Computational History and Data-Driven Humanities. CHDDH 2016 (=IFIP Advances in Information and Communication Technology, vol 482), Cham: Springer: 41–53, DOI 10.1007/978-3-319-46224-0_5 [Letzter Zugriff 25. 09. 2019].
                    

Donig, Simon / Christoforaki, Maria / Bermeitinger, Bernhard / Handschuh, Siegfried (2019): „Vom Bild zum Text und wieder zurück“, in: Sahle, Patrick (ed.): DHd 2019 - Digital Humanities: multimedial & multimodal – Konferenzabstracts. Mainz & Frankfurt a. M.: 227–232, https://zenodo.org/record/2596095/files/2019_DHd_BookOfAbstracts_web.pdf [Letzter Zugriff 25. 09. 2019].
                    

Johnson, Justin M. / Khoshgoftaar, Taghi M. (2019): „Survey of deep learning with class imbalance“, in: Journal of Big Data 6 (27): 2-54, https://doi.org/10.1186/s40537-019-0192-5 [Letzter Zugriff 25. 09. 2019].
                    

Krizhevsky, Alex / Sutskever, Ilya / Hinton Geoffrey E. (2012): „ImageNet Classification with Deep Convolutional Neural Networks“. In: Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, 1097–1105. NIPS’12. USA: Curran Associates Inc. http://dl.acm.org/citation.cfm?id=2999134.2999257 [Letzter Zugriff 25. 09. 2019].
                    

Lang, Sabine / Ommer, Björn (2018): „Attesting Similarity: Supporting the Organization and Study of Art Image Collections with Computer Vision“. In: Digital Scholarship in the Humanities 33 (4): 845–56. https://doi.org/10.1093/llc/fqy006 [Letzter Zugriff 25. 09. 2019].
                    

Lenci, Alessandro (2018): “Distributional models of word meaning”, in: Annual review of Linguistics, 4 (1) : 151-171.
                    

Mikolov, Tomas / Chen, Kai / Corrado, Greg / Dean, Jeffrey (2013): “Efficient Estimation of Word Representations in Vector Space.” ArXiv:1301.3781 [Cs]. http://arxiv.org/abs/1301.3781. [Letzter Zugriff 25. 09. 2019]
                    

Řehůřek, Radim / Sojka, Petr (2010): „Software Framework for Topic Modelling with Large Corpora“. In: LREC 2010. Valletta, Malta, 2010: 46–50.
                    

Sales, Juliano Efson / Souza, Leonardo / Barzegar,  Siamak / Davis, Brian / Freitas, André / Handschuh, Siegfried (2018) “Indra: A Word Embedding and Semantic Relatedness Server.” In LREC. Miyazaki, Japan, 2018.
                    
Abb. 2 / Abb. 3: Victoria & Albert Museum, London: Writing table, Neuwied, workshop of David Roentgen ca. 1774-1780. Ascension number: 1059:1 to 9-1882. http://collections.vam.ac.uk/item/O117298/writing-table-roentgen-david/ [Letzter Zugriff 25. 09. 2019].
Abb. 4: Victoria & Albert Museum, London: Vase [Sèvres Copy of the Medici Vase], Paris, 1813. Ascension Number 396-1874. http://collections.vam.ac.uk/item/O8978/vase-sevres-porcelain-factory/ [Letzter Zugriff 25. 09. 2019].
Abb. 5: 
                        Sheraton, Thomas / 
Bell, J. Munro (arr.)
 (1910): The furniture designs of Thomas Sheraton. London: Gibbings and Co., Ltd.
                    


