

Plädoyer für eine Fachgeschichte digitaler Geisteswissenschaften
Die Geschichte digitaler Geisteswissenschaften wurde bislang kaum erforscht. Schlaglichtartige Beiträge liegen vor (Hoover 2007, Kelih 2008, Cortelazzo/Tuzzi 2008, Viehhauser 2015, Weitin 2015, Jannidis 2015, Twellmann 2016, Thaller 2017, Schöch 2017, Lauer und Pacyna 2017, Bernhart 2018), doch umfassende Studien, die nicht nur die letzten Jahrzehnte, sondern auch die zahlreichen Vorläufe seit dem späten 18. Jahrhundert in systematischer und historischer Perspektive in den Blick nehmen, gibt es noch nicht.

Der Vortrag möchte für historisches Bewusstsein digitaler Geisteswissenschaften sensibilisieren und für eine Fundierung der Wissenschaftsgeschichte des Faches werben. Denn historisch informierte digitale Geisteswissenschaften sind in der Lage, auf Erfahrungen und Experimente aus mindestens zwei Jahrhunderten zurückzugreifen und diese für die Erkenntnisgewinnung zu nutzen und kritisch zu reflektieren. In ihrer genuinen Koppelung von Informatik, die tendenziell eher gegenwartsbezogen operiert, und geisteswissenschaftlichen Disziplinen, die tendenziell größere Aufmerksamkeit auf die Betrachtung historischer Wissensbestände legen, sind die digitalen Geisteswissenschaften dazu berufen, historische Perspektivierungen bei ihrer Arbeit an Modellierung und Interpretation kultureller Artefakte zu integrieren und zu systematisieren. Historische Perspektivierung macht die gesellschaftliche Relevanz digitaler Geisteswissenschaften transparent und dient der didaktischen Vermittlung des Faches, indem Zeitverlauf und Erkenntnisgewinn in Korrelation miteinander erzählt werden können. Unter den zahlreichen Strängen einer Wissenschaftsgeschichte digitaler Geisteswissenschaften greift der Vortrag den Aspekt künstlerischer Produktion als Modellierung heraus und demonstriert diese am Beispiel der Arbeiten von Theo Lutz und Wilhelm Fucks.


Künstlerische Produktion als Modellierung
Das Thema der Jahrestagung stellt Modellierung und Interpretation als zentrale Arbeitsfelder der Digital Humanities in den Vordergrund. Modellierung wird dabei jedoch vorwiegend theoretisch als Mittel der Erkenntnisgewinnung verstanden. Ein anderer Aspekt der Modellierung ist die künstlerische Produktion, die aber nicht zu den originären Arbeitsgebieten der Digital Humanities zählt. Während im internationalen Feld der Digital Studies die Grenze zwischen den Künsten und den Wissenschaften sehr viel stärker aufgehoben scheint und auch akademische Forschung sich an künstlerischer Produktion beteiligt, verharren die Digital Humanities, insbesondere jene des deutschsprachigen Raums, in eher beobachtendem Status. Sie sehen Analyse und Interpretation als ihre primären Zuständigkeitsbereiche und halten weiterhin die Dichotomie zwischen Medienkunst und Medienwissenschaft aufrecht. Digitale Kunst wird außerhalb der Digital Humanities produziert; für künstlerische Produktion werden Digital Humanities kaum genutzt. Dabei verweist gerade die Metapher der Spielräume auf den transgressiven Charakter experimenteller Laboratorien, die in den Digital Humanities bereitstehen.
Die Frühzeit digitaler Geisteswissenschaften und insbesondere die Kybernetik der späten 1950er und 1960er Jahre waren in dieser Hinsicht sehr viel verspielter und experimenteller. Vertreter akademischer Disziplinen wie etwa der Mathematik und Physik verstanden Modellierung ganz selbstverständlich auch im Sinne künstlerischer Produktion. Beispiele dafür sind der Mathematiker Theo Lutz und der Physiker Wilhelm Fucks. Wissenschaftsgeschichtlich bezeichnend ist ferner, dass sich in dieser Zeit aus Mathematik und Elektrotechnik ein neues Fach zu emanzipieren beginnt, das in den 1970er Jahren unter dem Namen Informatik sehr rasch an internationaler Bedeutung gewinnt. Dabei war in dieser frühen Zeit noch nicht ausverhandelt, für welche technischen, angewandten, theoretischen und geisteswissenschaftlichen Problemlösungen die Informationsverarbeitung zuständig sein soll; vielmehr waren Explorationen in sehr unterschiedliche Richtungen charakteristisch (Gunzenhäuser 1968).


„Stochastische Texte“ von Theo Lutz
Theo Lutz (1932–2010), Mathematikstudent an der vormaligen Technischen Hochschule, heute Universität Stuttgart, schrieb im Frühjahr 1959 an der hochschuleigenen Zuse Z 22 seine Diplomarbeit über elektrotechnische Netzwerke. In seiner Freizeit entwickelte er die Idee zu einem Umkehrschub: Wenn es mithilfe computergestützter und statistischer Verfahren möglich ist, Texte zu analysieren und zu interpretieren, muss es auch möglich sein, mithilfe derselben Verfahren Texte zu produzieren. Sein Lehrer Max Bense, Philosoph der rationalen Avantgarde, und sein Studienfreund Rul Gunzenhäuser, der später gemeinsam mit Helmut Kreuzer das Grundlagenwerk „Mathematik und Dichtung“ herausgeben (Kreuzer/Gunzenhäuser 1965) und die „Zeitschrift für Literaturwissenschaft und Linguistik (LiLi)“ begründen und in Stuttgart zu einem Pionier der Informatik avancieren wird, waren begeistert von Lutz’ Idee und unterstützten das Vorhaben. Das Ergebnis waren die „Stochastischen Texte“, die Wortmaterial aus Franz Kafkas Roman „Das Schloss“ (1926) wahrscheinlichkeitsmathematisch zu grammatikalisch sinnvollen Sätzen kombinierten (Lutz 1959). Programmiert wurde die Z 22 im sogenannten Freiburger Code. Eine besondere Herausforderung stellte dabei die maschinelle Generierung der für die Textherstellung erforderlichen Zufallszahlen dar. Mit seinen „Stochastischen Texten“ schrieb Lutz Literaturgeschichte: Sie waren nach den „Love Letters“ von Christopher Strachey die ersten mithilfe einer programmierten Rechenmaschine generierten Texte in deutscher Sprache (Strachey 1954). Doch der ursprüngliche Zweck der „Stochastischen Texte“ war ein anderer: Sie sollten als Vergleichstexte zur Untersuchung natürlichsprachlicher Texte dienen (Bernhart 2019: 329–331, Bernhart/Richter 2019, Reiter/Bernhart eingereicht). Die Rekonstruktion der Genese der „Stochastischen Texte“ wird im Vortrag flankiert von der Berücksichtigung der poetologischen, philosophischen, politischen und ästhetischen Voraussetzungen, die für maschinelle und programmgesteuerte Generierung von Kunst in den späten 1950er und 1960er Jahren stil- und programmbildend waren.



Wilhelm Fucks und Neue Musik
Wilhelm Fucks (1902–1990) war Physiker an der RWTH Aachen. In den Geisteswissenschaften ist Fucks vor allem für seine sehr zahlreichen kybernetischen Forschungen zu Literatur, Musik und bildender Kunst und für die beiden Monographien „Formeln zur Macht“ (Fucks 1965) und „Nach allen Regeln der Kunst“ (Fucks 1968) bekannt. Bislang kaum beachtet wurden dagegen seine kompositorischen Versuche.
Erstaunlich ist dabei der Echoraum, den sich Fucks für seine Kompositionen verschaffen konnte. Dank seines kommunikativen Talents und seiner wissenschaftlichen Adaptionsfähigkeit war er in der Lage, sich innerhalb weniger Jahre als zeitgenössischer Komponist zu etablieren und sich Ende der 1960er Jahre neben internationalen Vertretern der Neuen Musik wie Iannis Xenakis und John Cage zu behaupten, obwohl er auf dem Gebiet des musikalischen Schaffens Amateur war. Seine ersten Versuche reichen in die Zeit der letzten Monate des Zweiten Weltkriegs zurück, wie er im Diskussionsprotokoll des Bandes zum Symposium „Information Theory“ an der Royal Institution 1955 in London festhält. Zunächst habe er sich hobbymaßig, „as a hobby at first“ (Fucks 1956: 169), mathematisch mit Fragen literarischer Stilistik beschäftigt. Erst später habe er sich mit den Theorien und Ansätzen etwa von Benoit Mandelbrot, Gustav Herdan, Claude E. Shannon oder Norbert Wiener vertraut gemacht und seine Studien auf den Bereich der Musik ausgedehnt (ebd.).
Unter dem Eindruck der probabilistischen Logik von John von Neumann (Neumann 1956) intensivierte Fucks seine Beschäftigung mit stochastischer Musik, suchte in Paris den Austausch mit Abraham Moles und den Kontakt zu Iannis Xenakis, die ihn in den Kreis um den einflussreichen Experimentator und Theoretiker der Neuen Musik Hermann Scherchen einführten. Auf Scherchens legendärer Tagung in Gravesano im Schweizer Tessin stellte Fucks schließlich 1962 seine umfangreichen harmonischen Entropieforschungen sowie eigene Kompositionen vor, die – ähnlich wie Lutz’ Generierung der „Stochastischen Texte“ – aus der „umgekehrten“ Anwendung empirischer Verteilungen hervorgingen (Fucks 1962). Für den Vortrag seiner Stücke konnte Fucks die namhafte Pianistin Margot Pinter (1915–1982) gewinnen, Professorin für Klavier am Innsbrucker Konservatorium und Spezialistin für Neue Musik. Eine bislang unbekannte Tonbandaufzeichnung davon konnte ich kürzlich im Archiv der Akademie der Künste, Berlin, ausfindig machen (Akademie der Künste, Archiv, Signatur AVM-31 6332, Band 14 und Band 15). Die aufgezeichnete Musik und vor allem die ebenfalls aufgezeichnete Plenumsdiskussion im Anschluss an den Vortrag sind aufschlussreiche, bislang unbekannte Quellen, die im Vortrag vorgestellt werden.

Fucks’ weitere Stationen führten nach Berlin und London. Auf Einladung des Architekten und Präsidenten der Berliner Akademie der Künste Hans Scharoun stellte Fucks 1965 auf einer prominent besetzten Tagung zum Thema „Kybernetik“ in Berlin seine Musiken vor. In London war Fucks als Komponist auf der von Jasia Reichardt kuratierten Ausstellung „Cybernetic Serendipity“ 1968 vertreten (Reichardt 1968), die als eine der ersten internationalen Ausstellungen kybernetischer Künste gilt. Auf der gleichnamigen Langspielplatte, die im Rahmen der Ausstellung erschien, ist Fucks’ Stück „Quatro due [sic]“, gespielt von Margot Pinter, zu hören. Auf der Platte vertreten sind unter anderem John Cage, Iannis Xenakis und James K. Randall (Cybernetic Serendipity Music 1968).


Spielräume künstlerischer Produktion in Kybernetik und digitalen Geisteswissenschaften
Im Fazit des Vortrags wird danach gefragt, inwiefern sich Kybernetik und Digital Humanities hinsichtlich künstlerischer Produktion unterscheiden. Produktiv dafür kann die schwierige Vergleichbarkeit der beiden Bewegungen sein: Die Kybernetik operierte vorwiegend quantitativ und statistisch und delegierte die Interpretation an eine imaginäre Zukunft; die Digital Humanities dagegen integrierten quantitative und qualitative Verfahren von Anfang an und gleichermaßen in die Modellierung und Interpretation der Untersuchungsgegenstände. Die Dichotomie zwischen Medienkunst und Medienwissenschaft bleibt dabei tendenziell aufrecht, während die Kybernetik ihre Spielräume sehr viel transgressiver auch für künstlerische Produktionen nutzte. Spielerischer scheinen derzeit digitale Medienkünste zu agieren, die in und neben ihrer künstlerischen Produktion oft auch forschend tätig sind. Auch die zahlreichen KI-Labore (etwa von Google oder OpenAI) pflegen neben ihrer angewandten Forschung mitunter spielerischen Umgang mit Künstlicher Intelligenz, der bisweilen an die Experimente von Lutz und Fucks erinnert. Der Geist der Kybernetik entsprang der jungen und technikbegeisterten Aufbruchstimmung der Nachkriegszeit, während digitale Geisteswissenschaften und Künste mittlerweile auf kollaborative Projekterfahrungen, Tools und Formate eines halben Jahrhunderts digitaler Kompetenz zurückblicken und auch kritischere und differenziertere Positionen vertreten als die historische Kybernetik. Hinsichtlich der Einmischung in künstlerische Produktion liegen in den Digital Humanities Spielräume verborgen, über deren (Nicht-)Nutzung nachzudenken lohnen kann.

