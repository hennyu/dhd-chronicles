
            

                
Flurnamen sind Benennungen von Örtlichkeiten der Siedlungsflur, die vor allem der Gliederung der Landschaft dienen. Es handelt sich um die Bezeichnungen für Wälder, Felder, Wiesen, Berge, Gewässer und alle anderen natürlichen oder durch den Menschen beeinflussten Geländegegebenheiten, an denen sich der Mensch in der Landschaft orientiert. Auffallend ist ihr eingeschränkter Kommunikationsradius. Sie werden meist nur von Einheimischen benutzt, gelegentlich auch nur von einzelnen Familien. Flurnamen reagieren stark auf gesellschaftliche Veränderungen sowie Gegebenheiten wie Besitzwechsel oder variierende Bodenbewirtschaftung und sind deshalb nicht so stabil wie andere Örtlichkeitsbezeichnungen. Ihre schriftliche Überlieferung ist mundartlich geprägt. Flurnamen spiegeln das enge Verhältnis der Namengeber zu ihrem Lebens- und Arbeitsumfeld wider. Da es zumeist die bäuerliche Landbevölkerung war, die Flurnamen vergab, wurden diese Bestandteil regionaler Identität. 

            

            

                
Flurnamen gehören zum immateriellen Kulturerbe: Ihre Erforschung gibt Aufschluss über die Siedlungsgeschichte und ehemalige Raumstrukturen, sie liefert Erkenntnisse über die Entwicklung der deutschen Sprache und ihrer Dialekte. Viele andere Wissenschaftsbereiche wie die Volkskunde, die Wirtschafts- und Sozialgeschichte, die Rechtsgeschichte sowie die Botanik, Zoologie und Geologie profitieren ebenfalls von den Forschungsergebnissen.

            

            

                
In Thüringen wird seit über 110 Jahren Flurnamenforschung betrieben. Dabei wurden insgesamt etwa 200.000 Namen aus Thüringen und dem Süden Sachsen-Anhalts erhoben und in einem Zettelarchiv an der Universität Jena gesammelt. Zusätzlich wurden seit 1999 in einem von Barbara Aehnlich wissenschaftlich betreuten Ehrenamtsprojekt mehr als 500 Sammlungen mit insgesamt rund 40.000 Namen von über 360 aktiven Mitarbeitenden eingereicht. Insgesamt ist mit einem Bestand von 300.000 bis 350.000 Flurnamen für Thüringen zu rechnen.

            

            

                
Die Digitalisierung der umfangreichen Belegsammlung des Flurnamenarchivs wird seit 2019 von der Thüringer Staatskanzlei gefördert und zielt auf die öffentliche Sicht- und Nutzbarkeit des in Kooperation mit der Thüringer Universitäts- und Landesbibliothek (ThULB) entstehenden Portals.

                
In diesem Digitalisierungsprojekt werden die Belege transkribiert und in die Datenbank Collections@UrMEL

                
 eingetragen. Abkürzungen und bibliographische Angaben werden dabei nach Möglichkeit aufgelöst. Etwa 72.000 Namenbelege sind bereits im Portal sichtbar.

                
Parallel zur Abschrift werden die gescannten Belege mit den Gemarkungen verknüpft und in einem gemeinsamen Viewer zur Verfügung gestellt. Außerdem werden die Gemarkungen mit der zugehörigen Orts-ID der Gemeinsamen Normdatei (GND) und künftig die Flurstücke mit den offenen Geodaten des Thüringer Landesamtes für Bodenmanagement und Geoinformation verknüpft. Durch die in diesen Datensätzen enthaltenen Informationen wird eine Präsentation der Gemarkungen und Flurstücke in OpenStreetMap möglich. Das Portal macht also das Datenmaterial sicht- und nutzbar und ergänzt die Gemarkungen mit Kartenmaterial. Es stellt den bisher nur in Zettelform zur Verfügung stehenden Archivbestand dar und ist ein wichtiger Schritt, die Thüringer Flurnamenforschung in die Zukunft zu führen. 

            

            

                
Aktuell wird die Zusammenarbeit von Bürger*innen und Wissenschaft intensiviert. In engem Austausch erheben Ehrenamtliche die Flurnamen und erfassen das lokale Wissen ihrer Heimatorte, indem sie die mundartliche Lautung der Namen, Sagen und Legenden sowie Informationen zu landschaftlichen und historischen Gegebenheiten aufzeichnen und private Quellen erschließen. Die Bürger*innen werden durch verschiedene Maßnahmen geschult (Online- und Präsenzveranstaltungen, Flurnamensprechstunden, Workshops), so dass jede Altersgruppe ihren Fähigkeiten entsprechend mitwirken kann. Die Interaktion und der Austausch stehen dabei im Vordergrund. In der ThULB wird derzeit das Datenmodell des Flurnamenportals an diese Datenbestände angepasst und erweitert, damit die Belege passgenau eingegeben und dargestellt werden können. Aktuell wird die webbasierte Eingabemaske so angepasst, dass es in absehbarer Zeit möglich sein wird, dass die Bürger*innen ihre Ergebnisse selbst eintragen und diese nach einer wissenschaftlichen Überprüfung im Portal sichtbar werden. Die Arbeitsschritte im Projekt und deren Erfolge werden im Thüringischen Flurnamenportal, aber auch über Social-Media-Kanäle

                
und Tagungsbeiträge der Öffentlichkeit vorgestellt. 

            

            

                
Langfristig sollen die thüringischen Flurnamen flächendeckend erschlossen sowie sprach- und kulturwissenschaftlich ausgewertet werden. Alle Bestände sollen im Thüringischen Flurnamenportal zusammengeführt werden, um diese Informationen für die interessierte Öffentlichkeit und Forschende gleichermaßen zugänglich zu machen. Die digitale Auswertung und Darstellung des gesammelten sprachlichen Materials im Flurnamenportal verspricht im Sinne von Open Culture Auskünfte über die Geschichte der Orte, frühere Bodennutzungen und Landschaftsgestaltungen, Traditionen und Kultur, Siedlungsströme und Rechtsverhältnisse. Dies großräumig herauszuarbeiten und Verbreitungen auch kartografisch darzustellen, ist das große Ziel des thüringischen Flurnamenprojektes. Das Poster stellt die Arbeitsprozesse bei der Erstellung des Thüringischen Flurnamenportals sowie den bürgerwissenschaftlichen Ansatz vor und zeigt exemplarisch die Herausforderungen und unsere Lösungsansätze. Dabei werden auch der geplante Workflow für die Dateneingabe durch Ehrenamtliche, die Visualisierungsmöglichkeiten, die Social-Media-Aktivitäten und die Nutzung des Portals vorgestellt.

            

        

            

                
A complex starting point: from an inaccessible archive to FAIR data approach

            

                
The Vatican Archive is the private archive of the pope. Until 2019 it was called the Vatican Secret Archives and this particular name has given rise to legends, novels, films, rumours... Despite the fact that the archive has been open to the public since 1881 without any restrictions, and although the connotation of 'secret', which has always tickled the fancy of writers and journalists, derives from the ancient meaning of the adjective 

                
secretum

                
, indicating that the holdings were the pope’s private and personal property,

                
 this archive and its very rich heritage remain inaccessible to many in several respects. This concerns both the archive holdings as a whole (archival fonds and series) and the historical documents preserved there. 

            

            

                
In contrast to other historical archives whose inventories are based on International Standard Archival Description (International Council on Archives, 2000) that allow for interoperability of data, the Vatican Archive is so immense and complex

                
 that it has not yet been equipped with a modern archival description system. Bear in mind that there is still no uniform archive guide, no comprehensive inventory, and that researchers often have to consult indexes and handwritten inventories, even dating back to the 14

                
th

                
 century, in order to know the contents of the archive series and select the documents they are interested in. Moreover, for reasons of document ownership and authenticity, the archive's policy is clearly resistant to an open access approach to data: documents cannot be photographed by researchers. Digital reproductions are expensive and can only be used for personal research purposes. Handwriting recognition software,

                
 which usually foresee the sharing of digitized images with other users, cannot be used to 'read' these documents.

                
 A final aspect that makes the documents preserved in the Vatican Archives hardly accessible concerns the specific knowledge and skills needed to read, understand and interpret them. Although common to historical research in general, this aspect takes on an important significance in the case of papal documents for the exegesis of which it is necessary to master certain specific disciplines and techniques.

            

            

                
Among the various scientific objectives of our research project is also to improve the accessibility of data obtained from historical sources held in the Vatican Archive and to offer them to the scientific community according to FAIR principles 

                
(Findability, Accessibility, Interoperability, Reuse) (

                
Wilkinson 

                
et al.

                
, 2016.

                
)

                
, thus overcoming the inaccessibility of papal sources through modern technologies. In this paper we describe the methods and tools we are developing to address these challenges.

            

            

                
Our research project focuses on one of the most active bodies of the Roman Curia – the complex of organs and authorities that constitute the administrative apparatus of the Holy See - between the modern and contemporary ages: the Congregation of the Council, which we affectionately call “SCC”

                
. This dicastery was appointed for more than 350 years to oversee the correct interpretation and implementation of the Council of Trent

                
 and the administration of justice around all disciplinary matters contained in the council and later on other papal laws. The Congregation of the Council was composed of cardinals and other personnel and met periodically to discuss and decide legal cases that came to it from all over the Catholic world. It had consultative, judicial and gracious functions and its jurisdiction ideally extended to the whole world. Based on the 1.5 km of the SCC archive, preserved in the Vatican Archive, the group has compiled a dataset describing approximately 35,000 

                
positiones

                
, that is judicial cases that took place in front of the SCC between 1564 and 1680 and involved thousands of people and institutions from all over the Catholic world and beyond. 

            

            
In this paper we describe how we developed 1) 
                
modern standards for processing historical data
, 2) the semantic model and 3) visualization strategies for contextualizing data in global history. 
            

            

                
Establishing modern standards for processing data extracted from historical sources

            

                
Working with early modern sources require a complex approach. Source criticism considers such factors as uncertainty, unclearness, gaps, damaged fragments, mistakes, ambiguities etc. as a natural part of the context, which needs to be considered in the analysis.

            

            

                
For various reasons, the digitization of the sources covered by the project was never considered either possible or desirable.

                
 Instead, it was necessary to process the documents using specific techniques of source criticism applied to papal documents: all the data were collected directly from the original documents in accordance with academic standards of archival science, palaeography and papal diplomatics.

                
 The group can today rely in an orderly and logically organized dataset composed by all of the approximately 35,000 

                
positiones 

                
processed by the SCC from 1564 to 1680 and which constitute the oldest core of the dicastery's archive.

                
 Alongside the data collection phase, the group systematized data and defined classes

                
 in order to build the data model. For certain classes in the dataset the group also elaborated descriptive and structural metadata.

                
 These processes have been carried out always making the interpretative intervention of the researchers recognizable. Thus, the dataset presents several 'double fields' of which one collects the data as it appears in the document and other the data in its standardized version. This, in order to allow the scientific community to criticize our interpretation and to enrich it with new elements in the future. These resources become the basis for creating the knowledge graph for our data and the visualization application described below.

            

            

                

                
Figure 2: Section of the data model.
 
            

        

        

           
Building the semantic model of the SCC administrative structures.

            

                
The research group currently curates and manages the data using a graph database for the purpose of knowledge management. 

                
We choose a NoSQL graph database, i.e. Neo4j, because of its flexibility, which allow to work with 

                
unstructured, semi-structured, and structured data

                
, and create a schema-free model (Zhang, 2017). Figure 3 shows the data schema of

                
 

                
positiones

                

                    
 

                

                
built in the Neo4j graph database.

                
 The individual nodes represent persons (entitled as Persona), places (entitled as Location), institutions (entitled as Dioceses), 
                    
positiones
 (entitled as Positio). The archival sources are modelled with multiple nodes: 
                    
Volumen
 (volume for 
                

                
positiones

                
), 
                    
Extra
 (extra materials for cases), papal interventions (entitled as Intervention), papal fiat (entitled as 
                    
Fiat
), and 
                    
Nexus
 (ID for connecting the different parts of a 
                    
positio
 within the same volume and to indicate the links between different volumes. For each volume, a so-called 
                    
Vakat
 node is created for collecting information about empty pages. The Node 
                    
Phase
 is used to identify up to individual phases that a case may have. The edges represent the relationship structures between the nodes, such as the relations between a petitioner involved in a case and that case. 
                

            

            

                

                

                
Figure 3: Data schema of

                
 

                
positiones

                

                    
 

                

                
built in the Neo4j graph database.  

            

            

        

            

                
Modelling the legal administrative changes of the dioceses of the Catholic Church

            

                
In addition to usage of the graph database for curation of the 

                
positiones

                
, the research group is in the process of developing an ontology for modelling the historical changes of the legal administrative institutions, which appear in the data, with special attention to dioceses, which are of fundamental importance in our context as they represent the basic object in legal administrative structure of the Catholic Church. These data will be available as a knowledge graph. Using the Semantic Web technologies, we constructed an event-based ontology with 

                
key elements (Event, Time, Place, Institution, Source) 

                
that trace legal administrative events and changes of each diocese quoted in the 

                
positiones

                
. 

            

            

                
Our 

                
model proposes an extension of CIDOC-CRM (Doerr, 2003) through suggesting additional subclasses and object properties. Although CIDOC-CRM provides strong semantic expressiveness, this instrument was designed for modelling within the cultural heritage domain that set semantic limitations for modelling the structures of the administrative acts in historical perspective. Therefore, we proposed additional subclasses and object properties in order to fill a methodological gap.

                
 

            

            

                

                
Figure 4: Key elements in our ontology

            

            

                
Publishing open and FAIR data 

            

                
Based on 

                
our ontology data model and in accordance with FAIR principles,

                
we prepare the publication of our data as linked open data 

                
by using the combination of Resource Description Framework (RDF) triples and Internationalized Resource Identifiers (IRIs) 

                
which ensures for maintaining the semantic interoperability of data (Berners-Lee, 2009). Since 

                
for many of 

                
our actors, places, institutions there 

                
are no entries in any authority databases yet, therefore we will assign an URI to each resource and offer our data IDs as authority records for other projects. 

            

            

                
In accordance with FAIR principles, this achieves and guarantees, on the one hand, the findability and accessibility of research data, and on the other hand, it enables researchers to use our data as a reference as well as for their own purposes. This provides also a fundamental framework for reusability of the data. We believe that providing our data in RDF triple format, we fulfil the requirements for interoperability 

                
and standardization of data according to the World Wide Web Consortium (W3C) recommendations. To achieve these goals, we currently identify and match entries from our dataset, su

                
ch as dioceses and other actors, b

                
y using the reconciliation web service API providing OpenRefine that enables to align datasets to entries from the already existing dataset

                
 in OpenRefine (

                
Thalhath 

                
et al.

                
, 2021). Therefore, we enrich semantically our dataset via authority data and achieve useful degree of interoperability and connectivity between our dataset and external data.

            

            

                
Developing open source visualization instruments for contextualizing the SCC data and metadata in history

            

                
We consider that adequate historical contextualisation of data is necessary in order to avoid anachronistic or teleological biases and allow to interpret the results in scientific ways that correspond academic standards of historical research. To contextualize the SCC data and its metadata we developed visualization approaches that allow us to show our data not only as tables but also in interactive graphic format, to improve accessibility from the user perspective. There are various methods for visualizing historical data, focused on modern ideas of time

                
 and space

                
 that however don’t count uncertainty, unclearness and incompleteness of historical data

                
.

                
We stress on methods of visualization of unclear, uncertain and incomplete data for reducing the impact of modern gaze on time and history. We also aim to remain the complexity of the sources from the perspective of an historian, i.e. preserve the original data as presented in the sources even if unclear, uncertain or incomplete. This part of our project aims to extend the functionality of this kind of instrument with setting up the custom controllers, impossible to maintain in already existed tools and therefore make uncertainty, unclearness and incompleteness visible and accessible as an important specifics of the data. The principal languages we use for this part of the project are R and 

                
JavaScript

                
. 

            

            

                
We developed the 

                
SCC Timeline Explorer

                
 web app in which the history of the SCC is placed in global historical context. Through the visualization of parallel timelines, this application allows to explore different aspects of the history of the SCC (evolution of competences, turnover of the personnel, frequency of the meetings) within global history (Global Legal History, History of the Roman Curia, Pontificates). We enriched the original data with descriptive metadata, which provide information in case of uncertainty, unclearness and incompleteness of data. Since many inputs have incomplete information, we decided to use vis.js

                
 and R, for working with both – the data in an advanced way and the graphic visualization in dynamics, which also allows to visualize the uncertainty, unclearness and incompleteness in a functional (R) and aesthetic way (JS + CSS). We also combined the data and controllers, as we want to let users choose the settings. 

            

            
Using R we created a reactive dataset, which connects the original dataset and controllers. Since the graphic part of vis.js works only with dd-mm-yyyy format, we added and external CSS file to stylize uncertain entities, which set the uncertainty as semi-transparent elements. For setting the controllers, firstly, in R we wrapped them as a reactive function. In a basic way, this solution uses shiny for calling reactive and dynamic functions inside a server part. Secondly, these functions are visualized in vis.js.

            

                

                
Figure 5: A basic example of setting a subset with controllers.

            

            
            

                

                
Figure 6:
                
 
A timeline prototype using vis.js and R.
            

            

            
Each label has a trigger on click, which shows the information about the event or range, which is based on reactive values. What is unique in this approach is the possibility to set up reactive controllers for groups, custom reactive values, advanced aesthetics, and a way to bypass limits of the vis.js in the case of unclear, uncertain on incomplete information that is essential for historical datasets. 

            

                
Products of our development will be published in the format of web applications in the SCC Explorer Platform. The commented code will be available on our GitHub. The results can be validated and repeated with other datasets. R functions are designed universal and scalable for other Digital Humanities requests. 

            

        

        

            
Conclusion

            

                
Our research project offers various methods of modelling and visualization of a large scaled database and metadata, considered in a FAIR way with open access, open data and open code. Firstly, we developed a knowledge graph using a semantic data model, which offers a data-centred approach for the Congregation of the Council in global context in the early modern period. Secondly, we created open access research tools for contextualizing historical data, including unclear, uncertain and incomplete information, into a big picture of global legal history, providing a graphic and accessible visualization of the data. For the time being, our data covers the period from 1564 to 1680 and are concentrated on the SCC, but data collection will continue for the later periods and our methods can be profitably applied also to other bodies of the Roman Curia. Therefore, our research and the tools we are developing can be of great importance for the understanding of the administration of justice in the Western World from the Late Middle Ages to Contemporary period.

            

        

            
Historiker:innen setzen sich zur Erforschung der Geschichte mit einer Vielzahl ganz unterschiedlicher Quellenarten auseinander. Um das historische Quellengut entsprechend der individuellen Fragestellungen angemessen und kritisch-reflektiert zu bearbeiten, wurden mit dem historischen Werkzeugkasten eine Reihe von Methoden etabliert, die dieser Vielfalt Rechnung tragen (Überblick: von Brandt 18. Aufl. 2012). Die konkreten Verfahren müssen dabei nicht spezifisch geschichtswissenschaftlich sein, sondern können zum Teil auch aus anderen Wissenschaften entstammen, etwa den Philologien oder Wirtschaftswissenschaften. Entscheidend für die Auswahl des jeweiligen methodischen Vorgehens ist die konkrete Fragestellung in Kombination mit dem Quellenkorpus sowie die Befolgung methodischer Grundsätze, die auf die Plausibilität der Darstellung historischer Wirklich- und Wahrscheinlichkeiten zielen. Insofern wir als Historiker:innen also abhängig von unseren Fragestellungen das Untersuchungsinstrumentarium immer neu bestimmen, ist die Kritik der jeweiligen Methodik unverzichtbar (Sellin 2008, 84-96). Sie zielt darauf, die der Methode impliziten Einschränkungen zu verdeutlichen und bewusst zu machen. Erst auf dieser Basis ist dann eine angemessene Interpretation der gewonnenen Ergebnisse möglich, die wiederum Grundlage historiographischer Erzählungen ist.

            
Mit dem „digitalen Zeitalter“ und den Digital Humanities kamen vor allem seit den 2000er-Jahren durch die digitalen Methoden innovative Möglichkeiten zur Quellenlektüre und -auswertung für die Geschichtswissenschaften hinzu. Damit stehen nunmehr Werkzeuge zur Verfügung, die einerseits vormals analoge Tätigkeiten digital abbilden und unterstützen, wie Data Mining für die Historische Statistik, andererseits bieten quantitative Verfahren des Maschinellen Lernens wie etwa Topic Modeling mit ihrem explorativen Modellierungsansatz neuartige Ansätze, indem Texte als Daten verstanden und entsprechend flexibel skalierbar in Masse ausgewertet werden können. Für die Geschichtswissenschaften ist es entscheidend, sich mit den epistemologischen und methodologischen Konsequenzen dieser Digitalität in Bezug auf ihre Quellen und Methoden auseinanderzusetzen. Denn auch die digitalen Methoden wurden in anderen Disziplinen mit je eigenen theoretischen und methodologischen Annahmen respektive Erkenntnisinteressen entwickelt. Sie sind daher nicht ohne Weiteres auf historische Anwendungsfälle übertragbar. Um sie dennoch produktiv in den "Werkzeugkasten" der Historiker:innen zu integrieren, ist daher zunächst die "Kluft" zwischen historischer Fachdisziplin und fachfremder Methode zu identifizieren und durch geeignete Strategien zu überwinden. 

            
Diesem Anliegen widmet sich das diesem Poster zugrunde liegende Dissertationsprojekt “Mining the Historian’s Web – Methodenkritische Reflexion quantitativer Verfahren zur Analyse genuin digitaler Quellen am Beispiel der historischen Fachkommunikation”. Zwar wird in jüngerer Zeit zunehmend untersucht, welche Implikationen mit digitalen Methoden für die Arbeit mit historischen Quellen sowie für den Erkenntnisbildungsprozess einhergehen (u.a. Hiltmann et al. 2021; Fickers 2020; König 2017; Braake et al. 2016; Wettlaufer 2016), allerdings fehlt es weitestgehend noch an einer systematischen Werkzeug- und Methodenkritik, die den verantwortungsvollen Umgang mit digitalen Methoden in den Geschichtswissenschaften begleiten muss. In Anlehnung an Diskussionen rund um 
                
Tool
- und 
                
Algorithmic Criticism
 (Es/Schäfer/Wieringa 2021; Dobson 2019; Ramsay 2011) ist es Ziel der Dissertation, diese Lücke zu schließen. Dazu werden vergleichend etablierte Methodenkomplexe zunächst theoretisch-konzeptionell unter Berücksichtigung ihrer Entwicklungsgeschichte erarbeitet und anhand exemplarischer historischer Fragestellungen praktisch angewendet. Anschließend werden vor dem Hintergrund der besonderen Charakteristika historischer Fragestellungen und Daten die Erkenntnispotenziale und -grenzen kritisch geprüft. Auf dieser Basis wird dann abstrahierend ein Kriterien- und Fragenkatalog für die methodenkritische Evaluation und Auswahl digitaler Methoden entwickelt sowie konkrete Anwendungsempfehlungen gegeben.
            

            
Das Poster wird erste anwendungsbezogene Erkenntnisse am Beispiel von Topic Modeling vorstellen. In den digitalen Geistes- und Geschichtswissenschaften ist die 2003 vorgestellte 
                
Latent Dirichlet Allocation
 (LDA, Blei/Ng/Jordan 2003) hierfür am populärsten. Sie wird eingesetzt, um über die Identifikation statistisch signifikanter Sprachgebrauchsmuster in umfangreichen Textsammlungen beispielsweise die Entwicklung von Publikationstrends zu untersuchen (exemplarisch: Wehrheim 2019). Der frequente Einsatz von LDA scheint vor allem auf der hohen Verfügbarkeit zu basieren
 und weniger auf einer Evaluation der Eignung im Vergleich zu anderen Ansätzen. Für historische Fragestellungen etwa, die insbesondere die Temporalität und Kontextgebundenheit der Quellen fokussieren, weist LDA einige Limitierungen auf: Die Topic-Modellierung, deren vorrangiger Zweck die maschinelle Klassifikation umfangreicher und unstrukturierter Daten ist, berücksichtigt weder Relationen zwischen den Topics noch die Historizität der Daten. Diese im Algorithmus inhärenten Einschränkungen waren vielfach Anlass für technische Weiterentwicklungen (überblickshaft: Chauhan/Shah 2021; Vayansky/Kumar 2020); der Stand der Methodenentwicklung wird in den digitalen Geistes- und Geschichtswissenschaften allerdings bislang kaum rezipiert. Hier werden vor allem die grundsätzlichen Herausforderungen und Konsequenzen diskutiert, die mit LDA einhergehen und entsprechende Workflows zur nachhaltigen Integration in den Forschungsprozess vorgeschlagen (u.a. Hodel/Möbus/Serif 2022; Uglanova/Gius 2020; Maier et al. 2018; Fechner/Weiß 2017; Andorfer 2017). Da die Wahl des konkreten Modellierungsverfahrens aber darüber entscheidet, welche Aussagen sich über die sprachliche Struktur einer Dokumentensammlung treffen lassen, soll das Poster durch eine methodenkritische Bestandsaufnahme auch andere Topic-Modeling-Algorithmen für die historische Forschung präsentieren.
            

        

            
In diesem Beitrag wird eine regelbasierte Methode vorgestellt, um Figurenauf- und -abtritte in den Regieanweisungen dramatischer Texte zu klassifizieren. In der Forschung wurde Regieanweisungen meist nur wenig Beachtung geschenkt, etwa weil sie während einer Theateraufführung nicht textuell in Erscheinung treten (Schößler, 2017, S.3). Als eine der wenigen quantitativen Untersuchungen stellen Trilcke et al. (2020) fest, dass sich die vermutete Episierung des Dramas im Laufe der Jahrhunderte am Korpus GerDraCor bestätigt. Eine Differenzierung nach Funktionen der Regieanweisungen erfolgt nicht. 

            
Dabei ist die (Ko-)Präsenz von Figuren auf der Bühne eine häufig genutzte Grundlage für quantitative Dramenanalysen, insbesondere in der Netzwerkanalyse (z.B. Marcus 1973, Krautter et al. 2018, Fischer et al. 2018). Trilcke et al. (2017) konnten bereits am Beispiel von Lessings Emilia Galotti zeigen, dass in statischen Netzwerken, die das ganze Drama auf einmal darstellen, wichtige Informationen zur Dynamik der Beziehungen zwischen den Figuren verloren gehen können. Unserer Ansicht nach ist außerdem zu berücksichtigen, dass Figuren auch innerhalb von Szenen auf- und abtreten und die Anwesenheit von zwei Figuren in einer Szene nicht zwangsläufig bedeutet, dass diese Figuren auch gleichzeitig auf der Bühne stehen.

            

                
Korpus und manuelle Annotation

                
Das Deutsche Dramenkorpus GerDraCor enthält über 550 deutschsprachige, TEI-kodierte Dramentexte aus dem Zeitraum von 1650 bis 1947 (Fischer et al. 2019). Die Regieanweisungen sind als 
                    
stage
-Elemente kodiert, die bisher keine weiteren Attribute besitzen, die Auskunft über den Inhalt der Regieanweisungen, wie z.B. das Auf- oder Abtreten von Figuren, geben würden. 
                

                
Insgesamt wurden 16 Dramentexte manuell annotiert, wovon vier zur Implementierung
 und zwölf zur Evaluation genutzt wurden. Die Guidelines für die manuelle Annotation stehen unter 
                    
https://doi.org/10.5281/zenodo.6951465
 zur Verfügung. Die Annotation erfolgte direkt im XML-Format. Abbildung 1 zeigt, wie ein 
                    
stage
-Element mithilfe der Attribute 
                    
type
 und 
                    
who
 um die extrahierten Informationen erweitert wird.
                

                

                    

                
Abbildung 1: Exemplarische Ergänzung eines 
                    
stage
-Elements 

                

            

            

                
Regelbasierte Annotation

                
Das entwickelte Verfahren klassifiziert den unstrukturierten Text innerhalb der 
                    
stage
-Elemente mithilfe manuell erstellter Regeln. Diese basieren auf Schlüsselwörtern und -phrasen („treten herein“, „gehen ab“, …) und nutzen reguläre Ausdrücke. Zusätzlich wird die Position einbezogen, etwa bei Regieanweisungen am Anfang einer Szene, die oftmals ausschließlich die Namen der anwesenden Figuren enthalten.
                

                
Wurde im ersten Schritt ein Auf- oder Abtritt erkannt, folgt als zweiter Schritt die Zuordnung der betroffenen Figuren. Hierfür wird die im XML enthaltene Liste der Namen aller sprechenden Figuren genutzt, die auch die Abbildung auf die Figuren-IDs ermöglicht. Der Auf- oder Abtritt wird entweder einer in der Regieanweisung genannten Figur oder derjenigen Figur, deren Rede die Regieanweisung zugeordnet ist (vgl. Fall in Abb. 1), zugeschrieben.

            

            

                
Evaluation 

                
Die Evaluation erfolgt anhand von zwölf manuell annotierten Texten, die nicht zur Aufstellung der Regeln herangezogen wurden. Evaluiert werden 1) die Klassifikation der Regieanweisungen in Figurenauf- und -abtritte und 2) die Zuordnung der betroffenen Figuren. Im zweiten Schritt werden nur die Elemente in die Evaluation einbezogen, die im ersten Schritt korrekt klassifiziert wurden. Tabelle 1 zeigt, dass die durchschnittlichen Werte für Precision, Recall und F1-Score für die Auf-/Abtritterkennung bei 0,85 liegen. Auch die Figurenerkennung liefert gute Ergebnisse (F1 = 0,87). Zwischen den Texten zeigt sich allerdings eine erhebliche Variation in der Qualität. 

                

                    

                        
Autor*in

                        
Titel

                        
Jahr

                        
Auf-/Abtritterkennung

                        
Figurenerkennung

                    

                    

                        

                        

                        

                        
P

                        
R

                        

                            
F

                            
1

                        

                        
n

                        
P

                        
R

                        

                            
F

                            
1

                        

                        
n

                    

                    

                        
Gottsched

                        
Das Testament

                        
1745

                        
0,94

                        
1

                        
0,97

                        
543

                        
0,95

                        
0,65

                        
0,77

                        
63

                    

                    

                        
Schlegel

                        
Canut

                        
1746

                        
1

                        
1

                        
1

                        
23

                        
1

                        
1

                        
1

                        
23

                    

                    

                        
Gellert

                        
Die zärtlichen Schwestern

                        
1747

                        
0,95

                        
0,98

                        
0,97

                        
129

                        
0,99

                        
0,99

                        
0,99

                        
63

                    

                    

                        
Pfeil

                        

                            
Lucie Woodvil

                        

                        
1756

                        
0,95

                        
0,97

                        
0,96

                        
141

                        
1

                        
0,9

                        
0,95

                        
78

                    

                    

                        
Lenz

                        
Der Hofmeister

                        
1774

                        
0,86

                        
0,83

                        
0,85

                        
315

                        
0,89

                        
0,8

                        
0,84

                        
70

                    

                    

                        
Schiller

                        
Die Räuber

                        
1781

                        
0,8

                        
0,79

                        
0,79

                        
544

                        
0,86

                        
0,74

                        
0,79

                        
75

                    

                    

                        
Goethe

                        
Die natürliche Tochter

                        
1803

                        
0,93

                        
0,97

                        
0,95

                        
101

                        
0,97

                        
0,98

                        
0,98

                        
28

                    

                    

                        
Kleist

                        

                            
Die Familie Schroffenstein

                        

                        
1803

                        
0,85

                        
0,7

                        
0,76

                        
320

                        
0,91

                        
0,73

                        
0,81

                        
71

                    

                    

                        
Günderode

                        
Magie und Schicksal

                        
1805

                        
0,93

                        
0,95

                        
0,94

                        
84

                        
0,98

                        
0,88

                        
0,93

                        
37

                    

                    

                        
Günderode

                        
Udohla

                        
1805

                        
0,94

                        
0,62

                        
0,74

                        
41

                        
1

                        
0,96

                        
0,98

                        
16

                    

                    

                        
Weißenthurn

                        

                            
Das Manuscript

                        

                        
1817

                        
0,7

                        
0,89

                        
0,79

                        
675

                        
0,85

                        
0,72

                        
0,78

                        
64

                    

                    

                        
Hofmannsthal

                        
Der Rosenkavalier

                        
1911

                        
0,39

                        
0,47

                        
0,43

                        
715

                        
0,76

                        
0,41

                        
0,53

                        
38

                    

                    

                        
Mittelwerte

                        
0,85

                        
0,85

                        
0,85

                        

                        
0,81

                        
0,93

                        
0,87

                        

                    

               
                
Tabelle 1: Evaluationsergebnisse
 

                
Aufgrund des regelbasierten Verfahrens schneiden Texte, die stark von den zur Erstellung der Regeln verwendeten Texten abweichen, in der Evaluation schlechter ab. Besonders Texte mit langen Regieanweisungen sorgen dafür, dass viele Schlüsselwörter auch in anderen Kontexten vorkommen. Das zeigt sich insbesondere beim 
                    
Rosenkavalier
, dessen Regieanweisungen mit 12 Tokens im Mittel doppelt so lang sind wie der Durchschnitt aller Dramen. Ein weiteres Problem stellen von der Figurenliste abweichende Namen dar, etwa Varianten des Eigennamens oder Appellativa. Ersteres könnte in Zukunft durch die Nutzung von Ähnlichkeitsmaßen adressiert werden, die aber zu mehr Falsch-Positiven führen können. 
                

            

            

                
Analyse 

                
Abbildung 2 zeigt, dass die meisten Auf- und Abtritte tatsächlich innerhalb von Szenen stattfinden (ca. 46%). Etwas weniger erfolgen am Beginn einer Szene (41%) und etwa 13% am Ende. Erwartungsgemäß handelt es sich am Anfang der Szene fast ausschließlich um Auftritte, am Ende um Abtritte. Innerhalb der Szenen komme Auf- und Abtritte zu jeweils gleichen Anteilen vor. 

                

                    

                    
Abbildung 2: Verteilung der annotierten stage-Elemente auf den Beginn, die Mitte und das Ende einer Szene.

                

            

            

                
Fazit

                
In diesem Paper haben wir ein mit Figurenauf- und -abtritten annotiertes Teilkorpus zu GerDraCor präsentiert und einen regelbasierten Algorithmus vorgestellt, der diese Annotationen mit einem mittleren F1-Wert von über 0,85 reproduzieren kann. Ein Großteil der annotierten Auf- und Abtritte erfolgt innerhalb von Szenen. Diese Veränderungen in den Figurenkonstellationen werden bei einer szenenweisen Betrachtung der Figurenpräsenz nicht berücksichtigt, haben aber potenziell Auswirkungen auf beispielsweise netzwerkanalytische Arbeiten. Alle Daten und Skripte zu diesem Beitrag sind unter 
                    
https://github.com/quadrama/enter-exit
 verfügbar.
                

            

        

            

                
Einleitung

                

                    
Innerhalb der Literatur- und Kulturwissenschaften ist die historische Reiseforschung ein beliebtes Forschungsfeld, dennoch gibt es innerhalb der Digital Humanities wenige nennenswerte Fortschritte bei der Erschließung, Verarbeitung und Visualisierung von Reiseberichten. Konventionellen digitalen Editionen fehlt bisher die notwendige Ausdruckskraft und Flexibilität, um die diversen Anwendungsfälle und Forschungsfragen der historischen Reiseforschung zu erschließen. Anstatt einer starren Auszeichnung mit TEI zu folgen, müssen Informationen in digitalen Editionen erkannt, identifiziert, mit zusätzlichen Daten angereichert und Narrationen der Ereignisse explizit modelliert werden.

                    
 

                

                
Dieser Beitrag beschäftigt sich mit der Frage der Öffnung digitaler Reiseberichte für die wissenschaftliche Analyse und Visualisierung (von Zeit, Raum, Ereignissen u.a.) durch Textauszeichnung, semantische Annotation und ontologiebasierte Modellierung. Hierbei verfolgen wir einen disziplinübergreifenden, iterativen Ansatz, welcher sowohl geistes- als auch informationswissenschaftliche Perspektiven einbezieht.
 Erprobt wird dieser Ansatz an der digitalen Edition des Reiseberichts Franz Xaver Bronners (1758–1850), der 1810 als Professor für theoretische Physik von Aarau in der Schweiz an die russische Universität Kasan an der Wolga ging und 1817 in die Schweiz zurückkehrte. 
                

            

            

                
Problemstellung

                

                    
Datenmodellierung kann laut Flanders und Jannidis (vgl. 2015) in zwei Gruppen unterschieden werden: Curation-Driven und Research-Driven. Curation-driven beschreibt die Praktiken von Bibliotheken und Archiven, Objekte mit Hilfe von Standards einheitlich zu erfassen, um so die Auffindbarkeit und Transparenz zu gewährleisten. Die dafür notwendige Reduktion führt jedoch zu Ungenauigkeiten und Lücken im Datenmaterial. Research-Driven hingegen zielt auf die Beantwortung spezifischer Forschungsfragen ab und die Datenerfassung/Modellierung folgt einem konkreten Forschungsinteresse. Dabei werden nur selten Standards berücksichtigt. Dieser Gegensatz zwischen Forschungs- und Kuratierungspraxis erschwert die Kompatibilität und damit die Vergleichbarkeit der Daten. Um digitale Reiseberichte für wissenschaftliche Analysen verwertbar zu machen, müssen sie beiden Ansprüchen gerecht werden. 

                

                

                    
Im Bereich der digitalen Editionen haben sich die TEI-Guidelines zum De-facto-Standard entwickelt. “[TEI] is the most systematic effort so far to create standards for scholarly memory in an evolving digital culture.” (tei-c.org 2019) Die Guidelines beinhalten aktuell 585 Elemente und sind flexibel gestaltet, um für ein breites Spektrum von Forschungsfragen anwendbar zu sein. Ein Problem ist jedoch, dass dieselbe Information unterschiedlich kodiert und interpretiert werden kann (z. B. &lt;rs>, &lt;persName>) und der „Standard” damit nicht zwingend interoperabel ist (siehe Unsworth 2011; Burrows et al. 2021; Giovannetti und Tomasi 2022). Die TEI versucht die Brücke zwischen Standardisierung und Granularität zu schlagen, verliert damit aber ihre Eindeutigkeit (vgl. Kudella und Jefferies 2019). 

                

            

            

                
Textauszeichnung und Ontologie-Entwurfsmuster

                

                    
Einen Lösungsansatz für das Interoperabilitätsproblem auf Textseite bietet das DTA-Basisformat (Haaf et al. 2015), welches sich zum Ziel gesetzt hat: “[...] eine umfassende Textaufbereitung [zu] ermöglichen und dabei gleichzeitig Variationsspielräume bei der Annotation so ein[zu]schränken, dass die Kohärenz [...] untereinander gewährleistet wird.” (DTABf 2011-2020) Das DTABf richtet sich dabei nach den P5-Richtlinien der TEI. Um darüber hinaus auch spezifische Forschungsinteressen der historischen Reiseforschung zu adressieren, entwickeln wir auf Datenseite einen ontologiebasierten Textanreicherungs- und Bearbeitungsworkflow: Ontologie-Entwurfsmuster werden iterativ aufgebaut und für die Klassifizierung von Reise(teil)ereignissen (Abreise, Ankunft usw.) und Reisebeobachtungen (z. B. besuchte öffentliche Orte, Gewohnheiten von Personen) angewandt. 

                

                

                    
Während TEI für die Textauszeichnung verwendet wird, dient CRM zur Anreicherung des Textes mit explizitem Wissen, welches in einer Datenbank gespeichert wird. Wir modellieren mit den Ontologie-Entwurfsmustern nicht die Erzählung als solche

                    
, sondern den rekonstruierten und stellenweise interpretierten Reiseverlauf als Repräsentation der Realität. Aus narratologischer Sicht machen wir mit dem ereigniszentrierten Modellierungsansatz von CRM also nur die Fabula (chronologische Reihenfolge der Ereignisse) eines Reiseberichts explizit. Das Sujet (Erzählreihenfolge) kann allerdings bei Bedarf anhand der annotierten Textstellen abgefragt und rekonstruiert werden.

                

                

                    
Zur Erstellung des Annotationsschemas und der damit verbundenen Ontologie-Entwurfsmuster wenden wir die Frame-Semantik als theoretischen Rahmen an. Frames können als n-äre Relationen

                    
 repräsentiert und daher zur Entwicklung von Ontologie-Entwurfsmustern für Reiseereignisse und -beobachtungen verwendet und darüber hinaus als "knowledge patterns" zur Validierung der Entwurfsmuster herangezogen werden (vgl. Presutti et al. 2012).

                    
Die Ontology Design Patterns dienen uns als „Schablonen” zum Anlegen an den Text – wobei deren Orientierung an den Frames sehr hilfreich ist – um Reisedaten, Beobachtungen und Tätigkeiten unterwegs und während Zwischenstopps zu erfassen. Im ständigen Austausch mit der Bearbeitung von Forschungsfragen am Text werden die Frames und Design Patterns laufend überprüft und angepasst. Diese Form der forschungsgeleiteten Standardisierung (mittels expliziter und einheitlicher Modellierung) macht digitale historische Reiseberichte interoperabel und öffnet sie damit für vergleichende Analysen. Die möglichen Ansätze zur Verknüpfung von TEI-kodiertem Text und RDF-Daten mittels semantischer Annotation evaluieren wir (vgl. Eide 2015 und Borriello et al. 2016) und stellen im Poster Lösungswege mit EARMARK (Barabucci et al. 2013) und NIF (Hellmann et al. 2013) vor.

                

            

            

                
Verwandte Arbeiten und Schlussfolgerung

                

                    
Es gibt einige Beispiele wie das Hellespont-Projekt (Mambrini 2016) oder die Semantic Blumenbach-Edition (Wettlaufer 2015), die TEI-kodierten Text und CRM-modellierte Daten miteinander verknüpfen. Unser Ansatz geht jedoch über die bestehenden Projekte hinaus, da wir Ontologie-Entwurfsmuster für eine explizitere Datenmodellierung entwickeln und anwenden.

                    
 Kurz gesagt, wir lösen die Interoperabilitäts- und Ausdrucksprobleme von TEI und CRM mit Hilfe von DTABf und Frame-basierten Ontologie-Entwurfsmustern, was wiederum die Kategorien von Reiseereignissen und Reisebeobachtungen in historischen Reiseberichten für die weitere Analyse und Visualisierung explizit macht.

                

            

        

            

                
Einleitung

                
In den Editionswissenschaften ist nicht unbemerkt geblieben, dass Randnotizen in Briefen (fortan: RN) keineswegs von marginaler Bedeutung sind. Vielmehr können sie aufgrund ihrer Länge, ihres inhaltlichen Gewichts und ihres rezeptionsästhetischen Reizes sogar die Hauptsache eines Briefs darstellen. Und das gilt vor allem für solche, die den Briefschreibenden selbst entstammen.
 So wurde zu Theodor Fontanes „Kunst, auf Briefrändern zu schreiben“ etwa bemerkt, dass sie das Ergebnis „intensiver schriftstellerischer Arbeit” seien, deren imposante „Architektur” schwer zu entschlüsseln ist (Erler 1968: 318-319; Gabler 2020: 1239-1240). Damit stellen RN auch die Herstellung eines Transkripts und seiner grafischen Repräsentation vor eine große Herausforderung, die gewichtige editorische und im Fall von digitalen Editionen auch informationstechnologische Entscheidungen verlangt.
                

                
Trotzdem spielen RN in den 
                    
Digital Humanities
 bzw. in digitalen Editionen bisher nur eine marginale Rolle: Die Suche nach Ansätzen, die editorisch und in Bezug auf ein geeignetes Datenmodell bereits bewährte Bau- und Fahrpläne an die Hand geben, erweist sich als eher vergeblich. Bisherige digitale Brief-Editionen geben RN meist konventionell wieder: Diplomatischen Editionskonventionen folgend werden sie topografisch, teils auch zusätzlich farblich abgesetzt präsentiert
 oder an der zugehörigen Stelle hinzugefügt;
 bisweilen werden auch Arten von Randbeschriftungen unterschieden.
 Obgleich alle gesichteten Editionen ihre Transkriptionsregeln explizieren, begründen sie die Art ihres Umgangs mit RN nicht näher. 
                

                
In den meisten Fällen ist ein solches Vorgehen ausreichend. Problematisch wird dieses erst, wenn RN mehrere Seiten umgreifen, unterschiedliche Sinneinheiten bilden, deren Abfolge gar schwer durchschaubar bzw. interpretationsbedürftig ist. Eine rein diplomatische Transkription führt dann zu fragmentierten Brieftexten, deren Sinneinheiten im Lesen erst mühsam rekonstruiert werden müssen. Besonders unbefriedigend ist eine solche Präsentationsweise für ‚User’-Kontexte, deren Erkenntnisinteressen sich – wie in dem hier behandelten Fall von Ferdinand Tönnies – primär auf historische, politische, netzwerktheoretische, fach- und theoriegeschichtliche u.ä. Inhalte richten, aber nur selten poetische, literatur- und theorieästhetische Formen in den Blick nehmen. Für solche Fälle bedarf es anderer Lösungen. Wie ein solches hierfür spezifiziertes digitales Design aussehen kann, möchte dieser Vortrag zur Diskussion stellen. Er bezieht sich dabei auf das bereits entwickelte Konzept einer digitalen Edition der Briefe des Soziologen Ferdinand Tönnies. Diese von der DFG geförderte Briefedition entsteht momentan am Kulturwissenschaftlichen Institut Essen, der Schleswig-Holsteinischen Landesbibliothek Kiel und am Trier Center for Digital Humanities (Kompetenzzentrum 2022).

            

            

                
Beobachtungen zu Randnotizen in Tönnies-Briefen

                
Die Praxis, briefkommunikativ über Bande zu spielen, tritt bei Tönnies nicht durchgängig, sondern nur episodisch in bestimmten, besonders intensiven Beziehungskonstellationen auf. Und genau dann spielen RN – darin mit Fontane vergleichbar – im brieflichen miteinander ‚zu Rande- wird. Kommen‘ keine marginale, sondern eine zentrale Rolle. Wie zu zeigen sein wird, fungierten RN als Mittel einer Inklusionsstrategie, mit der die Intensität einer Freundschaft gesteigert und diese trotz Konflikten, divergierender Interessen oder Gefühlslagen aufrecht und produktiv gehalten 

            

            

                
Ferdinand Tönnies: Gemeinschaft und Gesellschaft 

                
Wer war Ferdinand Tönnies? Wissenschaftsgeschichtlich war er von besonderer Bedeutung, da er zu Beginn des 20. Jahrhunderts mit Max Weber, Georg Simmel und anderen die Soziologie als eine eigenständige Disziplin begründete. Sein 1887 erstmals erschienenes Werk „Gemeinschaft und Gesellschaft“ wurde nicht nur für die frühe Soziologie ein gesellschaftstheoretisches Standardwerk, sondern prägte auch die Kultur- und Bildungspolitik der Weimarer Republik. Das dort ausformulierte Konzept der „Gemeinschaft“, das er in Opposition zur strategischen Handlungssphäre der Gesellschaft konstruiert hatte, übte auf die politischen Diskurse der Republik einen großen Einfluss aus. Zugleich beeinflussten 
                    
Gemeinschafts-Konzeptionen
 auch Tönnies’ epistolare Praxis.
                

                
Besonders aufschlussreich ist hierbei die Rolle, die der Soziologe der Sprache attestierte. Er begriff sie nicht nur als Medium von Inhalten, sondern ebenso als das „wahre Organ” von „Verständnis“ (Tönnies 2019: 144). Sprache lasse die Äußerung tiefer Gefühle zu. Gemütserregungen wie Schmerz, Lust, Furcht und Wunsch würden in Laute übersetzt, so dass Menschen aufgrund ihrer anthropologischen Befähigung zur „lebhafte
                    
[
n
                    
]
 Sympathie“ zu einem wechselseitig „intimen Verständnis“ und zur gemeinschaftlichen „Übereinstimmung” gelangen könnten (Tönnies 2009: 147). In dem von Tönnies besonders geschätzten Fall des intellektuellen Austauschs unter Freunden übersteigerte er das Übereinstimmen sogar zu einer „Art von unsichtbarer Ortschaft, eine[r] mystische[n] Stadt und Versammlung“, die er als geistige Gemeinschaft von anderen Vergemeinschaftungen abgrenzte (Tönnies 2019: 139, 137). Gerade die Briefe, in denen er intensiv RN gebrauchte, waren von dieser übersteigerten Gemeinschaftserwartung getragen. Um diesen bislang unbekannten Zusammenhang sichtbar zu machen, musste auch für die digitale Edition eine spezifische Umsetzung gefunden werden.
                

            

            

                
Tönnies als Briefschreiber

                
Tönnies wurde am 26. Juli 1855 in eine Welt der voll entfalteten modernen Postinfrastruktur hineingeboren. Die Epochensignatur der Beschleunigung prozessierte gerade in der hochgradig brieflich geprägten Schriftkultur, wie sie für das Bürgertum des 19. und frühen 20. Jahrhunderts typisch war. In seiner „Kritik der öffentlichen Meinung“ von 1922 schrieb Tönnies: „In unendlichen Mengen schwirren heute Briefe […] hin und her, am meisten innerhalb eines Landes, noch intensiver in engeren Gebieten, aber auch über die Grenzen von allen Orten, zu allen Orten des Erdballes“ (Tönnies 2002: 370). Das galt auch für Tönnies selbst, der im damaligen Wissenschaftsbetrieb bis zur Etablierung von Soziologie eine periphere Stellung einnahm und seine globalen wie lokalen Wissenschaftsbeziehungen primär über Briefe pflegte.

                
Die hohe Abhängigkeit vom Medium des Briefes schlug sich auch in einer permanenten Reflexion über Möglichkeiten und Grenzen dieser Kommunikationsform nieder, zumal Tönnies den Brief für ein unvollkommenes Surrogat der Kommunikation unter Anwesenden hielt. Trotzdem schrieb er fast täglich Briefe, da diese nicht nur Mittel zum Austausch mit Kollegen waren, sondern zugleich ein Medium zur Stiftung und Pflege sozialer Verbindungen im obigen Sinne.

            

            

                
Phänomen und Bedeutung von RN in Tönnies-Briefen 

                
Das Aufkommen von RN lässt sich bei Tönnies ausschließlich in vertrauten Kommunikationssituationen beobachten. Nur nach einer Serie von Briefwechseln, nur nach wiederholten persönlichen Treffen in physischer Anwesenheit, nur nach oder einhergehend mit – diskret oder explizit – bekundeter Sympathie griff er auf dieses Stilmittel zurück. Je persönlicher und intensiver die Korrespondenz wurde, desto häufiger und komplexer setzte er RN ein. Kühlte die Beziehung wieder ab, wurden sie seltener und weniger komplex. Komplexität bezeichnet dabei das Maß an gleichzeitiger Rekursivität und Unübersichtlichkeit, wenn RN sich über mehrere Seiten hinweg erstrecken, in wechselnde Sinneinheiten und in unterschiedliche Arten untergliedern (z.B. fortlaufende Textabschnitte mit Apposition, Unterbrechung, Stern-Kommentar und Fußnotenzitat).

                

                    

                    
Beispiel für einen Brief mit mehreren RN (Tönnies 1879: 3)

                

                
                

                    

                    
Beispiel für eine Postkarte mit mehreren RN (Tönnies 1881: 2)

                

                
Der Effekt für die Lesenden bestand zunächst darin, dass sie, bei gleichzeitig undeutlich werdender Schrift, eine Übersicht gewinnen, Reihenfolgen bilden und voneinander abgegrenzte Sinneinheiten und Verweiszusammenhänge rekonstruieren müssen. Systematisch zusammengefasst sind mit RN noch weitere kommunikativen Effekte verbunden:

                
A) Zunächst führt das Hinausschieben des Briefendes zu einer Verlängerung der Produktionssituation, wodurch die briefliche Gemeinschaft mit der adressierten Person prolongiert wird.

                
B) Sodann bedingt der Zwang, die im Vergleich zum Kerntext i.d.R. schwerer lesbaren Randnotate zu entziffern und zu ordnen, eine längere Lesezeit und damit die Prolongierung der Rezeptionssituation. 

                
C) RN setzen Impulse zur Anschlusskommunikation. Das gilt sowohl für den Schreibenden, der sich selbst durch RN zur Abfassung neuer RN oder zur Eröffnung neuer Briefseiten reizt, als auch für die Brieflesenden, die mit zusätzlichem Material für weitere Anschlusskommunikationen ‚gefüttert‘ werden. Diese Wirkung verstärkt sich durch eine spezifische Themenwahl. So streute Tönnies in RN bevorzugt Nachrichten aus dem geteilten sozialen Umfeld, so dass das, was ‚Tratsch‘ (Bergmann 1987: 198-202) genannt werden kann, zusätzlich zur Fortsetzung animierte. 

                
D) Der Themenbereich Tratsch lässt das Bemühen erkennen, mittels diskret eingebrachter Informationen unterschiedliche Netzwerke miteinander zu verknüpfen und diese zu einem Kreis geistig Gleichgesinnter zusammenzuschließen (Simmel 1989: 237-257).

                
E) Häufig finden sich in RN jedoch auch Ergänzungen oder Präzisierungen literarischer oder terminologischer Art, so dass RN – insbesondere in wissenschaftlichen Kontexten – der literarischen Vernetzung dienten.

                
In der Summe des Zusammenwirkens von A, B, C und D ergibt sich nicht nur die Prolongierung der briefkommunikativen Gemeinschaft per Produktion und Rezeption, sondern auch eine verstetigte Kommunikation über epistolare Einzelepisoden hinweg.

            

            

                
Erfassung und Darstellung von Randnotizen

                
Aufgrund ihrer wichtigen Funktion musste für Randnotizen eine angemessene Präsentationsweise gefunden werden. Was aber heißt angemessen? Diese Frage lässt sich nicht allein aus der unterstellten Bedeutung von RN beantworten, sondern richtet sich zugleich an die Rezeptions- und Nutzungserwartung, die hochgradig durch die bisherige Leseerfahrung geprägt ist. Dabei ist zu beachten, dass in den Briefeditionen anderer zeitgenössischer soziologischer Fachklassiker wie Max Weber und Georg Simmel, an denen unser Editionsvorhaben gemessen wird, Randnotizen keine oder fast keine Rolle spielen.
 Hinzu kommt eine Fachkultur, die an der Entwicklung von Theorien großes Interesse zeigt, nicht aber an sogenannten ‘philologischen Feinheiten’. Der Brieftext hat daher primär das Kriterium der Lesbarkeit zu erfüllen. Sollte allerdings zugleich die Chance geboten werden, mithilfe des Transkripts das Faksimile entziffern zu lernen, dann wäre eine topografisch getreue Wiedergabe nötig. 
                

                
Aus diesen divergierenden Ansprüchen an den Editionstext ergibt sich ein Zielkonflikt: Eine rein topografische Wiedergabe vermag den seitenübergreifenden Sinnzusammenhang von Randtexten nicht wiedergeben, so dass kein ‚lesbarer‘ Text entstünde. Eine Präsentationsweise, die Randnotizen in ihrem ‘Kontext’ darstellt, eignet sich dagegen nicht als „Lesehilfe“. Die Lösungsmöglichkeit besteht somit darin, die digitale Briefedition mit beiden Möglichkeiten der Textwiedergabe auszustatten und zugleich ein möglichst elegantes Wechseln der Präsentationsweise zu gewährleisten.

                
Die Dualität des Ansatzes muss daher mit einem geeigneten Datenmodell beschrieben und mit einer passenden Erfassungsmöglichkeit ausgestattet werden. Im Projekt geschieht dies während der Transkription eines Briefes in der virtuellen Forschungsumgebung des Softwareprogramms FuD (
                    

                        
www.fud.uni-trier.de

                    
). Auch die Modellierung der RN erfolgt dann im Kontext der gesamten digitalen Briefedition innerhalb der dafür eingesetzten virtuellen Forschungsumgebung, die auf einem relationalen Datenbankmanagementsystem aufsetzt. Dabei erhalten die Randnotizen eindeutige Identifier, mittels derer die Verknüpfung von RN untereinander und innerhalb des Haupttextes abgebildet wird. Weitere Informationen, wie die topographische Positionierung der RN auf der Seite in Form von Metadaten (u.a. Angaben wie „Position: linker Rand; Drehung: 90“ bzw. „Position: unterer Rand; Drehung: 180“) werden auch erhoben. All diese Daten werden in unserem relationalen Entity-Relationship-Modell abgebildet und können somit in ihrer Gesamtkomplexität erfasst werden. Dadurch lassen sich die RN während der Transkription an den betreffenden Stellen einordnen; ebenso lässt sich die logische Lesereihenfolge konstruieren. Zusätzlich erlaubt das Analysemodul von FuD eine Annotation sämtlicher Textteile auf einer Metaebene und damit auch eine sich auf inhaltlicher Ebene bewegende Verknüpfung und Erschließung. Die konsequente Modellierung und Abbildung der Sachverhalte in einer relationalen Datenbank erlauben einen entsprechenden Export der Daten mithilfe einer geeigneten XML-Kodierung, für die im Laufe des Projektes eine Darstellung auf Basis der TEI-Guidelines (u.a. Modul „Linking, Segmentation and Alignment“) implementiert wird.
                

                
Über die Darstellung von Brieftexten und Randnotizen hinaus ermöglicht diese Art der Kodierung auch eine quantitative Analyse des Phänomens ‘RN’ sowie eine gezielte Suche nach RN-Inhalten. So können einerseits für Fragen der Art „Bei welchen Korrespondenzpartnern verwendete Tönnies häufig RN?”, „Ändert sich die Häufigkeit von RN über den zeitlichen Verlauf einer Korrespondenz?”, „Wie verhält sich der Textumfang in den RN zur Gesamtlänge eines Briefes?” usw. geeignete Visualisierungen erstellt werden. Andererseits lassen sich durch die Annotation der RN nach dem obigen Muster qualitative Fragen wie „In welchen Korrespondenzen treten vermehrt RN mit dem Thema ‘Tratsch’ auf?”, „Wann werden welche ‘Nachrichten’ bezogen auf die Gesamtkorrespondenz in den RN erwähnt?” usw. beantworten. Auf Basis des Datenmodells können in der grafischen Benutzeroberfläche der Edition dann einerseits die einzelnen Randnotate eines aktuell gezeigten Digitalisats synoptisch oder die RN in ihrem übergreifenden Gesamtzusammenhang wiedergegeben werden. Andererseits können die Visualisierungskomponenten einzelne Befunde zu den vorgenannten Fragen in interaktiver Form anbieten, so dass die Benutzenden jederzeit von der Visualisierung Zugriff auf die Korrespondenzen haben. Durch eine modulare Anordnung der Visualisierungen lassen sich entsprechende Vergleichsmöglichkeiten schaffen. Eine einfache quantitative Analyse der RN innerhalb einer Korrespondenz zeigt die folgende Abbildung, in der die chronologische Verteilung der RN in 159 Briefen von Ferdinand Tönnies an Friedrich Paulsen abgebildet ist:

                

                    

                

                
Man erkennt, dass Tönnies im Laufe der Kommunikation immer wieder RN verwendet, das Phänomen aber über die gesamte Dauer der Korrespondenz abnimmt.

            

            

                
Zusammenfassung

                
Wie gezeigt, spielen RN eine wichtige Rolle in der Art, wie Tönnies per Briefkommunikation bestimmte Netzwerke im Selbstverständnis nach 
                    
Gemeinschaft und Gesellschaft
 strukturiert; als ein komplex genutztes Instrumentarium sind sie daher sowohl für die Art der Briefkommunikation als auch für die gelebte Praxis der von Tönnies entworfenen Theorie äußerst aufschlussreich. Aus diesem Grund musste für die Repräsentation der RN eine philologische, programmiertechnische und ästhetisch angemessene Lösung gefunden werden, die hier zur Diskussion gestellt wird.
                

            

        

            

                
Einleitung

                
Unter dem Hashtag #aiart erscheinen täglich tausende künstlich erzeugte Bilder auf sozialen Medien und digitalen Marktplätzen z.B. als NFTs, flankiert von Kommentaren und Artikeln, welche die Qualität und Kreativität dieser Bildproduktionen diskutieren. 

                
Der Vortrag reflektiert diese Entwicklungen, indem er die Bilder und ihre Genese kritisch hinterfragt, wertet und die Prozesse analysiert. Dass Digital Humanities parallel zum wissenschaftlichen Rezensionswesen auch eine Kritik von Software entwickeln sollte, zeigt sich z.B. in der Gründung des CKIT Rezensionsjournal das aus dem AK digitale Kunstgeschichte und nfdi4culture entstanden ist. Im Zusammenhang generativer künstlicher Intelligenz weitet sich diese Softwarekritik in Form einer Werkkritik aus. Aufgrund der raschen Entwicklung innerhalb der Computer Vision und der generativen Modelle gibt es bislang wenige Reflexionen über deren Schöpfungshöhe (Gary et al. 2022) jenseits der informatischen Überbietungslogiken und der stetigen Diskussion über bias von AI. 

            

            

                
OpenAI und closed access. Technikkritik 

                
Digitale Kunstkritik setzt bereits ein, wo die Frage gestellt wird, ob dieses popkulturelle Phänomen Gegenstand der Kunstgeschichte oder der Medienwissenschaften ist oder – wie öffentlich viel diskutiert – es sich überhaupt um Kunst handeln kann. Die Frage ist leicht zu beantworten, denn in dem Moment, wo über Kunst oder nicht Kunst entschieden werden muss, bedarf es unvermeidlich der Methoden der Kunstgeschichte und Kunstkritik. #Aiart könnte auf einer rein ästhetisch-visuellen Ebene mit konventionellen Methoden der Kunstwissenschaft bewertet werden. Dadurch, dass die Bilder nicht von Menschen sondern in erster Linie von einem mathematischen Modell erzeugt werden, bietet sich an eine operative und technikzentrierte Medienkritik in die Kunstkritik zu integrieren. Die hier durchzuführende kritische Reflexion über AI Art findet somit in der digitalen Kunstgeschichte als Teil der Digital Humanities statt. Dies geschieht in Form Medienarchäologie, in der das maschinelle Lernen und die verschiedenen Modelle mit- und gegeneinander arbeitender neuronaler Netze untersucht werden. Dabei zeigt sich eine Vielzahl an Architekturen von GANs und Transformer-Netzwerken
 deren Ergebnisse teils zusätzlich noch durch verschiedene Trainingsdatenbanken
 variiert werden können. Die 2014 mit generative adversial networks (GAN) begonnene Entwicklung hat sich in den letzten drei Jahren deutlich beschleunigt und zu immer realistischeren Bildern geführt. Während einige Modelle selbstständig Bilder erzeugen, ist die Eingabe via Bildbeschreibung quasi als Kommando (prompt) zum Standard der künstlichen Bildproduktion geworden. Diese Texteingabe ist möglich, da in DALL-E ein Sprachmodell (GPT-3) mit Algorithmen zur Bilderzeugung kombiniert ist. Auch durch dieses Zusammenkommen von Bildverarbeitung/Visualisierung und Computerlinguistik besteht hier für die Digital Humanities ein großes Potential an Forschung, Anwendung und Kritik. Daneben verbinden sich durch die Bildsynthese die Forschungsfelder Computer Vision und Computer Grafik noch enger, so dass ein transdisziplinärer bildwissenschaftlicher Gegenstand vorliegt. Eine erste Phase von ‚Kunstkritik‘ ist den Architekturen schon eingeschrieben, da ständig entschieden wird, welche Bilder den Trainingsdaten ähneln oder durch andere Kriterien als angemessen erscheinen. 
                

                
DALL-E der Stiftung OpenAI lässt sich durch sieben Gründe nicht als ‚open‘ bezeichnen, da der Code ist bislang nicht publiziert worden und die Daten anhand derer das maschinelle Lernen durchgeführt wurde (oder werden) nicht bekannt sind. Bekannt ist lediglich, dass es sich um im Internet gescrapte Bild- und Textkombinationen (z.B. Bild und Bildunterschriften) handelt. Zudem gab es zunächst keinen freien Zugang zur Nutzung, sondern nur eine Registrierung über eine Warteliste und keine Transparenz über die Zugangsvergabe. Seit 20.07.2022 ist DALL-E monetarisiert, indem nach einer gewissen Anzahl unentgeltlicher Bilder, dann für weitere Bilderzeugungen bezahlt werden muss. Ähnlich wie bei Midjourney erscheinen diese Kosten allerdings relativ moderat. Die generierten Bilder gehören alle openai. Selbst wenn ein eigener Bildupload die Grundlage bildete oder das Kommando eine Innovation/Schöpfungshöhe besitzt. Allerdings räumt openai seit Juli 2022 eine Nutzung der Bilder auch zu kommerziellen Zwecken ein. Gleichzeitig wehren sich Künstler*innen international gegen die unfreiwillige Verwendung ihrer Bilder als Trainingsdaten, da ihr Stil so sehr einfach reproduzierbar wird. 

                
Die Offenheit von DALL-E wird zuletzt durch eine teils rigide Content Policy eingeschränkt, wodurch Bilder mit sensiblen Inhalten oder Deepfakes verhindert werden sollen. Andere Modelle wie Stablediffusion sind offen im Hinblick auf open source, open access und freier Texteingabe, was wiederum Kritik aufgrund der Missbrauchsmöglichkeiten erzeugt. 

                
Eine kritische Reflexion darüber, ob OpenAI mit dieser Herangehensweise nicht ihrer eigenen Stiftungscharta
 widerspricht, kann nur angerissen werden. Die Geschlossenheit der kommerziellen Systeme wie DALL-E, IMAGEGEN und PARTI ist allerdings nur ein Aspekt, der eine wissenschaftliche Erforschung und Kritik erschwert. Der Aufbau konkurrenzfähiger Modelle an Universitäten scheitert oft am Aufbau der Trainingsdatensätze sowie der Infrastruktur respektive Rechenleistung für das maschinelle Lernen. Die akademische Forschung hat somit Schwierigkeiten selbst unabhängige Architekturen zu erstellen und kann gleichzeitig mangels Verständnisses der führenden Modelle keine fundierte Kritik zu deren Funktionsweise artikulieren. Modelle wie Stablediffusion (Rombach 2022) zeigen hingegen, dass durchaus konkurrenzfähige Architekturen im akademischen Kontext entstehen klönnen. Insgesamt besteht allerdings die Herausforderung die Verfahren der oft als black box beschriebenen Netze zu interpretieren. Dies ist u.a. möglich durch den Vergleich der unterschiedlichen Modelle und Trainingsdatensätzen oder Ergebnissen aus unterschiedlichen Phasen der Bildgenerierung, sowie durch Prompt Engineering als experimentelle Methode. Bei letzterer kombiniert und ergänzt man in Versuchsreihen die Satzteile einer Befehlszeile in unterschiedlichen Reihenfolgen. Dabei können auch Marker gesetzt werden, z.B. indem im Kommando dazu aufgefordert wird, Personen einzufärben oder auf andere Weise Objekte zu markieren, um zu sehen, ob das Modell nicht nur Bilder des Konzepts kopiert, sondern ein tieferes Verständnis des Konzepts hat. Auch kann untersucht werden, wie ein Bild durch Zugabe oder Umformulierung von Text realistischer wird. An dieser Stelle zeigt sich auch wie subjektiv die Kriterien der Bewertung sind. Welche Grundlage hat die Wahrnehmung ein Bild realistischer oder die Umsetzung (man könnte auch sagen Inszenierung) eines prompts gelungener zu finden?
                

            

            

                
Kunstkritik 

                
Neben der Medienarchäologie zu neuronalen Netzen und dem maschinellen Lernen muss hier also eine zweite Form der Auseinandersetzung gefunden werden, die sich methodisch an klassische Kategorien und Werte der Kunstkritik anlehnt. Diese Werte sind nicht essentialistisch zu verstehen, sondern relativistisch. So kann ein visueller Turing-Test durchgeführt werden, indem gefragt wird, ob das Bild für computergeneriert oder eine Fotografie bzw. von Menschen digital oder konventionell erstellte Grafik gehalten wird. Hier zeigt sich, dass in DALL-E 2 viele Bilder erzeugbar sind (Photograph of a house in the Bauhaus style; Abb. 1), die in diesem Turing-Test bestehen, während andere unbefriedigende Ergebnisse liefern (17th century landscape painting). Das Problem ist in vielen Fällen nicht die Imitation eines Stils, sondern die Form der dargestellten Dinge also die Detailschilderungen (im Fall der niederländischen Landschaftsmalerei sind es monströse Kühe). Die Bildgeneratoren insbesondere DALL-E 2 sind intelligent im Sinne von Mimikry. Sie können die Oberflächen von Dingen, Stilen, Texturen und Beleuchtungen überzeugend widergeben ohne ein tieferes Verständnis der Gegenstände zu haben. Die für die Kunstgeschichte und Kunstkritik so wichtigen Pole, Kunst und Natur, haben auch bei der künstlichen Bildgenerierung eine Bedeutung. Das antrainierte „Weltwissen“ führt dazu, dass die Modelle auch in der Natur vorhandene Proportionen internalisieren – also visuelle Prinzipien der Kompositionalität befolgen. Entsprechend fällt es beispielsweise dem Algorithmus schwer Elefanten zu erzeugen, die kleiner als Schildkröten sind. Dadurch erscheint auch der Namensgeber Salvatore Dali für DALL-E eher unpassend, weil dessen radikal unnatürlichen Schöpfungen innerhalb seiner surrealen Kunst mit DALL-E kaum zu reproduzieren und noch weniger von diesem Modell zu erfinden wären. 

                
Auf diese Weise sollen Kriterien bzw. die so genannten Rubriken der Kunstkritik (Vogt 2010) wie Bilderfindung, Komposition, Ausdruck, Stil und Dekorum, Naturnähe (im Sinne von real-world-data) und Kunstrezeption untersucht werden. Auf ikonologischer Ebene führt die Bildkritik direkt in gesellschaftliche Zusammenhänge. Denn auch bei den neuen Generationen von Bildgeneratoren zeigen sich der Bias, der in Bezug auf das maschinelle Lernen bereits viel besprochen wurde (z.B. Stereotype Frauen- und Männerberufe sowie nicht-weiße Kriminelle werden wie selbstverständlich erzeugt), während die vollständige Unwissenheit bzgl. einiger kanonischer Werke der Kunst- und Kulturgeschichte auch die mangelnde Historizität der Trainingsdaten aufzeigt. 

                
Anhand der exemplarischen Beurteilung der synthetischen Bilder soll es weniger um Urteile der einzelnen Bilder gehen, als um die Aufstellung von Kriterien und Erkenntnisse über die Potentiale von aiart insgesamt. Dabei zeichnet sich ab, dass wir es mit einer Form von Mimikry zu tun haben, durch die Oberflächen und Stile immer besser imitiert werden können, während komplexere Kompositionen und Konzepte scheitern. Die Forschungen zu aiart entstanden im Rahmen des SPP „Das digitale Bild“ im Projekt „Bildsynthese als Methode des kunsthistorischen Erkenntnisgewinns“ (2019-2022). In Ausblick und Diskussion soll auch darauf eingegangen werden, welche weiteren Anwendungsfelder sich in der Kunstgeschichte für die künstlich generierten Bilder aufzeigen lassen.

                

                    

                

                

                    
Abb. 1: Mit Dall-E 2 erzeugte Bauhaus Architekturen. 

                

            

        

            
Beginen sind geistliche Frauen, die ohne Klausur und Gelübde in mittelalterlichen Städten lebten und im Gegensatz zu den klösterlichen Nonnen im Stadtbild präsent waren. Rechtlich waren sie Laien und unterstanden der weltlichen Obrigkeit. Daher sind sie in Rechtsquellen der Stadt präsent, auch in den berühmten Schreinsbüchern der Stadt Köln. 

            
Diese bieten eine für den Betrachtungszeitraum (13.–15. Jh.) außergewöhnlich dichte Überlieferungssituation (Militzer 1999), da in ihnen bereits seit dem 12. Jahrhundert Rechtsgeschäfte erfasst wurden, zu denen insbesondere Immobilienübertragungen zählten. Durch eine seit den 1990ern andauernde Auswertung der Schreinsbücher konnten bisher rund 2100 Beginen identifiziert werden. Die Ergebnisse dieser Auswertung liegen in Form von Regesten in einer Textdatenbank vor, die zudem Informationen zu Verwandtschaftsbeziehungen, der sozialen Stellung der Frauen und ihren Wohnstätten in der Stadt verzeichnet.

            
Im März 2022 ist an der Universität zu Köln ein DFG-gefördertes Projekt (DFG 2022) unter der Leitung von Frau Dr. Letha Böhringer angelaufen (Böhringer 2022). Das Projekt verfolgt zwei interdependente Ziele: 1.) die erste umfassende Monographie zur Sozialgeschichte der Beginen in Köln vorzulegen, 2.) die Wissensbasis, d.h. die Quellenauswertung, digital so aufzubereiten, dass diese nach prosopographischen, sozialgeschichtlichen und sozialtopographischen Fragestellungen hin ausgewertet und in einer Webplattform öffentlich zugänglich gemacht werden kann. Diese Arbeit wird am Cologne Center for eHumanities durchgeführt, das erste Ergebnisse zur Datenmodellierung und Georeferenzierung mithilfe historischen Kartenmaterials der Stadt Köln vorstellen wird.

            

                
Datenmodellierung und Informationsextraktion

                
Die vorhandenen Regesten mit 4848 Einträgen sind in eine XML-Struktur überführt worden und müssen, da wesentlich als Fließtext festgehalten, durch regex-Anweisungen und XSL-Transformationen in relevante Informationseinheiten vorstrukturiert werden. Hierbei geht es primär um die Identifikation von Akteuren, topographischen Angaben und Relationen (Verwandtschaftsbeziehungen, Transaktionen). Methoden des NLP, wie Named Entity Recognition, werden hierbei ebenfalls ergänzend erwogen.

                
Bei der Datenmodellierung orientiert sich das Projekt an methodisch verwandten Vorhaben und etablierten Standards (Grünwald 2021). Auf der Ebene der semantischen Verknüpfung wird die eventbasierte Beschreibung durch CIDOC CRM Anwendung finden (s. Abb. 1). Die spezialisierte Bookkeeping Ontology for Historical Accounts (Pollin 2022), die auf CIDOC CRM basiert, wurde ebenfalls evaluiert.

                
                    

                        

                    
Abb 1.: Regest Schrb. 12 fol. 61r, nach CIDOC CRM V.7.0.1

                

                
            

            

                
Kartographie/Topographie

                
Immobilienbesitz und Wohnstätten der identifizierten Beginen lassen sich in großer Zahl und mit erstaunlicher Genauigkeit lokalisieren. Dies ist vor allem deshalb möglich, weil die Quellengrundlage überwiegend Immobiliengeschäfte zum Gegenstand hat und für die Stadt Köln ein Standardwerk zur städtischen Topographie vorliegt, das ebenfalls auf der Auswertung der Schreinsbücher beruht (Keussen 1910) und ein Referenzierungssystem für Häuser der mittelalterlichen Stadt bietet. Die Referenzen auf Keussens System wurden bei der Erstellung der Fließtext-Regesten bereits systematisch nachgehalten und können so digital genutzt werden.

                
Dank der Bereitstellung von historischem Kartenmaterial durch die Historische Gesellschaft Köln e.V., in dem die Straßen-Parzellen-Einteilung Keussens in ursprünglich für den Druck vorbereiteten proprietären Formaten des Programms Adobe Freehand abgebildet wurde (die nun durch das Beginen-Projekt verlustfrei in offene Formate überführt worden sind), steht dem Projekt eine vektorisierte Gesamtkarte Kölns zum hohen und späten Mittelalter Kölns nach der zweiten Stadterweiterung zur Verfügung (s. Abb. 2).

                
                    

                        

                    
Abb 2.: J. Bigalke/Kliomedia/Historischen Gesellschaft Köln e.V./Greven Verlag

                

                
                
Dies eröffnet eine Reihe von Möglichkeiten, die vom Minimalziel eines Exports als hochauflösende Rasterdatei, die über einen Map-Tile-Server in Webviewer eingebunden werden kann und das Markieren von Positionen und Clustern auf der Karte ermöglicht, bis hin zum Maximalziel eines Export der Vektoren für unterschiedliche topographische Typen (Mauer, Straßenzüge, Bezirksgrenzen, Kirchen, Klöster, "Parzellen") und Einbindung in GIS-Programme und Webpräsentation reicht.

                
Das Projekt wird die verschiedenen Optionen vorstellen und in seiner Posterpräsentation der Frage nachgehen, ob sich das Referenzsystem aus Keussens Topographie systematisch auf die eingezeichneten Haussegmente übertragen und so als dynamische Visualisierung auf der Grundlage von Datenbankabfragen realisieren lässt.

            

            

                
Zusammenfassung

                
Dank der Vorarbeiten in der Auswertung der Schreinsbücher kann das Beginen-Projekt auf Regesten zurückgreifen, die eine Vielzahl an Informationen zu geistlichen Frauen im Köln des Mittelalters enthalten und es nach ihrer Strukturierung im weiteren Projektverlauf erlauben werden, prosopographische Netzwerkanalysen vorzunehmen. Daher wird das Poster als ersten Kernaspekt die Datenmodellierung eines exemplarischen Regesteneintrags visualisieren.

                
Zweiter Kernaspekt der DH-Komponente in dem Projekt und entsprechend der Posterpräsentation ist die Georeferenzierung und Verortung der geographisch gebundenen Daten mit den historischen Karten der mittelalterlichen Stadt Köln, sowie die Frage nach den damit verbundenen sozialtopographischen Auswertungsmöglichkeiten. Vorbehaltlich der lokalen Begebenheiten streben wir eine Live-Demonstration der Kartenvisualisierungen auf einem Laptop/Tablet an.

                
Jenseits dieser ersten vorläufigen Projektergebnisse möchten wir mit dem Poster zur Diskussion stellen, inwiefern das Projekt wichtige Vorarbeiten zum besseren Verständnis der Struktur und Semantik der Schreinsbucheinträge leisten kann, da die vollumfängliche Erschließung der Schreinsbücher ein lange bestehendes Desiderat der mediävistischen Forschung darstellt.

            

        

            
Die germanistische Mediävistik hat über Jahrzehnte viele digitale Ressourcen aufbereitet, die zu etablierten und zentralen Säulen des Faches gewachsen sind. Das DFG-Netzwerk 
                
Netzwerk Offenes Mittelalter
 widmet sich dem weiteren Erschließungspotenzial dieser heterogenen und historisch gewachsenen Forschungsdaten, ihrer Vernetzung und ihrer qualitativen Verdichtung und exploriert dabei den Einsatz von Linked Open Data (LOD). Das Potenzial und die Grenzen dieser Verfahren werden forschungsorientiert erprobt und methodisch reflektiert. 
            

            
Unter ‚offenes Mittelalter‘ verstehen wir dabei nicht nur die für LOD ohnehin erforderliche Einhaltung der FAIR-Prinzipien, sondern auch eine disziplinäre wie epochenübergreifende Durchlässigkeit, die den Austausch mit den relevanten Communitys und Gedächtnisinstitutionen, aber auch die Übertragbarkeit der Methoden widerspiegelt. 

            
Das Netzwerk liefert aus einer fachdisziplinären Perspektive heraus Impulse für den Einsatz von LOD in geisteswissenschaftlicher Forschung und befindet sich dabei in Austausch mit der Nationalen Forschungsdateninfrastruktur (NFDI), insbesondere den Konsortien Text+ und NFDI4Culture.

            
Das Netzwerk setzt sich zusammen aus 18 Wissenschaftler*innen aus Deutschland, Österreich und den Niederlanden. Die individuellen Hintergründe sind vielfältig, sodass neben verschiedener mediävistischer Expertisen auch Informationswissenschaften und Bildungsforschung vertreten sind. Über die einzelnen Forschungsaktivitäten und -projekte sind zudem zentrale Ressourcen der germanistischen Mediävistik unmittelbar eingebunden, etwa der Handschriftencensus, das Mittelhochdeutsche Wörterbuch, die Mittelhochdeutsche Begriffsdatenbank, das Referenzkorpus Mittelhochdeutsch, das Mittelalterblog und diverse einschlägige Editionen (s. KONDE). 

            
Ergänzt und erweitert wird das Netzwerk durch Expert*innen, die als Gäste wertvolle Impulse einbringen und Austausch und Vernetzung vorantreiben. Für diesen wichtigen Aspekt des Community-Building haben wir zudem die Beteiligungsmöglichkeit als „Assoziierte Mitglieder“ geschaffen, um das Netzwerk fruchtbar zu erweitern und der fachlichen und methodischen Diversität gerecht zu werden.

            
Konzeptionell beleuchtet das Netzwerk die Verfahren aus verschiedenen Perspektiven. Hierzu gehört ein editionsphilologischer Fokus: Als etablierter Standard für die Kodierung von Texten bildet TEI-XML die Grundlage vieler digitaler Editionen und dient als Archivformat (vgl. Wettlaufer 2018). Darin sind Informationen zunächst implizit erfasst, d. h. menschenlesbar, aber nicht ‚semantisch‘ nach Kriterien des Semantic Web (vgl. Hitzler 2021). Hier wurden etwa Möglichkeiten des Transfers eruiert (z. B. XTriples, Addlesee 2019), aber auch Herausforderungen in diesem Bereich diskutiert z. B. im Umgang mit Unsicherheiten und Ambiguitäten, wie sie besonders beim Umgang mit historischen Ressourcen häufig auftreten und zurzeit von besonderem Forschungsinteresse sind (Kuczera 2019; Andrews 2021-2026). Eine weitere wichtige Säule für LOD bilden persistente Identifier. Für eine forschungsgetriebene Anwendung, die über basale Metadatenkategorien hinausgeht, ergeben sich verschiedene Herausforderungen. Hier befindet sich das Netzwerk in Austausch mit der GND, beleuchtet Entwicklungen zu Normdatenstandards (z. B. Burrows et al. 2020) und exploriert zudem die Einbindung verschiedener Wikisysteme wie Wikidata und Factgrid.

            
Eine wichtige Rolle spielt auch das Spannungsfeld von Materialität und Text, in dem nicht nur die Stellvertreterfunktion digitaler Objekte beachtet werden, sondern auch Möglichkeiten zu domänenspezifischen Vokabularen und granularen Identifiern gegeben sein müssen. Die Materialität stellt einen disziplinenübergreifenden ‚Berührungspunkt‘ dar, der sowohl eine Schnittstelle für LOD bilden kann als auch große Herausforderungen und Chancen für eine Erfahrbarkeit im digitalen Raum bietet.

            
Die skizzierten Verfahren sind nur tragfähig, wenn für sie eine weitreichende Akzeptanz besteht und geeignete Forschungsinfrastrukturen vorhanden sind. Das Netzwerk bemüht sich daher um eine Methodenzusammenschau, liefert Best Practices und erstellt Showcases, die einen ertragreichen Einsatz von LOD illustrieren. Zu den Ergebnissen des Netzwerks gehört auch eine Wissensplattform, die eine Ressourcensammlung bietet, in der einschlägige Ressourcen zu LOD, Tutorials und Projekte gelistet werden, eine domänenspezifische Bibliographie zu LOD in der germanistischen Mediävistik gepflegt wird und die Showcases aus der Forschung der Netzwerkmitglieder präsentiert werden. Flankiert werden diese Ressourcen zudem von der ausführlichen Dokumentation der Aktivitäten des Netzwerks über Blogbeiträge (z. B. Borek et al. 2022), Meldungen auf der Website des Netzwerks und über Präsenz und Vernetzung über Social Media.

            
Mit unserem Poster informieren wir über unsere bisherigen Ergebnisse und Aktivitäten des 
                
Netzwerks Offenes Mittelalter
 und laden ein zu weiterem Austausch, um nachhaltige und innovative Forschungsinfrastrukturen in den digitalen Geisteswissenschaften gemeinsam mitzugestalten.
            

        

            
KoMuX, der Kompositamuster-Explorer, (www.owid.de/plus/komux) ist eine Webanwendung, die es ermöglicht, mehr als 50.000 nominale Komposita des Deutschen gezielt nach abstrakten oder lexikalisch-teilspezifizierten Mustern zu durchsuchen. Unterschiedliche Visualisierungen helfen dabei, Strukturen und Zusammenhänge innerhalb der Ergebnismenge zu erfassen.

            
Mit KoMuX machen wir einen Teil der Datengrundlage frei verfügbar, auf der unsere empirischen Forschungen zur Wortbildung basieren und integrieren Analysen und Visualisierungen aus unseren Arbeiten. Der Explorer ist damit auch ein Beitrag zu OpenScience, indem er es ermöglicht, unsere Forschungsergebnisse in Teilen nachzuvollziehen und zu reproduzieren.

            

                
Forschungshintergrund

                

                    
In der Wortbildungsforschung stellt die Einbeziehung authentischen Sprachmaterials nach wie vor ein Desiderat dar (vgl. z.B. Hein ersch. 2023; Elsen und Michel 2007). KoMuX ermöglicht es, Untersuchungen zur Komposition auf eine breite empirische Basis zu stellen und einer ‚empirischen Wortbildungsforschung‘ somit ein Stück weit näher zu kommen. Der Explorer basiert auf einer systematischen Datenerhebung, bei der alle nominalen Komposita automatisch aus 
dem KoGra-Untersuchungskorpus (KoGra 2022), 
                    
einem Ausschnitt 
des Deutschen Referenzkorpus DeReKo (Kupietz u. a. 2010)
                    
, extrahiert wurden. Diese Datengrundlage ist unseres Wissens nach die erste ihrer Art.

                

                

                    
Mit KoMuX wird erstmals eine Untermenge dieses Komposita-Inventars des Deutschen frei zugänglich und systematisch durchsuchbar gemacht, und zwar aus einer Muster-Perspektive (vgl. 
Stein und Stumpf 2019)
                    
heraus: Wir betrachten Komposita als konkrete sprachliche Realisierungen von zugrundeliegenden abstrakten oder lexikalisch-teilspezifizierten Mustern. Diese Muster aus spezifischen Paarungen von Erst- und Zweitgliedern wiederum können z.B. zur Erklärung von beobachtbaren Produktivitätsunterschieden herangezogen (vgl. 
Hein und Brunner 2020; Brunner u. a. 2021)
                    
oder – ganz allgemein – als Grundprinzip verstanden werden, das erklärt, wie die Komposition funktioniert bzw. wie sich das Inventar von Komposita grundsätzlich systematisieren lässt (vgl. 
Hein ersch. 2023)
                    
. Der Musteransatz 

                    
bietet darüber hinaus eine direkte Anschlussfähigkeit an Grammatiktheorien wie die Konstruktionsgrammatik bzw. die Construction Morphology 
(Booij 2010)
                    
.

                

            

            

                
Datengrundlage

                

                    
Das KoGra-Untersuchungskorpus umfasst ca. 7 Milliarden Tokens und besteht zum größten Teil (~90%) aus Pressetexten (zur genauen Zusammensetzung vgl. KoGra 2022; 
Bubenhofer, Konopka und Schneider 2014)
                    
. Es wurde mit einem automatischen Werkzeug annotiert, welches die 
Canoo Language Tools
                    

                        
[1]

                    

                    
 

                    
adaptiert

                    
, und für jedes Token detaillierte morphologische Informationen liefert, auf deren Basis nominale Komposita extrahiert wurden. KoMuX basiert auf einer Untermenge von 100.000 Komposita-Tokens, die zufällig aus der Gesamtmenge von ca. 489 Millionen Komposita-Tokens gezogen wurden. Daraus ergibt sich die Frequenzliste mit ca. 50.000 Komposita-Types, die durchsucht werden kann.

                

                
D
                    
ie automatischen morphologischen Analysen wurden manuell und semi-automatisch verbessert. Dies umfasste v.a. das Entfernen von falschen Einträgen (Tokens ohne Komposita-Status) sowie Korrekturen von falschen Zerlegungen und fehlerhaften Zuweisungen von Wortbildungstyp- und/oder Wortart-Klassifikationen für die Komposita-Konstituenten.

                

            

            

                

                    
Technische Details

                

                

                    
Die Komposita-Daten werden in einer MySQL-Datenbank verwaltet, die Anwendung selbst ist in JavaScript (node.js) programmiert. Für das Frontend wurden die Frameworks Vue.js und Vuetify.js verwendet, sowie die Bibliothek Apache ECharts für die Grafiken.

                

                

                    
KoMuX-Tabellen können im CSV-Format heruntergeladen werden. Zusätzlich verfügt die Anwendung auch über eine API, über die direkte Anfragen an die Datenbank gestellt und die Ergebnisse im JSON-Format heruntergeladen werden können (vgl. http://www.owid.de/plus/komux/komux_api_doku/KoMuX.html).

                

            

            

                
Funktionalitäten und Anwendungsbeispiele

                
Die musterbasierte Suche in KoMuX beruht darauf, dass grammatische Merkmale (Wortbildungstyp oder Wortart) oder lexikalische Eigenschaften (konkretes Lemma) für das Erst- und Zweitglied spezifiziert werden. Dies erlaubt es beispielsweise, gezielt alle Adjektiv+Nomen-Komposita (z.B. 
                    
Kleinkind
) oder Nomen+Nomen-Komposita (z.B. 
                    
Zeitpunkt
) zu extrahieren sowie systematisch nach Phrasenkomposita (z.B. 
                    
Nacht-und-Nebel-Aktion
) zu recherchieren. KoMuX ermöglicht zudem Untersuchungen zur Rekursivität von Komposita, indem gezielt nach Komposita gesucht werden kann, deren Erst- und/oder Zweitglied ebenfalls ein Kompositum ist (z.B. 
                    
Autobahnraststätte
)
                    
. 
Neben diesen formalen Suchebenen lassen sich auch konkrete Lemmata in Erst- oder Zweitgliedposition festlegen und somit Muster mit lexikalischen Ankern definieren, z.B. Komposita mit Farbwörtern (z.B. 
                    
Dunkelrot, Abendrot
).
                

                
Visualisierungen helfen dabei, die Ergebnismenge näher zu analysieren. Auch hier steht die musterhafte Betrachtung von Erst- und Zweitgliedposition im Mittelpunkt.

                
Quantitative Verteilungen in Hinblick auf Wortart, Wortbildungstyp und Lemma werden mit Hilfe von mehrstufigen Tortendiagrammen sichtbar gemacht.

                

                    

                        

                        
Quantitative Verteilungen (Suchebenen ‚Wortbildungstyp‘ und ‚Lemma‘) beim Erstglied für Komposita mit dem Zweitglied zentrum

                    
.
                

                
 

                
Die Konstituenten-Ansicht zeigt alle Erst- und Zweitglied-Lemmata der Ergebnismenge, sowie deren Vorkommenshäufigkeiten in den jeweiligen Positionen. So lässt sich untersuchen, in welcher Position die lexikalische Vielfalt größer ist und welche Lemmata starke Tendenzen zu einer der beiden Positionen aufweisen.

                
Die Verknüpfungsansicht zeigt Komposita, deren Erst- oder Zweitglied-Lemma in mindestens einem weiteren Kompositum der Ergebnismenge auftritt und weist so auf produktive Bildungsmuster hin.

                

                    

                        

                        
Ausschnitt aus dem Verknüpfungs-Diagramm für die Ergebnismenge mit dem Muster [PHRASE
                            
Erstglied
+NOMEN
                            
Zweitglied
].

                    

                    
                

            

            

                
Ausblick

                

                    
Wir streben an, in späteren Versionen auch semantische Suchebenen zu integrieren, z.B. die semantisch-thematische Klasse der unmittelbaren Konstituenten (z.B. ARTEFAKT, GEFÜHL) (vgl.
Brunner u. a. 2021) und die Visualisierungen auszubauen
                    
. Perspektivisch wäre es wünschenswert, unser gesamtes Komposita-Inventar über den Explorer verfügbar zu machen.

                

                

                    

                    
[1]
 Leider ist die Seite canoo.net nicht mehr online verfügbar. Teile der Inhalte wurden von LEO übernommen, allerdings nicht der morphologische Analysierer (vgl.
                    
 

                    
).
                

            

        

            

                
Hintergrund

                
Das Erstellen von digitalen Quelleneditionen gehört mit zu den etabliertesten und einflussreichsten Praktiken und Methoden im Bereich der Humanities Computing bzw. Digital Humanities (Burnard 2014). Die Vielzahl praktischer Erfahrungen und eine fundierte theoretische Reflexion führten zu einer Konvergenz, die sich sowohl an der Stabilität und weiten Verbreitung technischer Standards wie der TEI, in der zunehmenden Akzeptanz von Kriterien für die Bewertung digitaler Editionen (z.B. Sahle 2014) und einem breiten Konsens zu den methodischen, organisatorischen und sozialen Rahmenbedingungen ihrer Erstellung (Fritze et al. 2022) ablesen lässt.

                
Neben dieser Konvergenz zeigt sich in der Praxis allerdings ein sehr breites Spektrum digitaler Bereitstellungsarten, von Formaten und Zugängen zu historischen Quellen, die wir im Panel „Opening Sources“ adressieren möchten. Im Sinne der Open Humanities steht dabei der offene Zugang zu einem bestimmten Quellenbestand als Hauptziel im Zentrum des editorischen Tuns. Anhand von vier exemplarischen editorischen Projekten, angesiedelt an den Instituten der Max Weber Stiftung (MWS), soll diese Vielfalt von Formen und Zugängen verdeutlicht und als Ausgangspunkt für eine umfassendere Diskussion genutzt werden.

                
Die Mehrheit dieser Projekte erfüllt die Kriterien an eine wissenschaftliche Edition gemäß des erwähnten Kriterienkataloges nicht oder nicht ausreichend.
 Auch fallen sie nicht unbedingt unter den Begriff born digital editions, den Patrick Sahle dahingehend definiert, dass solche Projekte nicht ohne Verlust an wesentlichen Informationen und Funktionen in eine herkömmliche Papierform übertragen werden können (2013, Bd. 2, 149). Dabei teilen die Projektbeteiligten diese Kriterien durchaus. Jedoch erlauben die vorhandenen organisatorischen, finanziellen oder personellen Ressourcen nur Teilschritte der Quellenpublikation und -edition. Dank der Konvergenz von Formaten für Metadaten (MODS/teiHeader), Digitalisate (TIFF/PNG/JPEG), Transkripte und kritische Apparate (TEI) sowie deren Verschränkung (METS, IIIF) handelt es sich – so die erste These des Panels – beim datenzentrierten Ansatz, der den verschiedenen digitalen Zugangsformen zugrunde liegt, weniger um klar voneinander abzugrenzende Genres, sondern um ein gestuftes Editionsmodell, dessen Modularität wir ausgehend von vier kurzen Input-Referaten zur Diskussion stellen. Denn – so die zweite These des Panels – sehr viele Editionsprojekte dürften im Spannungsfeld von Anspruch und Wirklichkeit vor ähnlichen Herausforderungen stehen, sodass ein gemeinsames Nachdenken über Lösungsansätze lohnend erscheint (vgl. Sayers 2017).
                

            

            

                
Leitfragen

                
Die folgenden Punkte stehen im Zentrum der Input-Referate und sollen in der Diskussion im Panel und mit den Teilnehmer_innen vertieft werden:



                    

                        
Linearität des Editionsprozesses

                        
Wie weit deckt sich das informationswissenschaftliche Paradigma eines linearen Vorgangs von der Erfassung der Metadaten und Bereitstellung des Digitalisats über die Erstellung des Transkripts und dessen Annotation mit der Praxis des Edierens in den Geisteswissenschaften? Oberbichler et al. (2021) zeigen am Beispiel von Zeitungsarchiven, dass eine forschungsgeleitete digitale Hermeneutik immer wieder die prozessuale Linearität durchbricht. Nicht allein die Auswahl der zu edierenden Quellen, auch deren Datierung und Transkription setzt häufig Wissen voraus, das erst beim 
                        
close-reading
 bei der Quellenannotation entsteht. Ein zeitlich gestrecktes oder institutionell verteiltes Verfahren droht den Rückfluss neuer Erkenntnisse in einen vorhergehenden Arbeitsschritt zu erschweren, so dass auch bei digitalen Editionen eine Abkehr vom linearen Wasserfall-Modell hin zu agilen, iterativen Projekten zu beobachten ist (Ferraro et al. 2018). Und würden solche Teilschritte überhaupt von Forschungsförderern Finanzierung erhalten?
                        
An welchen Stellen lässt sich der Publikationsprozess unterbrechen und zu einem späteren Zeitpunkt weiterführen? Was sind Mindestanforderungen im Sinne der Open Humanities für dieses Vorgehen (freie Lizenzen, breit akzeptierte Standards, offene Schnittstellen, lückenlose Dokumentation)? Sind das bloß notwendige, oder auch hinreichende Bedingungen? Welche Parameter müssen von vornherein festgelegt werden (z.B. Browsing-, Such- und Auswertungsmöglichkeiten), welche können nachträglich ohne Nacharbeiten an Metadaten und Auszeichnung eingebracht werden?
                    

                    

                        
Vom Digitalisat zum Text

                        
(Putnam 2016) hat die fundamentale Bedeutung der Findbarkeit von (Voll-)Texten für den digitalen Wandel in den Geisteswissenschaften herausgearbeitet. „Digitale Suche bietet eine vermittlungsfreie Entdeckung“. Deshalb ist die einfache Durchsuchbarkeit für eine breite wissenschaftliche Rezeption eines Quellenbestandes meist wichtiger als dessen minutiöse Annotation. Der Durchbruch bei der automatisierten Handschriftenerkennung (HTR) in den letzten Jahren ermöglicht eine Automatisierung, die zuvor Druckerzeugnissen in lateinischen Zeichensätzen vorbehalten war. Da je nach Schreibhand passende Trainingsdaten die Voraussetzung sind, kann sich hier die Verschränkung mit Citizen-Science-Ansätzen zum Crowd-Sourcing anbieten.
                    

                    

                        
Vom Text zur Edition

                        
Die zentrale Frage beim Übergang zur Edition ist die Frage des 
                        
Edendum
. Soll und kann der Gesamtbestand an Transkripten inhaltlich annotiert und umfassend kommentiert werden? Falls nicht, sind Kriterien wie „Repräsentativität“, die „Vollständigkeit in Teilen“ beispielsweise für bestimmte Textgattungen oder Zeiträume oder gar „Popularität“ (häufig zitiert oder im Textarchiv oft aufgerufen) zielführend? Ist es auch in diesem Schritt hilfreich, automatisierte Verfahren oder Bürgerwissenschaftler_innen zur Annotation und Verlinkung mit Normdaten in Erwägung zu ziehen?
                    

                    

                        
Verlässlichkeit

                        
Eine schrittweise Edition ist keineswegs eine „Edition light“. Die Streckung des Prozesses kann nur gelingen, wenn jeder einzelne Schritt passende Qualitätssicherungsverfahren und die dauerhafte Sicherung der Zwischenergebnisse gewährleistet. Im Vordergrund stehen hier die FAIR-Prinzipien, die vor allem die Zugänglichkeit und die spätere Weiternutzung gewährleisten. Auch bei Quellenbeständen aus nicht-indigenen Kontexten, sollten zusätzliche Aspekte wie 
                        
Collective Benefit
, 
                        
Responsibilty
 und 
                        
Ethics
 aus den 
                        
CARE-Prinzipien
 berücksichtigt werden.
                        
Die Dauerhaftigkeit bezieht sich auf die langfristige Sicherung der Forschungsdaten wie auf die einfache Zugänglichkeit in einer niedrigschwellig verfügbaren Präsentationsumgebung (vgl. Fritze et al. 2022, Abschnitt 12).
                    

                

            

            

                
Format

                
Nach der Einführung durch die Organisator_innen (5 Min.) wird jede_r Panelist_in das eigene Projekt mit Bezug auf diese Leitfragen kurz vorstellen (je 5 Minuten). In einem zweiten Teil suchen wir Antwortvorschläge, zunächst in einer Diskussion unter den Panelist_innen (30 Min) und anschließend gemeinsam mit den anwesenden Kolleg_innen (30 Min.). Die Publikation der Ergebnisse in einem Blogbeitrag ist geplant.

                

                    
Statements

                    
                    

                        
Einleitung und Moderation

                        
Daniel Burckhardt (Deutsches Historisches Institut Washington) und Julian Schulz (Geschäftsstelle der Max Weber Stiftung, Bonn)

                    

                    

                        
Wenn Technik und Bewusstsein voranschreiten: die drei Leben der Korrespondenz der Constance de Salm (1767-1845) 

                        
Mareike König (Deutsches Historisches Institut Paris)

                        
Bisweilen verhindern zögerliche Projektpartner die freie Bereitstellung von Digitalisaten, begrenzen die Mittel das Maximalziel bei Digitalisierung und Erschließung von Quellen und eröffnet die technische Entwicklung neue Möglichkeiten. So geschehen beim Projekt der Digitalisierung und Inventarisierung der 
                            
Korrespondenz von Constance de Salm
. Zu Projektbeginn war weder die besitzende Institution bereit, die Digitalisate ohne Anmeldung und ohne Wasserzeichen online zu stellen, noch gab es die Mittel für eine zweisprachige Erschließung geschweige denn für eine vollständige Transkription der Briefe. Der Impulsbeitrag zeichnet exemplarisch nach, wie dennoch in den oben genannten Einzelstufen – vom Digitalisat zum Text und vom Text zur Edition – trotz Pausen die Korrespondenz der Constance de Salm schrittweise ediert werden konnte: durch einen langen Atem beim Verhandeln mit Partnern, Vernetzung mit ähnlichen Projekten, Nachnutzung von Workflows und Verfolgen kleiner Etappenziele. 
                        

                    

                    

                        
Quellen ohne Ressourcen: Ansätze des minimal computing für die Erschließung arabischer Periodika

                        
Till Grallert (Humboldt-Universität zu Berlin, ehem. Orient-Institut Beirut) 

                        
„
                            
Open Arabic Periodical Editions
“ ist ein Projekt, das die Rahmenbedingungen für die digitale Erschließung von Quellen in und aus Gesellschaften des Globalen Südens mit den Ansätzen des 
                            
minimal computing
 in der Praxis adressiert (Gil und Ortega 2016, Risam 2019). Hierbei geht es um mehrere, miteinander verwobene Schichten der Unzugänglichkeit, die DH-Projekte des Globalen Südens konstant verhandeln müssen (Grallert 2022): Hegemonie des anglophonen Globalen Nordens über die Infrastrukturen der Wissensproduktion (Wissensorganisation, Forschungsfelder, Fördergelder, Arbeitssprachen, vgl. Fiormonte 2021); computationelle Methoden und Werkzeuge der digitalen Editorik, die für nicht-lateinische Schriften und Sprachen des Globalen Südens nicht erprobt oder verfügbar sind (z.B. OCR, NER, vgl. Auddy 2022); begrenzter Zugang zur technischen Grundversorgung mit Strom, Internet und Hardware. 
                        

                    

                    

                        
The Wisdom of the Crowd: Quellen öffnen mit Citizen Scholars

                        
Jana Keck (Deutsches Historisches Institut Washington)

                        
„
                            
Migrant Connections
“ ist eine digitale Forschungsinfrastruktur (Omeka S) des DHI Washington, welche Zugang zu diversen Quellenmaterialien wie Briefen, Tagebüchern, und Zeitungsartikeln zur deutschen Migration in die USA bietet. Gemeinsam mit Citizen Scholars werden kontinuierlich Quellen gesammelt, digitalisiert, transkribiert, übersetzt, Metadaten erstellt und kontextualisiert. Durch die Expertise der Citizens aus Deutschland und den USA konnten bisher hunderte von Quellen erschlossen werden, die Einblicke vor allem in die Alltagsgeschichte von bisher wenig beachteten, „gewöhnlichen“ historischen Akteur_innen geben. Der Beitrag möchte die Potentiale einer schrittweisen Edition bei offener Kollaboration mit Citizen Scholars im Forschungsprozess beleuchten, dabei die Frage der Qualitätssicherung thematisieren und gleichzeitig hervorheben, wie diese Offenheit uns als Wissenschaftler_innen dazu bringt, Vertrauen zu schenken und Kontrolle abzugeben. 
                        

                    

                    

                        
„Königsweg“ Digitale Edition. Offen, aber für wie lange? 

                        
Jörg Hörnschemeyer (Deutsches Historisches Institut Rom)

                        
Der Impulsbeitrag geht der Frage nach, wie offen und nachhaltig die „letzte Stufe“ des eingangs skizzierten Modells der Bereitstellung digitaler Quellen, die Digitale Edition, wirklich ist. „
                            
Ferdinand Gregorovius
 – Poesie und Wissenschaft. Gesammelte deutsche und italienische Briefe" ist eine nach „allen Regeln der Kunst“ erarbeitete Digitale Edition, die über XML/TEI-annotierte, text- und sachkritisch erschlossene Brieftranskriptionen verfügt und mit umfangreichen Normdatenverknüpfungen, einem projektspezifisch entwickelten User Interface und einer REST-API versehen wurde. Trotz (oder auch gerade wegen?) dieser komplexen Architektur ist absehbar, dass die Edition in 10-20 Jahren vor der zentralen Frage stehen wird, ob und wie ein offener Zugang über die verschiedenen Zugänge auch weiterhin gewährleistet werden kann und wie neue Forschungsergebnisse in eine als abgeschlossen angesehene Edition einfließen können. Könnten z.B. Semantic Web-Technologien (Wettlaufer 2018), Social Editing (Crompton et al. 2016) und kollaborative, (Fach)community-getriebene Ansätze Antworten auf diese beiden Fragebereiche liefern? Und welche Ressourcen benötigt dann eine Langzeitstrategie für Editionen auf institutioneller Ebene?
                        

                    

                

            

        

            

                
Introduction: Narratives of Disinformation

                
Audiovisual media, such as film, TV, webseries, YouTube-videos and so on, have long become one, if not the, dominant cultural communication form of our times (Mirzoeff, 1999; Mitchell, 2001). Their role in shaping sociocultural configurations of all kinds appears incontestable. However, description and analysis of such media presents immense challenges which have so far resisted scalable solutions. Although it is becoming increasingly possible to conduct larger-scale stylistic and formal feature analysis (shot lengths, brightness, color profiles, dynamicity, etc.: cf., e.g., Heftberger, 2018), productive engagement with such media as powerful sociocultural artifacts demands in addition hermeneutic and functional interpretations as pursued in many branches of the humanities. Analyses of these kinds have not so far been possible within the digital humanities.

                
In an ongoing BMBF project on FakeNarratives
, we combine expertise, experience and tools crucial for making general advances towards these goals, practically directed at a soundly delimited and, at the same time, socially significant class of audiovisual artifacts: TV news. We address the research hypothesis that news of this kind increasingly employs strategies of 
                    
audiovisual narrative
 that may undercut, undermine or construct ideological positions and evaluations of news content 
                    
independently
 of what may be simply stated, for example, in accompanying spoken text. The increasing sophistication and technical possibilities available to news channels makes it possible to draw on techniques for storytelling well established in film, but this can by no means always be guaranteed to work as intended. Indeed, filmic storytelling techniques may also serve to more effectively dis-inform.
                

                
Although consideration of news reporting as narrative already has a considerably history and is nowadays hardly controversial (Sperry, 1981; Bell, 1999; Liebes, 1994; Langer, 1998; Hickethier, 2000; Dunn, 2005; Machill et al., 2006), methods for engaging specifically with their 
                    
audiovisual
 non-verbal properties remain limited. Analyses addressing the audiovisual are overwhelmingly based on small datasets and, as is usual within the humanities, are performed interpretatively. Consequently, just how widespread ‘narrativization’ strategies are, and to what effect and aims they are being applied, urgently demands in depth and larger-scale research from within the digital humanities.
                

            

            

                
Case study: Tagesschau vs. Bild TV

                
The current paper presents some early results of the FakeNarratives project illustrating how narrative audiovisual strategies might begin to be addressed at variable scales. Several distinct kinds of narrative strategies have been discussed in the journalism and newsreporting literature, but for the purposes of our first case study we focus on the production of ‘individual-centered narrative’ by means of editing devices such as showing layperson’s talk, individualized events and actions, close- ups of individual faces, and emotion shown either visually or exhibited in accompanying language. These features appear to be widely used across alternative and public news outlets (Vettehen et al., 2008). Our research hypothesis is that both kinds of news outlets use narrative strategies to some degree, but that there may still be significant differences in why and how often they are actually used. We consider it highly implausible that the use of narrative strategies can be reduced to any binary “good vs. bad” or “informing vs. dis-informing” characterisation and so more finely discriminating accounts are in any case necessary. The basic challenge we address here is then to set out how hypotheses concerning narrative construction may be made amenable to larger-scale data analysis. 

                
Here we select some of the most commonly discussed narrative features in news that contribute to highlighting individual stories, a technique that can effectively increase the viewers' memories of news contents and their perception of news severity (Aust &amp; Zillmann, 1996; Zillmann &amp; Brosius, 2012)
                    
. 
We examine whether and how these features are presented in both public and online alternative news channels. In particular, we consider how these features are employed to individualise, personalise or emotionalise people shown in news videos. In other words, we seek to operationalise just what constitutes the narrative strategy of 
                    
individualisation
 so that we can explore how the strategy is used across our selected news channels.
                

                

                    
Data

                    
Our data include a preliminary selection of 166 news videos, 70 videos from Tagesschau, which we use as an example for more traditional, serious media, and 96 comparable videos from Bild TV, which we use as an example for a more recent, alternative news channel. These videos were all produced between 1 January 2022 and 15 March 2022. The length of entire news programs in both channels is around 15-25 minutes. For current purposes, we analyze the first news report of each news video, namely, the top news story of the entire program, which usually lasts from 3 to 5 minutes. The most reported top news themes from January to the middle of March in 2022 were Covid-19 and the outbreak of the Ukraine war.

                

                

                    
Method

                    
For systematically analyzing the news videos, we employed the multimodal discourse approach of film cohesion (Tseng, 2013), which operates by picking out how the deployment of visual image, sound, verbal language, written language, camera movement, framing, color, and many more pattern together so as to introduce and coherently track people, places, objects and events within an unfolding event sequence. In other work, we have found that the cohesion structures resulting from such analyses appear to play significant roles for scaffolding an audience’s perception and understanding of narratives (cf., e.g., Tseng et al., 2021).

                    
Five distinct types of people commonly depicted in news reported were singled out for attention: (1) anchor persons, (2) reporters, (3) news commentators, (4) protagonists in the reported news events and (5) laypersons. The first three traditionally represent the authoritative voice in news and so we track and analyze whether these voices are personalized by including their own attitudes and emotions. The fourth type, protagonists, includes the main involved characters of news events, whereas the fifth type, laypersons, refers to people represented as generic exemplars effected by the news event.



                        

                            
Layperson in [talking head] mode
: whether a news video employs layperson’s interviews.
                        

                        

                            
Specific protagonist
: whether a particular individual in the news report is cohesively identified as a specific protagonist.
                        

                        

                            
Putin
: whether the Russian president Putin is depicted as protagonist co-patterned with particular personalized quality and emotional features.
                        

                        

                            
Close-up of laypersons and protagonists
: whether laypersons or protagonists are additionally shown with close shots.
                        

                        

                            
Self references of anchors, reporters or commentators
: whether the ‘authoritative’ voices make recurring uses of self-references, such as the uses of “I”, “my”, “me” suggesting subjective opinions or personal attitudes of the journalists.
                        

                    

                    
In addition to these five multimodal categories derived directly from the cohesive chains concerning individuals, we also analyze how these individuals are combined with the following four narratively relevant features: 



                        

                            
Conflict or violent events
: whether any conflict-related event is being mentioned, seen or written in news reports. 
                        

                        

                            
Negative evaluation
: whether particular, recurring quality features with negative evaluation can be found in news reports, for instance, the negative descriptions about the war or politicians. 
                        

                        

                            
Emotions
: whether there are multimodal realization of emotions, for instance, emotional terms mentioned in spoken or written texts and emotional reactions shown in visual images or sounds. 
                        

                        

                            
Wallpapering
: whether, in addition to the individuals and their evaluative features derived from the analysis of cohesive chains, background images that are not strictly related to the contents of the news report running in the foreground are shown.
                        

                    

                    
The process of deriving the applicability of these categories for a video segment on the basis of the analysis of cohesive chains is depicted graphically in Figure 1.

                    

                        

                        
Illustrative example of cohesive chains (right) developed on the basis of the unfolding video segment (Bild TV news, 25.02.2022, left). The chains show how the identities and elements tracked may occur in any modality and that their combination provides direct support for the narratively relevant features being targeted.

                    

                    
Initially, we coded and analyzed these nine categories in 166 video segments manually. Each segment was coded as ‘1’ when the category applies and ‘0’ when it does not apply. For simple coding of single news stories, the analyses could be entered into spreadsheets; for more complex coding, either of further narrative strategies or including more of the videos beyond the first news story, more structured annotations are essential, typically performed currently using tools such as ELAN (ELAN, 2021).

                

                

                    
Results and discussion

                    
The occurrences of the nine categories in our two selected TV channels are displayed in Figure 2. This shows the complex similarities and differences of the two channels when employing the distinct fine-grained strategies contributing to individualization. From the differing distributions among the fine-grained strategies, it becomes clear that, although both channels might be said to be employing individualization as a technique for raising the interest-value of their reports, they do this in differing ways. For example, the occurrences of closeup faces and the talk of laypersons show no significant difference across the two channel (Fisher’s exact test, p=0.101 and p=0.324 respectively); thus both channels can be seen to use the individualizing feature of closeups and laypeople reports. However, the comparison also shows that Tagesschau uses significantly more 
                        
protagonists 
to report news events than does Bild TV (Fisher’s exact test, p=0.001). This means that Tagesschau actually individualizes news stories more than Bild TV does, but in a very specific way. Nevertheless, the result in the category of 
                        
Putin 
indicates that Bild TV does specifically individualize Putin significantly more than Tagesschau. Indeed, in our data, Bild TV largely labels the Ukraine war or the violent conflict as Putin’s war, while Tagesschau mostly refers the war to the ‘invasion of Russia’ or Russian soldiers. Figure 2 also shows that the categories of conflict events, emotional reaction, negative evaluation and wallpapering all pattern similarly with significantly more use in Bild TV than in Tagesschau. Moreover, the substantial use of self references of reporters in Bild TV indicates the common insertion of personalized attitudes and opinions into their news reports.
                    

                    
In summary, Tagesschau minimizes Putin’s personification, negative evaluation, decorative editing and reporters’ self-references, presumably in order to balance its heavy use of protagonist- centered narrative model with objective representation and visualization, while the Bild TV news videos employ evaluative and decorative features to dramatize the news events.

                    

                        

                        
The percentage occurrences of the nine categories in Tagesschau and Bild TV and corresponding p-values indicating the significance of difference (Fisher’s exact test).

                    

                    
Apart from the narrative strategy of 
                        
individualisation
, we are currently analysing and formalising other crucial news narrative strategies such as 
                        
dramatisation
 and 
                        
 fragmentation
 (Bennett, 1988). Nevertheless, for enlarging the corpus of news videos and the scale of annotation categories, we require more effective annotation tools beyond the time-intensive manual annotation methods such as ELAN. In this pursuit, automatic annotation is one central goal of the FakeNarratives project.
                    

                

            

            

                
What’s ahead: Automatic annotation and exploratory visualization tool

                
The results of the previous section are encouraging concerning the use of more abstract audiovisual analyses as a means for revealing differences and similarities between audiovisual materials in ways that naturally relate to issues of narrativization. The characterizations of our selected news media clearly indicate where differences in broader strategies are occurring. It is also nevertheless clear that for more reliable and extensive results, expanding both the kinds of audiovisual materials analyses and the kinds of strategies at issue, it will be essential to move beyond time-intensive manual annotation. Augmenting this work further by the development and application of automatic and semi-automatic annotation is consequently a further central goal of the FakeNarratives project, for which we adopt a two-pronged approach combining foci on visualization and automated analysis. 

                
We now combine a range of computational techniques for which there are already good solutions available, such as shot detection, face detection, etc., into a flexible tool named 
                    
Zoetrope
, supporting the progressive annotation of larger data sets by means of a more interactive interface (see Figure 3). 
                    
Zoetrope
 enhances the existing landscape of tools for audiovisual annotation and analysis (for an overview see Pustu-Iren et al., 2020), as it is not only capable of visualizing automatic annotations, but also provides some basic functionalities for querying the video, for instance for specific key words or named entities. By these means, annotation becomes an activity that can leverage both automatic results of audiovisual processing and more abstract characterizations of data in terms such as cohesive structures as used above. 
                

                
The current 
                    
Zoetrope 1.0
 prototype already allows researchers to search for keywords (
                    
segment 1
). Any keyword can be used as a query, which will then be searched in the spoken (Mozilla DeepSpeech
 with a German model by Agarwal and Zesch
) and written (scene detection framework 
                    
easyOCR
) language of the video. We also embed the query using word embeddings
 so that we can find words semantically related to the query as well. Via a slider, the similarity threshold can be set looser or more exact (=100%). The results of such a query are visualized in 
                    
segment 2
, which basically uses a timeline metaphor. Query results found in spoken language are rendered in green, results in written language are rendered in red. Note that for the query “Krieg” we also find semantically related concepts such as “Truppen”, “Invasion” or “Militär”, due to the semantic similarity threshold, which was set here rather low, at 50%. In segment 3, we see some exemplary automatically determined features, including face detection and audio analysis by means of a spectrogram. Finally, segment 4 shows an interactive video player, which can be navigated by means of the timeline or by clicking on specific results, such as a keyword or a face. Visible features, such as written keywords or faces, can also be rendered with their bounding box withing the video.
                

                

                    

                    

                        
Zoetrope 1.0
 prototype for visualizing and exploring news videos.
                    

                

                
The tools developed within the project will continue to pursue advanced visualization and more extensive automatic analysis. The latter goal is supported by our project partners in Hanover and their continuing development of the web-based audiovisual analytics platform TIBAVA (TIB AV- Analytics).

            

            

                
Conclusions and Outlook 

                
We have shown in this paper some of the benefits that may accrue when the analysis of audiovisual media can build on more abstract discourse patterns carried out at scale. We have argued that to take this further, however, a progressive increase in computational support is necessary. To this end, we have set out some first steps taken towards bridging some of the gaps between current technical feature processing possibilities and discourse-related characterizations of data. This is being enabled first and foremost by an interactive tool which already seamlessly incorporates a variety of state of the art processing techniques relevant for more abstract annotation practices. We believe such a tool will greatly speed up the manual annotation that our current corpus analysis relies on and will allow us to discover and annotate many more narrative strategies in the future. Once the tool is beyond its prototype stage, we also plan to release it for the DH community, which might use it to search other video material than news videos, for instance YouTube Let’s Play videos for game studies, or any movie or TV series for film studies scenarios. A live demo of the prototype will be included in the presentation at DHd 2023.

            

        

            

                
Einleitung: Zur Rolle von Tools in den Digital Humanities

                
Software-Tools spielen in den Digital Humanities eine zentrale Rolle, ja, sind geradezu genre-prägend für diese Disziplin. Diese wichtige, praktische Rolle ist u.a. belegt durch die Existenz diverser Tutorials und vor allem auch von Tool-Katalogen (vgl. Tab. 1) sowie Versuchen der einheitlichen Kategorisierung von Tools, etwa der TaDiRAH-Taxonomie (Borek et al., 2016).

                

                    

                        
Name

                        
Typ

                        
URL

                    

                    

                        

                            
Programming Historian

                        

                        
Tutorials

                        
https://programminghistorian.org/

                    

                    

                        
forTEXT

                        
Tutorials

                        
https://fortext.net/

                    

                    

                        
CLARIAH-DE Tutorial Finder

                        
Tutorials

                        
https://teaching.clariah.de/search/

                    

                    

                        
TAPoR

                        
Katalog

                        
https://tapor.ca/home

                    

                    

                        
Digital Methods Initiative

                        
Katalog

                        
https://wiki.digitalmethods.net/Dmi/ToolDatabase

                    

                    

                        

                            
Alan Liu’s DH Toychest

                        

                        
Katalog 

                        
http://dhresourcesforprojectbuilding.pbworks.com

                    

                    

                        
SSH Open Marketplace

                        
Katalog

                        
https://marketplace.sshopencloud.eu/

                    

                    

                        
NFDI4Culture 

                        
Katalog

                        
https://riojournal.com/article/57036/instance/5947376/; Tabelle 9, Appendix

                    

                    
Tabelle 1: Übersicht zu verschiedenen Tutorials und Katalogen für Tools in den Digital Humanities.

                


                
Neben diesen stärker praxeologischen Aspekten finden sich auch diverse Diskurse um die Rolle und Implikationen von Tools in den Digital Humanities. Ein Thema ist dabei etwa „Tool Criticism“, also der kritische Umgang mit Tools, insbesondere deren spezifischer Funktionsweise und den Effekten, die diese auf die letztlichen Ergebnisse haben (Koolen et al., 2019; Traub &amp; van Ossenbruggen, 2015; van Es et al., 2018). Ein weiteres Themenfeld findet sich im Bereich der Mensch-Maschine-Interaktion, also den speziellen Anforderungen, die geisteswissenschaftliche Forschende an die Funktionalität und Usability von Software-Tools mitbringen (Burghardt &amp; Wolff, 2014; Wolff, 2015).

                
Besonders stark ausgeprägt ist die Diskurslinie, die sich mit der Frage um die epistemologischen Effekte – also die unmittelbare Auswirkung von digitalen Tools auf den geisteswissenschaftlichen Erkenntnisprozess – beschäftigen (Burghardt et al., 2022; Dalbello, 2011; Kaden, 2016; Ramsay &amp; Rockwell, 2012). 

                
Dabei erstrecken sich die genannten Diskussionen vornehmlich auf Einzelbeispiele, größere empirische Untersuchungen gibt es bislang nur wenige. Dazu zählt u.a. eine Studie aus dem Umfeld des Research Software Engineering, bei der die Art und Häufigkeit von Softwarezitationen in DHd-Abstracts analysiert wurde (Henny-Krahmer &amp; Jettka, 2022). Weiterhin sind hier die zahlreichen Experimente von Frank Fischer und Kollegen, die u.a. DH-Abstracts und Tutorials des Programming Historian auf Tool-Vorkommen hin untersucht haben (Barbot et al., 2019; Fischer &amp; Moranville, 2020b, 2020a; Zarei, Alireza et al., 2022) sowie auch erste eigene Experimente (Burghardt et al., 2022) zu nennen. Ein erster Austausch zwischen den unterschiedlichen Akteur*innen im deutschsprachigen Raum fand weiterhin im Rahmen einer gemeinsamen Veranstaltung mit dem Titel „Die Werkbänke der Digital Humanities: Zur Rolle von Tools und Software für die Forschungsarbeit“ bei der vDHd (2021) statt.

                
Als grundlegende Methoden zur Erkennung von Tools finden sich in den genannten Studien vor allem zwei Ansätze: (1) lexikonbasierte Ansätze, bei denen mithilfe bestehender Tool-Listen, wie sie bspw. in den in Tab. 1 genannten Ressourcen verfügbar sind, ein einfacher look-up in einem Zielkorpus von DH-Publikationen erfolgt. Dieser Ansatz lässt sich sehr schnell umsetzen, leidet aber unter den üblichen Einschränkungen lexikonbasierter Verfahren, bspw. der Unvollständigkeit von statischen Wortlisten und gleichzeitig die Ambiguität einzelner Einträge (R, Python, Gate, etc.). (2) Der zweite Ansatz versucht diese Einschränkungen zu überwinden, indem auf verschiedentliche Verfahren des maschinellen Lernens mit dem Ziel eines Klassifikationstasks gesetzt wird. Hier gibt es einerseits vortrainierte Modelle aus dem Bereich der Software Entity Recognition (Patrice &amp; Romary, 2015; Schindler et al., 2022), die aber zumeist aus dem naturwissenschaftlichen Bereich kommen und deshalb nur mäßig für den Einsatz in Digital Humanities-Publikationen geeignet sind (vgl. Henny-Krahmer &amp; Jettka, 2022). In einer aktuellen Publikation experimentieren Zarei et al. (2022) mit einem auf Prodigy und dem spaCy-Framework basierenden NER-Ansatz, welcher für ein sehr klar umrissenes Anwendungsszenario mit vorhergehendem Training gute Ergebnisse bringt. Für einen Ansatz, der in der Lage ist auch Tools zu erkennen, die nicht vortrainiert wurden, schlagen die Autoren weiterführende Ansätze mit aktuellen Transformer-Modellen wie bspw. BERT (Devlin et al., 2019) vor. Erste Experimente mit BERT für eine binäre Klassifikation von Sätzen mit / ohne Tools wurden von uns bereits im Rahmen einer Vorstudie (Burghardt et al., 2022) erfolgreich durchgeführt und zeigten sich nach einer ersten qualitativen Evaluation als sehr vielversprechend. Das vorliegende Paper knüpft hier nahtlos an und präsentiert Erkenntnisse aus aktuellen Experimenten mit dem RoBERTa-Modell (Liu et al., 2019), einer optimierten Variante des bekannten BERT-Modells. Der von uns verfolgte Ansatz ist neben einer grundlegenden Identifikation von Tools über deren Embedding-Vektoren auch in der Lage die Tools größeren Kategorien, wie bspw. 
                    
Textanalysetool
 oder 
                    
Visualisierungstool
, zuzuordnen. Wir glauben, dass wir mit einem solchen Ansatz einen wichtigen Beitrag zu den bestehenden Tool-Diskursen in den DH leisten können und hoffen damit weitere empirische “Tool Studies” zu befördern.
                

            

            

                
Tool-Identifizierung und -Klassifikation mit RoBERTa Tool-Embeddings

                
Der Task der Tool-Identifizierung und -Klassifikation lässt sich methodisch als Problem der Text-Klassifikation einordnen. Beim gewählten Ansatz handelt es sich um ein Verfahren des überwachten Maschinellen Lernens, welches konkret aus einer binären und einer mehrklassigen Klassifikationsaufgabe besteht. Die grundlegende Idee beim nachfolgenden Vorgehen ist, dass die Erwähnung eines Tools in einem Paper in einem spezifischen sprachlichen Kontext steht, der sich in Form eines Sequenz-Embeddings mithilfe eines transformer-basierten Sprachmodells abbilden und klassifizieren lässt.

                

                    
Korpus, Trainings- und Testdaten 

                    
Am Anfang steht die Erstellung spezieller, gelabelter Trainings- und Testdatensätze. Der Ausgangsdatensatz, der ebenfalls als Untersuchungsgegenstand für die folgende Analysen dient, besteht aus 3.737 englisch-sprachigen Zeitschriftenpublikationen aus dem Bereich der Digital Humanities, die zwischen 1966 und 2020 veröffentlicht wurden (Luhmann &amp; Burghardt, 2021). Eine manuelle Extraktion von Sequenzen in Papers, die die Nennung eines Tools beinhalten, ist in Anbetracht der vorliegenden Datenmengen nicht durchführbar. Stattdessen wurden automatisch bekannte Tools im Korpus gesucht, um entsprechende Sequenzen ausfindig machen zu können. Um qualitativ hochwertige Trainingsdaten zu erhalten, haben wir zunächst nach besonders populären Tools gesucht. Dazu haben wir insgesamt acht Listen und Tutorials mit Toolnennungen (vgl. Tab. 1) durchsucht und nur diejenigen Tools ausgewählt, die in mindestens zwei unterschiedlichen Listen genannt wurden. Aus diesen Tools haben wir dann zur weiteren Qualitätssteigerung diejenigen entfernt, die ein hohes Maß an Ambiguität aufwiesen. Dieses reduzierte Lexikon, mit insgesamt 246 Tools, dient als Ausgangspunkt für die Erstellung der Trainingsdaten. Im nächsten Schritt wurden sodann alle Papers des Korpus tokenisiert, mit dem Tool-Lexikon durchsucht und bei einem Treffer als Textausschnitt mit 15 Tokens vor und 15 Tokens nach dem Treffer als Sequenz-Label-Paar in den Trainingsdatensatz übernommen. Anschließend wurde die Menge der gefundenen Sequenzen durch zufällig gewählte und gleichmäßig über alle Papers verteilte, weitere Sequenzen ergänzt, die als Negativbeispiele für Toolnennung dienen.

                    
Als Label wird hier für den binären Trainingsdatensatz „Tool“ und „Kein Tool“ verwendet und für den mehrklassigen Trainingsdatensatz die Zugehörigkeit des Tools zur bestehenden Taxonomie von Alan Lius “DH Toychest”, da diese fast alle Tools in unseren Trainingsdaten bereits enthält und aus unserer Sicht auch gut nachvollziehbar ist. Langfristiges Ziel ist hier die Übernahme einer standardisierten Taxonomie wie TaDiRAH. Anschließend wurden die Datensätze für jeden der beiden Klassifikationstasks nach dem Prinzip einer 
                        
5-fold cross-validation
 in fünf Trainings- und Testsätze geteilt. Final besteht der Datensatz aus 3.780 Sequenz-Label-Paaren in jedem der fünf 
                        
folds
. Für jede Runde der Kreuzvalidierung wurden vier 
                        
folds
 für das Training und der fünfte 
                        
fold
 als Testdatensatz für die Validierung benutzt.
                    

                

                

                    
Modell-Evaluation

                    
Als transformer-basiertes Sprachmodell wurde eine RoBERTa Implementierung von Hugging Face gewählt.
 Aus Effizienzgründen wurde konkret „distilroberta-base“ als „case-sensitive“- und „knowledge distilled“-Variante verwendet. Die Ergebnisse dieser Modell-Evaluation sind sehr vielversprechend. So wird für den Klassifikationstask der binären Tool-Detektion für beide Klassen bei den 5 
                        
folds
 im Schnitt ein F1-Score von 0,99 erreicht. Für den Task der Tool-Kategorisierung wurden ebenfalls gute F1-Scores im Bereich 0,94 - 0,99 erzielt, die sich von Kategorie zu Kategorie geringfügig unterscheiden (vgl. Tab. 2). 
                    

                    

                        

                            
Class

                            
F1-Score*

                        

                        

                            
Authoring / Annotation / Editing / Publishing Platforms &amp; Tools (including collaborative platforms)

                            
0,97

                        

                        

                            
Exhibition/Collection/Edition Platforms &amp; Too

                            
0,96

                        

                        

                            

                                
Platforms and Communication

                            

                            
0,99

                        

                        

                            

                                
Programming Languages/Packages

                            

                            
0,98

                        

                        

                            
Text Analysis Tools

                            
0,97

                        

                        

                            

                                
Visualization Tools

                            

                            
0,94

                        

                        

                            

                                
No Tool

                            

                            
0,99

                        

                        
Tabelle 2: Ergebnisse des mehrklassigen Klassifikationsansatzes. Kategorien mit zu geringen Vorkommen für aussagekräftigen Klassifikationswert wurden entfernt (Testsamples > 100). *Mittelwert über alle Runden der Kreuzvalidierung

                    

                

            

            

                
Analysen

                
Die sehr guten Ergebnisse aus der Evaluation sollen im Folgenden im Rahmen einer beispielhaften Analyse weiter diskutiert werden. Dazu wurde der Ursprungsdatensatz mit den 3,737 DH-Papers mit einem „Sliding Window“-Ansatz und einer Fenstergröße von 31 Tokens (das entspricht der Größe unserer Trainingssequenzen, siehe oben) und einer Überlappung von fünf Tokens in insgesamt 753.210 Sequenzen geteilt. Diese wurden anschließend klassifiziert und in eine neue Datenbank geschrieben. 

                

                    
Ergebnisse der binären Toolidentifikation

                    
Für den binären Klassifizierungstask der Tool-Detektion werden 14.561 Sequenzen mit potenziellen Toolnennungen vorhergesagt. Obwohl die Trainingssätze auf Basis von nur 246 unterschiedlichen Tools erstellt wurden, ist der Anteil an gefundenen Sequenzen, die genau eines dieser Tools enthalten mit 45,5% überschaubar – mehr als die Hälfte der Sequenzen enthalten andere Tools.

                    
An dieser Stelle soll weiterhin ein Vergleich zu TAPoR, dem mit über 1.600 Einträgen größten Tool-Katalog, angestellt werden. 54,7% der von uns identifizierten Tool-Sequenzen enthalten Tools, die auch in TAPoR gelistet sind und die auch schon von vorhergehenden Lexikonansätzen gefunden wurden (vgl. Barbot et al., 2019; Fischer &amp; Moranville, 2020b, 2020a; Burghardt et al., 2022). In den restlichen 45,3% der Sequenzen finden sich allerdings neue Tools, also solche, die nicht schon in der sehr umfangreichen TAPoR-Liste dokumentiert sind. Mithilfe eines POS-Taggers wurden aus diesen Sequenzen Eigennamen gefiltert, um einen Überblick über die 
                        
neuen
 Tools zu erhalten.
                    

                    
Es sind dies einerseits Programmiersprachen (
                        
Java
, 
                        
SNOBOL4
, 
                        
Swift
, 
                        
Pascal
, 
                        
NetLogo
), Datenbanken (
                        
SQL
) und Markupsprachen (
                        
TEI
, 
                        
SGML
), aber auch diverse Belege aus dem Online-Bereich, etwa Social Media (
                        
YouTube
, 
                        
Wikipedia
, 
                        
Facebook
, 
                        
Instagram
, …) und Web Browser (
                        
Netscape
, 
                        
Mozilla
) sowie auch diverse Beispiele aus dem Bereich des Desktop Publishing (
                        
WordPerfect
, 
                        
Microsoft Word
, 
                        
WordStar
, 
                        
PageMaker
). Daneben finden sich zahlreiche weitere, teils antiquierte Tools, die nicht ohne Weiteres zu größeren Gruppen zusammengefasst werden können, etwa: 
                        
TreeForm
, 
                        
Storyspace
, 
                        
MtScript
, 
                        
Seshat
, 
                        
FarsiTag
, 
                        
PlotVis
, 
                        
LexStat
, 
                        
Galgo
, 
                        
Neurolingo
, 
                        
AustLit
, 
                        
XyWrite
, 
                        
StoryTrek
, u.v.m.
                    

                    
Es zeigt sich also, dass unser Ansatz das Tool-Inventar bestehender Kataloge wie TAPoR deutlich erweitern kann, und über die Embeddings auch weitere, neue Tools gefunden werden.

                    
Betrachtet man den jüngsten Aufruf (April 2022)
 von TAPoR, bei dem Mitglieder der DH-Community gebeten werden, weitere Tools in TAPoR zu ergänzen, dann könnte unser Ansatz hier im großen Stil automatisiert weitere Vorschläge unterbreiten, indem systematisch verschiedene DH-Publikationen klassifiziert werden.
                    

                

                

                    
Ergebnisse der multi-class Toolkategorisierung

                    
In einer zweiten Analyse wurden die Ergebnisse der automatischen Toolkategorisierung diachron ausgewertet (vgl. Abb. 1). Die Charts sind zur Paperanzahl pro Jahr im Datensatz normalisiert und zeigen einen zeitlichen Verlauf des Vorkommens der jeweiligen Kategorie.

                    
Dabei fällt bspw. auf, dass (a) Tools für „Authoring / Annotation / Editing / Publishing“ ebenso wie (b) Tools für “Exhibition / Collection / Edition Platforms” und (c) Kommunikationsplattformen alle erst Anfang der 2010er an Fahrt aufgenommen haben. Gleichzeitig zeigt sich im Falle der Programmiersprachen, dass diese in den Anfängen der DH, also den 1965er - 1985er Jahren, besonders populär waren, danach nimmt deren Nennung in Publikationen allerdings deutlich ab.

                    
Tools für die (e) Textanalyse hatten ihren ersten Peak in der Mitte der 1970er, für etwa 10 Jahre, und dann nochmals besonders extrem in den frühen 2000ern und später wieder von 2010-2020. Für eine weitergehende Analyse dieser Konjunkturen ist ein close reading der entsprechenden Publikationen vonnöten, um so zu untersuchen, ob bspw. besonders populäre Einzeltools wie 
                        
Voyant
 (Release 2003) für diese Spitzen verantwortlich sind. Spannend – und ggf. als Ursache für den Rückgang der Programmiersprachen zu interpretieren – ist weiterhin die steile Karriere von (f) Visualisierungstools, ab Anfang 2010.
                    

                    

                        

                        
Abbildung 1. Diachrone Ergebnisse der multi-class Toolkategorisierung

                    

                

            

            

                
Ausblick: Tool Studies 2.0

                
In diesem Beitrag haben wir dargestellt, dass es vielfältige Diskurse zur Rolle und Funktion von Tools in den DH gibt. Wir sehen großes Potenzial bei systematischen, empirischen Analysen von Tools, um die bestehenden Diskussionen zu ergänzen. Die bisherige, erste Welle der Tool Studies setzt primär auf lexikonbasierte Verfahren (vgl. Barbot et al., 2019; Burghardt et al., 2022; Fischer &amp; Moranville, 2020b, 2020a). Vereinzelt wurden auch einfache Ansätze aus dem Bereich des maschinellen Lernens (vgl. Burghardt et al., 2022; Henny-Krahmer &amp; Jettka, 2022; Zarei, Alireza et al., 2022) erprobt. Mit ersten Experimenten, die das große Potenzial von Transformer-Architekturen in Kombination mit Kontext-Embeddings aufzeigen, hoffen wir bestehende Tool Studies einen Schritt weiter zu bringen. Wir planen weitere Experimente, bei denen wir zum einen das Korpus erweitern wollen, aber auch weitere Modelloptimierungen vornehmen wollen. U.a. soll künftig ein Klassifikationsschema nach dem Vorbild von TaDiRAH trainiert werden. Neben einer diachronen Vermessung von Tooltrends in den Digital Humanities, soll in letzter Instanz mit einem ausreichend generalisierten Klassifikator ein großes interdisziplinäres Korpus nach dem Vorbild von Luhmann &amp; Burghardt (2021) bezüglich der dort vorkommenden Tools analysiert werden. Ziel wird es sein aufzuzeigen, welche Tools ggf. von anderen Disziplinen in die DH importiert wurden, welche Tools aus den DH erfolgreich exportiert wurden, und welche Tools ggf. DH-spezifisch sind, und in keiner anderen Disziplin vorkommen.

            

        

            

                
Einleitung

                

                    
Unser Beitrag zur Tagung 
                        
Open 

                        
Humanities

                        
, Open Culture
 präsentiert eine innovativen Vorschlag für die Datenmodellierung zukünftiger digitaler Werkeditionen. Das Modell ist hochgradig flexibel, indem es eine Diversität und komplexe Relationalität der Werkdaten zulässt, und zugleich stabil, wodurch es maximale Offenheit der Rezeption und Nachnutzbarkeit gewährleistet.
                    

                

                

                    
Entwickelt und erprobt wurde das Datenmodell im Zuge der Daten-Migration der Forschungsplattform 
                        
Handke
online (
                    

                    

                        

                            
https://handkeonline.onb.ac.at/

                        

                    

                    
), bei der eine vorerst “geschlossene” Modellierung von präbibliographischen Daten der verschiedenen, genetisch interpretierten Werkmaterialien aus dem Vorlass zusammen mit bibliographischen Daten der veröffentlichten Werke des Literaturnobelpreisträgers Peter Handke in eine “offene” umgebaut werden musste – wir nennen diese Modellierung eine “offene Werkgenese”. 

                

                

                    
In unserer Präsentation werden wir zuerst kurz in die Voraussetzungen der Plattform 
                        
Handke
online (Punkt 2) und in den aktuellen Forschungsstand der semantischen Modellierung von bibliographischen und präbibliographischen Daten (Punkt 3) einführen. Danach wird anhand von zwei Beispielen der generische Modellierungsansatz für „offene Werkgenesen“ präsentiert (Punkt 4).
                    

                

            

            

                
Forschungsplattform 
                    
Handke
online
                

                
Die Forschungsplattform 
                    
Handke
online wurde im Zuge eines FWF-Forschungsprojekts am Literaturarchiv der Österreichischen Nationalbibliothek Wien 2011-2015 entwickelt. Sie versteht sich als virtuelles Archiv, das sämtliche auf öffentliche und private Archive in Österreich, Deutschland und der Schweiz verstreute Vorlass-Materialien (wie Notizbücher, Bleistiftmanuskripte und Typoskripte unterschiedlicher Textfassungen sowie Druckfahnen, aber auch annotierte Bücher, Fotos, Landkarten oder sogar Wanderstöcke) zum veröffentlichten Werk Peter Handkes zusammenführt. Die Materialen werden tabellarisch und in Form von Texten beschrieben und in eine entstehungschronologische Beziehung gesetzt, die in Paratexten kommentiert wird. Die werkgenetische Interpretation und Beschreibung erweitern das virtuelle Archiv zu einer Edition. Ergänzt wurde diese durch eine umfangreiche Bibliographie zur Primär- und Sekundärliteratur des über 140 Bücher sämtlicher Genres umfassenden Werks Handkes. Inhaltliche und formale Vielfalt öffnen die Plattform für ein breites Publikum. Sie wird nicht nur für die universitäre Forschung intensiv genutzt, sondern bietet Schüler*innen Unterstützung für Hausarbeiten, wird von Theatern für die Erarbeitung von Stückinszenierungen oder Begleitheften herangezogen und, wie die Aufregungen um den Nobelpreis gezeigt haben, auch von Journalist*innen konsultiert. (Der faktischen Materialdokumentation der Plattform kam etwa innerhalb der kontroversen Debatte um Handkes “Serbien-Bücher” ein wichtige Rolle zu.)
                

                
Zentrales Anliegen des 
                    
Handke
online-Projekts war die Präsentation und Weitergabe des erarbeiteten biographischen, literaturwissenschaftlichen und archivalischen Wissens über Handke, die Entstehung seiner Werke, und damit zusammenhängend über die Beschaffenheit und Bedeutung der einzelnen Vorlass-Materialien. Die nachhaltige Modellierung dieser Forschungsdaten, welche erst eine langfristige Verfügbarkeit sowie Ausbau- und Anschlussfähigkeit des Wissens garantiert, wurde dabei vorderhand aus zeitökonomischen und budgetären Gründen vernachlässigt. Vor allem aber fehlte noch das geeignete konzeptuelle Modell das von Literatur-, Editions- oder Archiv- und Informationswissenschaft gemeinsam gedacht wird, und damit auch eine praktische Vorlage. Die Daten von 
                    
Handke
online wurden im Altsystem flach strukturiert und in einem proprietären Format vorgehalten. Die Folgen wurden bereits nach Projektende deutlich: die innovative Darstellung der Werkgenese war zwar Vorbild für andere Projekte, aber diese wurde weder in den Daten repräsentiert noch nachhaltig strukturiert. Deutlich zeigte sich das Problem in dem 2021, also nur sechs Jahre später begonnenen digitalen Editionsprojekt 
                    
Peter Handke Notizbücher
 der Österreichischen Nationalbibliothek Wien und des Deutschen Literaturarchivs Marbach (
                    

                        
https://edition.onb.ac.at/handke-notizbuecher

                    
): auf die wertvollen Daten über die Notizbücher konnte sogar intern nur mehr kompliziert über die dem Content-Management-System (Drupal) zugrunde liegende relationale Datenbank zugegriffen werden. Die damals getroffenen Zuordnungen der einzelnen Datenfelder konnten wegen des geschlossenen Datenformats teilweise nicht mehr vollständig rekonstruiert werden.
                

                
Aufgrund der Bedeutung von 
                    
Handke
online beschloss die Österreichische Nationalbibliothek die Daten-Migration in die erst später eingerichtete 
                    
Nachhaltige Infrastruktur für digitale Editionsprojekte an der Österreichischen Nationalbibliothek 
(ÖNB-DE) und nützt diese Gelegenheit zum vollständigen Re-Design der Daten und Datenstruktur. Ziel ist die Transformation sämtlicher für 
                    
Handke
online generierte Beschreibungsdaten zum Werk Peter Handkes nach TEI/XML und die Bereitstellung eines werkgenetischen Katalogs als 
                    
Linked Open Data
-Datensatz. Diese Umsetzung liefert einen wesentlichen Beitrag zu einer Forschungskultur, die Vernetzung und Dynamik von Forschungsprozessen ins Zentrum rückt. Folgende Herausforderungen galt es dabei zu bewältigen:
                



                    
Einen Modellierungsansatz zu finden, mit dem präbibliographische und bibliographische Daten gemeinsam repräsentiert und in Beziehung gesetzt werden können.

                    
Die geschlossene in eine offene Werkgenese umzuwandeln und den Datenkatalog entsprechend den FAIR-Prinzipien (Findable, Accessible, Interoperable, Reuseable) zur Verfügung zu stellen.

                

            

            

                
Modelle für präbibliographische und bibliographische Daten

                
Zur Erschließung und Verbreitung von bibliographischen Daten haben sich im wissenschaftlichen Bereich verschiedene bibliographische Datenbanken (
                    
Initiative for Open Citation
 (i4oc.org)) etabliert, wie z.B. 
                    
OpenCitation
 oder 
                    
GoogleScholar
, die sich allerdings auf die Erfassung bereits veröffentlichter Werke konzentrieren. Das gilt auch für eine Reihe weitverbreiteter Standardvokabulare für die grundlegenden Begriffe zur Dokumentation und Beschreibung bibliographischer Daten wie etwa 
                    
Dublin Core Element Set
 und 
                    
Metadata Terms
 (
                    

                        
https://www.dublincore.org/

                    
) und 
                    
Schema.org. 
Das 
                    
BIBFRAME
-Vokabular (
                    

                        
https://www.loc.gov/bibframe/docs/index.html

                    
) wiederum ist ein vor allem im wissenschaftlichen Bereich gängiges (Austausch-)Format und man kann MARC- und RDA-Daten im Linked-Data-Standard RDF abbilden – dieses Vokabular ist jedoch ebenfalls ungeeignet für die Repräsentation präbibliographischer Werkmaterialien. Für das Projekt 
                    
Handke
online sind diese gängigen Vokabulare somit zu limitiert, da hier unveröffentlichte Werke und unterschiedlichste Werkvorstufen (Werkmaterialien) weder erfasst noch in ihrer teilweise komplexen Relationalität zu den publizierten Werken abgebildet werden können.
                

                
Eine differenzierte bibliographische Basiskodierung, wie sie Martina Gödel und Sebastian Zimmer (2017) auf der Basis der 
                    
Functional Requirements for Bibliographic Records
 (FRBR) (Coyle 2016) für die Werke und vor allem den Zettelkasten von Niklas Luhmann gemacht haben (
                    

                        
https://niklas-luhmann-archiv.de/

                    
), ist die Voraussetzung für weitere Forschungen zur Werkgenese. In dem von uns gewählten Ansatz spielt gerade dieser Aspekt deshalb von Beginn an eine Schlüsselrolle. Auch unsere Modellierung bibliographischer Daten basiert wie zuvor schon bei Stefanie Gehrke et al. (2016), Frederike Neuber (2016), Gödel und Zimmer (2017) oder Andreas Lüschow (2020) auf den Grundkonzepten von FRBR, doch wir gehen für unsere Anforderungen noch einen Schritt weiter:
                

                
Wir greifen, um die für unsere Abbildung der Werkgenesen notwendige Flexibilität (Werkdatendiversität und -referenzierbarkeit) bei gleichzeitiger Stabilität zu erreichen, die für eine Anschluss- und Ausbaufähigkeit der Daten notwendig ist, auf das im Museumsbereich gebräuchliche Format FRBR
                    
OO
 zurück. FRBR
                    
OO
 ist eine Harmonisierung des FRBR-
                    
Entity-Relationship-Models
 und des 
                    
Conceptual Reference Models 
des 
                    
International Council of Museums
 CIDOC CRM (
                    

                        
https://www.cidoc-crm.org/

                    
). Letzteres wurde entwickelt, um die in der Museumsdokumentation relevanten Objekte und Konzepte (Ereignisse) zu beschreiben und institutionsübergreifend Daten austauschen zu können (Doerr, Crofts 2004, 15). FRBR
                    
OO
 führt diese ereigniszentrierte Sichtweise nun auf Werke aller Art ein. Die Anwendung von FRBR
                    
OO
 für die Modellierung von komplexen Werkgenesen ist, soweit wir die digitalen Editionen der letzten Jahre überblicken, neu; sie bringt viele Vorteile. FRBR
                    
OO
 ermöglicht eine exakte Darstellung der komplexen Genese von Handkes Werken: Erstens, weil das Modell differenzierte Werkbegriffe bereitstellt, die uns erlauben, nicht nur die zahlreichen literarischen Genres, die Handke bedient (Prosa, Theater, Hörspiel, Film, Briefe, Interviews, Essays, Gedichte, Journale), sondern auch seine bislang unveröffentlichten, aber Werken entsprechenden Notizbücher zu formalisieren. Zweitens, weil FRBR
                    
OO
 auf CIDOC-CRM basiert, ist es möglich auch individuelle, nichtbibliographische Materialien zu beschrieben. Drittens können mit FRBR
                    
OO
 die Vorstufen eines Buches (
                    
Manifestation Singleton) 
von einer veröffentlichten Ausgabe
                    
 (Manifestation Product Type
) in den Daten klar unterschieden werden. Viertens schließlich erlaubt uns FRBR
                    
OO
, über das Ereignis 
                    
Expression Creation
 die für eine Darstellung von Werkgenesen wichtige zeitliche und materielle Einordnung der einzelnen Fassungen.
                

            

            

                
Datenmodellierung von 
                    
Handke
online mit FRBRoo
                

                
Für die Modellierung von 
                    
Handke
online gilt es die vielfältigen Werkmaterialien aus dem Vorlass mit den publizierten Werken in Zusammenhang zu bringen. Peter Handkes Notizbüchern kommt dabei für die Genese seiner Werke ein besonderer Stellenwert zu: sie sind Materialsammlung oder Skizze für mehrere publizierte Werke; sie sind erste Textfassung für die gedruckte Notizauswahl in Journalen und darüber hinaus in ihrer Gesamtheit noch unveröffentlichtes Werk, das im Zuge des digitalen Editionsprojektes 
                    
Peter Handke Notizbücher
 erstmals publiziert wird. Wir gehen deshalb von zwei unterschiedlichen Werkbegriffen aus: einmal von dem von Handke autorisierten veröffentlichten Werk mit seinen verschiedenen Vorstufen und einmal von einem von Handke verfassten, aber nicht als Werk autorisierten vorerst noch unveröffentlichten, fortlaufenden Werk – den Notizbüchern, die für sich stehen und zugleich Vorstufe sein können. Dabei geht es in der Reflexion des Werkbegriffs bei der Datenmodellierung immer um das Ausloten von Grenzen, hier zwischen Flexibilität (Diversität und Relationalität oder Referenzialität) und Stabilität.
                

                
Im Folgenden wird das neue Modellierungskonzept von 
                    
Handke
online erstens für veröffentlichte Werke und zweitens für Notizbücher exemplarisch skizziert. Die kursiven Ausdrücke und in eckigen Klammern angegebene Informationen beziehen sich auf die Spezifikationen laut dem FRBR
                    
OO
–Modell (Bekiari et al. 2015). Die Begriffe 
                    
Manifestation 
und 
                    
Expression 
bleiben unübersetzt (Arbeitsstelle für Standardisierung 2006).
                

                

                    
Modellierung veröffentlichter Werke

                    
Handkes Werke werden grundsätzlich über die Entität 
                        
Work [1]
 erfasst. Die zugleich als Buch und Theaterstück oder Film publizierten Werke werden jedoch als 
                        
Complex Work [F15]
 beschrieben, dessen Werkkomponenten (
                        
Performance Work, Recording Work, Work
) über 
                        
has member [R10]
 zueinander in Beziehung gesetzt werden. Zur Rekonstruktion der Werkgenesen wird jede Textfassungsversion (z.B. Vorfassung, Druckfahnen, Druckfassung etc.) der Buchpublikation als Entität vom Typ 
                        
Expression [F2]
 angesehen. Die Erstausgabe wird als 
                        
Publication Expression [F24]
 mit 
                        
Manifestation Product Type [F3]
 modelliert. 
                    

                    
Die anderen Textfassungsversionen ordnen sich relativ zu dieser chronologisch ein. Um die zeitliche Abfolge dieser einzelnen 
                        
Expression-
Entitäten in FRBR
                        
OO
 beschreiben zu können, wird jeweils ein Ereignis 
                        
Expression Creation [F28]
 definiert, das Angaben zum Entstehungszeitraum über die Entität 
                        
Time-Span [E52]
 beinhaltet. Auf diese Art lässt sich die Werkhistorie als Abfolge von Ereignissen logisch rekonstruieren. Im Ereignis können auch Personen und Werke, die den Herstellungsprozess beeinflusst haben, repräsentieren werden. Es kann daher ein Unterschied zwischen Textfassungsversion und Notizbuch als selbständiges Werk in die Werkgenese integriert werden (
                        
was influenced by [P15]
).
                    

                    
Alle 
                        
Expressions,
 die das präbibliographische Werkmaterial repräsentieren, erzeugen (
                        
created [18]
) Entitäten des Typs 
                        
Manifestation Singleton [F4].
 Bei Bedarf kann die 
                        
Expression 
auch in einzelne 
                        
Expression Fragments [F23]
 unterteilt werden, um die Textstadien in einer feineren Granularität abbilden zu können. Abbildung 1 veranschaulicht die eben besprochene werkgenetische Modellierung am Beispiel des 1981 als Buch erschienenen und 1982 bei den Salzburger Festspielen uraufgeführten Theaterstücks 
                        
Über die Dörfer
 von Peter Handke.
                    

                    

                        

                        
Abbildung 1: Beispielhafter Graph der werkgenetischen Beschreibung zum Werk Über die Dörfer von Peter Handke.

                    

                

                

                    
Modellierung Notizbücher und digitale Edition

                    
Nicht veröffentlichte Notizbücher von Peter Handke werden im Modell durch die Entität 
                        
Individual Work [F14]
 repräsentiert; die daraus realisierte 
                        
Self-Contained Expression [F22]
 kann wiederum als Ereignis (
                        
Expression Creation [F28]
) detailliert beschrieben werden. Schließlich wird das Notizbuch als 
                        
Manifestation Singleton [F4]
 im Datensatz erfasst. Auf Werkebene werden die Notizbücher über 
                        
is logical successor [R1] 
in Beziehung gesetzt
                        
. 
Für alle Notizbücher, die in der digitalen Edition 
                        
Handke Notizbücher 
erstmals publiziert werden, werden Beziehungen auf Expression-Ebene (
                        
is component of [P148]
) zur digitalen Edition hergestellt. Die digitale Edition wird als fortlaufendes Werk (
                        
Serial Work [F18]
) repräsentiert, das kontinuierlich aktualisiert wird.
                    

                    
Vergleich zu 
                        
Handke
online
                    

                    
Im Vergleich zum Datenmodell von 
                        
Handke
online können mit dem vorgestellten System die Entitäten qualitativ miteinander in Beziehung gesetzt werden, was u.a. auch die erwähnte zeitliche Relation zulässt. Die ursprüngliche flache Struktur wird im neuen Modell durch ein Netzwerk repräsentiert, wodurch sich eine Bandbreite von neuen Auswertungsmöglichkeiten ergibt. Damit wird die Grundlage für ein durchsuchbares, interoperables Netzwerk geschaffen, welches das Auffinden von Ressourcen erleichtert und neue Forschungsfragen ermöglicht.
                    

                

            

            

                
Ausblick

                
Das von uns entwickelte generische Datenmodell ist für spezifische Beschreibungen erweiterbar, die darin gefassten Daten frei nachnutzbar. Was mit 
                    
Handke
online begonnen wurde, kann in der Tiefenerschließung durch andere Forschungsprojekte erstmals umfassend nachgenutzt werden. Davon profitiert vor allem auch das Editionsprojekt 
                    
Peter Handke Notizbücher.

                

                
Die hier präsentierte Modellierung unterstützt nicht nur das Verständnis der Werkgenesen von Peter Handkes Werken, sondern ist Vorbild für zukünftige DH-Projekte, die präbibliographische Daten zusammen mit bibliographischen Daten semantisch differenziert erfassen wollen.

                
Erst durch die hier besprochene Öffnung der Daten entsteht die Affordanz zur Repräsentation in vollem Umfang. Die Effekte sind vielfältig: Die Migration der isolierten Daten und deren Re-Modellierung nach anerkannten Standards machen das Material langzeitarchivfähig und schaffen die Grundlage für LOD. Durch den Einsatz von FRBR
                    
OO
 wird die Interoperabilität der vernetzten Informationen sichergestellt. Insgesamt werden durch die Öffnung neue Wege der Analyse geschaffen. Abfragen zu werkhistorisch relevanten Aspekten werden möglich: Wie die differenzierte Abfrage der Werkkomponenten oder zu aufführungsspezifischem Material oder sogar zu parallelen Entwicklungsprozessen; gleichzeitig wird auch eine Visualisierung von Stammbäumen zur Entstehung der Werke möglich. Wir erwarten mit Spannung, wie sich unsere Anwendung der formalen Semantik von FRBR
                    
OO
 in Zukunft auf die Reflexion über Werk und Werkgenesen auswirken wird.
                

            

        

            

                
GND, Normdaten und die Digital Humanities

                
Viele Projekte in den Digital Humanities verwenden Identifier für Entitäten wie Personen, Werke, Körperschaften oder Orte, die auf eindeutige Einträge in Normdaten-Verzeichnissen oder in Knowledge Bases verweisen 
                    
(

                    
Barth 2022 u. a.; 

                    
Rosenkötter und Fischer 2020; Fischer und Jäschke 2018; Herrmann und Lauer 2018; Dieckmann, Hermes, und Neuefeind 2017)
. Zu diesen Ressourcen gehören u. a. die Gemeinsame Normdatei (GND), andere Normdaten von Nationalbibliotheken, Wikidata, VIAF, DBpedia, Getty oder CERL. Jede dieser Ressourcen ist nach verschiedenen Kriterien aufgebaut und bietet unterschiedliche Funktionalitäten. Dies bringt Vor- und Nachteile für die Projekte mit sich, die sie nutzen. Zum Beispiel hat Wikidata keinen echten Normierungscharakter im Gegensatz zu Normdaten-Verzeichnissen wie der GND. Wikidata hat jedoch den Vorteil, dass Nutzende selbständig neue Entitäten anlegen können. Dies ist bei den durch Bibliotheken verwalteten Normdaten-Verzeichnissen meist nur auf Antrag möglich (in Zukunft sollen, zumindest für die GND, die Eingabemöglichkeiten durch angepasste Webformulare und Redaktionsumgebungen erweitert werden, vgl. 
                    
Kett u. a. 2022)
.
                

                
Nichtsdestotrotz ist die Sprache des Forschungsobjekts (z. B. von Textkorpora) und die Wahl der Normdaten-Ressource stark voneinander abhängig. Einige Projekte, die mit deutschsprachigen Texten arbeiten, haben sich für die GND entschieden, um Personen oder Werke zu identifizieren, u. a. die Digitale Bibliothek im TextGrid Repository,
                    

                     das Deutsche Textarchiv
                    

                     oder die deutschsprachigen Korpora aus DraCor 
                    
(Fischer u. a. 2019)
 und ELTeC 
                    
(Burnard, Schöch, und Odebrecht 2021)
. Projekte aus der deutschsprachigen Wissenschaftslandschaft, die im Bereich der nicht-deutschen Philologien forschen, entscheiden sich eher für andere Ressourcen, um Personen und Werke zu identifizieren. Die romanistischen Korpora der CLiGS-TextBox (Französisch, Spanisch, Italienisch und Portugiesisch; 
                    
Schöch u. a. 2019)
, die spanischen Korpora CoNSSA 
                    
(Calvo Tello 2021)
 und CONHA 
                    
(Henny-Krahmer 2018)
, und die französischen Korpora in ELTeC und DraCor benutzen für die Identifikation ihrer Entitäten überwiegend Wikidata und VIAF.
                

                
Die Bibliotheken und Fachinformationsdienste (FIDs) vieler Philologien in Deutschland verwenden die GND für die Sacherschließung ihrer Titel. Sie reichern umgekehrt die GND mit immer mehr Daten zu fremdsprachigen Autor*innen und deren Werke an. Es liegt nahe, dass die GND weiterhin hauptsächlich Autor*innen und Werke aus dem deutschsprachigen Raum verzeichnet. Das heißt jedoch nicht, dass die GND eine Quelle ist, die sich nur für die Germanistik eignet. Generell ist die GND in Form von Agenturen organisiert, die sich auf viele Bibliotheken und andere Institutionen in Deutschland verteilen und von der DNB koordiniert werden.
                    

                     Seit 2020 spielt die GND außerdem eine neue Rolle durch das Projekt “GND für Kulturdaten” 
                    
(Rosenkötter und Fischer 2020; Balzer u. a. 2019)
 und seit 2021 durch die Beteiligung in der NFDI
                    

                     (Text+, NFDI4Culture).
                    

                    
                

                
Daher fragen wir uns: Wie stark ist das Ungleichgewicht innerhalb der GND zwischen Einträgen zu deutsch- und fremdsprachiger Literatur?
                    

                     Können die Anglistik, die Romanistik, die klassischen Philologien, die Slavistik und andere damit rechnen, ihre Entitäten in der GND zu finden?
                

            

            

                

                    
ELTeC: mehrsprachige, vergleichbare Korpora für europäische Literatur
                

                
Unsere Frage beantworten wir anhand der multilingualen Korpora von ELTeC. Dabei handelt es sich um literarische Korpora in verschiedenen europäischen Sprachen, die in der COST-Action Distant Reading erstellt wurden 
                    
(Schöch u. a. 2021; Odebrecht, Burnard, und Schöch 2021)
. Das Ziel des Projektes war die Zusammenstellung vergleichbarer Korpora mit 100 Romanen pro Sprache. Aktuell wurde dies für 11 Sprachen erreicht, während für 10 andere Sprachen weniger Romane vorliegen. Außerdem wurde jedes Dokument mit Metadaten zu Autor, Werk, Edition und Text ausgezeichnet. Auf Grundlage dieser Metadaten konnten wir unsere Analysen erstellen.
                

                
Die Personen und Werke in ELTeC sind teilweise bereits mit Wikidata, GND oder VIAF eindeutig identifiziert. Die Abbildungen 1 und 2 zeigen im Vergleich die Annotation mit Normdaten für Autor*innen und Werke pro Sprache und die Wahl der Normdatenressource für die Identifikation von Autor*innen. 

                

                    

                        

                    

                

                

                    

                        

                    

                

                
Für die Sprachen Griechisch (GRE), Kroatisch (HR), Litauisch (LIT) und Rumänisch (ROM) konnten offenbar keine Normdaten verwendet werden. Um die Vergleichbarkeit der Ergebnisse zu gewährleisten, werden diese Sprachen bei den folgenden Analysen ausgeschlossen. Außer für das französische (FR) und in kleinen Teilen für das spanische (SPA), serbische (SRP) und ukrainische (UKR) Korpus wurden keine Werknormdaten eingetragen. Für die Autor*innen wurde überwiegend VIAF genutzt. Nur das französische und norwegische Korpus wurde auch mit Wikidata und das deutsche Korpus als einziges mit GND-IDs versehen. 

            

            

                

                    
Methode
                

                
Wir gehen in zwei Schritten vor, um ein vollständiges Bild über die mögliche Abdeckung mit Normdaten zu erhalten. Zunächst extrahieren wir die IDs der Autor*innen aus den TEI-Dokumenten der ELTeC-Korpora. Anhand der IDs werden die fehlenden Identifier aus Wikidata, GND und VIAF extrahiert. Das gelingt für die GND über die API von Lobid,
                    

                     und für Wikidata und VIAF durch ihre native API. Nach diesem Schritt erhalten alle Autor*innen eindeutige Identifier aus allen drei Ressourcen, falls Mappings gefunden werden konnten.
                

                
Im nächsten Schritt werden die Werke mit Rückgriff auf die Autor*innen-ID identifiziert. Auch wenn für vier ELTeC-Korpora Werk-IDs (überwiegend mit VIAF) bereits vom Projekt erfasst wurden, ignorieren wir diese, um die gleiche Methode für alle Korpora anzuwenden. Wir führen drei parallele 
                    
Reconciliation-
Prozesse mit den drei Ressourcen für jedes Werk durch. Genauer, werden alle Werke aller Autor*innen abgerufen, um die Ähnlichkeit zwischen dem Titel des Werks in ELTeC und allen Titeln der Werke der jeweiligen Autor*in aus den Normdaten zu vergleichen. Für jede mögliche Paarung wird ein statistischer Wert für die Ähnlichkeit zwischen beiden Titeln berechnet (0 für Titel, die gar keine Gemeinsamkeiten haben, 1 für Titel, die deckungsgleich sind). Der Werktitel mit der höchsten Ähnlichkeit wird ausgewählt. Für die weitere Analyse werden nur die Werke berücksichtigt, deren Wert höher als 0.5 liegt. Für diese drei parallelen und automatischen 
                    
Reconciliation- 
Prozesse werden die APIs von Lobid, Wikidata und VIAF benutzt.
                

            

            

                

                    
Ergebnisse
                

                
Ausgehend von den bereits in ELTeC identifizierten Autor*innen (vgl. Abb. 1) wird überprüft, ob diese auch in den jeweils anderen Normdatenressourcen (GND, Wikidata, VIAF) vorhanden sind. Daher sind hier Sprachen, für die keine Normdaten in ELTeC existieren (Griechisch, Kroatisch, Litauisch, Rumänisch), nicht berücksichtigt. 

                

                    

                        
Ergebnisse zu Autor*innen
                    

                    

                        

                            

                        

                    

                    
Abbildung 3 zeigt die Summe der Normdaten zu Autor*innen pro Ressource, die in den hier betrachteten Korpora gefunden oder ergänzt werden konnten. Für alle drei Ressourcen ist die Abdeckung hier sehr gut. Die GND liegt im Vergleich nur leicht zurück.

                    
Die Verteilung dieser Daten pro Sprache wird in Abbildung 4 gezeigt. Das Bild entspricht der Zusammenfassung aus Abbildung 3. Erwartungsgemäß hat das deutsche Korpus die höchste Quote in der GND. Das norwegische Korpus kann mehr Treffer mit Wikidata als mit VIAF erzielen. Für Tschechisch und Polnisch erreichen sowohl VIAF als auch Wikidata sehr gute Ergebnisse. Neben dem Deutschen bietet die GND eine gute Abdeckung für Sprachen wie Englisch, Französisch, Italienisch, Norwegisch, Schwedisch und Ukrainisch.

                    

                    

                        

                            

                        

                    

                

                

                    

                        
Ergebnisse zu Werken
                    

                    
Das Bild ändert sich erwartungsgemäß, wenn nicht Autor*innen, sondern Werke in den drei Ressourcen gesucht werden (vgl. Abb.5). Während VIAF und Wikidata mehr als 700 Werke aus ELTeC verzeichnen, erreicht die GND nur knapp über 200. 

                    

                        

                            

                        

                    

                    
Um diese Zahlen besser zu verstehen, zeigt Abbildung 6, dass nur das deutsche Korpus akzeptable Ergebnisse aus der GND erreicht (80 % der Werke). Alle anderen Sprachen bewegen sich zwischen null und knapp über 40 %. Die Abdeckung von Wikidata oder VIAF ist für viele Sprachen deutlich höher: von 60 % bis zu 100 %.

                    

                        

                            

                        

                    

                

                

                    

                        
Anzahl der Informationen pro Entität
                    

                    
Entscheidend für eine Bewertung der Ressource ist nicht nur, ob eine Entität vorhanden ist, sondern wie gut sie mit Metadaten beschrieben ist. Für die GND ist zu erwarten, dass Entitäten aus dem deutschsprachigen Raum ausführlicher beschrieben werden als Entitäten aus anderen Regionen. Um dies zu messen, werden die Daten von allen ELTeC-Autor*innen und deren Werke als XML-RDF-Dokumente aus der GND heruntergeladen und die Anzahl der XML-Elemente quantifiziert. Ohne den semantischen Gehalt der Elemente zu bewerten, gehen wir davon aus, dass mehr Elemente auch mehr Informationen pro Entität bedeuten. Abbildung 7 zeigt daher für die GND die Anzahl der Elemente pro Autor*in. Während für Autor*innen aus dem deutschsprachigen Raum 200-330 Elemente vorhanden sind, werden nur 100-240 für andere Sprachen verzeichnet. Auch wenn Sprachen wie Französisch oder Ukrainisch mittlere Werte (bis 240) zeigen, ist der Abstand zwischen diesen Sprachen und dem Deutschen immer noch sehr groß. 

                    

                        

                            

                        

                    

                    
Für die Werke (vgl. Abb. 8) erreichen die deutschsprachigen Entitäten wieder deutlich höhere Werte als alle anderen Sprachen in der GND. Hier ist der Unterschied im Vergleich weniger groß, weil insgesamt für Werke weniger Elemente angelegt werden.

                    

                        

                            

                        

                    

                    
Für einen Vergleich wurden diese Daten auch aus Wikidata extrahiert (Abbildungen 9 und 10). Wir prüfen, ob in Wikidata ähnliche Verzerrungen gegenüber dem Englischen oder anderen Sprachen zu beobachten sind. Jedoch hat in Wikidata keine Sprache einen so klaren Vorsprung im Vergleich zu allen anderen Sprachen wie das Deutsche in der GND.

                    

                        

                            

                        

                    

                    

                        

                            

                        

                    

                

                

                    

                        
Anreicherung durch ELTeC
                    

                    
Insgesamt ist ein möglicher Grund für die bessere Abdeckung für Autor*innen und Werke in Wikidata und VIAF ist die Tatsache, dass die Teilnehmenden von ELTeC die Entitäten in Wikidata selbst eingetragen haben 
                        
(Nešić u. a. 2022)
. Die Abbildungen 11 und 12 zeigen, dass zwar die Mehrheit der Autor*innen aus ELTeC bereits vor dem Start des Projekts (2017-2018) in Wikidata vorhanden waren, aber viele neue Werkeinträge entstanden. Die Metadaten zu Werken aus den sieben Sprachen, die 
                        
Nešić u. a. (2022)
 ausgewählt haben, führen zu einer deutlichen Verbesserung der Abdeckung in den letzten Jahren.
                    

                    

                        

                            

                        

                    

                    

                        

                        

                            

                        

                    

                

                

                    

                        
Kanonisierungsgrad
                    

                    
Eine weitere Hypothese ist, dass der Kanonisierungsgrad die Abdeckung in den drei Ressourcen beeinflusst. In den ELTeC Korpora wurde dies anhand des Metadatenfelds “reprints” belegt. Für Autor*innen und Werke, die keine oder wenige “reprints” (“low”) haben, zeigt Abbildung 13, dass alle drei Ressourcen eine niedrigere Abdeckung haben. Dabei ist der Unterschied für die GND deutlich größer als bei VIAF und Wikidata. Die Daten deuten darauf hin, dass die GND stärker vom Kanonisierungsgrad beeinflusst ist als die anderen zwei Ressourcen. Besonders niedrig ist die Abdeckung von nicht kanonisierten Werken in der GND (Abb. 13, “low reprints”). Zu beachten ist, dass die Verteilung von solchen Metadaten in den ELTeC Korpora nicht gleichmäßig ist. Die Ergebnisse können dementsprechend allein durch die Zusammenstellung der Korpora und die Metadatenanreicherung beeinflusst sein.

                    

                        

                            

                        

                    

                

            

            

                

                    
Abschluss
                

                
Wie gut können nicht-germanistische Projekte aus dem deutschsprachigen Raum mit der GND Autor*innen und Werke identifizieren? Sollten sie lieber auf Wikidata oder VIAF zurückgreifen? Um das zu beantworten, wurden die multilingualen Korpora von ELTeC analysiert. Auch wenn diese Korpora nicht vollständig ausgewogen hinsichtlich Repräsentation der Inhalte und der Persistenz ihrer Identifier sein können, sind sie eine wertvolle Ressource von und für die Community. Weitere ähnliche Evaluationen könnten in Zukunft durchgeführt werden, wenn umfassenderes Vergleichsmaterial identifiziert oder zusammengestellt wird. Das ELTeC Korpus ist zeitlich (19. Jh.), quantitativ und sachlich (100 Romane pro Sprache) notwendig beschränkt und vor diesem Hintergrund sind auch die vorliegenden Ergebnisse zu betrachten. 

                
Generell zeigt die GND eine gute Abdeckung von Personendaten und ist damit sehr nah an Wikidata oder VIAF. Jedoch füllt die GND deutlich mehr Felder (d.h. mehr Informationen) zu deutschen Autor*innen als zu anderen europäischen Autor*innen (für Personendaten außerhalb der Literatur mag das anders aussehen).

                
Hinsichtlich der Werknormdaten kann die GND nur für das Deutsche akzeptable Ergebnisse liefern. Auch für andere große Sprachen wie Französisch, Englisch oder Spanisch enthält die GND nur 40 % der enthaltenen Werke in ELTeC. Nicht nur die Abdeckung, sondern auch der Informationsgehalt ist für Werke deutschsprachiger Autor*innen höher als für alle anderen Sprachen. Darüber hinaus scheint die GND stärker vom Kanonisierungsgrad abhängig zu sein als VIAF oder Wikidata.

                
Wenn die GND damit als national-ausgerichtete Normdateninstitution erwartbar schlechter abschneidet, dann wäre zu prüfen, ob die Normdaten anderer Einrichtungen (vor allem von Nationalbibliotheken) eine ähnliche oder sogar stärkere Favorisierung der eigenen Sprache verzeichnen.

                
Durch die Einbindung der GND (der DNB und GND-Agenturen in anderen Bibliotheken) in die NFDI öffnet sich die GND nicht nur der Community, sondern es werden auch wichtige Diskussionen zu Multilingualität (GNDmul)
                    
 und Internationalität geführt. Wir sind zuversichtlich, dass die GND mithilfe der Community den Anteil fremdsprachiger Werke und Autor*innen in Zukunft erhöhen kann. Innerhalb von Text+ versuchen wir, Normdaten und Forschungsdaten zu verknüpfen. Die in dieser Analyse verwendeten Skripte werden auch für die Entwicklung von Pipelines zur Datenanreicherung im TextGrid Repository genutzt. So können die neu identifizierten Personen und Werke aus den ELTeC-Korpora mit den entsprechenden IDs zu den Daten aus dem TextGrid Repository hinzugefügt werden. Umgekehrt konnte damit auch ELTeC neue Daten für die Korpora gewinnen.
                

            

        

            

                
Ausgangslage

                
Das Selbstverständnis wissenschaftlicher Bibliotheken hat sich grundlegend gewandelt (z.B. Auberer et al. 2022): Neben ‚traditionellen‘ Aufgaben im Bewahren, Digitalisieren, Erschließen und dauerhaften Bereitstellen offener Kulturdaten übernehmen insbesondere Universitätsbibliotheken zunehmend Aufgaben im Forschungsdatenmanagement, stellen Arbeits- und Infrastrukturen inklusive Schulungs- und Beratungsdienste für DH-Projekte bereit. Ihnen kommt damit „eine wichtige Rolle bei der Sicherung guter wissenschaftlicher Praxis durch professionellen Umgang mit Forschungsdaten“ zu (Rösch 2021, 136). Hierin wird die 
                    
vermittelnde 
Rolle von Bibliotheken deutlich, gerade auch im Umgang mit Standards für Open Data. Der von den Bibliotheken selbst wie auch an sie herangetragene Anspruch an Offenheit (Berg-Weiß et al. 2022) ist jedoch nicht garantiert, vielmehr zeigt sich aufgrund der ausgesprochen heterogenen Ausgangs- und Rahmenbedingungen für Open Science (vgl. UNESCO 2021, 6) erst in konkreten Anwendungsfällen, ob der Zugang zu Kulturdaten, ihre Erstellung und (Nach-)Nutzung gemäß rechtlicher Vorgaben tatsächlich offen sind.
                

                
Ein solches Anwendungsszenarium ist mit der FAIRen Produktion und Nachnutzung von OER im Kontext der Lehre Gegenstand dieses Posters. Als Beispiel dient ein an der UB Kiel angesiedeltes Projekt, welches wiederum in ein partizipatives Forschungsdatenmanagement eingebettet ist, das mit Inklusion und Citizen Science zwei weitere Schwerpunkte bildet. Alle drei Bereiche teilen dabei die Grundidee der digitalen Interaktion auf Augenhöhe, die essentiell für das Gelingen partizipativer Forschung und FAIRer Lehre ist.

            

            

                
FAIRe Open Educational Resources

                
Freier Zugang zu und niedrigschwellige Nachnutzung von Lern- und Lehrmaterialien gehören zu einer offenen Wissenschaftspraxis. Open Educational Resources (OER) „kommt eine wichtige Funktion bei einem chancengerechten Wandel in der Bildung [...] zu“ (BMBF 2022, 2). Informations- und Kommunikationstechnologien bieten dabei, wie die UNESCO in ihrer Empfehlung zu OER betont, „ein hohes Potenzial für effektiven, chancengerechten und inklusiven Zugang zu OER und deren Weiterverwendung, Bearbeitung und Weiterverbreitung“ (UNESCO 2019, I.3). Dieses Potenzial umzusetzen ist jedoch anspruchsvoll, da neben der Verwendung offener Lizenzen und Dateiformate auch automatische Auffindbarkeit und didaktische Kontextualisierung gewährleistet werden müssen (Twillo o.J.). Herkömmliche Lernmanagement-Systeme an Hochschulen eignen sich für die Produktion FAIRer OER nur sehr bedingt (Dietrich, Zug 2020). Im zentralen Forschungsdatenmanagement der CAU kommt daher LiaScript – ein erweiterbarer, freier Markdown-Dialekt (Dietrich 2022) – zum Einsatz. Er wurde für die Erstellung digitaler Lern- und Lehrressourcen entwickelt und gewährleistet einfache Editier- und Versionierbarkeit der Materialien mittels Open-Source-Software ohne den Einsatz proprietärer Autor:innensysteme. Durch die Verwendung von Markdown bleiben die Materialien formatunabhängig und können für verschiedene Anwendungsfälle in passende Formate (u.a. HTML, PDF, SCORM für Lernmanagement-Systeme) exportiert werden. In zwei Kieler Pilotprojekten werden modulare LiaScript-Bausteine zu Digital-Literacy-Inhalten gemeinsam mit den Fachwissenschaften entwickelt, welche die didaktische Strukturierung und fachliche Einbettung sichern. Ein von der UB Kiel mit dem Germanistischen und Historischen Seminar der CAU Kiel durchgeführtes Projekt hat die Integration dieser Bausteine in bestehende curriculare Lehrveranstaltungen zum Ziel. Hierbei spielen organisatorisch-infrastrukturelle, fachliche, didaktische und technische Aspekte eine Rolle, die zusammen mit ersten Evaluationsergebnissen zum Einsatz des Tools in Lehrveranstaltungen diskutiert werden. 

            

            

                
Einbindung in ein Gesamtkonzept eines partizipativen und inklusiven Forschungsdatenmanagements

                
Eng verbunden mit der Forderung nach offener und freier Bildung sind die Themen Partizipation der Zivilgesellschaft mittels Citizen Science und eine inklusiv gedachte und praktizierte Wissenschaft als Bestandteile einer offenen Wissenschaftskultur. Universitätsbibliotheken bieten aufgrund ihrer bestehenden Erfahrung und Infrastruktur die Möglichkeit, partizipative, trans- und interdisziplinäre Forschungsprojekte anzustoßen und zu begleiten, zur Vernetzung zwischen Bürger:innen und der Scientific Community beizutragen, Citizen-Science-Projekte sichtbar zu machen und zu beraten (Vohland et al. 2021, 114; siehe auch Wiederkehr 2021). Die Transkription von Texten, Georeferenzierung von Objekten und Kategorisierung von kulturellen Artefakten bietet, etwa über eine App, einen niederschwelligen Zugang, um Bürger:innen an Forschungsprozessen zu beteiligen (vgl. Studie zu ‘Public Participation in Scientific Research’, Bonney et al. 2009) und sie, nicht zuletzt mittels OER, beim Erwerb von Datenkompetenzen zu unterstützen. 

                
Offenheit und Teilhabe sind wesentliche Forderungen der FAIR-Prinzipien. Wie aber gestalten sich FAIRe Daten für und barrierearmes Arbeiten in inklusiven Forschungsgruppen? Versteht man Behinderung als komplexes Phänomen und als Interaktion zwischen Individuum und Umwelt (UNESCO 2022, Abs. 1f.), liegt die Vermutung nahe, es könne keine allgemeingültigen Standards für einen inklusiven Umgang mit Forschungsdaten geben. Doch zeigen z.B. die Regelungen im Webdesign (nach WCAG 2.1 und BITV 2.0), dass sich die Rahmenbedingungen sehr wohl ändern und Barrieren abbauen lassen. Für das Forschungsdatenmanagement soll Ähnliches erreicht werden, hier gilt es den Fokus über die Präsentation von Daten hinaus um Datenerhebung, -analyse und -archivierung zu erweitern.

            

        

            

                
Introduction 

                
The endlessness of the digital space allows scholarly editors to conceive edition projects on a grand scale. Indeed, the eternal mission of philology can be identified in the quest for the best way to represent and/or reconstruct primary sources, in order to save them from oblivion and grant the public the most informed access to them. Consequently, the lack of space boundaries characterising the digital environment is one of the first aspects making such an environment perfectly suitable for hosting ambitious edition projects. Considering then the possibilities disclosed by hypertextuality, multimodality and multimediality, one can hardly argue against the digital way of editing, despite the ongoing challenges it must face (cf. for instance Rosselli Del Turco 2016), of which sustainability is clearly in the forefront. However, there are reasons to believe that state-of-the-art digital editions have not yet overcome the limits of a still rather bookish paradigm, obeying a mostly representational logic (Van Zundert 2018, Cugliana and Van Zundert 2022). 

                
In this contribution, I will argue that the key to the next level of digital scholarly editing is to be found in computation, that is, in the actual coding of the whole editorial workflow. I will present the theory, the advantages and the challenges of such a computational approach to the editing of pre-modern texts, backing up my claims with examples from the praxis and from my own scholarly work in the field of digital philology. Of course, the field of computational editing is still in its infancy, which means that fully computational editions have not been published yet. However, there are projects
 which can already prove the validity of the arguments presented in this paper or that at least seem to move in the same direction. 
                

            

            

                
Theoretical background 

                
While the digital paradigm has been widely described and commented upon (among others by Stella 2007 and Sahle 2016), there is a crucial aspect that has not been sufficiently underlined, which however could represent a turning point in the history of (digital) philology. It is the case of the use of programming code throughout the different phases of the editorial process, aiming at the realisation of what Barabucci and Fischer (2017) defined as the formalisation of textual criticism. The authors, in their conclusions, state that a “shared formalization would lead to the semi-automatization of the editorial process”, where “the responsibility of the editors would be to describe their choices and decisions”, while that of the computers would be to “deal with applying these rules and decisions in the best way”. 

                
The act of formalising the competence of the editor is to be seen as an achievement 
                    
per se
, for it can lead to more accountability and verifiability of the scholarly processes, which can be shared in its complexity with the community. As a matter of fact, such processes are of fundamental importance not only from a methodological perspective, but also for the proper understanding and contextualisation of the presented results. In this respect, Van Zundert (2018) points out how the current digital methods used in scholarly editing, instead, still aim at merely recording the results of the philological work in a representational fashion,
 without keeping track of the decisions, analyses and actions that led to those results. In a computational edition, on the other hand, the scholarly competence and actions are not only expressed in a formal language, but they are also operationalised. This, in turn, brings about new affordances, such as the replicability and reusability of the editorial work, as well as the introduction of a degree of indirection allowing for highly controlled modelling of the edition process. 
                

                
If methods are there to realise theoretical principles in the best possible way, at the same time they can in turn influence the principles themselves, opening up new perspectives uncovering, for instance, implicit biases and illogicalities. Indeed, the use of computation for the modelling and operationalisation of editorial knowledge can contribute to perfecting the editorial workflow in the way envisioned by McCarty (2005), who referred to the “meaningful surprise” often arising from the process of modelling and computing.

                

            

            

                
The praxis of the computational method

                
Some examples from the actual application of this approach will hopefully prove its potential for the field of Digital Scholarly Editing. 

                

                    
Achievements 

                    
During my PhD, which I completed in February 2022, I edited an Early High German version of Marco Polo’s travel account, also known as “Version DI” of the 
                        
Devisement dou Monde
. One of the main goals of my research was to automatise relevant parts of the editorial workflow, such as the normalisation of the texts. As a matter of fact, bearing in mind the concept of “text as a wheel” of different possible facets, as theorised by Sahle 2013, I wanted to produce (at least) three different levels of normalisation of the text (diplomatic, semi-diplomatic and interpretative). 
                    

                    
Together with my colleague Gioele Barabucci (Norwegian University of Science and Technology), we developed a method based on a rule-and-exception principle, featuring three XProc pipelines, one for each level of normalisation (Cugliana and Barabucci 2022). Each pipeline consists of a series of XSLT stylesheets which deal with the different steps of the normalisation process, such as the levelling of allographs, the expansion of abbreviations, the regularisation of capitalisation etc.
 In particular, the choice of using a pipeline system instead of a single stylesheet was due to the fact that some actions needed to be taken before other ones could follow: for instance, inserting a capital letter after a full stop implies that the full stops are already in place. Each pipeline generates a new TEI XML file with a different level of normalization.
                    

                    
This proved to be a very suitable strategy for dealing with the complexity of the normalisation process. Concerning the choice of XProc pipelines, it has already been shown that small-step pipelines making use of a stateless language such as XSLT prove to be advantageous in that they reduce the complexity of computer programs, they can be easily shared with the peers and they improve the sustainability of the code (Barabucci and Schaeben 2021). Not only were the pipelines successful in generating three levels of normalisation of the texts, but they could also be applied to the transcriptions of all the witnesses edited, despite the fact that some were written in the East-Swabian dialect, and some in Bavarian. This was probably due to the geographical proximity and the contemporaneity of their production, which leads to hypothesise the possibility of creating “pools of rules” for the editing of witnesses written in specific areas and historical periods. 

                    
In my talk I will present the system at the basis of the normalisation of the texts featured in the edition of DI, giving some insights into the very development of the pipelines, both from a strictly philological and from a computational perspective. In particular, I will focus on some tricky aspects such as the cases of ambiguities and exceptions, which might represent a hurdle for the full systematisation of the editorial workflow.

                

                

                    
Outlook 

                    
A case of reuse of the normalisation pipelines written for Version DI of the 
                        
Devisement dou Monde
 is the project for a computational edition of the medieval German versions of the 
                        
sortes
 text 
                        
Prenostica Socratis Basilei
 (Alonso Guardo 2015). This project is still in its early stages,
 but a taste of its complexity will be given in the talk. With a prototypical example I will show how the same or slightly adapted XSLT stylesheets can be used to normalise texts different from the ones which had served as a basis for their development, as well as how they can be embedded in the editorial workflow for the creation of a fully computational edition. This will open up the discussion on the reuse of program code for different editorial projects (which parts of the code can be reused, which need to be adapted, what rules can apply to different projects and what is instead determined by each individual edition). 
                    

                    

                        
Sortes
 texts (in German “Losbücher”) represent an extremely interesting genre, which unfortunately has not enjoyed much scholarly attention in the field of digital editing, although it has a long history dating back to antiquity.
 They were very interactive texts which were not meant to be read linearly (Heiles 2018) and which were used to predict the future with the help of sortition mechanisms, sometimes quite extravagant ones. The computational edition of the medieval German versions of the 
                        
Prenostica Socratis Basilei
 aims at establishing a new methodology for the editing of this genre, finding strategies to weave together games and textual criticism, with the goal of representing the 
                        
sortes
 in their genesis and reception throughout the centuries. Specifically, the whole editing process will find expression in an actionable formal language. 
                    

                

            

            

                
Conclusions

                
As can be evinced from this abstract, my contribution has a twofold purpose: on the one hand giving a short, but hopefully convincing introduction to the theoretical aspects justifying the scholarly value of a computational approach to editing and, on the other, presenting some meaningful results obtained in the praxis. In particular, the focus will mainly be on the success and challenges of the normalisation pipelines developed in collaboration with Gioele Barabucci, showing what it actually meant to translate into code the usually very analogue task of normalising medieval texts, what problems we encountered and how we solved them. Finally, as an outlook for the possible applications of the computational principle, I will briefly illustrate the work I am doing for my next project, the edition of the 
                    
Prenostica Socratis Basilei
, which was conceived from the start as a computational one. In my talk, I will show some prototypical examples concerning specific aspects of the computational workflow.
                

            

        

            

                
Hintergrund

                
Forschungssoftware zu entwickeln ist neben der Anwendung digitaler Methoden ein wichtiger Teil der Forschungstätigkeiten in den Digital Humanities, findet jedoch meist immer noch in einer wenig professionalisierten Form statt (siehe schon Schrade et al. 2018). Zwar ist es in manchen Bereichen der DH möglich, für bestimmte Aufgaben auf bestehende, außerhalb der DH selbst entwickelte und teilweise kommerzielle Softwarelösungen zurückzugreifen, z. B. auf den Oxygen XML-Editor (SyncRO Soft SRL 2022) für die Bearbeitung von XML-Dateien oder auf die Software Gephi (Bastian et al. 2009) zur Visualisierung von Netzwerkdaten. Häufig sind aber solche allgemeinen Tools für die geisteswissenschaftlichen Forschungsfragen und -projekte nicht geeignet und speziellere Werkzeuge nicht vorhanden. 
                    
Das führt dazu, 
dass die digitalen Geisteswissenschaftler*innen selbst als Software-Entwickler*innen tätig werden oder eng mit Research Software Engineers (RSE) zusammenarbeiten, um neue Forschungssoftware zu entwickeln.
                

                
Ein typischer Fall sind digitale Editionen, die einerseits auf einem zugrunde liegenden Datenmodell und einer Repräsentation in Daten – den Forschungsdaten selbst – basieren. Andererseits nehmen sie in den meisten Fällen erst durch eine digitale Webpräsentation Gestalt an, die oft je nach Gegenstand der Edition maßgeschneidert entwickelt wird. Auch für Analysen von geisteswissenschaftlichen Daten kann zwar häufig auf bestehende Softwarekomponenten und -module zurückgegriffen werden, diese werden aber oft erst dadurch sinnvoll für die Forschung nutzbar, dass sie programmtechnisch für einen spezifischen Workflow eingesetzt und miteinander verknüpft werden, wie z. B. einzelne Python-Module für einen Textanalyseworkflow. Oder es werden ganz neue Analysetools für geisteswissenschaftliche Anwendungsfälle entwickelt, wie etwa im Fall des Tools Stylo (Eder et al. 2016) für stilometrische Analysen. Schließlich beinhaltet geisteswissenschaftliche Softwareentwicklung auch das Entwickeln von Skripten zur Aufbereitung und Umwandlung von Daten, damit diese wiederum in anderen Tools verarbeitet werden können. 

                
Art und Umfang der Softwareentwicklung in den Geisteswissenschaften können also stark variieren. Genauso kann auch der Anteil und Stellenwert, den die Tätigkeit „Softwareentwicklung“ für Forscher*innen in den DH hat, sehr unterschiedlich sein. Das Entwickeln von Forschungssoftware kann einen Teil der gesamten Tätigkeit ausmachen, z. B. wenn ein Projekt individuell von Einzelnen umgesetzt wird, von einer geisteswissenschaftlichen Fragestellung über die Operationalisierung und technische Implementierung bis hin zur Interpretation und Aufbereitung der Ergebnisse. Softwareentwicklung kann aber auch für einzelne Forschende zur Haupttätigkeit werden, insbesondere in größeren Teams, die arbeitsteilig tätig sind, oder wenn die Entwicklung von Forschungssoftware im Mittelpunkt eines Vorhabens steht. Für diejenigen, die hauptsächlich mit der Entwicklung von Forschungssoftware beschäftigt sind, hat sich der Begriff 
                    
Research Software Engineer
 etabliert. 
                

                
Gerade dadurch, dass die Softwareentwicklung im geisteswissenschaftlichen Forschungskontext und in den DH in den wenigsten Fällen von hierfür ausgebildeten Softwareentwickler*innen oder Informatiker*innen durchgeführt wird, gibt es in diesem Bereich noch viel Raum und viele Möglichkeiten für eine stärkere Professionalisierung. Häufig entsteht geisteswissenschaftliche Forschungssoftware in den ersten Stadien aus der jeweiligen wissenschaftlichen Domäne heraus. Die Forschenden selbst eignen sich hierfür durch Kurse oder Zusatzausbildungen die Grundlagen einzelner Programmiersprachen an. Für diese zusätzliche Kompetenz und auch die zusätzliche Arbeit, die durch die Softwareentwicklung entsteht, gibt es allerdings aus der Community nur selten die verdiente wissenschaftliche Anerkennung. Da weder das Wissen um eine stabile Softwarearchitektur noch ausreichend Zeit für Tests oder gar eine umfangreiche Dokumentation vorhanden ist, ist auch die Software selbst oft in einem 
                    
verbesserungswürdigen 
Zustand und nicht für einen nachhaltigen Einsatz vorbereitet. Hier setzt dieser Workshop an.
                

            

            

                
Ziele des Workshops

                
Der Workshop wird von der AG „Research Software Engineering in den Digital Humanities“ des DHd-Verbands ausgerichtet und richtet sich an alle Wissenschaftler*innen, die im Rahmen ihrer Forschung oder in Forschungsprojekten Software entwickeln oder an der Entwicklung von Software beteiligt sind. Wir argumentieren, dass eine Offenheit 
                    
für stärkere Professionalisierung seitens 
aller an geisteswissenschaftlicher Softwareentwicklung Beteiligten helfen kann, sowohl die Software selbst zu verbessern als auch die Position ihrer Entwickler*innen in der Wissenschaft zu stärken. Auch wenn Professionalisierung mit steigender Bedeutung von Softwareentwicklung im jeweiligen Kontext im Forschungsprozess und als wissenschaftliche Tätigkeit generell wichtiger wird, können doch alle Bereiche der DH, in denen Softwareentwicklung betrieben wird, von einer stärkeren Professionalisierung profitieren, die grob auf drei Ebenen beschrieben werden kann:
                

                
1. technische Professionalisierung

                
Die technische Professionalisierung betrifft in erster Linie die Bereiche, die in der täglichen Arbeit in der Softwareentwicklung auf der Ebene von Code und Architektur von Bedeutung sind, z. B. 
                    
Clean Code
 (Martin 2009, Clean Code Developer 2022), Versionierung, Dokumentation, Tests, 
                    
DevOps
 (Halstenberg et al. 2020) usw.
                

                
2. organisatorische Professionalisierung

                
Die organisatorische Professionalisierung bezieht sich auf alle Aspekte der Planung, des Projektmanagements und der (Selbst-)Organisation von Softwareentwickler*innen. Dazu gehören u. a. die Bildung von Entwickler*innen-Teams, die Verwendung von Versionsverwaltungs- und Ticketsystemen, Methoden der agilen Softwareentwicklung (Beck et al. 2001), aber auch Softwaremanagementpläne (SMPs).

                
3. institutionelle Professionalisierung

                
Die institutionelle Professionalisierung schafft die Rahmenbedingungen für die beiden anderen Bereiche, indem Softwareentwicklung Teil von DH-Curricula und die Ausbildung für RSEs in den DH verbessert wird, aber auch Karrierewege ermöglicht werden. Es geht um die Bereitstellung von Mitteln und Infrastruktur für die Entwicklung von Software und die Verankerung der Tätigkeit einer/s RSE im wissenschaftlichen Bereich.

                
Ausgehend von den drei beschriebenen Ebenen soll der Workshop über mögliche Bereiche der Professionalisierung informieren und einen Erfahrungsaustausch und Diskussionen zum Thema ermöglichen. Konkretes Ziel des Workshops ist es, durch die Beteiligung aller Teilnehmerinnen und Teilnehmer in Gruppenarbeit ein White Paper zu erarbeiten, in dem Professionalisierungsoptionen für die Forschungssoftwareentwicklung in den DH festgehalten werden, so wie sich z. B. auch der Verein de-RSE fächerübergreifend für nachhaltige Softwareentwicklung ausgesprochen hat (Anzt et al. 2020).

                
Der Workshop soll so dazu beizutragen, dass in den DH mehr nachhaltige Forschungssoftware von hoher Qualität entsteht. Auf allen drei Ebenen - technisch, organisatorisch und institutionell - spielen hierbei auch Aspekte der Offenheit (Open Source, Open Access, Open Science) eine große Rolle, um eine vollständige Integrität und Transparenz aller Prozesse und Werkzeuge im Forschungszyklus zu gewährleisten und eine „gute wissenschaftliche Praxis“ zu ermöglichen. Mehr Professionalität in der Softwareentwicklung sorgt damit für eine bessere Forschungsunterstützung und bessere Forschung in den Geisteswissenschaften, gemäß dem Motto „Better Software, Better Research“ (Goble 2014). Zugleich trägt sie dazu bei, dass Softwareentwicklung als Forschungstätigkeit stärker sichtbar und anerkannt wird, was die Position von Entwickler*innen in den DH verbessert.

            

            

                
Ablauf und Inhalte des Workshops
                        
                
Der Workshop ist für zwei halbe Tage geplant und setzt auf eine starke Mitarbeit aller Teilnehmenden. Als Ergebnis des Workshops ist angestrebt, ein White Paper mit Best Practices und Professionalisierungsoptionen und -schritten zu publizieren. Das Papier soll als Argumentationshilfe für DH-RSEs dienen, die sich eine professionellere Arbeitsumgebung innerhalb ihrer Institution wünschen. Zudem soll es die Institutionen selbst dabei unterstützen, die Professionalisierung der Forschungssoftwareentwicklung in den DH voranzubringen.

                
Im ersten Teil des Workshops werden von den Convenern der AG zur inhaltlichen Einführung in das Thema Kurzvorträge zu den Ebenen technischer, organisatorischer und institutioneller Professionalisierung gehalten. Daran schließt sich eine offene Gesprächsrunde mit allen Teilnehmenden an, in der jede und jeder Gelegenheit bekommen soll, auf folgende Fragen einzugehen:



                    
Wie sehr sehen Sie sich als RSE?

                    
Wie genau sieht Ihre Rolle als RSE aus?

                    
Wie schätzen Sie Ihren zukünftigen Karriereweg ein?

                    
Wie würden Sie Ihren eigenen Professionalisierungsgrad und den Ihrer Institution in Bezug auf Softwareentwicklung einschätzen?

                    
Halten Sie eine stärkere Professionalisierung in der Softwareentwicklung in den DH für sinnvoll und warum/warum nicht?

                    
Welche Aspekte halten Sie für eine professionelle Softwareentwicklung für besonders wichtig?

                

                
Nach der offenen Gesprächsrunde folgt im zweiten Teil eine gemeinsame Arbeit in Teilgruppen, wobei sich jede Gruppe auf einen der drei Hauptbereiche für Professionalisierung konzentrieren wird. Die Arbeit in den Teilgruppen wird gemeinsam geplant, indem definiert wird, woran jede Gruppe arbeiten und was als Ergebnis der Gruppenarbeit erwartet wird. Für die Dokumentation der Ergebnisse werden Online-Dokumente vorbereitet, in denen kollaborativ gearbeitet werden kann. Jede Teilgruppe wird von einer der einreichenden Personen geleitet. Auch kann jede/r Teilnehmende entscheiden, in welcher Gruppe er oder sie mitarbeiten möchte. Um eine produktive Arbeit sowohl in der Gesamtgruppe als auch in den Teilgruppen zu ermöglichen, ist die maximale Teilnehmerzahl auf 30 Personen begrenzt. Im Folgenden wird ein Überblick über das Programm gegeben:

                
Teil 1:



                    
Begrüßung (10min)

                    
Input-Statements von Seiten der AG zu Teilaspekten von Professionalisierung (30min)

                    
Offene Gesprächsrunde zu Professionalisierung - Teil I (80min)

                    

                        Pause (30min)
                    

                    
Offene Gesprächsrunde zu Professionalisierung - Teil II (60min)

                    
Planung und Abstimmung zu Gruppenarbeit (30min)

                

                
Teil 2:



                    
Gruppenarbeit zu Teilaspekten I (90min)

                    

                        Pause (30min)
                    

                    
Gruppenarbeit zu Teilaspekten II (60min)

                    
Ergebniszusammenführung (30 min)

                    
Diskussion und Abschluss (30 min)

                

                
Zur Durchführung des Workshops werden ein Beamer, ausreichend Steckdosen und WLAN benötigt.

            

            

                
Kontaktdaten der Beitragenden

                
Alexander Czmiel ist Leiter von TELOTA - IT/DH, der IT und Digital Humanities-Abteilung der Berlin-Brandenburgischen Akademie der Wissenschaften. Er ist Mitglied im Institut für Dokumentologie und Editorik, in der Gesellschaft für Forschungssoftware - de-RSE und Co-Convener der AG „Research Software Engineering in den Digital Humanities“. Sein Forschungsinteresse liegt in der erfolgreichen Durchführung von Digital Humanities-Projekten und hier insbesondere in der nachhaltigen Softwareentwicklung, damit alle digitalen Forschungsergebnisse möglichst lange für die Nutzung erhalten bleiben. Kontakt: czmiel@bbaw.de

                
Ulrike Henny-Krahmer ist Juniorprofessorin für Digital Humanities an der Universität Rostock, Mitglied des Instituts für Dokumentologie und Editorik und Co-Convenerin der DHd-AG „Research Software Engineering in den Digital Humanities“. Ihre Forschung konzentriert sich auf Digitale Editionen und Textsammlungen, quantitative Textanalyse und die Evaluation und Nachhaltigkeit von digitalen Forschungsergebnissen. Kontakt: ulrike.henny-krahmer@uni-rostock.de

                
Daniel Jettka ist Wissenschaftlicher Mitarbeiter im Projekt NFDI4Culture, wo er im Arbeitsbereich „Forschungswerkzeuge und Datendienste“ tätig ist. Insbesondere arbeitet er an der Koordination der technischen Infrastruktur in NFDI4Culture mit und ist Teil der Beratungsagentur für nachhaltige Softwareentwicklung. Er ist Co-Convener der DHd-AG „Research Software Engineering in den Digital Humanities“. Kontakt: daniel.jettka@uni-paderborn.de

            

        

            

                

                
Geistes- und Kulturwissenschaften

                
 

                
bauen in ihren Erkenntnisprozessen wesentlich auf der Befragung von Quellen unterschiedlichster Materialität und Medialität auf: So bezeichnet der Begriff sämtliche Objekte und Überreste, die zum Erkenntnisgewinnungsprozess über das Vergangene beitragen, z. B. Gemälde, Musiknotenblätter, Texte, Fotografien, Münzen, Inschriften, Kleidung oder andere Alltagsgegenstände. Quellen liefern aber keine Wahrheiten, sondern müssen gedeutet und in die Sprache der (historischen) Wissenschaften übersetzt werden. Zudem können Quellen subjektiv, fehlerhaft, verfälscht oder auch nur in Teilen erhalten sein. Um diesen Herausforderungen zu begegnen, hat sich in den historischen Wissenschaften die Methode der Quellenkritik etabliert (vgl. Koselleck 1977). Sie dient dazu, die Aussagekraft einer Quelle für ein gegebenes Forschungsvorhaben (insbesondere in Relation zu anderen Quellen) zu beurteilen und stellt damit letztlich die Grundlage zu ihrer Analyse dar. Hierfür werden sie z. B. beschrieben, indexiert, kontextualisiert, übersetzt und daran anschließend ausgewertet. Die Quellenkritik ist damit eine der Grundsäulen des Forschens schlechthin, sowohl in den historischen Wissenschaften als auch darüber hinaus (vgl. z. B. Arnold 2001).

            

            
Die digitale Transformation verändert alle Bereiche der Gesellschaft – dies schließt die Wissenschaft insgesamt und die historischen Wissenschaften im Speziellen ein. Neben Quellen genuin digitaler Natur, sog. “born digital” Quellen wie z. B. Software, Websites, Social Media Beiträge und persönliche Textnachrichten, stehen “traditionelle” Quellen im zunehmenden Maße digitalisiert zur Verfügung. Diese digitalen Repräsentationen stellen die Quellenkritik allerdings vor neue Herausforderungen: Wer hat die Quelle wie digitalisiert und zu welchem Zweck? Welche Formate und Transformationsalgorithmen wurden verwendet? Wer hostet die digitale Quelle und gewährleistet die Langzeitverfügbarkeit sowie ihre Integrität? Wie wird die Quelle auffindbar für diejenigen, die sie in ihrer Forschung verwenden wollen? Zu all diesen Fragen müssen sich die Geistes- und Kulturwissenschaften verhalten und dabei sowohl philosophische Überlegungen (Was ist eigentlich ein digitales Objekt?), als auch Überlegungen zu Methodologie, Langzeitarchivierung, manueller / semi-automatischer / automatischer Erschließung, Forschungsethik und viele andere mehr berücksichtigen.

            

                
Die traditionelle Quellenkritik muss vor diesem Hintergrund um die Dimension einer digitalen Quellenkritik erweitert werden. Hierzu sind in jüngerer Zeit bereits einige Beiträge vorgelegt worden (vgl. z. B. Fickers 2020; Föhr 2017; Hering 2014; Pfanzelter 2015), doch mit dem rasanten technischen Wandel und vor dem Hintergrund sich stark verändernder digitaler Infrastrukturen und Arbeitsprozesse in der Wissenschaft (z. B. im Zusammenhang mit der Nationalen Forschungsdateninfrastruktur) braucht es eine stete und kritische Begleitung des Themas, wie es nur ein kontinuierlich und kooperativ geführter wissenschaftlicher Diskurs gewährleisten kann. Dieser Aufgabe hat sich der Arbeitskreis Digitale Quellenkritik (der lose an den DHd-Verband und die Arbeitsgruppe digitale Geschichtswissenschaft des Historikerverbands angeschlossen ist) verschrieben. Die Gruppe setzt sich aus Vertreter*innen mit unterschiedlichen institutionellen Hintergründen (Forschungs- und Infrastruktureinrichtungen) und aus diversen geistes- und kulturwissenschaftlichen Disziplinen zusammen, um die verschiedenen Aspekte digitaler Quellenkritik möglichst vielseitig zu beleuchten. Der Arbeitskreis hat es sich zur Aufgabe gemacht, die Thematik aufzufächern, die verschiedenen theoretischen, methodischen und inhaltlichen Aspekte zu identifizieren und sie in einem 

                
living handbook

                
 

                
zusammenzuführen. Das 

                
living handbook

                
 

                
stellt dabei sowohl eine Kondensationsfläche für den 

                
status quo

                
 

                
als auch eine Einführung in die Thematik und eine Diskussionsgrundlage dar.

            

            

                
Aktuell befinden sich bereits mehrere Kapitel des 

                
living handbooks

                
 

                
im Publikationsprozess, andere Kapitel sind noch in der Aufarbeitung bzw. Planung. Um den derzeitigen Stand im Detail vorzustellen und im Rahmen von Diskussionen weitere Impulse aus der DH-Community einzuholen und in die Kapitel zu integrieren, erscheint ein Panel auf der DHd2023 als ideales Format. Entsprechend soll dort nachstehende Auswahl an Kapiteln kurz präsentiert und zur Diskussion gestellt werden. Sowohl das Projekt des 

                
living handbooks

                
 

                
als auch die öffentliche Diskussion mit der DH-Community stehen damit ganz im Geiste des Tagungsmottos “Open Humanities – Open Culture”. 

            

            

                
Beitrag 1: Offenes, community-getriebenes Publikationsformat

                
Aline Deicke

                

                    
Das Handbuch ist ein Community-Projekt und per Definition nie abgeschlossen: Es hält sich offen für Korrekturen, Ergänzungen und Aktualisierungen. Die Prozesse der Arbeit mit Quellen, ihre kritische Reflexion und Interpretation und der Weg von einer Quelle zu einer wissenschaftlichen Aussage sollen damit transparent gemacht und aktualisiert werden. Das 

                    
living handbook

                    
 

                    
zur digitalen Quellenkritik ist so ein Stück gelebte Open Culture der digitalen Geistes- und Kulturwissenschaften.

                

                
Anhand der Arbeiten und Debatten um das Handbuch lassen sich Aspekte diskutieren, die Open Science in den Digital Humanities allgemein betreffen: Wie lässt sich eine möglichst weite Partizipation von Stimmen und Perspektiven aus der Gesellschaft und verschiedenen Fachcommunities mit der Sicherung von Expertise vereinbaren? Welche Verfahren der redaktionellen Überarbeitung erfordert eine offene und stetige Erweiterung von Texten? Wie lassen sich Elemente des traditionellen Publikationsbetriebs (Reputationsmetriken, Reviewverfahren etc.) in einen community-getriebenen, kollaborativen Prozess überführen?

            

            

                
Beitrag 2: Theorie der digitalen Quellenkritik

                
Moritz Feichtinger 

                

                    
Das Projekt des 

                    
living handbooks

                    
 

                    
ist getrieben von der gemeinsamen Einsicht, dass Quellenkritik im digitalen Zeitalter um neue Herangehensweisen und Fragestellungen erweitert werden muss. Unklar bleibt jedoch, inwieweit dieser digitale Wandel die theoretischen und methodischen Grundlagen der Disziplinen erfassen wird oder sollte. Handelt es sich bei digital unterstützten Praktiken des Befragens, Analysierens und Interpretierens lediglich um den Einsatz eines modernisierten Werkzeugkastens, oder ändert sich die Gewinnung historischer Erkenntnis fundamentaler? Wenn der Umgang mit Quellen von der Suche über die Bearbeitung bis zur Interpretation in erheblichem Umfang von digitalen Methoden bestimmt ist, bedeutet dies letztlich eine Erweiterung oder Erneuerung der Grundkenntnisse und Grundwissenschaften historischen Forschens?

                

                
Der Beitrag fragt also nach den Erfordernissen digitaler Quellenkritik und deren Konsequenzen für die Methodik und Hermeneutik historischen Forschens. Debattiert werden soll, welche Zugänge anderer Disziplinen übernommen werden können und unter welchen Bedingungen dies geschehen kann (oder sollte). Zudem möchten wir zur Diskussion stellen, welche Gestalt und welches Ausmaß der Einfluss digitaler Quellenkritik auf die Hermeneutik haben kann (oder sollte).

                
Erleben wir einen Wandel oder eine Pluralisierung der Erkenntnisverfahren? Bedeutet dies eine Schärfung des Profils akademischer Forschung (und ihrer hergebrachten Hermeneutik) oder eine stärkere Annäherung an andere Disziplinen?

            

            

                
Beitrag 3: Algorithmenkritik und die Grenzen des Algorithmus

                
Mark Hall

                
Im digitalen Raum werden Quellen zwangsläufig algorithmisch verarbeitet: von der Suche relevanter Quellen über ihre Analyse bis hin zur Visualisierung der Analyseergebnisse – Algorithmen sind allgegenwärtig. Jeder dieser Algorithmen hat das Potenzial, Quellen und Ergebnisse zu verzerren respektive zu beeinflussen (Van Es 2018). Eine kritische Reflexion der im Forschungsprozess verwendeten Algorithmen ist daher integraler Bestandteil der digitalen Methoden- und Quellenkritik (Dobson 2015).

                
In der Informatik wird der Algorithmus generell als unabhängig von den bearbeiteten Daten gesehen. Diese künstliche Trennung ist aber für die Algorithmenkritik aus mehreren Gründen problematisch: Erstens lässt sich die Frage nach potenziellen Verzerrungen nur im Hinblick auf die spezifischen Eigenschaften der zu verarbeitenden Daten beantworten. Ein Algorithmus kann für das eine Quellenkorpus geeignet sein und den Aussagewert eines anderen verzerren. Zweitens sind bei vielen Methoden die Grenzen zwischen Daten, Modell und Algorithmus nicht trennscharf. So nutzt Maschinelles Lernen etwa mehrere Algorithmen, um zum einen aus Daten ein Modell zu trainieren und zum anderen, um das Modell auf die Quelle anzuwenden. Diese Aspekte können nicht unabhängig voneinander betrachtet werden.

                
Es ist daher notwendig, eine holistische Algorithmenkritik zu entwickeln, die den Algorithmus im Kontext der Daten und Modelle analysiert. Zu diskutierende Fragen sind unter anderem: Ist diese enge Verknüpfung von Algorithmus, Daten und Modellen wirklich notwendig? Was für Methoden der Algorithmenkritik kann man ohne eine detaillierte Analyse des Codes anwenden? Wo zieht man die Grenze zur Datenkritik?

            

            

                
Beitrag 4: Für die verräumlichten Geisteswissenschaften: Von Karten zu Standorten

                
Francis Harvey

                
In einer digitalen Quellenkritik (Fickers 2020; Pfanzelter 2015) kann die Aufarbeitung von Orten und Standorten wichtige epistemologische und ontologische Hinweise für die Forschung geben (Bodenhamer et al 2010). Digitalisierte Karten und raumbezogene Daten stellen neue Herausforderungen dar. Forschende können zwar mit Einsichten aus der traditionellen Geographie und Kartographie die eigene Quellenkritik oft vertiefen, aber mit dem digitalen Wandel geht ein epistemologischer Wandel einher, der neue Herausforderungen an den verräumlichten Erkennungsgewinnungsprozess stellt (Capurro 2010). So kommen beispielsweise Fragen dazu auf, wie unterschiedlichen Zugängen zu Digitalisaten durch Informationsinfrastrukturen entgegengewirkt oder wie eine digitalisierte Kartenkritik in eine digitale Quellenkritik integriert werden kann.

            

            

                
Beitrag 5: Populäre Wissensproduktion – digitale Quellenlücke?

                
A. Nursen Durdaği

                
Die flächendeckende Nutzung von Computern in Wissenschaft und Gesellschaft, die rasante Entwicklung des Internets mit einer enormen Zunahme der Digitalisierung u.a. mit Hilfe von künstlicher Intelligenz führt zu einer unumkehrbaren Veränderung der menschlichen Informationsrezeption. Wissen wird nicht mehr ausschließlich über traditionelle Wege rezipiert, sondern oftmals über populär (wissenschaftlich)e Zugänge wie z. B. YouTube-Videos, Social Media Beiträge etc. Je mehr Informationen vorliegen und generiert werden, umso schwieriger wird es zu identifizieren, welche Beiträge wissenschaftlichen, peer-geprüften Forschungsprozessen entstammen. Insbesondere geht oft verloren, wie und von wem Informationen zu Sachverhalten erstellt, kommuniziert und verändert wurden. Für eine digitale Quellenkritik und -analyse bedeutsam sind u.a. soziale und geografische Unterschiede in der Verfügbarkeit von Ressourcen, in der Zugänglichkeit von Dokumenten angesichts (staatlicher) Geschichtspolitik und Zensur, eine sorgfältige Dokumentation der Datenhistorie sowie die Gewährleistung einer langfristigen Auffindbarkeit. Dennoch stellt sich die Frage, ob und in welchem Umfang es in Zukunft für Institutionen, Gremien oder auch Einzelakteur*innen überhaupt möglich sein wird, die Genese von Informationen umfassend zu recherchieren und nachzuvollziehen, und welchen Einfluss diese Unsicherheiten auf Prozesse der Wissensproduktion haben werden.

            

        

            

                
Einleitung

                
Für eine theoriegeleitete Analyse von Texten muss man eine Operationalisierung finden, die die theoretischen Konzepte im Text identifizierbar und damit messbar macht. Dies gilt in nicht-digitalen Forschungskontexten gleichermaßen wie in den Digital Humanities, allerdings ist in letzteren das Problem virulenter. Dies hat Moretti (2013) bereits prominent festgestellt. Jenseits von konkreten Fragestellungen (wie etwa von Moretti selbst oder in Fischer &amp; Trilcke 2016) fehlen allerdings generell umsetzbare Vorschläge.

                
In diesem Beitrag wollen wir an einem literaturwissenschaftlichen Anwendungsfall zeigen, wie die angewandte Kategorientheorie Operationalisierung als einen deutlich(er) definierten Workflow ermöglicht, mit dem man von einer theoretischen Grundlage über ein Modell zur Textanalyse – und zurück – kommen kann. Durch den theorieorientierten Fokus der operationalisierten Konzepte, ihre höhere Granularität sowie ihre Kompositionalität bietet der Workflow zudem eine Reihe von Vorteilen, die computationelle Analysen zugleich besser und einfacher machen können.

            

            

                
Angewandte Kategorientheorie

                
Um den Anforderungen sowohl der geisteswissenschaftlichen als auch der informatischen Komponenten der Digital Humanities gerecht zu werden, ist es nötig, einen möglichst flexible und abstrakte – d.h. inhaltsagnostische – Grundlage zu finden. Wir greifen bei unserem Unterfangen auf die angewandte mathematische Kategorientheorie zurück, da sie die Möglichkeit bietet, sowohl die Explikation und Modellierung geisteswissenschaftlicher Fragestellungen zu unterstützen als auch, die Anschlussfähigkeit an informatische Methoden zu gewährleisten (Ehrig 2001).

                
Die Kernidee der mathematischen Kategorientheorie ist es, beliebige Strukturen als Sammlungen von Objekten und ihren wechselseitigen Beziehungen zueinander zu charakterisieren. Im einfachsten Fall besteht eine Kategorie 
                    

                        

                            
C

                        

                    
 aus einer Klasse von Objekten 
                    

                        

                            
O

                            

                                

                                    
b

                                

                                

                                    
C

                                

                            

                        

                    
 und einer Menge von Beziehungen, oder Morphismen, 
                    

                        

                            
M

                            
o

                            

                                

                                    
r

                                

                                

                                    
C

                                

                            

                        

                    
 zwischen je zwei Objekten 
                    

                        

                            
A

                            
,

                            
B

                            
∈

                            
O

                            

                                

                                    
b

                                

                                

                                    
C

                                

                            

                        

                    
. Des Weiteren wird eine individuelle Beziehung als Morphismus 
                    

                        

                            
f

                            
∈

                            
M

                            
o

                            

                                

                                    
r

                                

                                

                                    
C

                                

                            

                            

                                

                                    
A

                                    
,

                                    
B

                                

                            

                        

                    
 bezeichnet und das Objekt 
                    

                        

                            
A

                        

                    
 Quelle (oder 
                    
domain
) und 
                    

                        

                            
B

                        

                    
 Ziel (oder 
                    
codomain
) genannt. Objekte und Morphismen lassen sich nun mithilfe von Pfeilen darstellen 
                    

                        

                            
f

                            
:

                            
A

                            
→

                            
B

                        

                    
. Die Darstellung komplexer Strukturen wird durch Komposition von Morphismen erreichen, d.h. durch Anfügen weiterer Objekte und Pfeile 
                    

                        

                            
A

                            
→

                            
B

                            
→

                            
C

                        

                    
. Die so konstruierten Verknüpfungen lassen sich diagrammatisch als gerichtete Graphen darstellen.
                

                
Durch wechselseitiges Ersetzen – markiert durch einen Asterisk (*) – von Morphismen oder Objekten durch Strukturen können Abstraktionsebenen integriert oder Modellierungen mit mehr Details angereichert werden:



                    
Mehr Details: Ein Objekt oder ein Pfeil wird rekursiv durch weitere Pfeile und Objekte ersetzt:


                            
Ersetzung des Objekts 
                                

                                    

                                        
B

                                    

                                
: 
                                

                                    

                                        
A

                                        
→

                                        
B

                                        
⇒

                                        
A

                                        
→

                                        
[

                                        
*

                                        
 

                                        
→

                                        
 

                                        
*

                                        
]

                                    

                                

                            

                            
Ersetzung des Pfeils zwischen 
                                

                                    

                                        
A

                                    

                                
 und 
                                

                                    

                                        
B

                                    

                                
: 
                                

                                    

                                        
A

                                        
→

                                        
B

                                        
⇒

                                        
A

                                        
 

                                        
[

                                        
→

                                        
 

                                        
*

                                        
 

                                        
→

                                        
]

                                        
 

                                        
B

                                    

                                

                            

                        

                    

                    
Mehr Abstraktion: Ganze Teilstrukturen durch Objekten oder Pfeile ersetzt werden:


                            
Ersetzung durch Objekt: 
                                

                                    

                                        
A

                                        
→

                                        
B

                                        
⇒

                                        
 

                                        
*

                                    

                                

                            

                            
Ersetzung durch Pfeil: 
                                

                                    

                                        
A

                                        
→

                                        
B

                                        
→

                                        
C

                                        
⇒

                                        
A

                                        

                                            

                                                
→

                                            

                                            

                                                
*

                                            

                                        

                                        
C

                                    

                                

                            

                        

                    

                

                
Der ursprüngliche Zweck der mathematischen Kategorientheorie war der Vergleich mathematischer Theorien (Mac Lane 2010), um so strukturelle Ähnlichkeiten zwischen unterschiedlichen mathematischen Disziplinen zu entdecken und sie zu vereinheitlichen. Um dies zu ermöglichen, werden Objekte rein formal, d.h. unter Absehung des konkreten Inhalts, betrachtet, was es uns erlaubt, auch nicht-mathematische Gegenstände zu modellieren. Hinzu kommt, dass dieser hohe Abstraktionsgrad die Abbildbarkeit verschiedener Theorien – oder in unserem Fall: Konzepte – aufeinander und damit auch ihren Vergleich ermöglicht.

                
Wir verwenden Elemente der angewandten Kategorientheorie als Gerüst, um klar und präzise darzustellen, worüber wir sprechen. Diese Darstellung ist nicht reduktionistisch in dem Sinne, dass wir behaupten, Literaturtheorie könne letztlich durch mathematische Strukturen ersetzt werden. Vielmehr zielen wir darauf ab, literarische Konzepte zu explizieren (vgl. Carnap 1945; Dutilh Novaes 2017) und zu operationalisieren. Mit Operationalisierung ist hier lediglich der Prozess oder Workflow gemeint, um Konflikte in literarischen Texten zu identifizieren und zu analysieren (vgl. Pichler &amp; Reiter 2021). Diese Form der Operationalisieren soll nicht mit Operationalismus verwechselt werden, d. h. der streng positivistischen Vorstellung von Operationalisierung, die Bedeutung mit empirischen Operationen gleichsetzt.

            

            

                
Ein Framework für die Operationalisierung von Analysekonzepten

                
Ziel dieses Beitrags ist es, ein Framework zur Operationalisierung von Analysekonzepten am Beispiel von literarischen Konflikten zu skizzieren. Dieses Framework besteht im wesentlichen aus drei Formalisierungs- bzw. Abbildungsschritten. Hierzu entwickeln wir einen der mathematischen Kategorientheorie entlehnten Formalismus, der es erlaubt drei Ebenen zu integrieren: 1. Theorie, 2. Modell und 3. Text. Wir zeigen die Schritte im Folgenden am Beispiel einer Analyse von Konflikten in literarischen Texten.

                

                    
Theorie

                    
Die Aufgabe einer Theorie literarischer Konflikte ist es, festzulegen, unter welchen Bedingungen ein Konflikt vorliegt. Nun gibt es zahlreiche Möglichkeiten, Konfliktkonzepte in der literaturwissenschaftlichen Textanalyse zu nutzen. Für diesen Beitrag fokussieren wir uns auf eine Analyse von Konflikten zwischen Figuren und nutzen ein Konzept aus den Sozialwissenschaften, welches bereits in Gius (2015) im Rahmen einer narratologischen Analyse erprobt wurde. Im Sinne des 
                        
Principle of Minimal Departure
 gehen wir davon aus, dass das Konzept des realen sozialen Konflikts auch in fiktionalen Welten eine gewisse Gültigkeit hat. Entsprechend halten wir uns an die Definition von Glasl (2011):
                    

                    
Sozialer Konflikt ist eine Interaktion
                        
 - zwischen Aktoren (Individuen, Gruppen, Organisationen usw.),
                        
 - wobei wenigstens ein Aktor
                        
 - eine Differenz bzw. Unvereinbarkeiten
                        
 im Wahrnehmen
                        
 und im Denken bzw. Vorstellen
                        
 und im Fühlen
                        
 und im Wollen
                        
 - mit dem anderen Aktor (den anderen Aktoren) in der Art erlebt,
                        
 - dass beim Verwirklichen dessen, was der Aktor denkt,
                        
 fühlt oder will eine Beeinträchtigung
                        
 - durch einen anderen Aktor (die anderen Aktoren) erfolge. (Glasl 2011, 17)
                    

                    
Um diese informelle Definition zu operationalisieren und einen Konflikt 
                        

                            

                                
K

                            

                        
 formell zu definieren, betrachten wir die darin verwendeten Entitäten und die Beziehungen zwischen ihnen. Glasl definiert die Klasse Aktor, die aus Individuen, Gruppen, Organisationen oder anderen Objekten bestehen kann, und legt fest, dass es mindestens zwei Aktoren gibt. Wir definieren folglich die Objekte eines Konflikts 
                        

                            

                                
O

                                

                                    

                                        
b

                                    

                                    

                                        
K

                                    

                                

                            

                        
 als die Menge der Aktoren: 
                        

                            

                                
A

                                
=

                                
{

                                
x

                                
|

                                
x

                                
=

                                
I

                                
n

                                
d

                                
i

                                
v

                                
i

                                
d

                                
u

                                
u

                                
m

                                
∨

                                
x

                                
=

                                
G

                                
r

                                
u

                                
p

                                
p

                                
e

                                
∨

                                
x

                                
=

                                
O

                                
r

                                
g

                                
a

                                
n

                                
i

                                
s

                                
a

                                
t

                                
i

                                
o

                                
n

                                
∨

                                
…

                                
}

                            

                        
 und 
                        

                            

                                

                                    

                                        
A

                                    

                                

                                
≥

                                
2

                            

                        
.
                    

                    
Glasl charakterisiert zwei Beziehungen zwischen den Aktoren, erlebte Unvereinbarkeit 
                        

                            

                                
u

                            

                        
 und Beeinträchtigung 
                        

                            

                                
b

                            

                        
, sodass 
                        

                            

                                
u

                                
,

                                
b

                                
∈

                                
M

                                
o

                                

                                    

                                        
r

                                    

                                    

                                        
K

                                    

                                

                            

                        
. Zwischen zwei Aktoren 
                        

                            

                                

                                    

                                        
A

                                    

                                    

                                        
1

                                    

                                

                                
,

                                

                                    

                                        
A

                                    

                                    

                                        
2

                                    

                                

                                
∈

                                
O

                                

                                    

                                        
b

                                    

                                    

                                        
K

                                    

                                

                            

                        
 gibt es dabei eine mindestens eine erlebte Unvereinbarkeit 
                        

                            

                                
u

                                
:

                                

                                    

                                        
A

                                    

                                    

                                        
1

                                    

                                

                                
→

                                

                                    

                                        
A

                                    

                                    

                                        
2

                                    

                                

                            

                        
, wobei:
                    

                    

                        

                            

                                
u

                                
 

                                
=

                                
 

                                

                                    

                                        

                                            

                                                

                                                    
W

                                                    
a

                                                    
h

                                                    
r

                                                    
n

                                                    
e

                                                    
h

                                                    
m

                                                    
e

                                                    
n

                                                

                                            

                                            

                                                

                                                    
D

                                                    
e

                                                    
n

                                                    
k

                                                    
e

                                                    
n

                                                    
 

                                                    
b

                                                    
z

                                                    
w

                                                    
.

                                                    
 

                                                    
V

                                                    
o

                                                    
r

                                                    
s

                                                    
t

                                                    
e

                                                    
l

                                                    
l

                                                    
e

                                                    
n

                                                

                                            

                                            

                                                

                                                    
F

                                                    
ü

                                                    
h

                                                    
l

                                                    
e

                                                    
n

                                                

                                            

                                            

                                                

                                                    
W

                                                    
o

                                                    
l

                                                    
l

                                                    
e

                                                    
n

                                                

                                            

                                        

                                    

                                

                                
 

                            

                        

                    

                    
Zwischen den zwei Aktoren gibt es mindestens eine Beeinträchtigung des Verwirklichens 
                        

                            

                                
b

                                
:

                                

                                    

                                        
A

                                    

                                    

                                        
2

                                    

                                

                                
→

                                

                                    

                                        
A

                                    

                                    

                                        
1

                                    

                                

                            

                        
, wobei:
                    

                    

                        

                            

                                
b

                                
 

                                
=

                                
 

                                

                                    

                                        

                                            

                                                

                                                    
D

                                                    
e

                                                    
n

                                                    
k

                                                    
e

                                                    
n

                                                

                                            

                                            

                                                

                                                    
F

                                                    
ü

                                                    
h

                                                    
l

                                                    
e

                                                    
n

                                                

                                            

                                            

                                                

                                                    
W

                                                    
o

                                                    
l

                                                    
l

                                                    
e

                                                    
n

                                                

                                            

                                        

                                    

                                

                                
 

                            

                        

                    

                    
Des Weiteren soll sich die Unvereinbarkeit 
                        

                            

                                
u

                            

                        
 auf die Beeinträchtigung 
                        

                            

                                
b

                            

                        
 beziehen, sodass 
                        

                            

                                
u

                                
⇒

                                
b

                            

                        
.
                    

                    
Ein Konflikt 
                        

                            

                                
K

                            

                        
 kann nun diagrammatisch wie in Abb. 1 dargestellt werden.
                    

                    

                        

                    
Abb. 1: Minimaler einseitiger Konflikt

                

                    
An diesem Diagramm ist eine für die literaturwissenschaftliche Analyse schwerwiegende Einschränkung der Theorie Glasls zu sehen: Die Beobachtung des Konflikts ist in der Beziehung zwischen den Aktoren versteckt, d.h. Erzähl- bzw. Beobachtungsinstanzen kommen nicht explizit vor. Dies ist dem Umstand geschuldet, dass Glasl Konfliktbegriff aus den Sozialwissenschaften stammt und entsprechend, anders als in literarischen Texten, die Konfliktparteien zu ihrer Einschätzung befragt werden können. Da es das Ziel unserer Operationalisierung ist, den Konfliktbegriff so zu explizieren, dass auch komplexe Erzählsituationen erfasst werden können, erweitern wir 
                        

                            

                                
O

                                

                                    

                                        
b

                                    

                                    

                                        
K

                                    

                                

                            

                        
 um Beobachtungsinstanz 
                        

                            

                                
B

                            

                        
, die in einer Wahrnehmungsbeziehung 
                        

                            

                                
w

                            

                        
 zu 
                        

                            

                                

                                    

                                        
A

                                    

                                    

                                        
1

                                    

                                

                            

                        
 und 
                        

                            

                                

                                    

                                        
A

                                    

                                    

                                        
2

                                    

                                

                            

                        
 steht und die Geschehnisse erzählerisch vermittelt (s. Abb. 2):
                    

                    

                        

                    
Abb. 2: Minimaler, um Beobachter erweiterter, Konflikt

                

                    
Im paradigmatischen Fall des von 
                        

                            

                                

                                    

                                        
A

                                    

                                    

                                        
1

                                    

                                

                            

                        
 wahrgenommen Konflikts sind 
                        

                            

                                

                                    

                                        
A

                                    

                                    

                                        
1

                                    

                                

                            

                        
 und 
                        

                            

                                
B

                            

                        
 identisch. Die Erweiterung der Theorie um beliebig viele weitere Beobachter:innen (als erzählte Figuren, Erzählinstanzen) erlaubt es, auch komplexe Narrationen zu erfassen bzw. die Darstellung der Konflikthaftigkeit durch andere Erzählinstanzen als die Konfliktbeteiligten zu berücksichtigen.
                    

                    
Der erweiterte Konfliktbegriff umfasst nunmehr als Objekte 
                        

                            

                                
O

                                

                                    

                                        
b

                                    

                                    

                                        
K

                                    

                                

                            

                        
 die Klasse der Aktoren 
                        

                            

                                

                                    

                                        
A

                                    

                                    

                                        
1

                                    

                                

                                
,

                                

                                    

                                        
A

                                    

                                    

                                        
2

                                    

                                

                                
,

                                
…

                                
,

                                

                                    

                                        
A

                                    

                                    

                                        
n

                                    

                                

                            

                        
 und die Klasse der Beobachter 
                        

                            

                                

                                    

                                        
B

                                    

                                    

                                        
1

                                    

                                

                                
,

                                
…

                                
,

                                

                                    

                                        
B

                                    

                                    

                                        
n

                                    

                                

                            

                        
 sowie die Beziehungen 
                        

                            

                                
u

                                
:

                                
A

                                
→

                                
A

                                
∈

                                
O

                                

                                    

                                        
b

                                    

                                    

                                        
K

                                    

                                

                            

                        
, 
                        

                            

                                
b

                                
:

                                
A

                                
→

                                
A

                                
∈

                                
O

                                

                                    

                                        
b

                                    

                                    

                                        
K

                                    

                                

                            

                        
 und 
                        

                            

                                
w

                                
:

                                
B

                                
→

                                
A

                                
∈

                                
O

                                

                                    

                                        
b

                                    

                                    

                                        
K

                                    

                                

                            

                        
. Zusammen mit der Möglichkeit, dass Aktoren auch Vermittler sein können, ergeben sich vielfältige Modellierungsmöglichkeiten.
                    

                

                

                    
Modell

                    
Ein Modell eines Konflikts liegt dann vor, wenn sich in literarischen Texten Kandidaten für Objekte und Morphismen finden, sodass es eine strukturerhaltende Abbildung auf den oben explizierten, erweiterten Konfliktbegriff 
                        

                            

                                
K

                            

                        
 gibt. Eine naheliegende Modellierung ist die Abbildung von Elementen der fiktiven Welt, etwa durch das Einsetzen von Figurenkonstellationen (
                        

                            

                                

                                    

                                        
F

                                    

                                    

                                        
1

                                    

                                

                                
,

                                
…

                                
,

                                

                                    

                                        
F

                                    

                                    

                                        
n

                                    

                                

                            

                        
) als Aktoren und von Erzählinstanzen (
                        

                            

                                

                                    

                                        
B

                                    

                                    

                                        
I

                                    

                                

                            

                        
) als Beobachtungsinstanzen. Eine Figurenkonstellation kann wieder als Tripel aus zwei Figuren und einer Relation konstruiert werden. So stehen in Kleists 
                        
Der Zweikampf
 Herzog Wilhelm von Breysach und sein Bruder, Graf Jakob, in der Beziehung Erbfolgestreit zueinander. Dieser Erbfolgestreit weist alle Merkmale nach der Definition von Glasl auf und wird primär durch eine unbeteiligte Erzählinstanz wiedergegeben (es gibt auch Passagen, in denen die Konfliktbeteiligten selbst über den Konflikt erzählen, insgesamt überwiegt jedoch eindeutig die Darstellung durch die Erzählinstanz). Eine Analyse dieses Verhältnisses besteht in der inhaltlichen Zuordnung der Teile der Figurenkonstellation und der Vermittlungsinstanz zu den Objekten und Morphismen der Theorie:
                    

                    

                        

                            

                                

                                    

                                        
A

                                    

                                    

                                        
1

                                    

                                

                                
→

                                
J

                                
a

                                
k

                                
o

                                
b

                            

                        

                    

                    

                        

                            

                                

                                    

                                        
A

                                    

                                    

                                        
2

                                    

                                

                                
→

                                
B

                                
r

                                
e

                                
y

                                
s

                                
a

                                
c

                                
h

                            

                        

                    

                    

                        

                            

                                
B

                                
→

                                
U

                                
n

                                
b

                                
e

                                
t

                                
e

                                
i

                                
l

                                
i

                                
g

                                
t

                                
e

                                
 

                                
E

                                
r

                                
z

                                
ä

                                
h

                                
l

                                
i

                                
n

                                
s

                                
t

                                
a

                                
n

                                
z

                            

                        

                    

                    

                        

                            

                                
u

                                
→

                                
U

                                
n

                                
v

                                
e

                                
r

                                
e

                                
i

                                
n

                                
b

                                
a

                                
r

                                
e

                                
r

                                
 

                                
H

                                
e

                                
r

                                
r

                                
s

                                
c

                                
h

                                
a

                                
f

                                
t

                                
s

                                
a

                                
n

                                
s

                                
p

                                
r

                                
u

                                
c

                                
h

                            

                        

                    

                    

                        

                            

                                
b

                                
→

                                
V

                                
e

                                
r

                                
a

                                
n

                                
l

                                
a

                                
s

                                
s

                                
u

                                
n

                                
g

                                
 

                                
d

                                
e

                                
r

                                
 

                                
Ä

                                
n

                                
d

                                
e

                                
r

                                
u

                                
n

                                
g

                                
 

                                
d

                                
e

                                
r

                                
 

                                
E

                                
r

                                
b

                                
f

                                
o

                                
l

                                
g

                                
e

                                
r

                                
e

                                
g

                                
e

                                
l

                                
u

                                
n

                                
g

                            

                        

                    

                    

                        

                    
Abb. 3: Diagramm Erbfolgestreit in Kleists “Der Kampf”

                

                    
Figurenkonstellationen und Beobachtungs- bzw. Erzählinstanzen können aufgrund ihrer Kombinierbarkeit sehr flexibel modelliert werden; ebenso lassen sich weitere Konflikttheorien integrieren, die auf andere Aspekte abstellen, und weitere Objekte, wie gesellschaftlichen Wandel, oder andere Relationstypen zwischen den Objekten einführen. So könnte man Dahrendorfs (1972) Unterscheidung zwischen latenten und manifesten Konflikten durch zusätzliche Bedingungen integrieren, u.a. dadurch, dass beide oder keine der Konfliktparteien den Konflikt wahrnehmen. Oder man kann Definitionen psychischer Konflikte integrieren, indem man u.a. die Identität 
                        

                            

                                

                                    

                                        
A

                                    

                                    

                                        
1

                                    

                                

                                
=

                                

                                    

                                        
A

                                    

                                    

                                        
2

                                    

                                

                            

                        
 annimmt. Man könnte auch das Modell auf extratextuelle Beobachtungsinstanzen erweitern und etwa die Einschätzung der Konflikthaftigkeit nicht anhand von deren Vermittlung, sondern anhand der Einschätzung von Leser:innen modellieren.
                    

                

                

                    
Text

                    
Auf der Textebene gilt es jene Textphänomene zu bestimmen, welche auf die Figurenkonstellationen und Erzählinstanzen abgebildet werden können. Diese sollen schließlich in der Analyse manuell oder automatisch identifiziert werden. Es geht also um die Bestimmungen von am Konflikt beteiligten Figuren und ihn wahrnehmenden Instanzen und die weiteren, zu Unvereinbarkeiten bzw. wahrgenommener Einschränkung führenden Aspekte. Für jeden Aspekt muss in einer Analyse festgelegt werden, wie er im Text realisiert werden kann. Die Operationalisierung auf der Textebene hängt auch vom anvisierten Untersuchungsmodus ab und kann auf sehr unterschiedliche Weisen erfolgen. So kann es sinnvoll sein, die Figuren als per NER-Analyse erkennbare Personen-Entitäten zu fassen – oder diese entsprechend zu erweitern –, wenn man eine automatisierte Analyse anstrebt. In einer manuellen Analyse ist es vermutlich eher möglich, Figuren als komplexere Phänomene zu fassen und auch Charakterzüge u.ä. zu annotieren. Bei der wahrgenommenen Unvereinbarkeit könnte der Fokus auf repräsentierte Prozesse des Denkens und Fühlens etc. liegen, die zumindest teilweise automatisch erkannt werden können. Oder es könnte eine Sentimentanalyse in Abhängigkeit von den Erzählinstanzen und den beteiligten Figuren durchgeführt werden, bei welcher automatische und manuelle Verfahren kombiniert werden können. Eventuell bietet es sich auch an, Indikatoren für Konflikthaftigkeit herauszuarbeiten.

                    
Unabhängig von den gewählten Phänomenen und ihrer Bestimmung im Text gilt: Durch Gruppierung von Textphänomenen in hierarchischen Tagsets und Kategoriensystemen können Texteigenschaften der Modellebene zugeordnet werden (vgl. Abb. 2, in der 
                        

                            

                                

                                    

                                        
T

                                    

                                    

                                        
1

                                    

                                

                                
,

                                
…

                                
,

                                

                                    

                                        
T

                                    

                                    

                                        
n

                                    

                                

                            

                        
 die Textphänomene, 
                        

                            

                                

                                    

                                        
F

                                    

                                    

                                        
1

                                    

                                

                                
,

                                
…

                                
,

                                

                                    

                                        
F

                                    

                                    

                                        
n

                                    

                                

                            

                        
 die Elemente der fiktiven Welt und 
                        

                            

                                

                                    

                                        
B

                                    

                                    

                                        
I

                                    

                                

                            

                        
 die Beobachtungs- oder Erzählinstanz bezeichnen):
                    

                    

                        

                    
Abb. 4: Diagramm Zuordnung zwischen theoretischem Konzept, Modell und Textphänomenen

                

                

            

            

                
Integration der Ebenen

                
Das hier vorgeschlagene Framework erlaubt es, den Workflow für die Operationalisierung auf jeder der drei Ebenen zu beginnen und bei Bedarf auf eine der anderen Ebenen zu wechseln. Es gibt keine starre Beschränkung des Workflows auf top-down, middle-out oder bottom-up. Vielmehr ist es je nach Fragestellung möglich, in die Tiefe zu gehen und mehr Daten und Details einzuarbeiten oder zu abstrahieren, um größere Strukturen sichtbar zu machen. Das bedeutet, dass die genutzte Konfliktdefinition ebenso verändert werden kann wie die Umsetzung der Objekte und Relationen für literarische Texte (wie wäre es etwa, wenn man die Definition auf eine Analyse von poetologischen “Konflikten” zwischen Autor:innen ausweiten wollte?) oder deren Realisierung auf der Textebene. Dabei kann die Änderung auf einer der Ebenen jeweils Anpassungen auf den anderen Ebenen nötig machen.

            

            

                
Abschließende Bemerkungen

                
Mit dieser Formalisierung eines Konfliktbegriffs für die Literaturwissenschaft und seiner skizzierten Überführung in ein Modell, welches in der Textanalyse genutzt werden kann, haben wir gezeigt, wie man ausgehend von einer Theorie zu ihrer Anwendung auf Texte gelangen kann. Der Prozess ist – wie die angewandte Kategorientheorie selbst auch – ein allgemein anwendbarer Workflow für die Operationalisierung von Analysekonzepten für eine quantifizierende und/oder computationell-algorithmische Analyse. Die angewandte Kategorientheorie ermöglicht dabei die in den Digital Humanities im Bereich von computationellen Analysen zwingend notwendige Formalisierung mit Fokus auf die Theorie. Insgesamt sehen wir vier substanzielle Vorteile in diesem Workflow: 

                

                    
Erstens
 spielen in der ausschließlichen Beschäftigung mit der Theorie im ersten Schritt Fragen wie “Was ist überhaupt realistisch computationell umsetzbar?” vorerst keine Rolle. Man kann also vermeiden, dass die genutzte Theorie aus pragmatischen Gründen vorschnell unterkomplex formalisiert wird, indem aus pragmatischen Gründen auf bereits etablierte, relativ gute Analyseverfahren zurückgegriffen wird.
                

                

                    
Zweitens
 erfolgt durch die angewandte Kategorientheorie und die in ihren Prinzipien festgelegte Kompositionalität immer eine feingranulare(re) und entsprechend genauere Formalisierung der zugrundeliegenden Theorie. In dieser sind die entsprechenden Unterkonzepte in Form von Objekten und Morphismen weniger komplex als das Gesamtkonzept. Die Bestimmung dieser einzelnen Elemente ist vergleichsweise einfacher, wobei das sowohl für manuelle als auch für maschinelle Zugänge gilt. Mit auf Kompositionalität beruhenden Operationalisierungen von Teilphänomenen kann man mehr und bessere Daten erzeugen, die zur Analyse genutzt werden, als wenn man versucht, die gesamte Theorie auf einmal anzuwenden. So ist es deutlich einfacher, jeweils zu bestimmen, welche Figur was fühlt und welche Handlungen Figuren vollziehen, und erst anschließend zu sehen, ob dort Konflikthaftigkeit erkennbar ist, als direkt Konflikte im Text zu annotieren.
                

                

                    
Drittens
 ermöglicht die Kompositionalität es, verschiedene Theorien zusammen zu formalisieren. Durch die Identifikation von Objekten und Morphismen auf einer möglichst abstrakten Ebene wird leichter erkennbar, an welchen Stellen weitere Elemente ergänzt oder Elemente ersetzt werden können, um eine andere, ähnliche Theorie mitzuerfassen. In unserem Fall wurde dies etwa beim Wahrnehmungsmorphismus offensichtlich: Während dieser nach Glasl nur zwischen den beteiligten Aktoren besteht, ermöglichte die Einführung weiterer Objekte eine Verknüpfung mit einem literarischen Kommunikationsmodell, welches auch Erzählinstanzen enthält. Dies ist für die Analyse von Texten genauso hilfreich wie für ihre Auswertung, die nun ohne großen Mehraufwand für mehrere Theorien durchgeführt werden kann.
                

                

                    
Viertens
 ist die Form, die eine nach der angewandte Kategorientheorie formalisierte Theorie hat, geeignet, bestehende Tripel-basierte Datenmodellierungen wie etwa RDF und darauf aufbauende Linked Open Data nachzunutzen.
                

            

        

            
Das an der Berlin-Brandenburgischen Akademie der Wissenschaften (BBAW) angesiedelte Akademienvorhaben 
                
Alexander von Humboldt auf Reisen – Wissenschaft aus der Bewegung 
erschließt und ediert die amerikanischen, russisch-sibirischen und europäischen Reisetagebücher des preußischen Naturforschers und Entdeckers Alexander von Humboldt.
 Begleitet werden die Tagebücher von thematisch zugehörigen Briefen aus seinem weltumspannenden Korrespondentennetz sowie von Manuskripten aus seinem umfangreichen Nachlass, von denen viele bis dato nie veröffentlicht wurden. Ergänzt werden diese edierten Texte durch Forschungsbeiträge, eine Chronologie zu Humboldts Leben und umfangreiche Register. Die Publikationsstrategie ist “digital first”, d. h. die veröffentlichten Dokumente erscheinen zuerst online ohne Verzögerung durch ‘Moving Walls’ oder sonstige verlagsseitige Einschränkungen, und die gesamte Ausgabe ist unter einer Creative-Commons-Lizenz
 vollständig frei zugänglich. Mit der Veröffentlichung der ersten Bände der Print-Edition im Verlag Springer Nature/J. B. Metzler
 wurde die Hybridstrategie des Projekts umgesetzt. Realisiert wird die digitale Edition mit ediarum
, das u. a. auf X-Technologien, der freien XML-Datenbank eXistdb und der Software Oxygen XML Author aufsetzt.
            

            
Fünf Jahre nach dem Launch der 
                
edition humboldt digital
 (
                
ehd
) haben wir acht Versionen dieser digitalen, textkritisch-dokumentarischen Edition
 vorgelegt. Mit der aktuellen Version 8 der 
                
ehd
 (veröffentlicht im Mai 2022)
 lösen wir nun das Versprechen ein, Humboldts komplexe handschriftliche und schwer zu entziffernde Texte auch 
                
als Daten
 bereitzustellen, indem wir (1) die kommentierten Texttranskriptionen von mehr als 500 Dokumenten (ca. 2.800 Seiten), (2) die umfassende Alexander von Humboldt-Chronologie mit ca. 1.600 datierten Ereignissen aus Humboldts fast 90-jährigem Leben und (3) ca. 18.000 Indexeinträge (z. B. Personen, Orte, Institutionen, bibliographische Einträge) auf GitHub (Ette et al. 2022) und (ab Winter 2022/23) auf Zenodo zur Verfügung stellen. Alle Datensätze liegen im TEI-XML-Format vor. Dem Single-Source-Prinzip folgend, basieren sowohl die digitale als auch die gedruckte Komponente (Buch, PDF und eBook-Derivate) vollständig auf denselben TEI-XML-kodierten Daten. Das TEI-XML-Subset der 
                
ehd
 wurde durch Übernahme etablierter TEI-Spezifikationen, v. a. des Basisformats für Manuskripte des Deutschen Textarchivs (DTABf-M
; Thomas/Haaf 2016-2019), entwickelt, um ein Höchstmaß an Standardisierung, Nachnutzbarkeit und Interoperabilität der Daten zu gewährleisten (Dumont/Haaf/Kraft/Czmiel/Thomas/Boenig 2016). Eine umfassende Dokumentation der Transkriptions- und Kodierungsrichtlinien steht zur Verfügung
 und kann von anderen, ähnlich gelagerten Projekten nachgenutzt werden
. Gleichzeitig bringen sich die Projektmitarbeiter:innen aktiv in die Verbesserung von bestehenden Richtlinien ein
. Dadurch fließen auf zweierlei Wegen Erfahrungen aus der editorischen Praxis in die Community zurück. 
            

            
Seit Abschluss der Betaphase im Mai 2017 wird die 
                
edition humboldt digital
 versioniert publiziert, d. h. die Daten werden nicht einfach aktualisiert und überschrieben, sondern durch jedes Update (mittlerweile einmal im Jahr) wird eine neue, zusätzliche Datenschicht hinzugefügt. Gleichzeitig werden alle vorangehenden Versionen weiterhin bereitgehalten und lassen sich über die Web-Oberfläche aufrufen – bis hin zu den Registereinträgen mit ihren dynamischen Verlinkungen. Die Datensätze werden in Zukunft immer als neue, zusätzliche Version publiziert. Ergänzt wird diese Versionierung seit 2022 durch die Einführung von “Editionsstufen”, die die unterschiedlichen Bearbeitungszustände systematisch abbilden.
 Zusammen mit der umfangreichen Dokumentation wird damit der gesamte Forschungs- und Editionsprozess offen gelegt und analysierbar. Einen ersten summarischen Überblick über den Fortschritt der 
                
ehd
 gibt die mit Version 8 neu hinzugekommene “Versionsgeschichte”, die die acht publizierten Versionen auch quantitativ auswertet.

            

            
Das nun zur Verfügung gestellte Datenset wird nicht direkt aus der eXistdb-Datenbank exportiert, sondern über die öffentlich zugängliche API abgerufen.
 Das ermöglicht diverse Optimierungen am Datenbestand für die externe Nachnutzung. So werden z. B. alle projektinternen IDs durch URIs aus Normdateien ersetzt – sofern eine solche in den einschlägigen Normdaten-Beständen vorhanden ist. Ist dies nicht der Fall, werden die projektinternen IDs als vollständige URIs ausgegeben und so immerhin technische Interoperabilität gewährleistet.
            

            
Die Registereinträge werden, wo immer verfügbar, mit URIs aus Normdateien versehen. Insbesondere das Personenregister weist dabei großes Potenzial für die unmittelbare Nachnutzung auf, da zahlreiche historische Personen in Humboldts Texten noch nicht in der wichtigsten Normdatei für die deutschsprachige Forschungsgemeinschaft, der GND
 der Deutschen Nationalbibliothek, dokumentiert sind.
 Diese ergänzenden Daten können dazu beitragen, die Normdateien der Community, wie GND, Wikidata etc. zu verbessern. Dabei stellen sich unterschiedlich große Hürden für die Zuarbeit: während Wikidata von Prinzip her ein offenes Communityprojekt ist, ist die GND institutionell angesiedelt und wird redaktionell betreut. Die Zuarbeit zur GND wurde im Projekt GND4C grundsätzlich für nicht-bibliothekarische Bereiche geöffnet, in naher Zukunft sollen die Ergänzungsmöglichkeiten seitens des Editionsvorhaben ausgelotet werden. Leider stehen solche Zuarbeiten immer unter dem Vorbehalt der Projektkapazitäten, da sie bei der 
                
ehd
 – ebenso wie in den meisten anderen Projekten – eigentlich nicht vorgesehen sind.
            

            
Die API selbst bietet nicht ‘nur’ den Volltext an, sondern ebenfalls diverse Metadaten, um eine umfassende Vernetzung der Edition zu gewährleisten. So bietet eine Schnittstelle die Metadaten zu den edierten Texten und Forschungsbeiträgen unter anderem im Dublin Core-Format via OAI-PMH an. Dadurch werden alle diese Texte in der Open Access-Suchmaschine BASE nachgewiesen. Daneben werden eine BEACON-Schnittstelle und eine CMIF-Schnittstelle für correspSearch (Dumont, Grabsch und Müller-Laackman 2021) angeboten, die mittlerweile zu den ‘klassischen’ Ausstattungen einer digitalen Edition zählen dürfen. Eine Schnittstelle ins Semantic Web gibt es derzeit noch nicht.
 Grund dafür ist v.a., dass andere Schnittstellen und vor allem Funktionen der 
                
ehd
 bisher im Fokus der Entwicklungsarbeit standen. Das Vorhaben läuft bis 2032 und lässt es daher grundsätzlich zu, in Zukunft auch diesen Bereich anzugehen. Denkbar wäre es, z.B. die Einträge der Chronologie oder der Register noch stärker zu schematisieren und tiefer zu kodieren, um dieses Wissen als Linked Open Data bereitzustellen. Das würde aber nicht nur Entwicklungsaufwand bedeuten, sondern auch erhebliche redaktionelle Arbeit, für die entsprechende Ressourcen geschaffen werden müssten. Die edierten Texte an sich werden für solch eine zusätzliche Aufbereitung - im Sinne einer “assertive edition” (Vogeler 2019) - wohl leider nicht in Frage kommen, dafür wäre der Aufwand (gegenüber dem Projektplan) zu hoch.
            

            

                

                
Abb. 1: Screenshot der Alexander von Humboldt-Chronologie in der 
                
edition humboldt digital
 mit eingeblendeten Daten aus dem Hofkalendarium 1848.

            

            
Die 
                
edition humboldt digital
 stellt nicht nur ihre eigenen Daten zur Verfügung, sondern nutzt auch andere Daten nach und verwendet externe Webservices zur Anreicherung oder zur Vergrößerung des eigenen Funktionsumfangs. So werden die TEI-XML-Dateien zur Lemmatisierung an den Webservice DTA::CAB
 (Jurish 2011) geschickt, dort analysiert und angereichert und wieder zurück in die Datenbank gespeichert. Dadurch wird eine lemmabasierte Suche ermöglicht, die die unterschiedlichen Schreibweisen und Flektionsformen bei der Suche abfangen kann.

            

            
Abgesehen von der Suche ist vor allem die Chronologie zu Alexander von Humboldts Leben ein zentraler Vernetzungs- und Integrationspunkt externer Ressourcen. Diese Chronologie wurde bereits in den 1960er Jahren an der damaligen Deutschen Akademie der Wissenschaften zu Berlin begonnen, in den 2000ern überarbeitet als HTML-Version im Web veröffentlicht und 2015/16 schließlich in TEI-XML überführt und in die 
                
edition humboldt digital
 integriert. Dort wird sie fortlaufend gepflegt, erweitert, mit externen Quellen sowie mit den edierten Texten und Registereinträgen verlinkt. Darüber hinaus integriert sie automatisiert verschiedene externe Angebote und Dienste in die Edition, wie z. B. die Metadaten der publizierten Korrespondenz Alexander von Humboldts aus correspSearch
, Schriften Humboldts aus dem Deutschen Textarchiv
 und Einträge aus dem Hofkalendarium der preußischen Monarchie
 mit Bezug zu Humboldts Leben (siehe Abb. 1). Dadurch öffnet die Chronologie die 
                
edition humboldt digital
 nach außen hin zu zahlreichen extern vorliegenden Materialien und Informationen.
            

            
Auch in den Registern werden externe Daten nachgenutzt. Zum einen werden die BEACON-Schnittstellen anderer ausgewählter digitaler Publikationen, wie die Kosmos-Vorlesungen Alexander von Humboldts im Deutschen Textarchiv, abgerufen und automatisiert verknüpft.
 Darüber hinaus werden ganze Datensätze der GND (also nicht nur die bloßen URIs) nachgenutzt. Mit ihrer Hilfe werden die Registereinträge der 
                
ehd
 automatisiert 
                
untereinander
 verlinkt – nämlich anhand der in der GND notierten familiären und freundschaftlichen Beziehungen. Außerdem können so Porträts von Wikimedia eingebunden werden. Ein Register, das in besonders großem Maße auf externe Dienste zurückgreift, ist das Pflanzenregister. Hier liegt in der Edition die Besonderheit vor, dass die Pflanzen nicht mit eigenen Registereinträgen versehen werden, sondern dieses Register ausschließlich automatisch über die taxonomischen Namen generiert wird. Dazu werden alle Pflanzennamen in den Transkriptionen von den Editor:innen auf ihren regulären Namen ergänzt oder korrigiert (natürlich nachverfolgbar). Anhand der taxonomischen Namen werden dann verschiedene Webservices abgefragt und automatisiert verknüpft. Damit öffnet die Edition v. a. die Tagebücher Humboldts für verschiedene Disziplinen wie die Biodiversitätsforschung.
            

            
Mit der intensiven Nachnutzung externer Webservices und Daten erhöht sich der Nutzen einer digitalen Edition signifikant. Gleichzeitig stellt diese Nachnutzung neue Probleme und Herausforderungen an die Entwicklung und den Betrieb digitaler Editionen, da externe Dienste sich grundsätzlich ändern können (aktualisierte Schnittstellen, Änderungen im Format etc.). Das - und Fragen der Performance - führt dazu, dass diese externen Daten auch in der 
                
edition humboldt digital
 vorgehalten werden müssen. Fraglich ist dann aber weiter, ob und in welchem Rahmen diese externen Daten auch in der Datenpublikation mitveröffentlicht werden können und müssen. Darüber hinaus kann man nicht garantieren, dass ein externer Dienst auch in Zukunft verfügbar sein wird. Das ist insbesondere ein Problem vor dem Hintergrund der relativ langen Laufzeit des Projekts: Werden die externen Services zur Anreicherung von Texten und Daten, die erst in den nächsten Jahren hinzukommen, noch vorhanden sein? Der Vortrag möchte am Beispiel der 
                
edition humboldt digital
 diese und weitere Herausforderungen und Chancen einer ‘offenen Edition’ vorstellen und die damit zusammenhängenden, skizzierten Themenfelder Datenpublikation, Bereitstellung und Nachnutzung von APIs und externen Daten sowie Transparenz im Editions- und Forschungsprozess diskutieren.
            

        

            

                
Introduction

                

                    
Due to copyright law, Text and Data Mining (TDM) with copyrighted texts faces a lot of restrictions in terms of storage, publication and follow-up use of the resulting corpora, which, however, is against the spirit of open data in digital humanities (DH). As a solution to the problem, the concept of derived text formats (DTFs) has been suggested and discussed (see e.g., 
                    
Lin et al. 2012
, 
                    
Bhattacharyya et al. 2015
, 
                    
Jett et al. 2020
, 
                    
Schöch et al. 2020
). In DTFs, although some information (primarily copyright-relevant features) has been removed from the texts, the texts can still be used for various relevant TDM tasks in DH, such as authorship attribution or topic modeling. 
                    
Schöch et al. (2020)
 also provides a very detailed examination of several DTFs from the perspectives of Computational Literary Studies, Computer Science, memory institutions and law. DTFs are extremely meaningful for the DH community, because they match the spirit of open data and make it possible for researchers and libraries to provide more text data for DH research. It also supports the pursuit of open science by encouraging researchers to publish their research data without worrying about violating copyright laws. 
                

                
However, as far as I know, there is not much research dedicated to the question, how much the loss of information caused by DTFs affects the TDM results. 
                    
Eder (2013)
 presented an empirical study of verifying the impact of unwanted noise in texts on authorship attribution and emphasizing that the usefulness of damaged texts should not be underestimated in stylometric studies. He brought noise into texts by (a) randomly replacing a portion of characters, (b) increasing standard deviation of word counts and (c) randomly replacing original words with other words in the same corpus, in order to show the correlation between a dirty corpus and the attribution accuracy. The presented paper did a similar empirical study by transforming texts into token-based DTFs and provide a review on the correlation between information loss caused by these DTFs and the loss of accuracy in authorship classification.
                

            

            

                
Token-based DTFs

                
In 
                    
Schöch et al. (2020)
, three kinds of token-based DTFs are introduced, to enable the free reusability of text data: 
                



                    
Simple document-term-matrix: The idea is to transform a corpus into a matrix, which only saves the frequency of each term in each text in the corpus.

                    
Sequence randomization in segments: The idea is to split a text into segments, randomize the sequence of words in each segment, and reassemble all the segments into a text. 

                    
Selectively reduced information on individual tokens: The idea is to replace a portion of the words (e.g., all the function words) in text with their POS-tags.

                

                
Applying the first and the second DTFs to frequency-based authorship attribution does not present any challenge. Take the most well-known method in authorship attribution Burrows’s Delta (
                    
Burrows, 2022
) as an example: Delta test follows the “bag of words” model for representing documents and only requires the frequency of each word in each text to distinguish between authors. The sequence information of words in texts is not necessary. Therefore, the first and the second DTFs keep all the information needed for the Delta test and the transformation does not affect the test results. As a matter of fact, if one only wants to publish a corpus so that the reported classification results of authorship could be verified, all one must do is to publish the document-term-matrix and the metadata table of the corpus. 
                

                
In comparison, if the texts are transformed into the third DTF, although the frequency information of some words in the text will not match the original situation, the sequence information of words could be kept. This opens the possibility of using the data in this form for other TDM tasks such as sentiment analysis or named entity recognition that require the sequence information of words. If a corpus is published with the expectation that it can be applied to multiple TDM tasks, it makes more sense to prepare the corpus in this format. And of course, it is important to understand how much this format will affect the outcome of different TDM tasks. Therefore, this paper evaluates the usefulness of the third token-based DTF on authorship attribution as a start. In the next sections, the method and the results of the evaluation are reported. 

            

            

                
Method

                
For the evaluation, three corpora representing different languages and text types have been constructed: deu_DraCor (German plays), fra_ELTeC (French novels) and eng_RSC (English journal articles). The relevant information about the corpora is shown in Table 1. 

                

                    

                        
corpus

                        
corpus size (million words)

                        

                            
average text length (words)

                        

                        
no. of texts

                        
no. of authors

                        
period

                        
language

                        
text type

                    

                    

                        

                            
deu_DraCor (

                            
Fischer et al., 2019

                            
)

                        

                        
5.69

                        
18237

                        
312

                        
55

                        
1650 - 1928

                        
German

                        
play

                    

                    

                        

                            
fra_ELTeC (

                            
Odebrecht et al., 2021

                            
)

                        

                        
11.33

                        
80370

                        
141

                        
30

                        
1840 - 1912

                        
French

                        
novel

                    

                    

                        

                            
eng_RSC (

                            
Kermes et al., 2016

                            
)

                        

                        
7.92

                        
6206

                        
1276

                        
69

                        
1665 - 1869

                        
English

                        
journal article

                    

                
                
Table 1: Overview of the corpora.

                
The test is designed as follows: First, for each document in a corpus, a certain percentage of words (0%, 10%, ..., 100%) were randomly selected and replaced by their corresponding POS-tags. Since function words are crucial to authorship attribution, instead of only replacing function words as suggested in 
                    
Schöch et al. (2020)
, any kind of word (including punctuation) may be replaced. The next step is the standard procedure for Delta test: creating a document-term-matrix, computing the z score of each value in it and then select the most frequent word types as feature to classify documents. The classification was done by a linear SVM classifier with 5-fold cross-validation. Following the results in 
                    
Evert et al. (2017)
, the 2000 most frequent word types in each corpus were taken as feature for all the classification tasks. A further test was also performed: All POS-tags that replaced their corresponding words were removed from the text and the Delta test were then conducted again. By comparing the classification results of these two tests, we can also determine the contribution of POS-tags to the results of the classification. Since the words in texts are randomly replaced or removed which could introduce some random variation into the results, each of the above-described tests is repeated 10 times.
                

                
Before presenting the classification results, three text passages are prepared to give an impression of readability of the texts in DTF. The original text, the texts with 10% and 50% of words replaced by their corresponding POS-tags, are listed in Table 2.

                

                    

                        
percentage 

                        
text

                    

                    

                        

                            
0%

                            
(original)

                        

                        
The members of this new group of alkaloids are so numerous, their deportment is so singular, and their derivatives raimify in so many directions, that I have not as yet been able to complete the study of these substances in all their bearings; nor is it my intention to go fully into the chemistry of the subject in the present communication, my object being merely to establish the existence of these bodies, and to give a general outline of their connection with the volatile bases, and of their most plominent chemical and physical properties, reserving a detailed description of their salts and derivatives to a future memoir .

                    

                    

                        
10%

                        
The members of this new group of alkaloids are so numerous, their deportment is so singular, and their derivatives raimify in so many directions, that I have not as RB VBN able to complete the study of these substances in all their bearings ; nor is it my intention to go fully into DT chemistry of DT subject in the present communication, PP$ object being merely TO establish DT existence of these bodies, and to give a general outline of their connection IN the volatile bases, and of their most plominent chemical and physical properties, reserving a detailed description of their salts and derivatives to a JJ memoir.

                    

                    

                        
50%

                        
DT members IN DT JJ NN IN NNS VBP so JJ, PP$ deportment is RB JJ, CC their NNS raimify in RB many NNS, WDT PP VHP RB RB yet been JJ TO VV DT study IN these NNS IN all PP$ NNS : CC is it my NN TO go RB into DT chemistry IN the subject in DT JJ communication, my object NN merely to VV DT NN of DT NNS, and TO VV DT JJ outline IN PP$ connection IN DT volatile bases, CC IN their RBS plominent chemical and physical properties, reserving DT JJ NN IN their NNS and derivatives TO a future NN.

                    

                
                
Table 2. Text passages in original format and DTF (The percentage value indicates the proportion of words replaced or removed.).

            

            

                
Results

                
The classification results on the German play collection, the English article collection, and the French novel collection are presented in Figures 1, 2 and 3, respectively. The y-axis is the F1(macro)-score, and the x-axis shows the portion of words that are replaced or removed. The blue boxplots and the yellow boxplots represent the classification results, when the words in texts are replaced with POS-tags or removed, respectively. As the reference value, the classification results for the original data are also shown in the figures. The Welch’s t-test is also performed to determine the difference in classification results. The “ns” in the figures means non-significance. 

                
In all the three figures, the same trend can be observed: Step by step, the median of F1-score distributions get worse as the percentage increases. Especially when more than half of the words were replaced or removed, the tendency for the classification results to become worse became particularly obvious. In addition, the variance of the F1-scores always becomes larger, if a certain percentage of words in texts are replaced or removed. According to the Welch’s t-test, in all cases, whether the words are replaced or removed does not affect the classification. This observation indicates that the POS-tags do not contribute to the distinction of authorships.

                
Another interesting observation is, when all words in texts are replaced by POS-tags, the classification results improve, relative to a reduction of 90%, in the case of the German and English data, but not for the French data. To understand this situation, the change in the number of word types in each corpus was checked. As presented in Figure 4, when 90% of the words are replaced or removed, there are still around 20,000 word types in each text collection. But when all the words are replaced, only a few dozen types remain. Their number becomes so small that it looks like it is reduced to zero in the Figure 4. Since the classification is based on the most frequent 2000 types, although 90% of the words are replaced or removed, the 2000 features used for classification are still mostly from the remaining 10% of words. In the German and English collections, these words bring apparently noise to the classification task. In contrast, the remaining 10% of words in the French corpus are still able to guarantee a relatively good classification result. From the data in Table 1 we can see that number of authors in the French corpus is smaller, which indicates the classification task on the French corpus is easier. More importantly, as presented in previous studies (e.g., 
                    
Eder 2015
, 
                    
Romanov &amp; Grallert 2022
), that pulling random samples of at least 5000 words length out of texts will be sufficient for ensuring reliable authorship attribution. Considering the average length of the French novels is over 80,000 words, when 90% of the words are replaced or removed, the remaining 10% (that is, about 8000 words) is still sufficient to guarantee a good classification result. To clarify this issue, further research would certainly be of great interest.
                

                

                    

                
                
Figure 1. Authorship classification on the German play collection

                

                    

                
                
Figure 2. Authorship classification on the English article collection

                

                    

                
                
Figure 3. Authorship classification on the French novel collection

                

                    

                
                
Figure 4. Change of the number of word types in three text collections

            

            

                
Conclusion

                
This paper provides an exploration of the usefulness of three token-based DTFs for frequency-based authorship classification with Delta. As presented, selectively reducing information on individual tokens could ensure, to a certain extent, that the authorship classification results are not affected too much. The impact of token-based DTFs on the results of Delta test can be reduced by considering only replacing or removing content words, while all function words remain unchanged. But this limits the application of the texts on other TDM tasks such as topic modeling. For the future work, a series of tests are planned on evaluating the usefulness of token-based DTFs on other TDM tasks. The goal is to find DTFs that could balance various factors (e.g. word frequency, sequence information, content vs. function words, copyright) so that texts could be published and used for as many TDM tasks as possible without violating copyright law.

            

        

            
Die Ansprüche an Wissenschaftskommunikation verändern sich derzeit deutlich: Das Bedürfnis Forschender, ihre Ergebnisse offen zu teilen, sich aktiv mit anderen Forschenden zu vernetzen und selbstorganisiert zusammen zu arbeiten, ist Teil einer umfassenden institutionellen Transformation, die ihre Infrastrukturen noch entwickeln oder weiterentwickeln muss. Digitale Technologien eröffnen hier neue Optionen, die einer im Wandel begriffenen Wissenskultur entgegenkommen und neuen Formen der Konnektivität und des Arbeitens entsprechen können.

            

                
Mit der 

                
Architecture Research Stage - ARS

                
 

                
(www.architectureresearchstage.de) 

                
wird eine solche Vernetzungsplattform für die Architekturforschung entwickelt und erprobt. 
Die Architekturforschung steht dabei vor einer besonderen Herausforderung, da sie in vielen unterschiedlichen und unverbundenen Fachdisziplinen entwickelt wird. Zudem findet diese Forschung nicht allein im akademischen Bereich statt, sondern auch im außer-akademischen Bereich, in den Architektur- und Ingenieurbüros und in der Industrie. Das Potential der Architekturforschung liegt jedoch – wie bei der Architekturpraxis auch – gerade in der Synthese, die das Wissen aus diesen unterschiedlichen Disziplinen zusammenführt und vereint. ARS richtet sich deshalb explizit an alle Beteiligten der Architekturforschung: Ob sie im akademischen oder außer-akademischen Bereich forschen, alleine oder im Verbund, ob sie selbst Forschungsergebnisse produzieren oder diese herausgeben, ausstellen, diskutieren oder managen, als Profis, Studierende oder interessierte Laien.
            

            
Eine passende Infrastruktur für die spezifischen Bedürfnisse der Forschenden in der Architektur fehlt bisher. Die bestehenden institutionellen Repositorien der Hochschulen oder die überinstitutionellen Repositorien aggregieren und archivieren vornehmlich Forschungsergebnisse und -daten, bieten aber keine überzeugenden Funktionen für das Vernetzen an. Es sind die sogenannten akademischen Netzwerkplattformen wie Academia und ResearchGate, die auf diesen Bedarf reagieren und dort ihre Marktnische gefunden haben. Sie entsprechen jedoch weder den Open-Access-Standards, noch folgen sie den FAIR-Data Prinzipien oder garantieren eine langfristige Speicherung der Daten. Will die Forschungsgemeinschaft sich nicht von wenigen privatwirtschaftlich orientierten Plattformen oder Softwaresystemen abhängig machen, muss sie aus sich selbst heraus disziplinspezifische Vernetzungs-Plattformen entwickeln.

            

                
ARS

                
bietet für eine kollaborative Architekturforschung ein übergreifendes Modell und eine aktiv vernetzende Infrastruktur. Erstmalig werden nicht nur Ergebnisse und Daten, sondern auch Kontexte der Architekturforschung auf einer webbasierten Plattform gemeinsam generiert, vernetzt und sichtbar gemacht. 
Auf Grundlage eines Akteursmodells für interdisziplinäre Zusammenarbeit können die jeweiligen Entstehungskontexte durch die Forschenden eigenständig beschrieben, bewertet und mit Alternativen, Wünschen und Bedarfen ergänzt werden. Mit Bezug auf die Akteurs-Netzwerktheorie von Bruno Latour (Latour und Woolgar 1979) und die Netzwerktheorie von Harrison White (White 2008) werden dabei menschliche und nicht-menschliche Einflussfaktoren auf den Forschungsprozess unter dem Akteurs-Begriff versammelt. Aus vorhergehenden Forschungen zu interdisziplinären Forschungskontexten (Dürfeld et al. 2021) konnten dabei elf unterschiedliche Akteursklassen als relevant identifiziert werden: Personen, Organisationen, Themen, Methoden, Quellen, Ereignisse, Aufgaben, Werkzeuge, Orte, Zeiten und Gelder. Diese Akteur:innen sind miteinander durch spezifische, semantisch ausformulierte Bindungen verbunden, wodurch sich komplexe Forschungszusammenhänge einheitlich und vergleichbar modellieren, analysieren und planen lassen.
            

            
Zentrales Element von ARS sind die 
                
plusPublikationen
, die Forschungsergebnisse mit deren konkretem Entstehungskontext verbinden. Die von den Forschenden erstellten 
                
plusPublikationen
 berichten so immer auch etwas über ihre eigene Entstehung: Welche Werkzeuge und Methoden wurden verwendet, mit welchen Personen wurde diskutiert, welche Organisationen unterstützten die Forschung, welche Quellen wurden verwendet, welche Ereignisse haben den Forschungsprozess beeinflusst, an welchen Orten wurde gearbeitet, welche Aufgaben mussten erledigt werden, wieviel Zeit und Geld stand zur Verfügung? Mittels Graphentechnologie auf der Basis der Open-Source-Graphdatenbank Neo4j werden die Forschungsergebnisse und Forschungskontexte automatisch vernetzt. Über ein bildbasiertes, variables Interface wird das Akteursnetzwerk der Architekturforschung dargestellt und durchsuchbar gemacht.
            

            
Der Mehrwert von ARS liegt in fünf Bereichen: (1) 
                
Vergrößerung der Sichtbarkeit eigener Forschungen und Vernetzung mit Architekturforschenden durch Präsentation auf einer innovativen Forschungsplattform 
(2) Passgenaues Auffinden von Kooperationspartner:innen für neue Projekte, (3) Veröffentlichen und Finden von detaillierten, bewerteten Forschungsdaten, (4) Reflektion des eigenen Forschungsverhaltens durch das Erkennen von Wiederholungen, Lücken und Mustern und (5) Partizipation an einer aktiven Architekturforschungs-Community.
            

            
Hauptaufgaben des von der DFG für drei Jahre (2021-2024) geförderten experimentellen Pilotprojektes sind Aufbau, Erprobung und Evaluierung von ARS. Das Projekt wird als ein Verbundprojekt der TU Berlin und der UdK Berlin, unter der Beteiligung von Kooperationspartner:innen aus dem akademischen und außer-akademischen Bereich in Berlin-Brandenburg durchgeführt.

        

            

                
Das D-WISE Projekt

                
Das Verbundprojekt D-WISE (
                    
www.dwise.uni-hamburg.de
) ist ein 2021 gestartetes BMBF gefördertes interdisziplinäres Kooperationsprojekt an der Universität Hamburg zwischen den Geisteswissenschaften, vertreten durch das Institut für Empirische Kulturwissenschaft, und dem Fachbereich Informatik, vertreten durch die Arbeitsgruppe Language Technology. D-WISE entwickelt eine prototypische Arbeitsumgebung, die D-WISE Tool Suite (DWTS), in der sowohl innovative KI-Verfahren für die Analyse von multimodalen Daten entwickelt als auch hermeneutische Analysen der wissenssoziologischen Diskursanalyse digital unterstützt und erweitert werden.
 Dabei wird in dem Projekt herausgearbeitet, zu welchen Zwecken, Zeitpunkten und in welcher Form Digital Humanities (DH)-Verfahren in qualitative diskursanalytische Wissensproduktion analytisch sinnvoll eingebunden werden können. Die erkenntnistheoretische Reflexion und Weiterentwicklung hermeneutischer Methoden in der Nutzung (halb-)automatisierter diskursanalytischer Forschungsprozesse sind integraler Bestandteil innerhalb der konzeptuellen Forschung des D-WISE Projekts. Es sollen Fragen danach beantwortet werden, wie Automatisierung und DH-Methoden sinnvoll in qualitative diskursanalytische Ansätze und Wissensproduktion integriert werden können, welche bestehenden Methoden und Tools dafür übernommen werden können und welche dafür neu entwickelt werden müssen.
                

                
Das D-WISE Projekt zielt darauf ab jene Herausforderungen zu adressieren, denen sich Forschende im Hinblick auf methodologische Ansätze aus den Digital Humanities (DH) und der Diskursanalyse sowie dem zunehmenden Umgang mit digitalen Tools und offenen Korpora, bestehend aus heterogenen, multimodalen und großen Datenmengen gegenübersehen. Dabei zielt das Projekt auch darauf ab, den Mangel an digitalen Lösungen für die Analyse von Pluralität von Bedeutungen in multimodalen Materialien zu beheben sowie Dokumentations- und Reflexionsprozesse diskursanalytischen Arbeitens und die Weiterentwicklung der Diskursanalyse als Methode selbst zu unterstützen.

                
In Co-Creation Ansätzen zwischen Informatik und Geisteswissenschaften wird die diskursanalytische Arbeitsweise durch digitale Methoden erweitert und an digitale Modalitäten angepasst. Das Projekt innoviert dabei sowohl die informatische KI-Technologie kontextorientierter Embedding-Repräsentationen als auch hermeneutische Arbeitsweisen zur wissenssoziologischen Diskursanalyse. Dabei steht auch die Überbrückung der Lücke zwischen strukturellen Mustern, die mit digitalen Methoden aufgedeckt werden, und interpretativen Prozessen menschlicher Bedeutungsproduktion im Mittelpunkt des kollaborativen Ansatzes der Empirischen Kulturwissenschaften und Computerlinguistik innerhalb des D-WISE-Projekts. Dabei legt der D-WISE-Ansatz einen besonderen Akzent auf die Interaktion zwischen Mensch und Algorithmus bzw. Maschine, um KI-Forschungssysteme zu verbessern, die sich auf den menschlichen Erkenntnisprozess auswirken (Oeste-Reiß et al. 2021: 221, 149); Als Datenanalysewerkzeug mit Komponenten des maschinellen Lernens wird die DWTS zukünftig einen interaktiven und wechselseitigen Prozess bieten, in dem maschinell erstellte Vorschläge von Annotator:innen akzeptiert, abgelehnt oder korrigiert werden können, wodurch das maschinelle Lernen mit der Zeit verbessert wird (Yimam et al. 2017, Koch et al. 2022: 77).

            

            

                
Computergestützte Diskursanalyse und ihre Entwicklungspotenziale

                
Anthropologisch und soziologisch orientierte Diskursanalysen haben zum Ziel diskursive Konstruktionen von Wirklichkeit, von Wissen oder von Macht-/Wissensbeziehungen zu untersuchen, wobei sie von der Annahme ausgehen, dass der Sprachgebrauch in diskursiven Praktiken eben jene Gegenstände konstituiert, die sie als Wissen behandelt. Grundlage digitaler Ansätze in der Diskursanalyse sind Kodierungsprozesse in Anlehnung an die Grounded Theory: Annotieren und Extrahieren von Zitaten aus Transkripten, Erstellen von Codes, Sub-Codes und Parent Codes und der Identifizierung von Konzepten oder Themen. Die Analyse von Diskursen erfolgt in zirkulären Prozessen der Suche, Auswahl, Analyse und Interpretation von Forschungsdaten, unterstützt durch Literaturarbeit, und wird iterativ durchgeführt, bis die Forschungsfrage beantwortet ist. 

                

                    

                    
Abb. 1: Schaubild des D-WISE Projektziels der Unterstützung und Erweiterung soziologisch und anthropologisch orientierter Diskursanalysen, © D-WISE

                

                
Die Verwendung von digitalen Werkzeugen in der Analyse von Diskursen ist weit verbreitet: Ob Organisation, Kodierung, Annotation oder Analyse von Forschungsmaterial – digitale Tools bieten eine nützliche Ergänzung in der qualitativen und quantitativen Datenanalyse. Lizenzbasierte und kostenpflichtige qualitative Datenanalysetools wie MAXQDA, Atlas.ti oder NVivo sind bemüht umfangreiche ‚All-in-one-Lösungen‘ zu bieten und verschiedenen Anforderungen gleichzeitig gerecht zu werden. Zur Verfügung stehende Open Source Tools hingegen konzentrieren sich oftmals auf wenige spezialisierte Funktionen, die dafür teilweise sehr gut beherrscht werden.
 Catma oder WebAnno sind entsprechende Beispiele für offene Werkzeuge, die jedoch noch viele Wünsche für die qualitative Diskursanalyse offenlassen – Lücken, die mit D-WISE geschlossen werden sollen (Koch et al. 2022: 81; Gius et al. 2021; Eckart De Castilho et al. 2016).
 Dabei konzentriert sich das Projekt vornehmlich auf drei Aspekte, in denen Entwicklungspotenzial im Bereich digitaler Tools und semi-automatisierter Funktionen identifiziert wurden: Analysen, Kodierungen und Annotationen von multimodalen Daten, die Text, Bild, Audio und Video beinhalten; das Crawlen, Filtern und Management von großen digital zu Verfügung stehenden Datenmengen (‚big data‘); sowie Prozesse der Dokumentation und Reflexion von diskursanalytischen Verfahren und Methoden sowie der Verwendung und des Einflusses digitaler Tools.
                

            

            

                
Digitale Diskursanalyse und Hermeneutik in D-WISE

                
Mit der zunehmenden Integration von IT-Werkzeugen und -Infrastrukturen in die geisteswissenschaftliche Forschung kommt es zu ontologischen Veränderungen in der Wissensproduktion; neue soziale, ethische und/oder politische Konstellationen werden verfestigt, gestört oder geschaffen (Koch 2018: 71). Die Interaktion mit und Interpretation von Datenmodellierung oder Mustererkennung als grundlegende Operationen in den Digital Humanities können die Perspektive von Geisteswissenschaftler:innen auf ihre Quellen verändern (Schwandt 2020: 19). Die zunehmende Integration solcher strukturellen Ansätze in den hermeneutischen Ansätzen diskursanalytischer Forschung kann als sich verfestigende Infrastruktur für die Sozial- und Kulturforschung verstanden werden. Dies wirft methodologische und epistemologische Fragen hinsichtlich der Erforschung sozialer Wirklichkeit und der Pluralität von Bedeutung auf und bringt die Herausforderung mit sich, dass Diskurs- und Arbeitspraktiken in Standardformen gepresst werden (Koch 2018: 70; Koch et al. 2022: 73). Die wachsende Hybridität von manuellen und digitalen Herangehensweisen im Forschungsalltag macht eine Neuverhandlung von einer Hermeneutik notwendig, die „das Dazwischen“ gegenwärtiger geisteswissenschaftlicher Praktiken problematisiert (Fickers und Tatarinov 2022: 7, 11).

                

                
Im Mittelpunkt dieser Entwicklung steht auch im D-WISE Projekt die Frage, wie Menschen und digitale Strukturen interagieren und auf welche Weise diese Interaktion menschlichen und methodologischen Bedürfnissen dient (Koch et al. 2022: 70). Unterschiede in Forschungsdesign und Methodik (quantitatives bzw. maschinengestütztes ‚distant reading‘ und qualitatives bzw. individuelles ‚close reading‘ von Korpora) sowie in den Ambitionen (Auffinden allgemeiner wissenschaftlicher Gesetze versus Produktion originärer subjektiver Interpretationen in den Geisteswissenschaften) schaffen neue Herausforderungen und Entwicklungspotenziale für das Forschungs- und Tool-Design (Fickers und Tatarinov 2022: 5).

                
Die Digital Humanities sind davon geprägt eine vielfältige und nach wie vor im Bestehen begriffene Forschungslandschaft zu sein, die eine Reihe von geisteswissenschaftlichen und informatischen Forschungen, Methoden und Technologien umfassen. Dabei kommt immer wieder die Frage auf, inwiefern die Geisteswissenschaften durch ihre Interaktion mit Technologie, Medien und digitalen Methoden Teil an diesen haben (Schlicht 2021: 26 zit.n. Svensson; Fickers und Tatarinov 2022: 6-7). Während komplexe Mensch-Computer-Interaktionen zunehmend in Forschungsprozesse eingebunden werden, können diese zumeist nur in phänomenologisch-deskriptiver und ethisch-normativer Hinsicht ausreichend verstanden werden (Fritz et al. 2020: 3). Teil dabei entstehender Reflexionsprozesse sind auch Herangehensweisen wie die sogenannten „screwmeneutics“, wobei in die digitalen Werkzeuge als Mittel der explorativen Hermeneutik eingetaucht wird und durch spielerisches Herumprobieren und „screwing around with data“ (Fickers und Tatarinov 2022: 9 zit.n. van Zundert 2016 und Ramsay 2014) Erkenntnisse gewonnen, ‚user stories‘ und Kritiken formuliert werden gegenüber angewandten Methoden und Tools (Koch et al. 2022: 73-76; Schwandt 2020: 20).

                

                
Für die Entwicklung zentraler Innovationen für die qualitative Datenanalyse werden in D-WISE in einem konzeptuellen Rahmen der ‘manuellen Analyse’ bereits vorhandene Features und Tools sowie deren technische und methodische Nutzbarkeit von Projektinternen und -externen Forscher:innen aus verschiedenen Bereichen der Geisteswissenschaften und der Informatik erprobt. Funktionalitäten wie das Kodieren, Dokumentieren oder Visualisieren werden in Co-Creative Prozessen auf ihre Nützlichkeit für Diskursanalyse im Allgemeinen sowie sinnvolle Implementierung für die Tool Suite getestet, eruiert und reflektiert. Diese Integrierung von Erfahrungen und Feedbackschleifen von Wissenschaftler:innen mit verschiedenen Forschungsschwerpunkten macht die DWTS robust für disziplinübergreifende diskursanalytische Ansätze. 

                
Diese wechselseitige Arbeitsweise, die sich in der Entwicklung der D-WISE Tool Suite manifestiert hat, wird als methodischer Ansatz zur Kombination von manuellen und digitalen Analysemethoden in der qualitativen und quantitativen Sozial- und Kulturforschung angewandt. Kern dieser konzeptuellen Forschung beinhaltet auch die epistemologische Reflexion über Relevanz und Validität der erhobenen Daten, der angewandten Methoden sowie verwendeten Tools und hermeneutischen Prozesse. In diesem wechselseitigen Ansatz von manueller und digitaler Arbeit sollen die User und der Prozess der Diskursanalyse mit digitalen Lösungen, Algorithmen und KI effektiv unterstützt werden und jene Forschungsaspekte ausgemacht werden, in denen hermeneutische Interpretationen notwendig werden. Damit einher geht eine Reflexion darüber, wie digitale Tools und Methoden diskursanalytische Forschungsprozesse generell verändern. Direkt im Forschungsprozess entstanden, legt eine solche Zirkulation von Agency in der Mensch-Computer-Interaktion den Grundstein für diesen Ansatz von wechselseitiger Beeinflussung, Anreicherung und entsprechenden Reflexionsprozessen.
 Während für die Entdeckung und Produktion von Wissen, von Innovation, Integration und Wiederverwendung von Daten eine gute Verwaltung zentral ist, wird die D-WISE Tool Suite den Anforderungen der Prinzipien der Open Science angepasst, um den Forderungen nach einem größtmöglichen und inklusiven Nutzen von Daten, Kollaboration und ihren Werkzeugen gerecht zu werden (Wilkinson et al. 2016; Schlicht 2021: 29).

                

                
Neben der Tool Entwicklung findet sich der Wert des Projekts in der Konzeptualisierung selbst. Zentraler Aspekt ist dabei der hermeneutische Ansatz in D-WISE: Die epistemologische Reflektion und die Weiterentwicklung hermeneutischer Methoden und semi-automatisierter Prozesse. Mit diesem methodisch-konzeptuellen Ansatz will das Projekt die Lücke schließen zwischen manuellen und digitalen diskursanalytischen Ansätzen: Dies beinhaltet auf forscherischer Projektseite einerseits den interdisziplinären Brückenschlag durch Co-Creation Konzepte sowie andererseits die Überbrückung auf technischer und methodisch-diskursanalytischer Ebene: zwischen strukturellen Mustern, die durch digitale Methoden erkannt werden sowie hermeneutischen und interpretativen Prozessen der menschlichen Bedeutungsgebung und Wissensproduktion – zwischen qualitativen Ansätzen und ‚close readings‘ und quantitativen Ansätzen des ‚distant readings‘. Teil dessen ist auch die Reflexion der Veränderung empirischer Forschung durch digitale Technologien. 

                
Dabei dient die Tool Suite als Bindeglied zwischen manueller und digitaler sowie quantitativer und qualitativer Analyse; Durch Hin- und Anleitung zu proprietären Tools, durch semi-automatisierte Kodierungen, Visualisierungen und Mappings können Lücken zwischen quantitativer und qualitativer Analyse überbrückt werden. Durch Weiterentwicklungen von semi-automatisierter Annotation und der Produktion von Sub-Korpora oder Code-Gruppen werden quantitative Verfahren beschleunigt und es kann sich schrittweise einem axialen Kodieren und einer qualitativen Feinanalyse angenähert werden. Der forscherische Schwerpunkt in D-WISE zu der Frage, welche digitalen Tools und Funktionen sinnvoll eingesetzt werden können und wo hingegen der Mensch mit hermeneutischer und interpretativer Leistung sowie mit Skalierungen und Medienwechsel eingreifen muss, ist der methodisch-theoretische und konzeptuelle Rahmen für die Erforschung dieser Überbrückung von quantitativ und qualitativ, von ‚distant‘ und ‚close reading‘, von digital und manuell geleiteten sowie hermeneutisch-interpretativen Zugängen zu Diskursanalyse.

            

            

                
Die D-WISE Tool Suite als Open Source Software 

                
Der Prototyp der DWTS wird als webbasierte Open Access Serverarchitektur entwickelt und auf Basis bereits erfolgreich etablierter DH-Methoden und digitalen Tools und deren Reflektion und Evaluation konzipiert.
 Die Tool Suite ist projekt-zentriert aufgesetzt, und soll kollaboratives Arbeiten und alle diskursanalytischen Schritte unterstützen. Gestartet im Mai 2021, befindet sich die Tool Suite in einem frühen prototypischen Stadium, in dem zunächst erste grundlegende Funktionen und erwartete Standards umgesetzt wurden, wie das Suchen, Filtern, Kodieren, Annotieren sowie Dokumentieren.

                

                
Der im Rahmen des D-WISE Projektes entwickelte Prototyp der D-WISE Tool Suite wird angelegt als Free and Open Source Software (FOSS) zur digitalen Unterstützung qualitativer Datenanalyse und Diskursanalysen (Schlicht 2021: 34). Eines der Ziele ist es dabei die DWTS so zu entwickeln, dass sie entsprechende Kriterien in Bezug auf Klimafreundlichkeit, Nachhaltigkeit, Langlebigkeit und Offenheit erfüllt (Jentsch und Porada 2020: 91).
 Dabei wird auch den FAIR-Leitprinzipien für wissenschaftliches Datenmanagement gefolgt, die entwickelt wurden, um den Umgang mit Forschungsdaten, die Ausweitung des Geltungsbereiches von Open Access sowie eine breite Implementierung der Open Science Prinzipien zu unterstützen.
 Die Idee der Tool Suite im Sinne eines Werkzeug- oder Baukastens strebt an, Tools und Features sowohl innerhalb der Tool Suite anzubieten als auch die Verwendung externer Tools zu ermöglichen und anzuleiten. Dabei sollen, wo immer möglich, extern wie intern, Open-Source-Features, -Tools und -Lösungen implementiert werden (Fischer et al. 2023). Vor allem proprietäre Tools kommen dabei zum Einsatz, die Ansprüche an die Arbeitsweise der qualitativen Diskursanalyse erfüllen und digitale Verfahren entsprechend sinnvoll einbinden. Aus der Implementierung jener Open Science Prinzipien ergeben sich folglich Fragen nach Inklusivität und Gleichberechtigung; Sie sollen zu einer höheren Sichtbarkeit und Auffindbarkeit führen sowie schnellere Verbreitung von Forschungsergebnissen und freien Zugang unabhängig von institutioneller Zugehörigkeit ermöglichen (Schlicht 2021: 28, 29 zit.n. Drucker 2009). Die Wahl von freier und quelloffener Software (FOSS) stellt zudem sicher, dass der Quellcode dieser Werkzeuge stets nachvollzogen werden kann, von Nutzer:innen veränderbar ist und Verarbeitungsschritte dokumentiert, reproduzierbar sowie interoperabel sind – im Idealfall über einen längeren Zeitraum über die Projektphase hinaus (Jentsch und Porada 2020: 92).

                

            

            

                
Schlussbemerkung: Offener Zugang und Interdisziplinärer Austausch

                
Kooperatives und interdisziplinäres Arbeiten sowie Fragen nach Open Access gewinnen in den Digital Humanities zunehmend an Bedeutung. Vor allem geisteswissenschaftlich ausgerichtete Forschungsteams interdisziplinärer Projekte können einen wichtigen kritischen Beitrag leisten, um mittels der Verwendung von Werkzeugen, Paradigmen und Konzepten digitaler Technologien, die Idee von digitalen Tools, deren Zugänglichkeit und Instrumentalität neu zu überdenken. Durch die kontinuierliche Zusammenarbeit zwischen Informatik und Geisteswissenschaften wird im D-WISE Projekt sichergestellt, dass in iterativen Zirkeln der Software-Entwicklung die Umsetzung von inklusiven und möglichst unbefangenen Wissens- und Untersuchungsparadigmen unterstützt werden (Koch et al. 2022: 71; Schlicht 2021: 46; zit.n. Liu 2012: 501-502). Dieser Forschungsprozess ist technisch als zyklischer Prozess entwickelt, somit flexibel auf iteratives Arbeiten anpassbar und wiederverwendbar und entspricht selbst einem hermeneutischen Zirkel, der durch wiederholtes Hinterfragen und ständige Erweiterung des Wissensstandes immer wieder zu neuen Fragen, Erkenntnissen und damit zu einem tieferen Verständnis des Phänomens führt. Kritisch reflektiert werden dabei neben der Entwicklung der Tool Suite, auch Prozesse der Wissensproduktion und das zugänglich machen von Wissen und Werkzeugen, angewandten Methoden und Fragestellungen. Die Geisteswissenschaften dienen dabei, wie Liu es formuliert, als „Vektor“ für den Import fremder Wissensparadigmen: Sie bringen neue Phänomen-Ebenen ein durch beispielsweise quantitativ definierte Strukturen, Formen und Zyklen; in den Analyse- und Interpretationsverfahren werden durch digitale Technologien Modellierungen eingeführt und in der Wissensproduktion wird das Repertoire von Ergebnispräsentationen um Visualisierungen, Programmen oder Datenbanken erweitert (Schlicht 2021: 47; zit.n. Liu 2009: 27).

            

        

            

                
Panelthema

                
Der Nutzen der Digitalisierung von Objekten kulturellen Erbes scheint auf den ersten Blick selbstevident: Der Zugang zu Objekten wird unabhängig vom physischen Zugriff und somit die artefaktbezogene Forschung deutlich erleichtert. Artefakte, die bislang an verschiedenen Orten aufbewahrt werden, können digital zusammengeführt, kontextualisiert und miteinander in Beziehung gesetzt werden. Und nicht zuletzt kann die umfassende digitale Dokumentation auch angesichts der Bedrohung des kulturellen Erbes durch Raub, Krieg oder Umweltzerstörung zumindest ein digitales Gedächtnis sicherstellen.

                
Zugleich steht diese vermeintliche Selbstevidenz zuweilen einer kritischen Auseinandersetzung mit den epistemologischen Grundlagen und politischen Konsequenzen im Wege. Oder, wie es Monika Stobiecka zusammenfasst: “Digital archaeology has been used largely to avoid the politics and ethics of dealing with difficult questions concerning the field of heritage studies” (Stobiecka 2020; siehe auch Thompson 2017). Dabei bietet die Praxis der Digitalisierung von Kulturerbe genügend Anknüpfungspunkte für eine kritische (Selbst-)Reflexion, insbesondere dann, wenn sie in postkolonialen Kontexten stattfindet. In diesem Panel geht es uns daher darum, einen Diskussionsraum zu eröffnen, in dem Anfragen an die Praxis der Digitalisierung und die eigene Rolle gestellt werden können.

                
Eine solche Auseinandersetzung muss letztlich schon beim Konzept des kulturellen Erbes ansetzen. Wie Stuart Hall in seinem Essay “Whose Heritage?” schreibt, hat der scheinbar unverdächtige Begriff eine unsichtbare, eingeschriebene Agenda, indem darin ein bestimmtes Bild der Vergangenheit und die Konstruktion einer oft national gefassten Identität in der Gegenwart eingeschrieben ist. Um diese unsichtbare Funktion kulturellen Erbes sichtbar zu machen, sei zu fragen: Wofür Kulturerbe, und für wen (Hall 2004)? Dies ist vor allem dann von besonderer Bedeutung, wenn das Kulturerbe im Kontext postkolonialer Machtgeflechte thematisiert wird. So wird etwa die Digitalisierung des kulturellen Erbes in Krisenregionen des Mittleren Ostens in erster Linie von westlichen Akteuren vorangetrieben, was Fragen nach einem neuen „digitalen Kolonialismus“ aufwirft (Thompson 2017, 155). Diese Fragen können und müssen dabei auf ganz verschiedenen Ebenen verhandelt werden. Grundlegend kann etwa gefragt werden, inwieweit Digitalisierungsprojekte hier in der Tradition der Archäologie des 18. und 19. Jahrhunderts stehen, in der westliche Akteure als wahre Kenner und Bewahrer vergangener Kulturen auftraten und vergleichbare Narrative der Errettung des Kulturerbes eine Rolle spielten (Thompson 2017, 162). In der konkreten Praxis kann gefragt werden, welche Kriterien für die Auswahl für Digitalisierungsprojekte eine Rolle spielen oder ob Open-Access-Mandate mit indigenen Vorstellungen von Zugang und Sichtbarkeit bestimmter Objekte in Einklang zu bringen sind (Manžuch 2017, 4–5). Auch rechtliche Fragen spielen hier eine Rolle, wenn etwa die physischen Objekte selbst aufgrund ihres Alters als gemeinfrei klassifiziert werden, auf die Digitalisate aber neue Schutzrechte reklamiert werden (Thompson 2017, 172).

                
Als Reaktion auf einige dieser Herausforderungen und als Gegengewicht zu den stark technisch formulierten FAIR-Prinzipien (Findability, Accessibility, Interoperability, and Reusability; Wilkinson u. a. 2016) wurden die CARE-Prinzipien (Collective Benefit, Authority to Control, Responsibility, Ethics) für “Indigenous Data Governance” formuliert (Carroll u. a. 2021). Diese Prinzipien lenken den Blick auf die Rechte und den Nutzen der Herkunftsgemeinschaften. In der Diskussion sind sie zunehmend als Korrektiv zu einem rein technischen Blick auf Daten eingeführt worden. Sie beziehen sich allerdings explizit auf die konkreten Bedarfe indigener Gemeinschaften und können daher nicht unterschiedslos auf andere postkoloniale Konstellationen übertragen werden.

                
Während die FAIR- und CARE-Prinzipien zumeist in Forschungsprojekten zum Tragen kommen, ist ihre Umsetzung in der Digitalisierung von großen Sammlungsbeständen, wie etwa in Museen, Archiven oder Bibliotheken, oft von vielen Ungewissheiten geprägt und stellt Institutionen vor enorme Herausforderungen. Die Rückgabe von Benin-Bronzen hat etwa die Frage des Umgangs mit den Objektdaten sowie der intellektuellen Autorität, die durch die Daten ausgeübt wird, aufgeworfen (Geismar 2018, 111-2; Pavis und Wallace 2019; Wallace und Euler 2020). Die intellektuelle Autorität zeigt sich beispielsweise in den Objektbeschreibungen und den eingesetzten Vokabularen, die in kleinteiliger Arbeit von Kurator*innen erstellt wurden, um Objekte beschreiben und finden zu können. Einerseits ist es notwendig koloniale Begriffe in Sammlungsdatenbanken zu entfernen, damit koloniale Weltbilder durch die rasche Verbreitung von offenen Daten nicht weiter reproduziert werden. Andererseits sollten diese Begrifflichkeiten nicht vollständig eliminiert werden, da sie über die historische Entwicklung von Institutionen Auskunft geben können. Noch weitgehender lässt sich fragen, wie Metadatenschemata gestaltet werden müssten, um nicht nur einen westlichen Blick auf die Artefakte zum Ausdruck bringen. Das digitale Medium eröffnet neue Wege, um die intellektuelle Autorität neu zu verteilen, nämlich die Aufnahme von unterrepräsentierten Stimmen in Sammlungsdatenbanken und Objektbeschreibungen (Risam 2019, 9), doch stellt sich hier die Frage wer an der Wissensproduktion beteiligt sein darf, kann oder soll und in wie weit Infrastrukturen anderes Wissen und Wissensstrukturen zulassen (Scholz et al. 2021, 299-315). .

                
Dieses Panel will diese und ähnliche theoretische Fragen in Beziehung setzen zu unserer eigenen Praxis in den Digital Humanities und in Digitalisierungsprojekten. Dabei geht es uns darum, einen Schritt zurückzumachen und mit einer gewissen Distanz noch einmal auf unser eigenes Tun und seine epistemologischen Grundlagen zu blicken. Das Ziel ist nicht, moralisch eindeutige Urteile zu fällen, sondern einen Raum zu eröffnen, in dem kritische Fragen gestellt, aber auch eigene Ansätze zum Umgang mit ihnen vorgestellt werden können.

            

            

                

                    
Beiträge
                

                

                    
Claudia Berger: Im Projekt „Kartographien Afrikas und Asiens“ (KarAfAs) digitalisieren wir einen ganz besonderen Bestand, der von Kolonialismus in verschiedenen Vermittlungsgraden durchdrungen ist. Unser Digitalisierungsvorhaben speist sich daher zu nicht unerheblichem Anteil aus dem Anliegen, dieses Material, das teils autoritativ koloniale Weltbilder zu manifestieren geholfen hat, teils durch indigene Mitproduzent*innen entstanden ist, global verfügbar zu machen und damit zugänglich für eben jene, die in jenen Gegenden leben und forschen, die von diesem kartographischen Material beschrieben wurden und betroffen waren. Die Frage ist allerdings, ob ein Digitalisierungsvorhaben diesem Anspruch gerecht werden kann. Hierein spielen Fragen der Digitalisierungskultur und des 

                    
digital divide

                    
, aber auch der Aufbereitung der Sammlung zur Zugänglichmachung und Kommunikationsstrategien.

                

                
Nicole High-Steskal: Das Linking Viennese Art through Artificial Intelligence - Projekt beschäftigt sich mit dem Einsatz von Künstlicher Intelligenz, um die offenen Bestände von drei Museen in Wien zusammenzuführen. Die Digitalisierungsgrundlage der Museen ist sehr gut und ein Großteil der Objekte wurden zwischen 2002 und 2010 digital erfasst als koloniale Bezüge in Objektbeschreibungen und Vokabularen noch nicht thematisiert wurden, gleichzeitig lag der Fokus der Provenienzforschung vielfach auf Objekte, die zwischen 1938 und 1945 in die Sammlungen gelangt sind und nicht auf koloniale Bezüge. Im Rahmen des LiviaAI-Projektes stellt sich daher die Frage, wie wir sicherstellen können, dass unser Zugang rassistische oder koloniale Sichtweisen nicht reproduziert oder verstärkt, insbesondere bei großen Datensätzen, wo es nicht möglich ist alle Datensätze durchzuschauen. Um mögliche Biases in den Datensätzen besser einschätzen zu können, wurde im Projekt ein spezieller Fokus auf die Aufarbeitung der Digitalisierungsgeschichte und Kontextualisierung der einzelnen Datensätze gelegt. Der Beitrag stellt das LiviaAI-Projekt vor und greift ausgehend davon theoretische Fragestellungen auf.

                
Clemens Neudecker: Die Staatsbibliothek zu Berlin - Preußischer Kulturbesitz verfügt durch den Vollständigkeitsanspruch der „Sammlung Deutscher Drucke“ der SBB über besonders dichte Bestände aus den Jahren 1871 bis 1912. Im Zuge eines geplanten Digitalisierungsvorhabens “Digitalisierung von Quellen zur deutschen Kolonialzeit 1876-1919” beabsichtigt die SBB die reichen Quellen aus der deutschen Kolonialzeit zu digitalisieren um so insb. Forschung und Projekte zu Fragen der De-Kolonisation, aber auch der Herausbildung des Kolonialgedankens und der Gründung der deutschen Kolonien, zu unterstützen. Die einschlägigen Titel sind mittlerweile ganz überwiegend urheberrechtsfrei und können via Open Access digital bereitgestellt werden. Parallel wird in einem Forschungsprojekt der SBB zu Künstlicher Intelligenz (KI) für das digitale Kulturelle Erbe untersucht, wie digitalisierte Kulturdaten als Datensets (“collections as data”) besser Eingang in die Entwicklung von Verfahren und Modellen aus dem Bereich der KI finden können, wobei insbesondere Fragen zur Provenienz und Kontextualisierung der Daten eine Rolle spielen, da im Bereich der KI verbreitete Daten und Modelle oft bereits über ethisch und sozial problematischen Bias verfügen (vgl. z.B. Bender et al. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?”, 2021, und Inke Arns: “Can Artificial Intelligence be biased? On the critique of AI's 'algorithmic bias' in the arts”, 2022). Besonderer Fokus wird daher auf die Dokumentation und Kontextualisierung von digitalen Kulturdaten aus der Kolonialzeit als Datensets gerichtet, so dass die im Zuge des Digitalisierungsvorhabens entstehenden Daten als Use cases dienen können, um gemeinsam mit der Community Empfehlungen und Richtlinien zu erarbeiten, wie digitalisierte Kulturdaten aus kolonialen Kontexten angemessen dokumentiert, kontextualisiert und als Datensets für die Forschung zugänglich gemacht werden können.

                
Jessie Pons: Das Projekt „Digitalisierung Gandharischer Artefakte“ (DiGA) digitalisiert buddhistische Skulpturen, die in zwei Sammlungen pakistanischer Museen in der Provinz Khyber-Pakhtunkhwa aufbewahrt werden. Die Digitalisate und beschreibende Metadaten werden frei zugänglich gemacht. Dennoch stellen wir uns im Projekt die Frage, inwieweit unser Vorhaben Teil eines neuen digitalen Kolonialismus ist, wenn Infrastrukturen, Metadatenstandards und Einkommen innerhalb des westlichen Wissenschaftssystems verbleiben. Wir sind daher im engen Austausch mit lokalen Stakeholdern, insbesondere dem Direktorat für Archäologie und Museen (KP), um die Interessen und Bedürfnisse der pakistanischen Partner in der Planung und Umsetzung zu berücksichtigen. Im Rahmen des Panels möchte ich einige unserer Ansätze vorstellen, aber auch offene Fragen und Herausforderungen diskutieren.

            

            

                

                    
Zeitplan
                

                
Einführung ins Thema: 6 Minuten

                
Vier Kurz-Inputs à 6 Minuten: 24 Minuten

                
Diskussion im Panel: 30 Minuten

                
Diskussion mit dem Publikum: 30 Minuten

            

        

            

            
Ausgangslage

            

                
Mit dem Sammlungsschwerpunkt auf schriftlichen Unterlagen aus dem Bereich der Bildenden Kunst unterscheidet sich das Deutsche Kunstarchiv im Germanischen Nationalmuseum in Nürnberg von Kommunal- oder Staatsarchiven. Die ca. 1450 Bestände, die seit den 1960er Jahren gesammelt werden, umfassen Vor- und Nachlässe aus dem deutschen Sprachraum der Gebiete Architektur, Bildhauerei, Bildwissenschaft, Design, Fotografie, Kunstgeschichte, Kunsthandel, Kunsthandwerk, Malerei und Restaurierung und decken den Zeitraum vom späten 19. Jahrhundert bis in die Gegenwart ab.

            

            

                
Durch den konkreten Schwerpunkt und die strukturelle Ähnlichkeit der einzelnen Bestände wuchs der Wunsch nach einem auf die Archivbestände zugeschnittenen Datenbanksystem.

            

            

                
2019 wurde der Entschluss gefasst, die seit 2009 genutzte hierarchische Datenbank durch eine semantische und graphbasierte Datenbank zu ersetzen. Ein Systemwechsel war unter anderem notwendig, weil die vorherige Archivsoftware nicht 64-bit fähig war. Anpassungen bei der Onlinepräsentation sind ebenso stark eingeschränkt und berücksichtigen nicht die adäquate Darstellbarkeit auf mobilen Geräten. Ein Upgrade auf eine neuere Version desselben Systems ist kostenpflichtig.

            

            

                
Während viele Archive zur Erschließung und Onlinestellung ihrer Bestände auf kommerzielle Softwarelösungen zurückgreifen, haben die Archive des Germanischen Nationalmuseums - das Historische Archiv und das Deutsche Kunstarchiv - einen alternativen Weg beschritten. 

            

            

            
Anforderung

            

                
Aufgrund der negativen Erfahrung mit der vorherigen Archivsoftware, soll die neue Software Open Source sein. Darüber hinaus ist eine flexible Webpräsentation wichtig, die an die sich stetig ändernden Anforderungen der Nutzer:innen und der Geräte jeweils angepasst werden kann. Die im Archivinformationssystem zur Verfügung gestellten Daten müssen persistent und zitierfähig sein, um Nachhaltigkeit und Zukunftsfähigkeit sicherzustellen. Außerdem ist ein strukturell standardkonformes System zentral für die Implementierung von ISAD(
G) (Internationale Grundsätze für die archivische Verzeichnung
                
) und dem ISO-zertifizierten CIDOC (
International Committee on Documentation
                
) Conceptual Reference Model des International Council of Museums (ICOM).

            

            

            
Evaluierung

            

                
Die zugrundeliegende Ontologie für das semantische Datenmodell ist das CIDOC CRM. Ausschlaggebend für diese Entscheidung waren die Flexibilität und die Erweiterungsmöglichkeiten. So konnten die Anforderungen, die der Wechsel eines Erschließungssystems mit sich bringt, diskutiert und eine für die Bedürfnisse des Deutschen Kunstarchivs maßgeschneiderte aber zugleich den archivischen Standards entsprechende Erschließungsstruktur modelliert werden. 

            

            

                
Bei der Konzeption der Erweiterung des CIDOC CRM wurden nicht nur die Vorgaben der inhaltlich beschreibenden Bestandserschließung berücksichtigt, sondern auch Anforderungen der Dokumentation, der Digitalisierung und der Vernetzung innerhalb der Bestände mitbedacht. Aus intensiven Diskussionen mit allen archiv- und datenbankerprobten Mitarbeiter:innen ging die Konzeption von Mechanismen zur Erfassung von archivinternen Workflows hervor. So wurden im Datenmodell Eingabefelder für restauratorische Maßnahmen, Akzessionierung oder das Depotmanagement ergänzt. Die weiteren beschreibenden Erfassungsfelder basieren auf den Erschließungsstandards ISAD(G) sowie RNAB (Ressourcenerschließung mit Normdaten in Archiven und Bibliotheken für Personen-, Familien-, Körperschaftsarchive und Sammlungen). Das entwickelte Archivinformationssystem bietet den Verzeichner:innen auf diese Weise die Möglichkeit auf allen Ebenen - vom Gesamtbestand bis hin zum Einzeldokument - inhaltliche Informationen mit Daten zur Bestandsverwaltung und -konservierung zu verknüpfen. 

            

            

                
Die Anwendungsontologie des Deutschen Kunstarchivs bietet neben der Ontologie des International Council on Archives Records in Contexts (RiC-O) eine praxisnahe Diskussionsgrundlage für eine allgemeinere Erweiterung des CIDOC CRM für die archivische Erschließung. Bisher gibt es in diesem Kontext nur vereinzelte Versuche, wie den des Portuguese National Archive (vgl. Melo 2022) oder für Kommunalarchive allgemein (vgl. Vitzthum 2021). Das Portuguese National Archive weicht in der Modellierung des graphbasierten Datenmodells vom Deutschen Kunstarchiv ab. Das heißt, es werden unterschiedliche Klassen des CIDOC CRMs benutzt. Die Modellierung am Beispiel der Kommunalarchive wendet die RiC-Ontologie auf die Archivbestände an. Das Archivinformationssystem des Deutschen Kunstarchivs zeigt exemplarisch, dass sich mithilfe des CIDOC CRM und WissKI Archivbestände adäquat strukturieren sowie erfassen lassen und sich darüber hinaus auch deren Binnenstruktur abbilden lässt.

            

            

                
Bei der Evaluierung existierender Systeme hinsichtlich der vorliegenden Anforderungen zeigte sich, dass für die Umsetzung des Graphnetzwerks mit einer spezifischen Domänenontologie lediglich WissKI alle Aspekte erfüllt.

            

            

                
Umsetzung, aktueller Stand und Ausblick

            

                
Innerhalb von zwei Jahren wurden die Daten ins neue Datenbanksystem migriert. Aktuell verzeichnen die Mitarbeiter:innen des Deutschen Kunstarchivs neue Bestände bereits im WissKI. Um in nationale und internationale Portale Daten liefern zu können, muss ein Export der auf CIDOC CRM basierenden Metadaten in die gewünschte EAD (Encoded Archival Description) xml Struktur entwickelt werden. Zusätzlich wird an einem Konzept zur Onlinestellung der Daten gearbeitet, das im folgenden Jahr umgesetzt werden soll.

            

            

                
Das Poster veranschaulicht zum einen die Entscheidungsgrundlagen und Vorstellungen der Mitarbeiter:innen des Deutschen Kunstarchivs bezüglich der Umsetzung eines Linked Open Data basierten Archivinformationssystems. Zum anderen wird die Modellierung der Domänenontologie auf Basis von CIDOC CRM abgebildet, die für die Umsetzung des Datenmodells notwendig ist.

            

            

        

            

                

                    
1. Forschungshintergrund
                

                
Marcel Reich-Ranicki kam 1955 und 1956. Der sowjetische Autor Michail Scholochow besuchte die DDR 1964, zwei Jahre bevor er den Nobelpreis erhielt; ähnlich der guatemaltekische Schriftsteller und spätere Nobelpreisträger Miguel Asturias, der 1965 nach Ostberlin reiste. Friederike Mayröcker folgte einer Einladung im Mai 1987. Andere kamen wiederholt, wie der ungarische Dichter Gábor Hajnal, der sich zwischen 1957 und 1986 dreizehn Mal in Ostberlin aufhielt. Eingeladen hatte jeweils der Deutsche Schriftstellerverband (DSV), über den der Großteil der internationalen literarischen Kontakte in der DDR organisiert wurde. Tausende Daten zur Einladungspolitik des Verbandes sind in einer Kartei in der Akademie der Künste in Berlin hinterlegt, deren Bestand im Rahmen der Archivarbeiten für das Forschungsprojekt »Writing Berlin« digitalisiert wurde.
                    

                    

                

                
»Writing Berlin« ist Teil des Exzellenzclusters »Temporal Communities. Doing Literature in a Global Perspective« (EXC 2020) und befasst sich mit den facettenreichen Aktivitäten zur Förderung des internationalen literarischen Austauschs in der geteilten Stadt nach dem Bau der Berliner Mauer. Ein besonderes Augenmerk liegt dabei auf den Auswahlprozessen und den kulturpolitischen Implikationen dieser Aktivitäten, ihrem Niederschlag in literarischen Texten sowie auf der Frage, inwiefern die sich verändernde politische Gemengelage Biografien und die soziale Stellung der betreffenden Autor*innen beeinflusste. Die Internationalisierung der Berliner Literaturszene ist bislang nur in einigen wenigen Fallstudien untersucht worden, vor allem im Hinblick auf die Netzwerktätigkeit einzelner Schriftsteller*innen (vgl. Böttiger 2005, Berbig 2005). Der institutionalisierte Austausch, der einen Großteil der internationalen Kontakte im Osten der Stadt ausmachte, war bislang noch nicht Gegenstand weitergehender Studien – zwar liegen allgemeine Untersuchungen zum Schriftstellerverband der DDR vor, diese erwähnen die politisch so relevante Auslandsarbeit der Organisation jedoch bestenfalls beiläufig (vgl. zum DSV allgemein Pamperrien 2004, Walther 2006, Michael et al. 1997) und betrachten lediglich einen sehr eingeschränkten Zeitraum (vgl. insbesondere zu den 1950er-Jahren Degen 2011, Gansel 1997). Die Einreisekartei des DSV, der wichtigsten nichtstaatlichen Literaturinstitution im Ostteil der Stadt, erlaubt es nun, die internationalen Kontakte und ihre Konjunkturen insbesondere in der spannungsgeladenen Zeit während des Bestehens der Berliner Mauer zu erforschen: den Verlauf dieser Aktivitäten insgesamt, die länderbezogene Einladungspolitik, die Umstände individueller Aufenthalte und ihre politische Rolle für das Herkunftsland. Sie ermöglicht auch, Literaturkontakte weniger um besonders hervorstechende Einzelpersonen zentriert zu denken und dabei gerade auch Autor*innen zu berücksichtigen, die durch Kanonisierungsprozesse der Vor- und Nachwendezeit ggf. in Vergessenheit geraten sind.

            

            

                

                    
2. Digitalisierung und Anreicherung mit OpenRefine
                

                
Zunächst wurden die Einreisekarteien im Archiv des DSV transkribiert. Als Grundlage dafür wurden die nach Ländern und Autor*innennamen geordneten Karteien verwendet. Für jede*n einreisende*n Autor*in existiert so mindestens ein separates Blatt, auf dem die verschiedenen Aufenthalte vermerkt sind. Die Mitarbeiter*innen der Auslandsabteilung des DSV ergänzten ggf. noch biografische Informationen oder auch ein Presse- oder Passfoto. Über die Jahrzehnte änderte sich vielfach die Art der Aufzeichnung, ein Großteil der etwa 3.000 Karteien orientiert sich jedoch an dem in Tabelle 1 wiedergegebenen Schema, das am Beispiel des kubanischen Dichters Nicolás Guillén in Abbildung 1 illustriert werden soll.

                

                    

                        
Name, ggf. Vorname

                        
Land

                    

                    

                        
[ggf. kurze biografische Information]

                    

                    

                        
Datum

                        
Aufenthaltsgrund

                    

                    

                        
Datum

                        
Aufenthaltsgrund

                    

                    

                        
…

                        
…

                    

                

                

                    

                        

                        
Abbildung 1: Beispiel für die Datengrundlage anhand der Karte zum kubanischen Autor Nicolás Guillén. Archiv des Schriftstellerverbands der DDR, Literaturarchiv der Akademie der Künste Berlin, Signatur SV 2848. (© Archiv der Akademie der Künste, Veröffentlichung mit freundlicher Genehmigung.)

                    
Qualität und Umfang der angegebenen Informationen variieren stark: Besonders in den 1970er-Jahren etwa wurden teils nur die Nachnamen der Autor*innen notiert, ggf. mit Zusätzen wie »Herr«/»Frau«, der Abkürzung »Gen.« (= Genosse) oder mit akademischen Titeln. Auch die Zeiträume der Aufenthalte sind nicht einheitlich angegeben, gelegentlich finden sich nur Jahres- oder Monatsangaben. Zudem handelt es sich überwiegend um per Hand getätigte Vermerke; nicht immer ist dabei die Schrift der Mitarbeiter*innen leicht zu entziffern. Auch erfolgten viele Angaben offenbar nicht auf Grundlage von Ausweisdokumenten, sondern durch Hörensagen. Gerade bei Autor*innen aus dem Globalen Süden etwa kann die Transkription der Namen inkorrekt sein oder sie wurde unsystematisch an deutsche Schreibgewohnheiten angepasst. Der nordkoreanische Autor Chŏng Tŏk-ch’ŏl wird beispielsweise mit »Dok Tschol Dschong« transkribiert, der afghanische Dichter Vāṣif Bākhtarī taucht in den Karteien als »Wassef Bachterie« auf, der indische Schriftsteller Harivansh Rai Bachchan unter der Schreibweise »Harbans Rai Bachhan«. Auch Namen von Autor*innen aus sozialistischen Bruderstaaten, etwa der ČSSR, Bulgarien oder Rumänien sind häufig fehlerhaft notiert.
                

                
Um die Informationen aus der nach Autor*innennamen sortierten Einreisekartei zu komplettieren, wurden auch die chronologischen Karteien herangezogen sowie etwa punktuell weitere Akten aus dem Archiv des DSV, etwa die zu etlichen Aufenthalten vorhandenen Freundschaftsverträge, Korrespondenzen und Zeitpläne. Als Ergebnis dieser Transkriptionsarbeit entstand eine Excel-Tabelle mit insgesamt 3.709 Einträgen. Die Tabelle enthält Informationen zum Zeitraum des jeweiligen Aufenthalts, den Autor*innen (Name und Staatsangehörigkeit), zu beteiligten Institutionen sowie Angaben zum Anlass bzw. Einladungsgrund.

                
Mit OpenRefine (Version 3.5.0) wurden die in der Excel-Tabelle enthaltenen Daten vereinheitlicht. So konnten Einträge, die zwar denselben Anlass betrafen, aber unterschiedlich verschriftlicht waren, zusammengeführt werden. 

                
In einem weiteren Schritt wurden die teils in problematischer Weise notierten Autor*innennamen über OpenRefine aufbereitet und mit Normdatensätzen verknüpft. Dadurch wurde zum einen die Verifizierung bzw. Identifizierung der in der Kartei verzeichneten Einträge vereinfacht; zum anderen konnten aus den verknüpften Datenbanken weitere Informationen zu den Autor*innen importiert werden.

                
Ein erstes umfangreiches Reconciling erfolgte mit dem Virtual International Authority File (VIAF). Als Grundlage hierfür diente der von Jeff Chiu über Codefork bereitgestellte Reconciliation Service (Version 3.0.5, 
                    
), der einen erfolgreichen Abgleich von zunächst etwa 25 % der Einträge ermöglichte. Nach manuellen Suchstrategien in der Datenbank, etwa dem Ausprobieren verschiedener Schreibweisen und dem Abgleich mit im VIAF hinterlegten biobibliografischen Daten, konnte die Trefferquote auf fast 90 % erhöht werden.
                

                
Ein weiteres Reconciling wurde – über das in OpenRefine integrierte Tool – mit Wikidata vorgenommen. Auch hier konnte durch einige Nachjustierungen eine hohe Trefferquote von 75 % erzielt werden. Der erfolgreiche Abgleich ermöglichte nun den Import weiterer Informationen aus Wikidata, etwa Angaben zu Sprachen, Parteizugehörigkeit oder Geschlecht der Autor*innen. Zudem konnten weitere Identifier über Wikidata importiert und somit Schnittstellen zur Gemeinsamen Normdatei der Deutschen Nationalbibliothek (GND) und zum WorldCat geschaffen werden, wodurch nun auch bibliografische Informationen zu den eingeladenen Autor*innen recherchierbar sind.

                
Jeder einzelne der hier dargestellten Schritte stellt eine Interpretationsleistung der Daten dar, die ihrerseits wieder nur heuristisch erfolgen, unvollständig und fehlerbehaftet sein kann. Bei dem über OpenRefine bereinigten und abgeglichenen Datensatz handelt es sich somit nur um eine mögliche Lesart der ursprünglichen Einreisekarteien, die der fortwährenden Überprüfung und Modifizierung bedarf.

            

            

                

                    
3. Schnittstelle, Web-App und Modellierung unvollständiger Datumsangaben
                

                

                    

                        

                        
Abbildung 2: Prototypische Ansicht der Web-App für den Eintrag zu Nicolás Guillén.

                    

                    
Um den digitalisierten und erschlossenen Datensatz besser erforschbar zu machen, wurde für die Daten eine Schnittstelle geschaffen, über die sie maßgeschneidert ausgeliefert werden können. Diese Schnittstelle dient auch als Grundlage für die Web-App, die einen explorativen Zugang zum Datenmaterial ermöglichen soll (Abbildung 2). Die App wurde in TypeScript mit dem SvelteKit-Framework geschrieben (vgl. 

                    

                    
), welches das Front-End-Framework Svelte (vgl. 

                    

                    
) in ein umfangreiches Web-Framework mit serverseitigem Rendering (SSR) integriert. Svelte agiert, anders als das vergleichbare React, nicht während der Laufzeit, sondern als Compiler. Zum Build-Zeitpunkt werden alle HTML-Vorlagen zu nativen JavaScript-Funktionen kompiliert. Ein Vorteil ist der Verzicht auf ein virtuelles DOM (Document Object Model) und die Berechnung der damit einhergehenden Deltas (vgl. Harris 2018). SvelteKit, das sich derzeit noch in der Beta befindet, gilt als Svelte-Pendant zu Reacts Next.js. Damit läuft Svelte auch auf dem Server und ist in der Lage, Seiten dort entweder zum Build- oder Anfragezeitpunkt vorzurendern und anspruchsvolle Datenoperationen auszuführen, während clientseitig die volle Flexibilität eines reaktiven Frameworks erhalten bleibt (vgl. 

                    

                    
). Im 

                    
vorliegenden Projekt sind das etwa Daten aus Wikidata, die zur Anreicherung des bestehenden Datensatzes gebündelt serverseitig abgerufen und transformiert werden.

                

                
Wegen der teils unvollständigen Datumsangaben haben wir auf das Extended Date/Time Format (EDTF) gesetzt. Dieses 2019 von der International Organization for Standardization als Erweiterung zu ISO-8601 gedachte Datumsformat erlaubt es unter anderem, verschiedene Arten von Ungewissheit formalisiert auszudrücken. Für die Zwecke dieses Projekts besonders fruchtbar ist die Einbeziehung von »unspecified digits« (Library of Congress 2019), die unbekannte Teile eines Datumsformats explizieren: Ein nicht spezifizierter Tag im Februar 1972 kann etwa als »1972-02-XX« dargestellt werden, der gleiche Fall bezogen auf einen Tag im Jahr 1986 als »1986-XX-XX«. Darüber hinaus muss die Ungewissheit nicht zwingend von den niedrigstwertigen Stellen herrühren – auch »XXXX-09-24« oder sogar »19XX-05-XX« sind gültige EDTF-Werte. Zwar existiert eine JavaScript-Bibliothek zum Parsen von EDTF-Datumsangaben (vgl. Keil 2022), nicht jedoch zur menschenlesbaren Darstellung. Die Logik zur sprachenübergreifenden Darstellung unvollständiger Angaben wurde deshalb eigens in TypeScript implementiert.

            

            

                

                    
4. Statistische Narrative und Ausblick
                

                
Mit den vorliegenden Daten kann ein spezifischer Aspekt des literarischen Lebens in der DDR nun zum ersten Mal auch statistisch ausgewertet werden. Durch chronologische Verlaufsdiagramme zeichnen sich Einladungstendenzen ab, die sich unter anderem politisch deuten lassen.

                

                    

                        

                        
Abbildung 3: Einladungsfrequenz von Autor*innen nach Staatsform.

                    

                

                

                    
So zeigt etwa Abbildung 3, dass fast durchgängig signifikant mehr Autor*innen aus sozialistischen Bruderstaaten eingeladen wurden (vgl. dazu Müller-Tamm 2021). Eine Ausnahme bildet das Jahr 1965 mit dem Internationalen Schriftstellertreffen im Mai, das in Berlin und Weimar stattfand (dokumentiert in: Deutscher Schriftstellerverband 1965).

                

                
Ein Blick auf süd- und westeuropäische Länder zeigt nur sporadische Besuche, mit der Ausnahme Frankreichs, dessen breit aufgestellte Linke teils verstärkt mit dem Schriftstellerverband der DDR kooperierte (vgl. Fabre-Renault 2015). Auch mit Autor*innen aus dem englischsprachigen Ausland, vor allem den USA und Australien, gab es noch in den 1960er-Jahren einen vergleichsweise regen Austausch, der in den 1970er-Jahren allerdings vollends zum Erliegen kam.

                
Dokumentieren lässt sich auch ein hohes Interesse des DSV an Autor*innen aus den sich als neutral verstehenden Staaten Finnland und Schweden, die in den 1960er-Jahren von der SED zu Schwerpunktländern auslandspropagandistischer Aktivitäten erkoren wurden:
                    

                    
 Im Laufe der Jahre ergingen vom Schriftstellerverband etwa 100 Einladungen in das recht dünn besiedelte Nordeuropa, unter anderem an die Nobelpreisträger Halldór Laxness (Island), Eyvind Johnson (Schweden) sowie die finnische Star-Autorin und PEN-Präsidentin Eeva Joenpelto.
                

                
Autor*innen aus Ostblockstaaten waren jedoch weitaus regelmäßiger bei literarischen Terminen in Ostberlin zu Gast. Hier zeigen die Daten, dass sowjetische Besucher*innen stets in der Überzahl waren, ein Beleg für die Quotenregelung, die der Einladungspolitik zugrunde lag.

                
Die von uns angebotene Schnittstelle ermöglicht viele weitere statistische Anfragen. Ihre Funktion ist aber nicht auf die projektbezogene Auswertung beschränkt. Vielmehr kann der von uns erstellte, semantisch angereicherte Datensatz auch langfristig eine Funktion im wachsenden Ökosystem der digitalen Literaturwissenschaft übernehmen und bietet sich für den Austausch mit komplementären Projekten wie der »Forschungsplattform Literarisches Feld DDR« an (vgl. 
                    
). Reisedaten von Autor*innen können biografische Datenbanken ergänzen. Gleichzeitig konnten wir den Datensatz durch das Ausstatten mit Normdaten breiter kontextualisieren und mit Zusatzinformationen speisen. Insofern bietet der Datensatz jenseits der statistischen Auswertung auch die Möglichkeit einer enzyklopädischen Nutzung für Fallstudien: Interessierte Wissenschaftler*innen unterschiedlichster Disziplinen und Philologien
                    

                    
 können ihn als Einstiegspunkt für Erkundungen von bi- und multilateralen Literaturkontakten nutzen, indem sie sich innerhalb der Web-Anwendung durch Länderlisten und Personeneinträge bewegen. Auf diese Weise können sie etwa nachvollziehen, welche Autor*innen aus dem Ausland zu welchen Anlässen und Zeitpunkten über den Schriftstellerverband in die DDR entsandt wurden, in welchen Delegationen sie durch die DDR reisten, wem sie bei Kongressen, Rundtischgesprächen oder Studienaufenthalten gegenübersaßen. Über die Verknüpfung mit Wikidata und VIAF ist es dabei auch möglich, Bezüge zu biografischen Eckdaten (Parteizugehörigkeit, Ehrungen und Literaturpreise), Veröffentlichungen und Übersetzungen im In- und Ausland herzustellen. In Planung ist zudem ein analoges Vorhaben zu den internationalen, institutionalisierten Literaturkontakten im Westen der Stadt (DAAD, Literarisches Colloquium Berlin etc.), für das bereits etliche Daten vorliegen. Wenngleich sich der Literaturaustausch in Ost und West schon allein durch unterschiedliche politische Strukturen nur bedingt vergleichen lässt, wäre es damit auch möglich, Korrespondenzen, (personelle) Überschneidungen und Konkurrenzsituationen zur Zeit der Berliner Mauer aus unterschiedlichen Perspektiven zu beleuchten.
                

            

            

                

                    
Fördernachweis
                

                
Gefördert durch die Deutsche Forschungsgemeinschaft (DFG) im Rahmen der Exzellenzstrategie des Bundes und der Länder innerhalb des Exzellenzclusters Temporal Communities: Doing Literature in a Global Perspective – EXC 2020 – Projekt-ID 390608380.

            

        

            

                
Das D-WISE Projekt

                
Das Verbundprojekt D-WISE (
                    
www.dwise.uni-hamburg.de
) ist ein BMBF gefördertes interdisziplinäres Kooperationsprojekt an der Universität Hamburg zwischen den Geisteswissenschaften, vertreten durch das Institut für Empirische Kulturwissenschaft, und dem Fachbereich Informatik, vertreten durch die Arbeitsgruppe Language Technology (Koch et al. 2022). D-WISE entwickelt neue informatische Analyseverfahren unter Einsatz von kontextorientierten Embedding-Repräsentationen und eine Arbeitsumgebung (die D-WISE Tool Suite) als digitale Unterstützung von wissenssoziologischen Diskursanalysen (Keller 2011). Dabei orientiert sich D-WISE an zwei grundlegende Fragestellungen: (1) Zu welchen Zwecken, wann und wie können Automatisierung und DH-Methoden sinnvoll in qualitative diskursanalytische Ansätze und Wissensproduktion integriert werden; welche bestehenden Methoden und Tools können übernommen werden und welche müssen neu entwickelt werden? (2) Wie können hermeneutische Methoden durch die Nutzung (halb-)automatisierter Forschungsprozesse weiterentwickelt werden?
                

                
Das D-WISE Projekt zielt darauf ab, den Mangel an digitalen Lösungen für multimodale Diskursanalysen zu beheben. Die Lösungen sollen in der Lage sein, mit der Multimodalität von Materialien sowie der Pluralität von Bedeutungen umzugehen, welche vielfältige Herausforderungen und hohe Komplexitätsanforderungen für digitale Lösungen darstellen. Gleichzeitig müssen die ständig wachsende Zahl digitaler Materialien bewältigen werden, indem die Methodik durch digitale Methoden erweitert wird. 

                
Die Überbrückung der Lücke zwischen strukturellen Mustern, die mit digitalen Methoden aufgedeckt werden, und interpretativen Prozessen menschlicher Bedeutungsproduktion steht im Mittelpunkt des kollaborativen Ansatzes von Kulturanthropologie und Sprachtechnologie im D-WISE-Projekt. Durch die Kombination von manuellen und digitalen Ansätzen zur Diskursanalyse wird die D-WISE Tool Suite für die digitale qualitative Diskursanalyse in enger Co-Creation Arbeitsweise (Eiser et al. 2023) zwischen Wissenschaftler:innen aus den Geisteswissenschaften und der Informatik geschaffen.

            

            

                
Die D-WISE Tool Suite für KI-gestützte Wissenssoziologische Diskursanalysen

                
Die D-WISE Tool Suite
 (DWTS) wird als offen zugängliche Webapplikation entwickelt und ausgehend von der Evaluation und Reflektion von etablierten DH-Methoden sowie digitalen Tools (u.a. MAXQDA, CATMA, WebAnno) konzipiert. DWTS erweitert das bestehende Angebot und verbessert den Forschungsprozess, insbesondere Diskursanalysen, mittels künstlicher Intelligenz (KI) gestützten Funktionalitäten und teilautomatisierten Prozessen. Hierbei spielen die sich ergänzenden Konzepte von Human-in-the-Loop (Holzinger 2016) und AI-in-the-Loop (Koch et al. 2022) eine zentrale Rolle und werfen für D-WISE relevante Fragestellungen hinsichtlich der Interaktion von Mensch und Algorithmus zur Verbesserung von KI-Forschungssystemen und deren Auswirkungen auf den menschlichen Forschungsprozess auf. Zur Umsetzung werden, extern wie intern, Open-Source-Software, -Bibliotheken und -Lösungen verwendet. 
                

                
Das Projekt befindet sich in seinem zweiten Projektjahr mit einer entwickelten Tool Suite, in der zunächst Standard-Funktionalitäten implementiert wurden, wie das Suchen und Filtern, Kodieren und Annotieren und Dokumentieren. Insbesondere wurde auch ein besonderer Fokus auf projekt-zentriertes und kollaboratives Arbeiten, sowie die Unterstützung aller diskursanalytischen Schritte gelegt – vom Crawling und Filtern von Daten, über die Spezifikation des Forschungsphänomens bis hin zu Analyse, Auswertung und Interpretation, aber auch Dokumentation und Reflexion. Die DWTS unterstützt dabei die drei Kodierungsphasen des offenen, axialen und selektiven Kodierens der Grounded Theory, wodurch Muster und Konzepte identifiziert, verknüpft und analysiert werden können. Auch implementiert wurde die kollaborative Arbeit und Memofunktion. Die D-WISE Tool Suite bietet ferner die automatische Erkennung und Codierung von Entitäten
, wie z.B. Akteur:innen, Organisationen oder Orten sowie Bild-Annotationen. Das integrierte Logbuch ermöglicht einerseits eine automatisierte Erfassung aller getätigten Änderungen zur Dokumentation des Forschungsprozesses sowie die Möglichkeit eines manuell zu führenden Feldtagebuchs.
                

                
In den letzten Jahren gab es bedeutende Durchbrüche bei verschiedenen Aufgaben der Computer Vision (ViT, Dosovitskiy et al. 2021), der Verarbeitung von Natural Language Processing Tasks (BERT, Devlin et al. 2019) sowie bei multimodalen Aufgaben (CLIP, Radford et al. 2021). Entsprechende Erkenntnisse und Entwicklungen erlauben dem D-WISE Team multimodale Daten, die aus Text, Bild, (zukünftig auch Video oder Audio) oder einer Mischung aus allen Modalitäten bestehen, innerhalb der D-WISE Tool Suite zu verarbeiten. Insbesondere wird dadurch eine multi-modale Ähnlichkeitssuche
, also beispielsweise das Auffinden ähnlicher Bilder zu einem Suchtext, ermöglicht. Diese semantische und multi-modale Suchfunktionalität ist neben einer lexialischen Suche in DWTS integriert.
                

                
Im Gegensatz zu meist lizenzbasierten und kostenpflichtigen, umfangreichen ‚All-in-one-Lösungen‘ für qualitative Datenanalyse wie MAXQDA, Atlas.ti oder NVivo
                    
, 
wird die D-WISE Tool Suite als frei verfügbare Open-Source-Software mit Fokus auf die Wissenssoziologische Diskursanalyse konzipiert. Daher sind hermeneutisch-zirkuläre Methoden, Filterung und skalierbares Lesen vorherrschende Konzepte. Die Tool Suite unterstützt alle diskursanalytischen Schritte und erlaubt einen zyklischen Forschungsprozess, der eng an den hermeneutischen Zirkel angelehnt ist: DWTS soll beim Hinterfragen und der ständigen Erweiterung des Wissenstandes unterstützen, um zu neuen Fragestellungen, Erkenntnissen und somit zu einem tieferen Verständnis des Phänomens zu gelangen.
                    
Im nächsten Projektjahr wird KI-gestützte Sprach- und Videoverarbeitung sowie die Entwicklung von Datenanalysefunktionen vorangetrieben.
                

            

        

            
Das Ziel wissenschaftlicher Editionen besteht seit jeher in der Nachnutzung durch die (wissenschaftliche) Community. Unter den Vorzeichen von Open Data und Open Science ändern sich allerdings die Möglichkeiten der Bereitstellung und Nachnutzung des erschlossenen Materials. Gleichzeitig haben Wissenschaftler:innen, die mit Methoden der Digital Humanities arbeiten, andere Anforderungen und Bedarfe an bereitgestellte Daten z.B. im Hinblick auf den Umfang der Korpora und die spezifischen Datentypen. Ziel unseres Beitrags ist es, anhand der unterschiedlichen, von uns im digitalen Editions- und Forschungsprojekt 
                
Dehmel digital
 (Nantke 2022) produzierten Datentypen systematisch Szenarien der digitalen Nachnutzung durchzuspielen und anhand von Beispielen zu präsentieren. Die Darstellung ist projektbezogen und nicht erschöpfend in Bezug auf alle denkbaren Datentypen und Nutzungsmöglichkeiten. Dennoch sollen die dargestellten Szenarien in ihrer Bandbreite exemplarisch auch für andere Projektkontexte fungieren können.
            

            
Wir beziehen uns auf folgende Datentypen: 1) Metadaten von Briefen unterschiedlicher Schreibender aus dem Korrespondenznetz von Ida und Richard Dehmel, 2) digitale Bilder der Dokumente, 3) maschinenlesbarer Text der Briefe, 4) Annotationen von Entitäten sowie 5) algorithmische Modelle.

            
In Abhängigkeit vom jeweiligen Datentyp ergeben sich unterschiedliche Nachnutzungsszenarien. Diese reichen von dem aus editorischer Sicht klassischen Szenario der Nutzung der bereitgestellten Daten in (literatur)wissenschaftlichen Analysen über die Nutzung zur Produktion eigener Korpora und Modelle, die dann wiederum Gegenstand der Nachnutzung werden können, bis hin zur Algorithmen-gestützten Reflexion der konzeptuellen Grundlagen einer solchen Datensammlung.

            

                

            
Abb.1: Datentypen und Nachnutzungsszenarien im Überblick

            
Szenario 1: Analyse von Briefinhalten auf der Basis von Datentyp 3

            
Briefe stellen relevante Quellen für die Rekonstruktion historischer Diskurse dar (Baillot 2011). Diese in einem Gesamtüberblick und nicht nur in Einzelbeispielen zu erfassen, ist mittels Close Reading-Verfahren kaum zu bewältigen. Ein zentrales computergestütztes Nachnutzungsszenario für unsere Daten sind daher Analysen mittels Distant Reading-Verfahren. Auf der Basis der erzeugten Transkripte kann u.a. eine automatisierte Exploration der zentralen Briefinhalte über Topic Modeling umgesetzt werden (Andorfer 2017; Henny-Kramer/Neuber 2023). Ergänzend hierzu lassen sich z.B. stimmungsmäßige Gewichtungen in den Briefen durch Sentiment Analysis ermitteln und zu den Topics ins Verhältnis setzen.

            
Szenario 2: Korrespondenznetze sichtbar machen auf der Basis von Datentyp 4

            
In den im Projekt 
                
Dehmel digital
 produzierten Daten sind Entitäten (Personen, Orte, Institutionen, Werke) annotiert. Netzwerkanalysen auf der Basis dieser Annotationen bieten die Möglichkeit, Dynamiken innerhalb der vernetzten Kommunikationspraxis offenzulegen und genauere Einblicke in personelle Kontakte, organisatorische Strukturen und räumliche Bewegungen zu erlangen (Nantke/Bläß/Flüh 2022).
            

            
Szenario 3: Vernetzung mit anderen Briefeditionen auf Basis von Datentyp 1 und 3

            
Die von uns erzeugten Briefmetadaten können über die Plattform 
                
correspSearch
 abgerufen werden. Dadurch können Nachnutzende unsere Daten im Rahmen individueller Suchanfragen in Kombination mit den Daten anderer Briefeditionen nutzen.
            

            
Szenario 4: Texte erschließen auf der Basis von Datentyp 5

            
Neben den erschlossenen Dokumenten stellen wir auch die im Projekt von uns trainierten HTR- und NER-Modelle zur Nachnutzung zur Verfügung. Auf Basis dieses Datentyps können weitere Dokumente, die nicht Teil des Projekts sind, erschlossen und somit neue Daten für die weitere Nachnutzung produziert werden. Dies gilt zum einen für handschriftliche Dokumente der Schreibenden, für die wir HTR-Modelle trainiert haben (z.B. Stefan Zweig, Detlev v. Liliencron, Julie Wolfthorn). Zum anderen können die Named Entity-Classifier für die Erschließung weiterer deutschsprachiger Briefe aus einem ähnlichen Zeitraum genutzt werden. Es besteht auch die Möglichkeit, auf der Basis unserer Trainingsdaten spezifische Modelle für andere Anwendungsfälle nachzutrainieren (Flüh/Lemke 2022).

            
Szenario 5: gemischte HTR-Modelle trainieren auf Basis von Datentyp 2 und 3

            
Die in 
                
Dehmel digital
 teilautomatisiert generierten, qualitativ hochwertigen Transkripte zahlreicher unterschiedlicher Schreibender bieten in Kombination mit den zugehörigen Bilddigitalisaten den idealen Ausgangspunkt für das Training sog. ‘gemischter Modelle’, mit deren Hilfe sich deutlich mehr unterschiedliche Handschriften aus dem Zeitraum um 1900 transkribieren lassen.
            

            
Szenario 6: Reflexion der theoretischen Fundierung von Datensammlungen auf Basis von Datentyp 5

            
Unsere NER-Classifier wurden auf den Dokumententyp ‘Brief um 1900’ trainiert. Eine experimentelle Anwendung z.B. des Orte-Classifiers auf ein Korpus mit Texten eines deutlich abweichenden Dokumententyps (z.B. fiktionale Texte) kann insbesondere in Kombination mit einem auf den Dokumententyp zugeschnittenen Classifier dazu beitragen, die theoretisch-konzeptuelle Fundierung offenzulegen, welche in die Modellierung des Classifiers eingegangen ist, indem die Ergebnisse der Classifier vergleichend betrachtet werden (vgl. dazu die Fallstudie von Flüh/Schumacher/Nantke im Erscheinen).

        

            
Oral History sieht sich durch die Digitalisierung großen Veränderungen ausgesetzt. Angesichts der digitalen Aufnahmetechnologien stellte sich schon vor Jahren die Frage nach der Archivierung der „born digital“ Quellen. Diese Herausforderung bietet aber auch neue Chancen und Perspektiven: etwa die automatische Spracherkennung digitaler Audiosignale, computergestützte Transkription und die komfortable Suche in Online-Repositorien (Leh 2015, Leh 2018, Gref/Köhler/Leh 2017). In den letzten Jahren konnten große Fortschritte im Bereich der automatischen Spracherkennung gemacht werden, weshalb auch die Zahl digitaler Transkripte deutlich angestiegen ist (Köhler et al. 2019). Mit Oral-History.Digital entsteht zurzeit das größte Portal zur Archivierung und Präsentation lebensgeschichtlicher Interviews in digitaler Form im deutschsprachigen Raum - damit sind auch die historische und qualitative Forschung auf dem Weg in die „Big Data“ (Graham et al. 2015).
 Um die wachsende Quellenbasis erschließen zu können, bedarf es perspektivisch automatisierter Verfahren, die das klassisch hermeneutische Arbeiten ergänzen. Mit den Digital Humanities haben Verfahren maschinellen Lernens Einzug in die Oral History gehalten, um Texte systematisch inhaltlich zu analysieren.
            

            
Als Heuristik zur Erschließung großer Textkorpora hat sich etwa das Topic Modeling etabliert (Graham et al. 2015, Lemke/Wiedemann 2016, Adelmann et al. 2019). Mit diesem Verfahren werden über Wahrscheinlichkeitsrechnung Gruppen von Wörtern extrahiert, die miteinander in Zusammenhang stehen. Gut trainierte Topic Models ermöglichen zum Beispiel die Extraktion thematischer Zusammenhänge aus kompletten Sammlungen lebensgeschichtlicher Interviews und vermitteln einen ersten inhaltlichen Überblick (Hodel et al. 2022). Darüber hinaus können bei explorativer Durchsicht der Ergebnisse unerwartete Phänomene an die Oberfläche gespült werden, die in den vielschichtigen Interviews allzu leicht verschütt gehen (Möbus 2022). Gerade für Sekundäranalysen ist das Verfahren deshalb vielversprechend (Franken 2022).

            
Allerdings wird im Machine Learning noch zu selten qualitativ evaluiert, um die automatisch generierten Ergebnisse zu validieren (Dobson 2021). Des Weiteren mangelt es in den DH an systematischen Studien, die computationelle und menschliche Inhaltserschließung vergleichen, auch wenn erste Ansätze bestehen (Andorfer 2017, Baumer 2017, Fechner/Weiß 2017, Andresen et al. 2020). Schließlich ist die Hemmschwelle zum Einstieg in die Verwendung digitaler Methoden bei qualitativ Forschenden besonders hoch (Franken 2020). Um diesen Desiderata und Vorbehalten zu begegnen, wurde das Potential des Topic Modeling für die Aufbereitung größerer Datenmengen nach Grounded Theory durch die Autor:innen systematisch getestet. In einem am Turing-Test orientierten Versuchsaufbau haben wir im Rahmen eines zweitägigen Workshops lebensgeschichtliche Interviews verschlagwortet - einmal klassisch qualitativ (also manuell), einmal auf Grundlage von Topic Modeling (also maschinell). Leitendes Erkenntnisinteresse des Workshops war, wie das maschinelle Verfahren qualitative Analysen bereichern kann, welche Unterschiede entstehen, wenn Interviewmaterialien durch ein Topic Modeling strukturiert gesichtet werden und wie sich Perspektiven auf (unbekannten) Text unterscheiden. Topic Modeling wurde als Verfahren gewählt, weil es einen intuitiven Zugang zu großen Textmengen ermöglicht und zudem mit Interviewtranskripten gut zurechtkommt (was viele computerlinguistische Verfahren aufgrund der abweichenden Satzstruktur bisher leider nur begrenzt tun). Es ging uns dabei nicht darum, Grounded Theory und Topic Modeling als vergleichbar zu setzen, sondern zu prüfen, wie sich die beiden Methoden ergänzen und wie sich Sinnstiftungsprozesse je nach Zugang ausgestalten. Die schlussendlichen Annotationen an den Transkripten wurden mit im Vorfeld vorbereiteten Projekten in Catma (Gius et al. 2022) durch die Gruppen umgesetzt.

            
Im Mittelpunkt des Workshops stand das konkrete Arbeiten am Textkorpus aus unterschiedlichen Perspektiven sowie die gemeinsame Reflexion dieser Perspektiven. Der Beitrag stellt die Ergebnisse dieses Experiments vor und diskutiert die Mehrwerte sowie weitere Anschlussmöglichkeiten. Dafür wurden insgesamt sechzehn Teilnehmende in vier interdisziplinäre Gruppen mit jeweils gemischten Kompetenzen und Vorwissen unterteilt, sodass beispielsweise Teilnehmende mit Erfahrung in qualitativer Forschung und Teilnehmende, die bereits mit Topic Modeling vertraut waren, zusammenarbeiteten. Insgesamt setzten sich die Gruppen sehr heterogen aus Studierenden, Promovierenden und Promovierten zusammen.

            
Gearbeitet wurde parallel in den Gruppen mit einem umfangreichen, digital erschlossenen Korpus von Transkripten lebensgeschichtlicher Interviews zur Sozialgeschichte des 20. Jahrhunderts, dem Bestand Lebensgeschichte und Sozialkultur im Ruhrgebiet (LUSIR). Dieser geht zurück auf das erste großangelegte Oral-History-Projekt in Deutschland, das von Lutz Niethammer und Alexander von Plato 1981 bis 1988 durchgeführt wurde (Niethammer 1983). Die Interviews sind transkribiert und mit Timecodes versehen und können über das Archiv “Deutsches Gedächtnis” online eingesehen und angehört werden.
 Für das dem Experiment zugrunde liegende Sample wurden 166 Volltexte herangezogen. Durch die Laufzeit der Interviews von bis zu acht Stunden hat das Korpus einen Umfang von 3,7 Millionen Wörtern, wovon nach Stoppwordbereinigung gut 700.000 übrigbleiben. Das Topic Modeling wurde mit Mallet, allerdings mit Hilfe des Gensim-Wrappers in Python, umgesetzt, nutzt somit LDA mit Gibbs-Sampling als Inferenzalgorithmus. Nach der Evaluation verschiedener Modelle und umfangreichem Parametertuning konnte festgestellt werden, dass eine Aufteilung der Interviews in kürzere Einheiten (Chunks) zu 25 Sätzen inhaltlich konsistente und aussagekräftige Topics hervorbringt, als optimale Topic-Anzahl hat sich 50 herausgestellt (vgl. ausführlich: Hodel et al. 2022, 188f., 194f.). 
            

            
Als Sample hatten die Organisator:innen im Vorfeld in einer Pilotstudie zahlreiche Interviewpassagen gesichtet, die mit arbeitsspezifischen Topics gelabelt waren, und ein Sample zusammengestellt. In zwei der vier Gruppen wurden drei ausgewählte Interviewpassagen zunächst manuell annotiert (Kategorienvergabe oder Codierung), während die beiden anderen Gruppen mit einer Erschließung der noch unbekannten Texte durch das vortrainierte Topic Modeling starteten. Danach tauschten die Gruppen die Rollen, um die Unterschiede systematisch vergleichen zu können. Als Fragestellung wurde das Thema „Arbeit“ in all seinen Facetten gesetzt, um vergleichbare Ergebnisse zu erhalten. Der exemplarische Zugang wurde gewählt, weil der arbeitsspezifische Wandel im 20. Jahrhundert gesellschaftlich relevant ist (Stichwort: vom Normalarbeitsverhältnis zu prekärer und/oder entgrenzter Arbeit). In Oral-History-Interviews finden sich fast durchgängig Aussagen zur persönlichen Arbeitsbiografie und Einschätzungen zum Wandel des eigenen Arbeitsumfeldes. Sie sind deshalb als Daten für die computationelle Analyse zum Thema Arbeit besonders vielversprechend.

            
Die qualitative Annotation erfolgte auf Grundlage der Grounded Theory (Glaser/Strauss 2010 [1967]; Charmaz 2014). Diese ist als Analysemethode in der qualitativen Sozialforschung und den empirischen Kulturwissenschaften weit verbreitet und auch für die Erweiterung qualitativer Forschungsprozesse gut geeignet (Franken 2022). Sie ermöglicht ein induktives Vorgehen, das aus dem Material heraus Bedeutungen und Kontexte erschließt und besonders in offenen und selektiven Annotationen als Prozess (Franken/Koch/Zinsmeister 2020) Teil des Erkenntnisprozesses ist. Allerdings wurde das Methodensetting im Workshop nicht in seiner vollen Komplexität realisiert, sondern für den experimentellen Aufbau auf den Schritt des offenen Annotierens reduziert. Dabei werden Textabschnitte gelesen und aus dem hermeneutischen Sinnerschließen heraus Kategorien gebildet. Diese werden direkt an einzelne Textstellen vergeben, so dass das Kategoriensystem bei der Texterschließung nach und nach wächst und strukturiert wird (Holton 2007). Weitere Schritte, wie das theoretische Sampling, auf dessen Grundlage zentrale Quellen aus Korpora ausgewählt werden (Morse 2007), oder das anschließende selektive Annotieren, mit dem das Kategorienset je nach Erkenntnisinteresse wieder reduziert wird, wurden aufgrund der begrenzten Zeit im Workshop nicht realisiert. Auch eine die Analyse begleitende Verschriftlichung von Gedanken in Memos (Lempert 2007) wurde nicht umgesetzt, da eine umfassende Auswertung der im Vorfeld ausgewählten Textstellen nicht Ziel war, sondern der Vergleich der unterschiedlichen Texterschließungen im Mittelpunkt stand. Konkret war der Arbeitsauftrag an die Gruppenmitglieder, die Textstellen zu lesen und offene Kategorien zu vergeben, um die im Text enthaltenen Inhalte möglichst gut zu beschreiben. Im Anschluss an einzeln umgesetzte Annotationen vergaben die Gruppen dann in einer Diskussion gemeinsame Kategorien an den Textstellen, abstrahierten also von der individuellen Interpretation.

            
Die Analyse der Topic Models erfolgte zunächst auf globaler Ebene. Dazu standen in vorher vorbereiteten Jupyter-Notebooks verschiedene Funktionen zur Verfügung: Topiclisten mit variabler Keyword-Anzahl, Balkendiagramme, welche die Topic-Verteilung zeigen, und Heatmaps, die einerseits die globale Verteilung der Topics auf die Interviews, andererseits die Verteilung der Topics über die in Textpassagen zerteilten Interviews im zeitlichen Verlauf zeigen. Zunächst wurden die Ergebnisse des Topic Modelings - also je eine Wortliste zu jedem der fünfzig Topics - einzeln gesichtet und dann in der Gruppe diskutiert. Topics mit Bezug zum Thema “Arbeit” wurden anschließend mit einem Schlagwort gelabelt. So wurde dem Topic „'chef', 'büro', 'angestellt', 'abteilung', 'thyssen', 'sekretärin', 'abteilungen', 'arbeit', 'herren', 'damen' [...]“ von einer Gruppe etwa die Kategorie “Anstellung/Verwaltung” zugewiesen, dem Topic „'betriebsrat', 'gewerkschaft', 'betrieb', 'kollegen', 'gewerkschaften', 'betriebsräte', 'vorsitzend', 'wählen', 'metall', 'belegschaft' [...]“ die Kategorie „Interessenvertretung/Betriebsperspektive“. Anschließend wurden mit Hilfe des Jupyter-Notebooks stichprobenartig Interviewpassagen ausgegeben, die den relevanten Topics zugeordnet waren, um die Qualität der Topics qualitativ zu überprüfen. Im nächsten Schritt gab die Workshopleitung die vorher ausgewählten Interviewpassagen bekannt, um den Vergleich mit den qualitativ arbeitenden Gruppen sicherzustellen.
 Die Teilnehmenden sollten beurteilen, ob sie anhand ihrer in der freien Exploration gesammelten Eindrücke auch zu diesem Sample gelangt wären. Abschließend wurden die Oberbegriffe der stärksten Topics einer Textpassage (Chunk) in Catma an die Interviewtranskripte getagged. Die Vergabe der Topics für die jeweiligen Interviewpassagen konnten die Teilnehmenden ebenfalls dem Jupyter-Notebook entnehmen.
            

            
Insgesamt stehen aus den Gruppen acht verschiedene Schlagwort-Sets pro Chunk zum Vergleich zur Verfügung (vier Gruppen, pro Gruppe rein menschliche Schlagwörter sowie Oberbegriffe für die Topics). Im Ergebnis lässt sich feststellen, dass für ein Topic, also eine Wortliste, zwar unterschiedliche, aber dennoch vergleichbare Oberbegriffe gewählt wurden. Beispielsweise wurden für das bereits erwähnte Topic, für das eine Gruppe den Begriff „Anstellung/Verwaltung“ vergab, von den anderen Gruppen die Oberbegriffe „Positionen in einer Firma“, „Organisation von Arbeit“ und „Arbeitsorganisation“ festgelegt. In den rein menschlich vergebenen Schlagwörtern für eine Textstelle ergeben sich ebenfalls Unterschiede, die jedoch grundsätzlich auf ein vergleichbares Textverständnis schließen lassen. Die Unterschiede der gewählten Kategorien bestehen hier eher in der thematischen Schwerpunktsetzung. Während eine Gruppe „Technische Entwicklung“ benannte, wählte eine andere Gruppe „Wandel Arbeitstechnik“ für den gleichen Textabschnitt. Bei den durch eine Gruppe benannten „Arbeitskonflikte[n]“ annotierte eine andere Gruppe „Arbeitsbedingungen“, nahm also ebenfalls ähnliche Interpretationen vor, wenn auch in geringerer Deutlichkeit. In der Annotation wurden für die maschinengenerierten Topics wesentlich allgemeinere Kategorien vergeben, die menschliche Annotation erfolgte zielgenauer und hat in verschiedenen Dimensionen die den Forschenden bekannten Kontexte (etwa zu historischen Ereignissen) einbezogen.

            
Im Anschluss an die Arbeit in den Gruppen wurde in einer Abschlussdiskussion das unterschiedliche Vorgehen verglichen und diskutiert. Besonders sticht hervor, dass die Teilnehmenden - unabhängig von Qualifikationsstufe und Vorwissen - übereinstimmend der Meinung waren, dass Topic Modeling die qualitative, textnahe Arbeit vorbereiten und anleiten kann. Es ermöglicht, so eine Teilnehmerin, das „Springen“ zwischen Nähe und Distanz und damit einen anders informierten Umgang mit dem Quellenmaterial. Die Teilnehmenden waren sich einig, dass die aus dem Topic Modeling erzeugten Ergebnisse ihren Interpretationsvorgang angeregt haben. Das Verfahren wurde auch als „kreative Methode“ bezeichnet. Mit ihrer Einschätzung stimmen die Teilnehmenden damit bisherigen theoretisch-konzeptionellen Überlegungen (Jacobs/Tschötschel 2019; Nelson et al. 2018) zu.

            
Die Gruppen, die zuerst die Topic Models betrachtet hatten, gingen gezielter an den qualitativen Arbeitsschritt. Allerdings wurde mehrfach darauf hingewiesen, dass parallel oder im Anschluss „Rücksprache mit dem Text gehalten“ werden müsse. Denn allein aus den Mustern des Topic Modelings könne man sich nicht erschließen, was die Interviewpartner:innen gemeint haben. Zudem würden sich hierdurch nur beschreibende Kategorien entwickeln, die um analytische Kategorien ergänzt werden müssten, wie sie für den qualitativen Interpretationsprozess typisch sind. Die Notwendigkeit der Verbindung von qualitativen und maschinellen Schritten wurde also mehrfach betont. Wie es ein Teilnehmender zusammenfasste: „Topic Modeling ist eine Suchmaschine, bei der ich Parameter gut beeinflussen kann. Es eignet sich, um Themenkomplexe zu finden, die ich mir danach anschauen kann.“ Dennoch kann Topic Modeling als der Grounded Theory in vielen Punkten entsprechend verstanden werden, da es ohne vorher gebildete Kategorien an Text herangeht und Cluster von Bedeutungen aus dem Text selbst herausstellt. Es eignet sich also für induktives Vorgehen, wie Kitchin (2014, 5) es in seiner 
                
data driven science
 fordert und auch Salganik als 
                
empirically driven theorizing
 (2018, 61) vorschlägt.
            

            
Zur weiteren Bewertung der im Workshop generierten Schlagwörter wurde im Nachgang eine Befragung unter 13 Bachelor-Studierenden der Soziologie sowie mit einigen Monaten Abstand auch unter 10 der Teilnehmenden des Workshops selbst durchgeführt. Über die Schlagwortmengen und Chunks hinweg entstanden so 1.148 Bewertungen. Es sollte die Passung der acht verschiedenen Mengen an Schlagwörtern zu jedem Textabschnitt bewertet werden. Dazu lasen die Teilnehmenden sieben Textabschnitte und bewerteten die jeweiligen Schlagwörter auf einer Skala von eins bis fünf, wobei bei den Teilnehmenden des Workshops die Schlagwörter der eigenen Gruppe jeweils ausgelassen wurden. Mit Kontrolle der kategorialen Variablen (1) Gruppe im Workshop, (2) Befragungsgruppe (Bachelor-Studierende als Referenzkategorie) und (3) Chunk hatten die mithilfe des Topic Modelings erstellten Schlagwörter im multiplen linearen Regressionsmodell eine um 0,94 (p &lt; 0,001) schlechtere Bewertung auf der Skala von 1-5 als die rein menschlichen Schlagwörter. Das heißt, die Schlagwörter des Topic Modeling wurden statistisch höchst signifikant als schlechter bewertet. Allerdings variiert der Performance-Unterschied je nach Chunk: Für den Textabschnitt mit der geringsten Differenz zwischen maschinengestützter und menschlicher Annotation beträgt der Unterschied lediglich 0,18 und ist statistisch nicht signifikant. Daran wird deutlich, dass je nach Kontext des Textes Topic Modeling sehr gute Ergebnisse der inhaltlichen Vorstrukturierung liefern kann.

            
Die nachgängige Befragung der Workshop-Teilnehmenden umfasste zudem eine Art Turing-Test, bei dem die Befragten bestimmen sollten, ob die ihnen präsentierten Schlagwörter jeweils durch manuelle Annotation oder über das Topic Modeling entstanden sind. Cramérs V zwischen tatsächlicher und zugeschriebener Annotationsform beträgt 0,28 und beschreibt damit einen moderaten Zusammenhang.

            
Selbst Personen, welche die unterschiedlichen Generierungsprozesse der Schlagwörter kennen, können bei separater Betrachtung also nicht mehr eindeutig auf den Generierungsprozess schließen. Die über das Topic Modeling generierten Schlagwörter besitzen für menschliche Betrachter folglich durchaus eine sinnhafte Qualität.

            
Als Fazit kann festgehalten werden, dass, wenig überraschend, epistemologische Unterschiede zwischen der computationellen und der manuellen Texterschließung bestehen. Gerade am Vergleich wurde jedoch sichtbar, und von den Teilnehmenden so diskutiert, dass in beiden Zugängen wir als Menschen es sind, die subjektiv Sinn zuschreiben und neu ordnen. Wie kontrovers das Thema „Sinnzuschreibung“ ist, zeigte sich in einer Diskussion während des Abschlussplenums: Während einige die Art der statistischen Kondensierung, wie sie im Topic Modeling durchgeführt wird, bereits als einen Akt der Interpretation auffassten, widersprachen andere vehement, dass eine Interpretation Verstehen - konkreter: Sinnverstehen - voraussetze, was bei computationellen Auswertungen nicht der Fall sei. Ein Indiz, das letzteres Argument stützt, ist die Tatsache, dass letztlich den rein deskriptiven Topics analytische Kategorien zugeordnet werden, um die Texte zu annotieren. Immerhin konnte die Auswertung des nachgelagerten Surveys zeigen, dass sich die direkt aus dem Text und die aus den Wortlisten generierten Kategorien nicht grundlegend unterschieden, wenngleich die analytische Tiefe teils signifikant abwich. Zielführend im Sinne einer Erschließung digitaler Großbestände qualitativer Daten erscheint daher ein Mixed-Method-Approach, der die komplementären analytischen Zugänge kombiniert und etablierte Forschungsprozesse ergänzt.

            
Gleichzeitig hat sich die hohe Relevanz systematischer Evaluation digitaler Methoden gezeigt. Im Setup des Workshops haben sich die verschiedenen Ansätze gut ergänzt und zu einer vertieften Reflexion der Vor- und Nachteile der Zugänge geführt: So können mit Hilfe der Grounded Theory - insbesondere im diskursiven Austausch innerhalb einer Gruppe - Sinnstiftungsprozesse minutiös und akkurat herausgearbeitet werden. Das Topic Modeling hingegen spielte seine Stärken in der rasanten Verschlagwortung ganzer Interviewkorpora aus. Auf der anderen Seite ist das Arbeiten nach den Regeln der Grounded Theory äußerst zeitintensiv und konsistente Topics können Sinnstiftung suggerieren, wo letztlich reine Statistik am Werk ist. 

            
Die Schwächen des Experiments liegen am Ende vor allem in der notwendigerweise gesetzten Ausschnitthaftigkeit des Materials. Um in der begrenzten Zeit vergleichbare Ergebnisse zu erzeugen, wurden nur Interviewausschnitte durch die Gruppen gelesen und annotiert. Das ist in der Grounded Theory unüblich, da das Ausschnitthafte den Blick auf das Material verzerrt. Auch für Ansätze in den Digital Humanities wird üblicherweise das gesamte Korpus analysiert. Für Folgeprojekte sollte ein größeres Zeitbudget eingeplant werden, um komplette Interviewtranskripte analysieren zu können.

        

            

                

                    
Einleitung

                

                
Rituell reine Torarollen sind ein außerordentliches kodikologisches, theologisches und soziales Phänomen der jüdischen Schrifttradition. Zum ersten Mal sollen diese besonderen Objekte im Zuge unseres Forschungsprojekts umfassend mit digitalen Mitteln erforscht werden. 

                
Die Abschrift der heiligen Schriftrollen ist seit der Antike in ein dichtes Geflecht religionsgesetzlicher Regulierungen eingebunden. Die stark idealisierten Vorstellungen und Theorien der jüdischen Tradition zum Verhältnis von Material, Reinheit und Heiligkeit im Schreibkontext werden durch eine reiche Kommentarliteratur ergänzt. Diese behandelt aus ethisch-philosophischer, mystischer oder magischer Perspektive die symbolische Bedeutung der materialen Elemente einer Torarolle, den rituellen Schreibprozess oder die außergewöhnlichen Charakteristika des Schreibers (Martini 2022). 

                
Trotz der immensen Bedeutung, die den Torarollen als Mediatoren zwischen dem Heiligen und Profanen, der Vergangenheit und der Zukunft, aber auch in der jüdischen und nichtjüdischen Gesellschaft beigemessen wurde, beschränkte sich die bisherige Forschung weitestgehend auf die Untersuchung der überlieferten materialen Artefakte selbst. Dabei standen insbesondere die Schriftrollen vom Toten Meer im Fokus diverser Studien, die Textvarianten sowie die Beschaffenheit der Schreibhäute, der Tinten und bestimmte Merkmale des Layouts thematisieren. In der letzten Dekade führten Forscher*innen wie Judith Olszowy-Schlanger (Olszowy-Schlanger 2019), Mauro Perani (Perani 2019), Jordan Penkower (Penkower 2019, 2014), Josef M. Oesch (Oesch 2005, 2003) und Franz D. Hubmann (Hubmann und Oesch 2012) diesen Ansatz an ausgewählten Zeugnissen der mittelalterlichen Schrifttradition fort und bereicherten damit das Wissen um regionale Schrifttraditionen enorm – jedoch ohne das metaphysische Potential der Thematik auszuschöpfen. 

                
Dieses Forschungsdesiderat wird im Rahmen des Verbundprojekts „Materialisierte Heiligkeit. Torarollen als kodikologisches theologisches und soziologisches Phänomen der jüdischen Schriftkultur in der Diaspora“ eingelöst. Gegenstand des Forschungsprojektes sind mittelalterliche Torarollen und Torarollenfragmente vornehmlich europäischer Provenienz sowie die umfangreiche Schreiberliteratur zur Herstellung rituell reiner Torarollen. Das Schriftbild und seine Besonderheiten bilden dabei einen eigenen Forschungsschwerpunkt, wobei erstmals auch Texte in den Fokus gerückt werden, die sich explizit mit den dekorativen Elementen der Buchstaben, den ‚Krönchen‘ (
                    
Tagin
) oder den sogenannten ‚besonderen Buchstaben‘ (
                    
Otijjot meshunnot
), beschäftigen. 
                

                
Ziel ist es, den bedeutenden Schatz an bislang vernachlässigten Texten zu heben und erstmalig im Zusammenhang mit den überlieferten Torarollen und deren Geschichte zu lesen. Hierfür sollen die Torarollen als auch die relevanten Texte mit digitalen Mitteln aufbereitet und so deren Verständnis und Verknüpfung darstellbar gemacht werden. 

            

            

                
Der Forschungsschwerpunkt: 
                    
Tagin
 und 
                    
Otijjot meshunnot

                

                
Laut traditionell-jüdischer Auffassung ist der Text der Tora heilig und jedes einzelne Element von semantischer und paläografischer Bedeutung - auch die dekorativen Elemente der einzelnen Buchstaben, deren Form, Ausgestaltung und Anordnung. Der unveränderliche heilige Text bildet das Zentrum der Kommentarliteratur. 

                
In den Torarollen findet man neben den regulären Buchstaben der hebräischen Quadratschrift auch Buchstaben mit Verzierungen und besonderen Schreibweisen. Die Darstellung dieser Krönchen und besonderen Buchstaben weicht jedoch in den überlieferten Handschriften stark voneinander ab und durchläuft erst im Laufe der Zeit einen Standardisierungsprozess. Im Mittelalter gibt es hingegen einen regelrechten „Wildwuchs“ der Buchstabenformen und 
                    
Tagin
, der sich in den Torarollen und in der Kommentarliteratur widerspiegelt. Die Quellen zeigen, dass es keine einheitliche und standardisierte Schreibweise der 
                    
Tagin
 und besonderen Buchstaben in den Torarollen gab, was durch die unterschiedlichen Auslegungen und Diskurse in der Kommentarliteratur bestätigt wird.
                

                
Allein der kleine Midrasch Rabbi Aqiva al ha-Tagin (Midrasch Rabbi Aqiva über die Krönchen) bietet in jeder Version des in den Handschriften überlieferten Textes unterschiedliche Buchstabengestaltungen (beispielhaft in den Abbildungen 1-3).

                
unterschiedliche Buchstabengestaltungen (beispielhaft in den Abbildungen 1-3).

                

                    
 
                    
Abb. 1: rechts: Aleph ohne 
Tagin
, MS Bodleian Libraries, Can. Or. 1 (1303-1304).

                        Abb. 2: Mitte: Darstellung des Buchstaben Aleph mit drei 
                        
Tagin
 oben, zwei auf dem linken Plateau und einen auf dem rechten Plateau, MS BL Harley 5510 (14.-15. Jahrhundert). 

                        Abb. 3: links: Darstellung des Buchstaben Aleph mit drei 
                        
Tagin
 oben rechts und zwei unten links, MS Parma 2295 (13. bis 14. Jahrhundert).

                

            

            

                
Problematik 

                

                    
Die Quellen

                    
Eine grundsätzliche Herausforderung stellt der heterogene Charakter und die Intertextualität des von uns bearbeiteten Textkorpus dar - sowohl im Hinblick auf die Textebene als auch auf die Komplexität der Handschriftenlage (die im Projekt bearbeiteten Texte liegen größtenteils nur in Handschriften vor). Der Midrasch Rabbi Aqiva al ha-Tagin beispielsweise ist in mind. 24 Handschriften und dabei in unterschiedlichen Versionen überliefert. Die Unterschiede in den Versionen des Textes betreffen teils nur marginal einzelne Wörter, teils recht gravierend die zentralen Inhalte des Textes. Ein umfassender Vergleich der Diskrepanzen innerhalb der Texte verlangt nach einer digitalen synoptischen Edition mit zentraler Darstellung.

                

                

                    
Intertextualität und Bezug zwischen Texttradition und Torarollen

                    
Welchen Rückschluss erlauben die Texte der jüdischen Tradition auf die erhaltenen Torarollen und wie lassen sich Bezüge darstellen? Um die Vielzahl an relevanten Texten mit dem unveränderlichen Text der Tora zu analysieren und zu verknüpfen, bedarf es einer innovativen Herangehensweise. Die systematische Erfassung, Analyse und Interpretation der Texte sowie deren kodikologischer Besonderheiten ist erstmalig in diesem Umfang zu bewältigen. Diese komplexen Anforderungen zeigen deutlich, dass eine neue, digitale Forschungsmethodik unabdingbar ist, um die Texttraditionen zu kontextualisieren und intertextuelle Beziehungen hervorzuheben. 

                

                

                    
Verknüpfung von Form und Inhalt 

                    
Wie gezeigt, ist die Ausgestaltung der Buchstaben höchst variabel und lässt auf Entwicklungsschichten in der Ausprägung der besonderen Buchstaben und der 
                        
Tagin
 schließen. Eine Schwierigkeit stellt die Beschreibung und Darstellung der 
                        
Tagin
 und besonderen Buchstaben dar, die mit derzeitigen Möglichkeiten der (digitalen) Textverarbeitung unzureichend ist. Gleichzeitig führt die Erfassung der Gestaltung und Bedeutung der 
                        
Tagin
 und 
                        
Otijjot meshunnot
 zu noch unbekannten Zusammenhängen und neuen Erkenntnissen. Die quellenübergreifende und strukturierte Analyse der Buchstabenverzierungen im Kontext der Torarollen und Sekundärliteratur bringt das Verständnis der 
                        
Tagin
 und 
                        
Otijjot meshunnot
 auf eine neue inhaltlich-philologische Ebene. Auch hier zeigt sich, dass eine umfassende Erforschung mit altbekannten Mitteln unzureichend ist und es neuer digitaler Zugänge bedarf.
                    

                    
Für alle Schwerpunkte des Projektes sind digitale Methoden und standardisierte Richtlinien unabdingbar. Diese werden in unserem Forschungskonzept vereint und stellen die Basis unseres Projektvorhabens dar. Die Konzeption und Verknüpfung der Methodiken sollen in diesem Vortrag erläutert werden.

                

            

            

                
Methodik

                

                    
Das Editionskonzept

                    
Insbesondere die digitale Edition, Kommentierung und englische Übersetzung der Schreiberliteratur, in der über kulturelle Grenzen hinweg in einem Zeitraum von etwa 1700 Jahren die Herstellung von rituell reinen Torarollen diskutiert wird, ist ein zentraler Baustein des Vorhabens. Ein entwickeltes digitales Editionskonzept sowie ein zugrundeliegendes Schema gemäß den Richtlinien des TEI-Konsortiums (TEI Consortium 2022) stellen die Basis einer einheitlichen historisch-kritischen Edition und inhaltlich-semantischen Kodierung für rabbinische, narrative und mystische Schreiberliteratur dar. Die eigens entwickelten Editionsrichtlinien ermöglichen es, die 
                        
Tagin
 in den Torarollen und der Kommentarliteratur in einer digitalen Edition systematisch zu identifizieren und zu beschreiben. 
                    

                

                

                    
Nachhaltigkeit und Vernetzung

                    
Um die gesamtheitliche Erforschung dieser gesammelten Daten und Metadaten nicht nur innerhalb dieses Projekts, sondern auch in Zukunft zu ermöglichen, wird ein digitales Forschungsdatenrepositorium gemäß der FAIR Prinzipien (Wilkinson 2016) zur zentralen Speicherung verwendet. In diesem werden sowohl die ausgewählten Torarollen als auch die diskutierten Handschriften und Sekundärliteratur in Form von strukturierten, digitalen Objekten inklusive eindeutiger Identifier und standardisierter Metadaten angelegt, um die Wiederverwendbarkeit der Daten und eine Vergleichbarkeit mit anderen Datensätzen zu gewährleisten. Mit Hilfe von standardisierten Schnittstellen können die digitalen Objekte sowohl maschinenlesbar als auch durch die Forschenden abgefragt werden. Dies ermöglicht nicht nur eine nachhaltige (Nach-) Nutzung der Daten, sondern auch eine gezielte Verknüpfung untereinander und eine kontinuierliche Erweiterung des Wissensspeichers durch neue Quellen.

                    
Wie schon bei den Editions- und Erfassungsrichtlinien sowie dem Repositorium betont, legt unser Forschungskonzept Wert auf Standardisierung und eine leistungsfähige Forschungsdateninfrastruktur, weshalb zur Nomenklatur von spezifischen Charakteristika der Torarollen der Vokabulareditor EVOKS (Ernst 2022) verwendet wird. Das zugrundeliegende Konzept des Vokabulars beruht dabei auf dem von W3C empfohlenen Datenmodell SKOS (Simple Knowledge Organization System) (Miles 2009). Durch festgelegte Begriffe der schriftlichen Besonderheiten entsteht eine kontrollierte kodikologische Sammlung, welche dynamisch im Laufe der Forschung ergänzt und editiert werden kann. Jedoch sollen die handschriftlichen Merkmale nicht nur erfasst und thesauriert werden, sondern auch in den digitalen Versionen der Handschriften ausgezeichnet werden. Dazu steht ein Annotationsdienst zur Verfügung, der mit dem digitalen Repositorium in Verbindung steht (Tonne et al. 2019). Die heterogene und aussagekräftige Auszeichnung sowie Validierung der Annotationen folgt nach W3C-Empfehlung den Richtlinien des Web Annotation Data Model (Young et al. 2017).

                

                

                    
Virtuelle Torarolle

                    
Das Zusammenspiel aller genannten Komponenten wird in einer Virtuellen Torarolle gebündelt. Die Virtuelle Torarolle dient zum einen der Visualisierung und Präsentation der editierten Handschriften und der erfassten Torarollen, zum anderen bildet sie das Verbindungsstück von Forschungsdatenrepositorium und methodischen Werkzeugen. Die Anbindung des Vokabulareditors und des Annotationstools an die Virtuelle Torarolle bietet so die Möglichkeit, paläographische Details der Schrift und Besonderheiten des Schriftbildes der überlieferten mittelalterlichen Artefakte aufzunehmen und im Verhältnis zueinander und zu den Vorgaben der mittelalterlichen Regelwerke zu verknüpfen. Die qualitative und quantitative Analyse und Präsentation der Forschungsergebnisse in der Virtuellen Torarolle hat das Ziel, den Ursprung, den Wissens- und Praxiswandel und schließlich die Bedeutung der schriftlichen Besonderheiten im kulturellen Gedächtnis des Diasporajudentums freizulegen.

                    
Anhand eines Fallbeispiels möchten wir zeigen, wie die verschiedenen Komponenten in unserem Forschungsablauf ineinandergreifen und vielfältige Potentiale und Forschungsmöglichkeiten der Handschriften und Torarollen bieten.

                

            

            

                
Fallbeispiel

                
Präsentiert wird der bereits oben erwähnte Midrasch Rabbi Aqiva al ha-Tagin, ein kurzer exegetisch-mystischer Text, der die Gründe für die Anzahl der Krönchen auf den Buchstaben diskutiert. Der ins Forschungsdatenrepositorium eingespeiste Text der Handschrift liegt in TEI-konformer Repräsentation als digitales Objekt vor, wobei nicht nur die Daten, sondern auch angereicherte strukturierte Metadaten gespeichert sind. Im Fokus des Fallbeispiels stehen die 
                    
Tagin
 und 
                    
Otijjot meshunnot, 
deren erfasste Varianten im kontrollierten Vokabular eingesehen werden können, wobei jede Variante eine Relation zum übergeordneten unverzierten Buchstaben erhält. Des Weiteren zeigen wir die Visualisierung des Midrasch-Textes in der Virtuellen Torarolle und gehen auf ihre Konzeption und Implementierung ein. Anhand eines ausgewählten Buchstabens wird erläutert, wie dieser in der Virtuellen Torarolle annotiert ist und welche Analyse- und Visualisierungsschritte auf diese Weise möglich werden.
                

                
Das beschriebene Fallbeispiel zeigt nicht nur eine spannende kodikologische Anwendung des Forschungskonzepts, sondern illustriert das enorme Anknüpfungspotential des Vorhabens. Denn auch Erkenntnisse aus anderen Disziplinen wie der Material- und Sozialwissenschaft können in den Wissensspeicher einfließen und bisher unerforschte Verknüpfungen ermöglichen. Die Virtuelle Torarolle und der umfangreiche Werkzeugkasten eröffnen folglich neue methodische Zugänge zur Erforschung von Torarollen aus kodikologischer, geschichtlicher, material- und religionswissenschaftlicher Perspektive. Solch eine disziplinübergreifende Interpretation derart komplexer und heterogener Datenbestände trifft dabei auf den Kern der aktuellen Forschung und bietet immenses Forschungs- und Innovationspotential im Bereich der Digital Humanities. 

            

        

            

                
Introduction

                
Tyrolean tangible cultural heritage is manifested in various artefacts and documents spread across different regions in Austria and northern Italy, spanning the territory of historical Tyrol as it was delineated in the early 20
                    
th
 century. The written testimonies are preserved in historical institutions, private collections and public libraries. Efforts to digitize historical Tyrolean documents to safeguard their long-term preservation began about two decades ago with the Austrian Literature Online (ALO) project (Egger and Mühlberger, 2000) and Europeana Newspapers (Pekárek and Willems, 2012). However, the digitization of all relevant historical documents is still far from complete and digitization processes are often pursued by institutions with limited coordination between them.
                

                
The fact that many historical documents are only available as paper copies limits their access to expert groups and the public alike.
 It follows that historical text sources are known and used by only a few people when they could be valuable sources of information for larger parts of society, be those relating to the educational sector, public culture or research.
                

                
This is where the Zeit.shift project comes in. Using historical newspapers as a use case, this interdisciplinary and interregional project aims at increasing the visibility and use of historical text sources by bringing together dispersed items in digital form, making them accessible and promoting their value and use through educational and participatory campaigns. 

            

            

                
Project consortium and project goals

                
Zeit.shift is funded by the European Regional Development Fund and Interreg V-A Italia - Austria 2014-2020. The project started in October 2020 and will run until June 2023. It is a cooperation between two libraries, the Dr. Friedrich Teßmann Library (Bolzano, Italy, project lead) and the University and State Library of Tyrol (Innsbruck, Austria), together with the research Institute for Applied Linguistics at Eurac Research (Bolzano, Italy). The library partners contribute their data and expertise in collecting, cataloguing, digitizing and publishing historical text sources. The research institute contributes its expertise in computational linguistics, Digital Humanities, crowdsourcing and participatory approaches for the (semi-)automatic processing of textual resources with the aim of enhancing their informational content, their searchability and thus their overall use. Seven cultural institutions support the project as associated partners.

                

                
The Zeit.shift projects seeks to preserve, develop and communicate the cultural and textual heritage of the historical region of Tyrol by, firstly, digitizing and bringing together Tyrolean historical newspapers from the early 20
                    
th
 century (mostly written in German Fraktur); secondly, by upvaluing the data, improving their searchability and making them accessible via a dedicated and free online portal; and, thirdly, by disseminating the data to be used by various stakeholder groups through free educational materials, workshops and participatory activities. The activities are promoted through campaigns, and complemented by free educational workshops and an e-learning course.
                

            

            

                
Research and development 

                
The aims of Zeit.shift to extend access, visibility and use of historical text documents call for an open research and development approach. Indeed, the project fosters open procedures and open access in three distinct but interconnected areas of the project implementation: (1) openness in collaboration and knowledge sharing, (2) openness of data and tools, and (3) openness in education and participation.

                

                    
Open collaboration and knowledge sharing

                    
Collaboration openness mostly concerns the work of the two library partners, who join forces in digitization efforts which were previously carried out in parallel with little interaction. This includes sharing experiences on good practices, exchanging information on the cataloguing work, agreeing on naming conventions and harmonizing workflows. In addition, both data and digitization costs are shared between the partners and the digitized data will be published through a joint online portal.

                    
The collaboration with the research partner also builds on openness in data and knowledge exchange, but it is less challenging as in this typical research collaboration the competences of the partners are complementary and thus responsibilities clearly defined. 

                

                

                    
Open data and tools

                    
All data that is produced within the project is and will be openly available. The digital copies of the historical newspapers and their metadata are published under Creative Commons Attribution licenses. The computational linguistic processing and enhancement of the newspaper data is based on non-commercial open tools; all scripts, toolchains and other code developed within Zeit.shift are and will be made available via Eurac Research’s GitLab repository.
 Data collected through the participatory activities is and will also be accessible under open licenses. 
                    

                

                

                    
Open education and participation

                    
Zeit.shift pursues an active and open approach towards citizen participation and dissemination of project results. One of its goals is to engage the public and raise interest for the historical data that is digitized within the project. This is done by developing initiatives that invite the local population to interact with the data and contribute to their processing and annotation (see §4.2.). These initiatives are promoted through specific campaigns, including stands at conferences and transfer-oriented events such as the 
                        
Long Night of Research
, as well as radio and TV broadcasts, announcements in local print media, flyers, stickers and a prize competition (ibid.). 
                    

                    
Additionally, Zeit.shift is investing in the formation of multipliers for different stakeholder groups, ranging from professional archivists and library staff to hobby historians (‘Chronisten’ in German), teachers and teacher trainers. The course materials are jointly elaborated by the partners and are openly shared with the community via multiple institutional repositories. They introduce users to both the basics of digital and automated procedures in text processing and historical studies, and to the participatory activities offered by the project. In addition, the project has published a free Massive Open Online Course (MOOC) to show students and interested lay citizens how historical daily newspapers from North, East and South Tyrol can be used as online sources for research, to illustrate the potential of Digital Humanities and to provide insights into some computational linguistic techniques that facilitate work with historical newspapers.

                    

                

            

            

                
Project results

                
Two years into the project, we can report the following results. 

                

                    
Digitized historical newspapers

                    
Some 47.631 issues (amounting to 423.782 pages) from 41 newspapers published between 1880 to 1950 in the historical region Tyrol have been digitized and are currently being made available through the online portals of the Teßmann
 and Innsbruck University Libraries
. The digitized data includes OCR’d text in ALTO-XML format, aligned at the page level with scanned 400 dpi resolution color images.
                    

                    
The data is automatically annotated for Named Entities using the spaCy named entity tagger (Honnibal and Montani, 2017) augmented with a trimmed list of local toponyms (‘Flurnamen’ in German
). Each newspaper page is automatically tagged with one or more topic labels distinguishing between local news, global news, announcements, advertisements, and serial stories. The final, joint newspaper portal, which is still under development, will make use of these annotations to enhance search functionality.
                    

                

                

                    
Participatory activities

                    
Two participatory activities have been developed and are available online. Both are open to all participants alike but were designed for specific user groups. 

                    
The first activity (
                        
macro-task
), targets German-speaking specialists (librarians, chroniclers, historians, etc.) and invites them to geo- and semantically tag historical advertisements automatically extracted from the Zeit.shift newspaper collection using the Historypin platform (Fig. 1).
 Currently, the Zeit.shift collection in Historypin contains some 7,000 adverts from ten different newspapers and, as of December 2022, 27 people have (geo)tagged 351 adverts with a total of 1,668 unique tags. As a byproduct of the project, the Zeit.shift team produced the German translation of the Historypin platform to the benefit of the German-speaking community.

                    

                    

                        

                        
Figure 1. An advert from the 
                        
Schwazer Lokalanzeiger

                        about cough sweets in Historypin. The pink and black pin on the map marks the farmacy’s location. Source: 
                        
https://tinyurl.com/mzkzzhwz

                    
The second activity is Ötzit!, a custom web game inviting participants between 11-14 years of age to speed-type words extracted from the newspapers, which is designed to help them learn this script whilst contributing to OCR post-correction efforts (
                        
micro-task
).
 In the game, alpine animals walk in the direction of Ötzi the Iceman
 looking to harm him while Fraktur words appear on the screen; players must type the words correctly as fast as possible to fend off the animals and thus preserve Ötzi’s health (Fig. 2). Knowledge of German is not necessary to play the game but certainly an advantage. 
                    

                    
Ötzit! is released under an MIT license and can be played on both desktop and mobile devices. It is made up of two software components.
 The 
                        
frontend
 implements user interaction, that is, the gameplay; it is written in Javascript using Phaser
 and is distributed via Itch.io
, the 
                        
de facto
 standard platform for indie games. The 
                        
backend
 implements all data flow (providing words to the frontend and receiving game data via an API) and data analysis; it is written in Javascript using Express
 and is hosted on Eurac Research infrastructure. Ötzit! sources assets (audio files and graphics) published under open licenses only.
                    

                    
Transcriptions anonymously typed by players are collected to test the efficacy of the game as an OCR manual post-correction tool. As of December 2022, the game has been played by 1,754 unique devices and out of the 6,909 words typed thus far, 890 confirm the OCR output, 442 provide corrections and 5,577 are pending automatic evaluation.

                    

                    
In September 2022, the project launched a prize competition for middle schools in South Tyrol to advertise the game to the younger population.
 The competition and advertising campaign, which ran between 12
                        
th
 September and 31
                        
st
 October 2022, was very successful in recruiting players from both the target group and the general public, with 1,820 games initiated, 729 games completed, 88.72 hours of total game time (an average 3 minutes per initiated game and 7 minutes per completed game) distributed across 914 unique devices.
                    

                    

                        
                    
                        
Figure 2. Ötzit! screenshot. The player types Fraktur words as they appear on the screen. At the top of the screen, points accumulated are shown in the left corner, elapsed game time in the center and health status points in the right corner.

                    

                

                

                    
Dissemination and training

                    
Multiple training and dissemination activities have been carried out in both South Tyrol and North Tyrol. To date, the project has conducted nine training workshops for the Historypin activity for a rough total of 100 participants, and another 200 people have been informed about the project activities through presentations at local stakeholder institutions, including Tiroler Landesmuseum Ferdinandeum and Standtarchiv Bozen. The MOOC contains six modules and is followed by 63 participants. Flyers and posters have been distributed by all partners in their vicinity. Several press releases have been published
, and scientific publications have been presented at three events: a demo session at the Engaging Citizen Science Conference 2022 (CitSci2022)
 in Aarhus (Denmark) (Franzini et al., forthcoming), and two poster presentations at the annual conference of the 
                        
Associazione per l’Informatica Umanistica e la Cultura Digitale
 (AIUCD)
 in Lecce (Italy) (Franzini et al., 2022) and at the 7
                        
th
 Austrian Citizen Science Conference
 in Dornbirn (Austria). 
                    

                

                

                    
Joint newspaper portal

                    
The design phase of the Zeit.shift newspaper portal addressed the requirements and technical needs of the three project partners. The portal is currently being implemented by an ICT service provider and will go live in the spring of 2023.

                

            

            

                
Conclusions and future work

                
In this article we describe how an open approach is helping to increase the visibility and support the use of historical resources within the Zeit.shift project. Indeed, we believe that the wider promotion and use of cultural heritage data should move beyond the mere provision of open data to foster the engagement of citizens through education and participatory initiatives, as well as the collaboration among cultural institutions in relation to data, procedures and networks. 

                
At the same time, historical sources are often difficult to process automatically, which makes manual processing by researchers and wider stakeholder groups highly necessary. Follow-up work on the project might focus on developing additional participatory activities to enhance the historical data sources in a playful and educational fashion. This could also include further research on how any data collected from a non-expert audience can be processed and aggregated to improve the original sources in a reliable way. These activities would serve a twofold purpose: improve the quality of digital copies of fragile historical items for long-term preservation, and increase their visibility and use for educational purposes and public cultural awareness. 

            

            

                
Acknowledgements

                
Zeit.shift has received funding from the European Regional Development Fund under the Interreg Italia - Österreich 2014-2020 Programme (Project no. ITAT3030). We thank the conference reviewers for their helpful suggestions and are especially grateful to our contributing citizens.

            

        

            

                
Minimal Editing

                
Digitale Editionen sind in der Regel kostspielige Großprojekte, die von Drittmittelförderung abhängig sind. Projektmitarbeiterinnen und -mitarbeiter müssen umfangreiche technische und philologische Fähigkeiten mitbringen und die Projekte werden meist in sehr spezifischen Editionsumgebungen entwickelt – Elena Pierazzo nennt derartige digitale Editionen “Haute Couture”-Editionen (Pierazzo 2019, 213).

                
Kleinere Editionsvorhaben werden oft nur von Einzelpersonen gestemmt oder vielleicht im Kontext einer Lehrveranstaltung oder Qualifikationsarbeit umgesetzt; eine Vermittlung der Ergebnisse in Form eines Webauftritts oder die Archivierung in einem zertifizierten Repositorium ist in derartigen Fällen nur selten möglich; oft scheitert es am entsprechenden Know-How oder an finanzieller Unterstützung. Diese Arbeiten verschwinden eher früher als später ungenutzt in dunklen Schubladen. Um dieser Problematik Herr zu werden, müssen nachhaltige Lösungen her! Eine solche Lösung für derartige Fälle wäre sicher 
                    
minimal editing
.
                

                

                    
Minimal Editions
 stellen reduzierte digitale Editionen im Gegenzug zu kostspieligen, aufwendigen Editionsgroßprojekten dar. Wie eine derartige Minimaledition zu definieren wäre, ist Gegenstand des aktuellen Forschungsdiskurses: Basiert sie auf dem effizientesten Workflow, um den Arbeitsaufwand der Datenverarbeitung zu reduzieren, der kleinstmöglichen technischen Infrastruktur (i.e 
                    
minimal computing
, cf. Gil 2015), um die Hemmschwelle vor der Technik möglichst niedrig zu halten, der sparsamsten Modellierung der Objekte, um Bearbeitungszeit zu sparen und die weitere Verarbeitung zu vereinfachen oder zeichnet sie sich durch einen reduzierten Funktionsumfang bei der Onlinepublikation aus? 
                

                

                    
Minimal editing
 kann es möglich machen, digitale Editionen mit vorgefertigten Workflows, bereits etablierten, einfachen technischen Lösungen und leicht zu integrierenden Basis-Funktionalitäten im Bereich der Datenmodellierung wie auch der Datenpräsentation schnell, einfach und mit einem geringen Kostenaufwand umzusetzen. Das bringt allerdings die unterschiedlichsten Einschränkungen mit sich! Demgegenüber stehen aber die Erwartungen und Ideen der Editorinnen und Editoren und die Vorgaben der Förderorganisationen. Allerdings würde der Aufbau einer zeitgemäßen Infrastruktur, die Langzeitverfügbarkeit garantiert, mit Komponenten, die 
                    
ad hoc 
für unterschiedliche Editionsprojekte benutzbar sind, den Aufwand für die Umsetzung einer digitalen Edition reduzieren und somit vermutlich zu einer größeren Anzahl von digitalen Editionsprodukten führen. Pierazzo bezeichnet diese Editionen als “Prêt-à-Porter”-Editionen (Pierazzo 2019, 213). 
                

                
Weitere Stichworte im Zusammenhang mit 
                    
minimal editing
 sind 
                    
minimal functionalities
 und 
                    
minimal computing
. Federico Caria und Brigitte Mathiak untersuchten, welche minimalen Funktionalitäten der Webauftritt einer digitalen Edition unbedingt aufweisen sollte, damit diese von der Community auch zufriedenstellend genutzt werden kann (2018); Greta Franzini, Melissa Terras und Simon Mahony führten ebenfalls eine Umfrage zu Erwartungen und dem Gebrauch von digitalen Editionen durch (2019). Die dabei gesammelten Ergebnisse können helfen, die wichtigsten Komponenten einer 
                    
Minimal Edition
 zu definieren, wie z. B. eine Such- und Exportfunktion über bzw. in den Daten oder eine grundsätzlich benutzerfreundliche Navigationsstruktur (Caria/Mathiak 2018, 360), oder die Einbindung von Faksimiles, leicht erkennbare Angabe von Lizenzen, die Möglichkeit der Nachnutzbarkeit der Daten und eine umfassende Dokumentation (Franzini/Terras/Mahony 2018, 1).
                

                

                    
Minimal computing
 hingegen ist ein Ansatz, bei dem versucht wird, die technischen Ansprüche zu reduzieren und für die Arbeit nur jene Technologien zu verwenden, welche tatsächlich notwendig für die Umsetzung eines bestimmten Vorhabens sind. Erst 2022 erschien eine ganze Ausgabe des 
                    
Digital Humanities Quarterly
, welche sich den unterschiedlichsten Aspekten des 
                    
minimal computing
 widmet (Risam/Gil 2022a). Um zu bestimmen, welche Technologien tatsächlich notwendig und ausreichend sind, schlagen Risam und Gil vor, folgende vier Fragen zu stellen (Risam/Gill 2022b): Was brauchen wir, was haben wir, was müssen wir priorisieren und was sind wir bereit aufzugeben?
                

                
Alle diese Publikationen zeigen einen Trend in Richtung maßgeschneidertes Editionsmanagement auf. Es gibt bereits Lösungen, Tools und Plattformen, die unter dem Aspekt Minimal Editing zusammenzufassen sind – darunter fällt z.B. EVT (Edition Visualization Technique), ein Open-Source-Tool, welches es ermöglicht, XML-TEI Dokumente als digitale Editionen im Browser anzuzeigen. Dasselbe gilt für CETEIcean und TEI-Publisher, welche ebenfalls Lösungen anbieten, um XML- und TEI-Dokumente ohne Erfahrung in der Webprogrammierung im Browser zugänglich zu machen. Transcriptiones erlaubt es, Transkriptionen zu teilen und kollaborativ zu überarbeiten, wobei hier jedoch kein Fokus auf die Modellierung mit XML/TEI oder eine nachhaltige Archivierung gelegt wird. Den Tools, die eine Kodierung mit TEI unterstützen, ist gemeinsam, dass zwar niederschwellig eine Webansicht generiert werden kann, allerdings wird hier der Aspekt der Langzeitarchivierung, d.h. dass die Daten permanent zitierbar verfügbar bleiben, nicht berücksichtigt. 

                
Ein Zugang zur “Minimal Edition”, der sich am Paradigma eines effizient gestalteten Workflows orientiert und den Aspekt der Langzeitarchivierung miteinschließt, ist in Österreich im Rahmen des bundesgeförderten Projektes 
                    
DiTAH - Digitale Transformation der österreichischen Geisteswissenschaften
 umgesetzt worden.
                

            

            

                
Hyper: Hyperdiplomatische Transkriptionsplattform

                
Im geplanten Vortrag soll eine Variante des 
                    
minimal editing
 anhand der hyperdiplomatischen Publikationsplattform vorgestellt werden: Die Hyperdiplomatische Transkriptionsplattform (Hyper, 
                    

                        
http://gams.uni-graz.at/context:hyper

                    
) wurde entwickelt, um das Archivieren und Publizieren von Editionsdaten, die auf dem standardisierten Datenmodell der hyperdiplomatischen Transkription (Böhm/Klug 2021) beruhen, möglichst ressourcenschonend und im Sinne von
                    
 minimal editing
 zu gestalten – mit den Ressourcen, die im vorliegenden Fall möglichst sparsam verwendet werden sollen, sind vor allem die nötigen Arbeiten und der finanzielle Aufwand im Zusammenhang mit einer Publikation und Langzeitarchivierung der Editionsdaten gemeint.
                

                
Eine hyperdiplomatische Transkription versucht, die historische Quelle möglichst detailreich bis hin zur Teilzeichenebene (z. B. Superskripte oder sogar Teilstriche eines Zeichens) bzw. unter Berücksichtigung der Quellentopographie (Verortung der Informationseinheiten in einem digitalen Abbild der Quelle) in ein modernes Zeichensystem zu übertragen. In Grazer Projekten (
                    

                        
Mittelalterlabor

                    
, 
                    

                        
Cooking recipes of the Middle Ages

                    
) wird nach einer hyperdiplomatischen Transkriptionsmethode gearbeitet, die ursprünglich auf die Philosophie der “Grazer dynamischen Editionsmethode” (Hofmeister-Winter 2003) zurückgeht: Es wird dabei eine “Basistransliteration” (Hofmeister-Winter 2003, S. 101) erstellt, deren Ziel es ist, den Text der Handschrift möglichst getreu mit typographischen Mitteln abzubilden, wobei die Quellentopographie dabei keine bzw. nur bedingt eine Rolle spielt. In dieser Phase soll noch keine inhaltliche Interpretation stattfinden, sondern vorerst nur der paläografische Informationsgehalt der Handschrift, also die Schriftsymbole wiedergegeben werden. Es werden in einem Graphinventar nicht nur alphabetische Schriftsymbole, sondern alle graphischen Phänomene wie Abbreviaturen, Superskripte oder Verzierungen mit bestimmten von den Transkribierenden individuell festlegbaren Kodierungen festgehalten. Im Graphinventar werden die von der Schreiberin oder dem Schreiber verwendeten Zeichen beschrieben und mit einer proprietären Kodierung versehen, sodass es gleichzeitig als Transliterationsschlüssel dient, der im Rahmen der digitalen Workflows eine zentrale Stellung einnimmt. Die Transkription soll damit sowohl sprach- und literaturwissenschaftlichen als auch geschichtswissenschaftlichen Ansprüchen genügen bzw. sie in manchen Fällen übertreffen – das Ziel dieser Herangehensweise ist es aber prinzipiell, eine Transkription zu erstellen, die über den konkreten Anwendungsfall hinaus auch von anderen Forscherinnen und Forschern verwendet werden kann (Böhm/Klug 2020). 
                

                
Die Hyperdiplomatische Transkriptionsplattform bietet daher ein vorgefertigtes Datenmodell für die hyperdiplomatische Transkription (Klug/Böhm 2021), vorgefertigte Routinen und Transformationen, die darauf aufsetzen, und unterschiedliche Beschreibungszugänge, um den Workflow zu verdeutlichen und einfach nachnutzbar zu machen. Die Plattform (Klug/Galka 2022) wurde mit Testusern entwickelt, die sich willig auf das Experiment eingelassen haben. Die dafür produzierten Daten stehen mittlerweile offen in einem funktionell umfangreichen Webauftritt zur Verfügung: 33 Texte zum Thema “Jagd in den deutschsprachigen Texten des Mittelalters und der Frühen Neuzeit” (
                    

                        
http://jagd-im-mittelalter.de/

                    
), transkribiert und modelliert von Timo Bülters und Simone Schultz-Balluff, und die hyperdiplomatische Transkription der Handschrift Wien, ÖNB, Cod. 3085, fol. 1r-39v von Astrid Böhm (2022).
                

            

            

                
Workflow

                
Die hyperdiplomatische Transkription erfolgt mittels Transkribus, einer Transkriptionssoftware, die eine automatische Textsegmentierung und Text-Bild-Verknüpfung erlaubt und darüber hinaus eine äußerst benutzerinnenfreundliche Arbeitsoberfläche bietet. In Transkribus wird mittels proprietärer Codierung transkribiert, wobei die proprietäre Kodierung, die im Transkriptionsprozess verwendet wird, im TEI-XML in der Zeichenbeschreibung (&lt;charDecl>) festgehalten wird. Mit Daten aus Projekten, die diese Transkriptionsmethode anwenden (
                    

                        
Mittelalterlabor

                    
, 
                    

                        
Cooking recipes of the Middle Ages

                    
), wurde in Transkribus außerdem ein HTR-Modell für spätmittelalterliche Bastarda trainiert (German Bastarda, hyperdiplomatic), das kurz vor der Publikation steht. Auch Textauszeichnungen (Initialen, Überschriften usw.) und die Annotationen wie z.B. Revisionen der Schreiberin oder des Schreibers oder Anmerkungen/Notizen der Editorinnen und Editoren erfolgen in Transkribus. Generell wird aber versucht, die Annotationen in Transkribus auf ein notwendiges Minimum zu beschränken. Die Software ermöglicht einen TEI/XML-Export der Transkription – die exportierten Daten werden mittels X-Technologien bzw. mit auf der Plattform bereitgestellten XSLT-Stylesheets weiterverarbeitet und ins finale Datenmodell transformiert. Die Kollationierung der Transkription mit der Quelle erfolgt außerhalb von Transkribus und setzt auf den selben Transformationen auf. Das publikationsfähige TEI-Dokument enthält einen umfangreichen &lt;teiHeader> mit &lt;editorialDecl>, &lt;msDesc> und &lt;charDecl> und den Editionstext, der die Struktur des Faksimiles abbildet. Die XSLT-Stylesheets, Schema-Dateien zur Validierung, Templates für Handschriftenbeschreibung (&lt;msDesc>) und Editionsrichtlinien (&lt;editorialDecl>) werden auf GitHub zum Download bereitgestellt (
                    

                        
https://github.com/ditah-at/hyper

                    
).
                

                
Die Archivierung und Publikation erfolgt mittels des Geisteswissenschaftlichen Asset Management Systems (
                    
GAMS
). GAMS ist ein Asset Management System zur Verwaltung, Publikation und Langzeitarchivierung digitaler Ressourcen aus allen geisteswissenschaftlichen Fächern, welches auf der Open-Source-Lösung Fedora (
                    
Flexible Extensible Digital Object Repository Architecture
) basiert und am Zentrum für Informationsmodellierung an der Universität Graz ständig weiterentwickelt wird. Mit Hilfe des Systems können unterschiedliche Ressourcen verwaltet, mit Metadaten angereichert und persistent zitierbar publiziert werden (Stigler/Steiner 2018). Das Asset-Management-System entspricht den Empfehlungen des OAIS-Referenzmodells (
                    
Open Archival Information System
), welche eine verlässliche Langzeitarchivierung von digitalen Daten garantieren sollen. Die Auffindbarkeit der Daten wird durch die Vergabe von persistenten Identifikatoren und der Anreicherung mit umfassenden Metadaten ermöglicht. GAMS ist seit 2019 mit dem Core Trust Seal als vertrauenswürdiges digitales Repositorium und seit Frühjahr 2020 außerdem als CLARIN Datencenter zertifiziert.
                

                
Das finale TEI/XML-Dokument wird gemeinsam mit den Faksimiles in das Repositorium ingestiert und unter einem systeminternen Permalink langzeitarchiviert. Die Daten werden außerdem mit einem PID aus dem Handle.net-System versehen und stehen stabil referenzierbar zur Verfügung. Im Zuge des Ingest wird automatisch die HTML-Anzeige 
                    
on the fly
 mittels gängigen Webtechnologien basierend auf vorgefertigten XSLT-Transformationen generiert. Der Webauftritt der Hyperdiplomatischen Transkriptionsplattform umfasst eine vertikale wie auch horizontale Text-Bild-Synopse (wahlweise mit diplomatischer Transkription oder Lesetext), die Editionsrichtlinien, die Handschriften- und Zeichenbeschreibung, Metadaten und Lizenzinformationen sowie Zitiervorschläge.
                

                
Die Hyperdiplomatische Transkriptionsplattform stellt zur Nachvollziehbarkeit und Nachnutzbarkeit des Workflows sämtliche XSLT-Stylesheets etc. zur Verfügung. Außerdem werden in einer umfangreichen Dokumentation (
                    

                        
http://gams.uni-graz.at/o:hyper.documentation/sdef:TEI/get?mode=overview

                    
), die sich der Transkription, dem Datenmodell, der Kollationierung und Validierung sowieder Archivierung und Publikation der Daten widmet, die einzelnen Schritte mit Verlinkungen zu weiteren Ressourcen, wie z.B. dem 
                    
KONDE-Weißbuch 
(Klug/Galka/Steiner 2021) mit Einträgen zur digitalen Edition, genau erläutert. Eine Kurzbeschreibung (
                    

                        
http://gams.uni-graz.at/o:hyper.documentation/sdef:TEI/get?mode=shortdescription

                    
) der einzelnen Schritte erspart das ständige zur Rate ziehen der ausführlichen Dokumentation und ein Aktivitätsdiagramm veranschaulicht die einzelnen Arbeitsschritte und bietet eine Verlinkung zu den jeweiligen Anleitungen (
                    

                        
http://gams.uni-graz.at/archive/objects/context:hyper/methods/sdef:Context/get?mode=diagram

                    
).
                

            

            

                
Fazit

                
Die Plattform und das Editionskonzept richtet sich grundsätzlich an all jene Editorinnen und Editoren, die bereits Vorkenntnisse in Bezug auf digitale Editionen mitbringen, wie z.B. Wissen über XML/TEI, XSLT, oder den Umgang mit dem Oxygen-XML-Editor. Das Paradigma der “Minimal Edition” bezieht sich hier auf einen nachhaltig erarbeiteten und von weiteren Editionsprojekten nachnutzbaren Workflow. In der Regel evaluiert jedes Editionsprojekt in unterschiedlichen Stadien mögliche Tools und Workflows, mit denen im Team gearbeitet werden soll, sei es für die Transkription, Annotation oder für die Publikation. Im Projekt Hyper wurde jedoch ein standardisierter Workflow entwickelt, der auf wenigen, zur sofortigen Nutzung bereitstehenden Tools und Routinen aufbaut und in einer umfangreichen Dokumentation beschrieben wird, wodurch Editorinnen und Editoren,  welche diesen Text auch einem größeren Publikum im Zuge einer digitalen Edition möglichst einfach zugänglich machen wollen, auf diesen zugreifen können. Die “Einschränkung” ist, dass die hyperdiplomatische Transkribtionsmethode übernommen werden muss. Risam/Gill 2022b schlagen für ihre Variante des 
                    
minimal editing
 vor, nur die absolut notwendigen Technologien für die Umsetzung eines bestimmten Vorhabens zu verwenden. Da einer der Grundpfeiler von digitalen Edition aber die Modellierung der Daten in einem standardisierten Datenformat ist, wurde bewusst entschieden, dass bei der hyperdiplomatischen Modellierung mit TEI/XML keine Abstriche gemacht werden sollen. Da sowohl Edierende wie auch Benutzerinnen und Benutzer von digitalen Editionen gewisse Standards in der Datenpräsentation erwarten (Caria/Mathiak 2018), werden auch dafür komplexe Technologien verwendet, die allerdings an die verwendete Repositoriumslösung angeknüpft sind. Sowohl die Modellierung der Quelle als auch die Präsentation sind in einem projektspezifischen Datenmodell verwirklicht. 
                    
Hyper
 unterscheidet sich von anderen Tools und Lösungen auch dadurch, dass die Daten langzeitarchiviert werden und unter einem PID erreichbar sind. “Minimal Editing” bedeutet im Rahmen dieser Plattform also nicht nur, dass es ein fertiges Datenmodell und einen erprobten Workflow gibt, sondern auch, dass der Aufwand seitens der Repositoriumsbetreiber für die Archivierung und Präsentation der Daten sehr gering ist. Alle veröffentlichten Quellen stehen 
                    
open access
 zur Verfügung, außerdem werden sämtliche Dateien, die im Workflow benötigt werden, wie XSLT-Stylesheets oder Schema-Dateien offen auf GitHub bereitgestellt.
                

            

            

                
Ausblick

                
Das Ziel der 
                    
Minimal Edition
 ist es nicht, möglichst viele Editionsprojekte möglichst schnell umzusetzen, sondern einen niederschwelligen Zugang anzubieten, damit ohne großen Aufwand “reduziertere” digitale Editionen geschaffen werden können, die trotzdem qualitativen Standards entsprechen. Die 
                    
Hyperdiplomatische Transkriptionsplattform
 ist zwar momentan hauptsächlich für all jene interessant, die hyperdiplomatisch transkribieren – allerdings ist es ein erster Schritt in eine Zukunft, in der auch andere Editionen, die dem 
                    
minimal-editing
-Konzept folgen, ohne großen organisatorischen Aufwand publiziert werden können.
                

            

        

            
Die Digitale Edition stellt ein äußerst wichtiges Forschungsfeld innerhalb der Digital Humanities dar. Es gibt eine Reihe an Publikationen und Ressourcen, die sich sowohl einführend als auch theoriebildend diesem Thema widmen. So verfasste z.B. Patrick Sahle ein umfangreiches Standardwerk dazu (2013: 
                
Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels.
), bzw. 2019 den einschlägigen Artikel “What is a scholarly digital edition?” (2019). Ein weiteres Standardwerk, 
                
Digital scholarly editing: theories, models and methods
, wurde 2015 von Elena Pierazzo publiziert; von ihr und James Driscoll wurde 2016 der Sammelband 
                
Digital Scholarly Editing. Theory, Practice and Future Perspectives
 herausgegeben. Daneben erschienen unzählige Artikel in einschlägigen Zeitschriften, wie z. B. zum Aspekt der Zukunft von Digitalen Editionen von Elena Pierazzo aus dem Jahr 2019, Digitale Edition im Zusammenhang mit rechtlichen Aspekten von Wout Dillen und Vincent Neyt 2016 oder zum Thema Kommentar in Digitalen Editionen (Bleier/Klug 2020) oder bestimmten Textsorten wie Briefen oder Tagebüchern (Dumont 2019). In diesem Zusammenhang müssen natürlich auch die Leistungen des Instituts für Dokumentologie und Editorik (
                

                    
https://www.i-d-e.de/

                
) genannt werden, die mit dem Rezensionsorgan R.I.D.E. und den dafür geschaffenen Rezensionskriterien (Sahle 2014), wie auch einer mittlerweile umfangreichen Buchreihe (SIDE) federführend an der Theoriebildung und Weiterentwicklung Digitaler Edition arbeiten. Unterschiedliche Ausbildungsangebote wie das bereits 2017 ausgelaufene Digital Scholarly Editions Initial Training Network DiXiT (
                

                    
https://dixit.uni-koeln.de/

                
) runden dieses Bild ab.
            

            
Anfang 2021 wurde als Abschluss des vom österreichischen Bundesministerium für Bildung, Wissenschaft und Forschung geförderten Projekts "KONDE - Kompetenznetzwerk Digitale Edition" eine als Nachschlagewerk konzipierte Publikation, das sogenannte 
                
Weißbuch
 zum Thema Digitale Edition veröffentlicht: 
                

                    
https://www.digitale-edition.at

                
. Im HRSM-Projekt wurde im Rahmen von thematisch einschlägigen Arbeitsgruppen (z.B. Transkription und Textauszeichnung, Netzwerkanalyse und Datamining, Archivierung oder Quellendigitalisierung) Inhalte und Lemmata des 
                
Weißbuchs
 erarbeitet. Die Weißbucheinträge wurden von den Autorinnen und Autoren mithilfe eines vorgefertigten Google-Doc-Template, das eine überschaubare Menge an relevanten Formatierungen enthielt, erstellt. Nach der Redaktion durch das Projektteam wurden die Dokumente nach einem Export aus Google Drive mittels XSLT in das für das Projekt erarbeitete TEI/XML-Datenmodell transformiert und in das Geisteswissenschaftliche Asset Management System (GAMS) eingespeist, welches am Institut Zentrum für Informationsmodellierung in Graz entwickelt wurde (und wird). Die TEI-Dokumente stehen unter einem PID (Persistent Identifier) langzeitarchiviert und stabil referenzierbar zur Verfügung und wurden mit umfassenden Metadaten versehen. Die HTML-Ansicht des 
                
Weißbuchs
 wird direkt aus den Daten mittels XSLT generiert.
            

            
Das 
                
Weißbuch
 enthält neben 25 Portraits von Editionsprojekten über 200 Beiträge, verfasst von 50 Autorinnen und Autoren aus 10 österreichischen Forschungseinrichtungen, die Begriffe aus 12 Themenbereichen primär für Einsteiger in diese Thematik aufbereiten:
            



                
Editionswissenschaft im Allgemeinen: Erklärung allgemeiner Begriffe und Konzepte (Analysemethoden, Digitale Edition, Editionstext usw.), Editionstypen, Darstellung allgemeiner Themen wie Interpretation, Kollationierung, Normalisierung, Paläographie

                

                    
Digitale Editionswissenschaft: Erklärung allgemeiner Begriffe und Konzepte (Ontologie, XML, Usability usw.), Analysemethoden, Apparat, diverse Editionstypen, Benutzerinnen und Benutzer Digitaler Editionen, Diplomatische Transkription, Editionstypographie, Elemente digitaler Editionen, FAIR-Prinzipien, Informationsarchitektur, Kataloge Digitaler Editionen, Kommentar, Lagenvisualisierung, Linked (Open) Data, Persistent Identifier, Social Edition, Textkritik in digitalen Editionen, Zielgruppen digitaler Editionen, Zitierbarkeit digitaler Ressourcen

                

                
Digitalisierung: Digitalisierungsdienste, Checkliste Digitalisierung, Transkriptionswerkzeuge, Kosten, Crowdsourcing, HTR, OCR

                

                    
Metadaten: Metadatenformate (CIDOC CRM, METS, PREMIS), Metadaten Harvesting 

                

                

                    
Annotation und Modellierung: Datenmodellierung und -modelle, Modellierungsstandards, Schemata, Normdaten, Semantic Web, Markup und Markup Sprachen

                

                

                    
NLP: Artikel über computergestützte natürliche Sprachverarbeitung (Distant Reading, Historische Korpora, Lemmatisierung, Named Entity Recognition, Tagger, Tagsets)

                

                

                    
Schnittstellen: Design und Komponenten (Barrierefreies Webdesign, Benutzerinnen und Benutzer Digitaler Editionen, Editor-testing, Informationsarchitektur, Interface, Usability, Zitiervorschlag)

                

                
Datenanalyse: Analysemethoden, Datamining, Datenvisualisierung, Dramennetzwerkanalyse, Visualisierungstools

                
Archivierung: Archivierungsstrategien, digitale Nachhaltigkeit, österreichische Archivanbieter, Metadaten, Versionierung

                
Software und Softwareentwicklung: Software zur Erstellung digitaler Editionen und theoretische Aspekte der Softwareentwicklung

                
Rechtliche Aspekte in Bezug auf Digitalisierung, Bereitstellung von Digitalen Editionen, Urheberrecht, Lizenzierung und Lizenzmodelle

                
Institutionen: einschlägige Beschreibungen der am Projekt beteiligten Institutionen mit einer Darstellung der digital-editorischen Schwerpunkte.

            

            
Als Begleitmaterialien stehen unter anderem eine umfangreiche, öffentlich verfügbare Zoterobibliothek zur Verfügung, die weit mehr Literatur zum Thema beinhaltet, als in den Artikeln zitiert ist. Außerdem gibt es eine nach Anwendungsbereich systematisierte Sammlung von Tools, Ressourcen und Standards, die für das digitale Edieren relevant sein können. 

            
Die einzelnen Einträge des 
                
Weißbuchs
 bieten neben der Erläuterung des jeweiligen Begriffes in der Regel Links zu verwandten Lemmata innerhalb des 
                
Weißbuchs
 bzw. zu Einträgen in den relevanten digitalen Lexika zum Thema Digitale Edition (
                
Edlex: Editionslexikon
, 
                
Lexicon of Scholarly Editing
, 
                
Parvum Lexicon Stemmatologicum
), einschlägiger Software oder prototypischen Projekten. Zusätzlich werden Literaturhinweise angeboten, die mit einer umfangreichen Zotero-Bibliothek verlinkt sind, die Literatur zur Digitalen Edition auflistet. Die Artikel werden sowohl über eine thematische Gliederung als auch über einen Index erschlossen; eine Volltextsuche ist ebenfalls implementiert.
            

            
Neben der Darstellung auf der Website als HTML werden die Weißbucheinträge auch als TEI/XML- bzw. als PDF-Download zur Verfügung gestellt. Ein Netzdiagramm veranschaulicht die Beziehungen zwischen den einzelnen Artikeln und die Häufigkeit, mit der ein Stichwort genannt wird. Inhaltlich sind die Artikel breit angelegt; sie richten sich primär an interessierte Laien, sie können aber auch für Fachleute aus der Forschung als Einstieg in die Thematik oder pointierte Zusammenfassung dienen. 

            
Das 
                
Weißbuch Digitale Edition
 dient als Einstiegswerk in unterschiedlichste Aspekte des Forschungsfeldes, wobei bei jedem Weißbuchartikel weiterführende Literatur angegeben wird, damit sich Benutzerinnen und Benutzer selbstständig vertiefend in die Materie einlesen können. Es zeichnet sich außerdem durch eine starke hypertextuelle Vernetzung der Artikel untereinander aus, die einen explorativen Zugang für alle Nutzerinnen und Nutzer ermöglicht. 
            

            
Nach der Publikation des 
                
Weißbuchs
 Anfang 2021 konnten die Herausgeber im Sommer 2022 eine Förderung für eine Aktualisierung (Überarbeitung und Ergänzung bestehender Artikel) bzw. für die Erweiterung (Verfassen neuer Artikel, Vorstellung weiterer einschlägiger Projekte) des 
                
Weißbuchs Digitale Edition
 einwerben: Ein 
                
Call for Contribution
 erging an die deutschsprachige Community digital Edierender, der ca. 60 Einreichungen erbracht hat. Nach einem Redaktionsworkflow, der auch ein 
                
peer-review
 der neuen Artikel beinhaltet, ist die Aktualisierung der bestehenden Ressource für Frühjahr 2024 geplant. Die Umsetzung dieses Kleinprojekts wird zeigen, ob ein derartiges Update-System in regelmäßigen Abständen durchgeführt werden kann, sodass der möglicherweise schnellen Überalterung der Inhalte entgegengewirkt wird.
            

        

            

                

                    
Towards a Shared Infrastructure for Metaphor Analysis

                

                

                    

                    
The collaborative research center (CRC) 1475

                    

                    

                    
 

                    
studies the role of metaphors in religious meaning-making.

                    
 

                    
In metaphors, meaning is transferred from one semantic domain to another. Metaphors can thus serve as a means to express abstract concepts with reference to more concrete ones closer to human experience. Religion, which cannot directly address its ultimate subject (the transcendent, i.e., gods, otherworlds, etc.), is especially dependent on this procedure. By adopting conceptual metaphor theory 

                    
(

                    
Lakoff and Johnson, 1980; Steen et al., 2010; Nacey et al., 2019)

                    
 

                    
the CRC seeks to more thoroughly understand this process theoretically and grasp it methodologically to research its semantic forms empirically and comparatively. Through its multidisciplinary subprojects the CRC contributes to the historiography of religions and to answering systematic questions in the comparative study of religions. It covers a plethora of religious traditions from across the globe, including Christianity, Islam, Judaism, Zoroastrianism, Jainism, Buddhism, and Daoism. The time frame ranges from 3,000 BCE to the present day, including texts from multiple languages and diverse genres, from Korean Confucian ego-documents to Christian online forums.

                

                
Motivated by the challenge of comparability and interoperability between its extremely heterogeneous subprojects, the CRC deliberately puts emphasis on digital methods. Thus, the shared infrastructure, provided by the information infrastructure (INF) project, does not only support the individual research projects, but fosters conceptual integration. Utilizing this infrastructure the subprojects annotate religious texts to make metaphorical language explicit. For this process we adapt the Five Step Method (Steen, 2011) to not only mark the presence of metaphors, but to include complex analysis of the structural functioning of the metaphor and the resulting domain mappings as well. As a standardization measure we append an additional step, where each concept is linked to a conceptual thesaurus.

            

            

                
The Metaphor Workbench

                
Within the INF project, scholars of religion, computational linguists, and computer scientists jointly establish the digital research infrastructure of the CRC and provide the necessary tools for the subprojects.

                

                    
Shared text repository

                    
An instance of the KIT Data Manager Base Repo (Jejkal et al., 2014) is used as a research data repository for all CRC subprojects. Existing data from the subprojects is stored as structured data objects including their respective metadata in the form of TEI compliant XML files (Burnard and Bauman, 2010) and is available for further processing, enrichment, analysis, etc. via standardized interfaces. In close collaboration with the subprojects, the INF project is evaluating existing representations of their data, required format conversion, legal aspects and rights management, as well as assisting the provision of required descriptive metadata. 

                

                

                    
Annotation services

                    
To annotate the metaphors the INF team provides tools for annotation implementing a shared metaphor annotation schema, which is based on the web annotation data model (Young, Sanderson, and Ciccarese, 2017). Because the available annotation tools (like INCEpTION, CATMA, WissKI etc.) are lacking the possibility to create the complex annotations needed for the CRC’s methodology, a new metaphor analysis tool will be developed, which guides and documents the interpretative analysis process. In the future, we will be using NLP expertise present in the CRC’s subprojects to integrate methods of (semi-)automatic metaphor detection and analysis.

                    
Furthermore, the INF project is advising the subprojects that aim for additional, project-specific annotation of their data, particularly with regard to the use of existing annotation standards and best practices. All of the annotations are provided in a Web Annotation Protocol Server (Tonne et al., 2019) as RDF triples to foster analysis across the subprojects and to ensure interoperability and reuse.

                

                

                    
Conceptual thesaurus

                    
To facilitate comparability of our metaphor annotations across barriers of languages and cultural traditions, the CRC is developing a conceptual thesaurus (CT) as a shared reference system. Its taxonomy is based on the Historical Thesaurus of English (Kay, 2009), albeit extending and adapting it for the languages and topics prevalent in the CRC. Using the SKOS data model (Miles and Bechhofer, 2009) and principles from Linked Open Data (LOD; Berners-Lee, 2006), the CT will provide a language-independent framework for the annotation of domains used in metaphorical mappings. Linking concrete metaphorical expressions with a central semantic resource enables retrieving conceptually related metaphors from different corpora, and comparatively studying semantic domains used in metaphors.

                

                

                    
Thesaurus of religious metaphors

                    
Linking texts, analysis, and concepts, the resulting annotations will make up the thesaurus of religious metaphors (TRM). The TRM will enable studies of metaphors in a systematic and comparative way by providing a semantically indexed collection of religious metaphors, as well as query and analysis tools.

                

            

            

                
Conclusions and Future Scope

                
The CRC’s infrastructure – research data repository, annotation services, conceptual thesaurus and thesaurus of religious metaphors – fosters reusability as well as interoperability with external knowledge graphs by focusing on open data principles and will be published under open licenses. In particular, the emerging TRM will act as a unique resource for scholars worldwide studying religious metaphors. The INF project itself is an integral part of the CRC in providing this shared infrastructure and thus enabling comparative studies on an unprecedented scale in the field.

            

        

            
Seit rund 20 Jahren steigt die online verfügbare Menge an digitalen Daten in unterschiedlichster Form und Qualität. Dies erleichtert den Geisteswissenschaftler|innen den Zugang zu neuen Materialien und senkt die Hemmschwelle sich mit Big Data zu beschäftigen und der „methodological monoculture of close reading“(Karsdorp et al. 2021, 
                
preface
) zu entkommen. Dadurch müssen sie sich aber auch vermehrt mit den technischen, sprich, computergestützten Anwendungen und Programmiersprachen beschäftigen. Zudem müssen Daten meist noch gesammelt, von überflüssiger Information bereinigt und Ergebnisse visuell aufbereitet werden. Alle diese neuen Schritte sind je nach Datengrundlage aufwändig, da Techniken erst erlernt und korrekt angewendet werden müssen. Als zusätzlicher Faktor werden für Berechnungen großer Datenmengen fachfremde Methoden aus der Mathematik oder Informatik entlehnt. Die Anwendung dieser arithmetischen Methoden werden jedoch oftmals nicht ausreichend hinterfragt. Zusätzlich fehlt zur Überprüfung der Methoden auch die empirische Evidenz vor allem dann, wenn das Untersuchungsmaterial keine faktischen Hinweise liefern kann. 
            

            
Auch in der Geschichtswissenschaft wenden sich, angeregt durch niedrigschwelligen Zugang zu online verfügbaren Texten, Geisteswissenschaftler:innen neuen Forschungsfragen zu. So eröffnet beispielsweise die Stilometrie neue Möglichkeiten, die Anonymität eines mittelalterlichen Textzeugens aufzuheben und neue Thesen bezüglich Überlieferung und Organisation in Schreibstuben und Kanzleien aufzustellen. Dabei bewegen sich jedoch erstens die Forschungsfragen häufig um eine konkrete Identifizierung eines Stiles, einer Kanzlei oder einer Schreibschule und weniger über Makrosignale wie übergeordnete geographische, kulturelle oder sprachliche Dimensionen. Zweitens vernachlässigt die Arbeit am konkreten Korpus auch die Auseinandersetzung mit der Auswahl der geeigneten mathematischen Verfahren, die hinter den computergestützten Berechnungen stehen. Eine Auseinandersetzung auf der Makroebene über die Effektivität von beispielsweise einem Distanzmaß-Verfahren wie Burrows’s Delta auf die konkrete Textgattung der Urkunden findet bis heute noch nicht statt. Zwar gibt es Messungen über den Wirkungsgrad des Verfahrens für Lyrik und Prosa in lateinischer Sprache und kurzer oder auch fehlerhafter Texte
, dennoch finden darin die spezifischen Eigenschaften von Urkunden nicht ausreichend Berücksichtigung: Individuelle, orthographische Signale, formelhafte Sprache und lückenhafte Überlieferungen sind nur einige der spezifischen Faktoren, welche noch nicht ausreichend für Burrow‘s Delta untersucht wurden (vgl. Eder 2013, 2015). 
            

            
Betrachtet man das weitere Forschungsfeld zum Thema Autorschaftsattributionen werden aktuell vermehrt andere statistische Ansätze in Erwägung gezogen, die nicht zwingend auf Textfeatures wie Wortlängen oder inhaltlichen Merkmale wie Worthäufigkeiten basieren. Eine Fokussierung auf syntaktische oder semantische Zusammenhänge beispielsweise könnte bei der Untersuchung der Signale in Urkunden ebenso Distinktionen herausheben. Eine Loslösung vom Vector-Space-Model und Burrows Delta hin zu Topics und Neuronalen Netzen wurde zwar bisher auf dem Typus Urkunden noch nicht im großen Rahmen angewandt, dennoch könnte man dadurch potentiell das Manko der geringen Textlängen und der formelhaften Sprache umgehen. Neuere Ansätze haben sich zudem das Ziel gesetzt, durch eine Kombination mehrerer methodischer und textimmanenter Ansätze Merkmale herauszuarbeiten. Diese umschließen dann folglich nicht mehr nur die klassische Stilometrie, sondern auch die oben genannten syntaktischen und semantischen Features. Ob diese neuen Ansätze bei Urkunden Wirkung zeigen, soll ein Ziel dieser Dissertation werden.

            
Ein grundsätzliches Problem bezüglich der Urkundentexte ist allerdings, dass diese häufig nicht von ihren originalen Textzeugen, sondern nur aus digitalisierten Editionen entnommen sind, die nicht dem eigentlichen Überlieferungstext entsprechen müssen. Wie sehr vertraut man Texten aus älteren Editionen vor 1945, in denen die Lachmannsche Editionstechnik angewendet wurde, mit der die 
                
emendatio 
angeblicher Fremdeingriffe unbedarft angewendet und zudem schlecht oder gänzlich undokumentiert in den Druck gegeben wurde (vgl. Plachta 2006, S. 29)?
            

            
Aus diesen Überlegungen heraus ist es naheliegend, die stilometrischen Verfahren einmal aus der Makroperspektive zu untersuchen: Anstatt sich mit einer These zu beschäftigen, die aus dem konkreten Material, vielleicht sogar aus dem close reading-Prozess selbst, entstanden ist, sollten die mathematischen Methoden an einer großen und diversen Menge an Urkundenmaterial untersucht werden. Eine mannigfaltige Auswahl bieten dazu mehrere Urkundenportale, wie monasterium.net, Cartae Europae Medii Aevi, Codice diplomatico della Lombardia medievale oder Telma.
 Dabei spielen zunächst weder die Urkundentypen, noch die zeitliche Dimension, in der die Urkunden entstanden sind, eine Rolle. Die Annahme ist eher, dass verschiedene Verfahren unterschiedlich starke Distinktionen der Urkunden unterstreichen. Ihre jeweilige Sensibilität für bestimmte Eigenschaften des Textmaterials gilt es herauszuarbeiten und methodisch zu begründen. So lassen sich Stärken und Schwächen der Methoden analysieren und gegenüberstellen, nicht im Sinne eines Rankings (‚bessere vs schlechtere Methode‘), sondern im Hinblick auf ihre Eignung für spezifische Korpora und Fragestellungen. Ein solcher Ansatz erlaubt es Forschenden nicht nur, eine fundiertere Entscheidung über die anzuwendende Methode zu treffen, sondern im Idealfall sogar, diese gezielt auf den eigenen Anwendungsfall abzustimmen und zu verfeinern.
            

        

            
Harkening back to the “big tent” metaphor (e.g. Terras 2013) which characterized debates about the inclusivity of the field ten years ago, the topic of ‘openness’ in the conference theme invites associations of ‘blue skies’, endless horizons, and the sense that everything is possible – in terms of participation, dissemination and objects of observation. This notion is complicated by several issues that discourses of cultural criticism have identified in the Digital Humanities in recent years (although they are not exclusive to the field): Among them monolingualism (Fiormonte 2021), a heritage of colonialism (Risam 2019) and gender imbalance (Gao et al. 2022, 330), to name but a few.

            
It is as of yet unclear whether and how the aspirations of the field can be reconciled with the realities of its practices. Supposing, however, that there is something to be learned from shifting the gaze towards the “borderlands” (Earhart 2018) of the field’s perceived center of activity – especially in a continental European context –, it would appear paramount to interrogate the theme of the conference and probe the boundaries of its ‘openness’ against this backdrop of socio-economic, political and infrastructural inequalities.

            
In order to remedy the 
                
invisibility
 of feminist, postcolonial and multilingual approaches or indeed their marginalization as topics with niche interest only intended for those directly affected by them, the panel will consist of six panelists that will each bring a different thematic focus to bear:
            



                
Dr. Sarah-Mai Dang, Philipps University Marburg | 
                    
data feminism

                

                
Tinghui Duan, University of Trier | 
                    
multilinguality

                

                
Dr. Till Grallert, Humboldt University of Berlin | 
                    
decolonization

                

                
Jana Keck, German Historical Institute Washington (GHI) | 
                    
transnationality

                

                
Prof. Dr. Julianne Nyhan, Technical University of Darmstadt | 
                    
hidden history

                

                
Dr. Antonio Rojas Castro, Berlin-Brandenburg Academy of Sciences and Humanities | 
                    
cultural context

                

            

            
All of these statements will have their own unique perspective but they will also intersect in many ways, which is why the panel chose a broad view on potential borders and blind spots: Sarah-Mai Dang will address film-historical desiderata while Julianne Nyhan’s statement on the history of DH will also take gender aspects into account. Antonio Rojas Castro’s presentation of the German-Cuban collaboration in the 
                
Proyecto Humboldt Digital
 project will argue for the necessity of decolonial and postcolonial considerations. Till Grallert’s statement, which will focus on the neo-colonial invisibility of the cultural heritage of large parts of the societies of the Global South, will also provide input on the challenges for DH of under-resourced languages. This in turn connects to Tinghui Duan who will address challenges of multilinguality in the Global East. Jana Keck will cover perspectives of the Global North by presenting the transnational forum of the DH working group of the German Historical Institutes that engage, for example, in transnational discussions of data feminism which leads us back to Sarah-Mai Dang’s statement.
            

            
Each panel member will give a short statement (5 minutes each, 30 minutes in total). These will be interwoven with the first round of discussion among the panelists (30 minutes). Due to the broad range of topics, the expert statements as well as this first round of discussion will address a shared set of key points to anchor and focus the conversation. After this initial hour, the discussion will be open to questions from audience members (30 minutes), making for a total length of 90 minutes.

            
The panel will be held in English due to it being an international panel with an international topic. This does not mean, however, that the panel will be oblivious to the cultural academic context in which it will take place. Moderation will make the conscious effort of connecting the panel discussion with discourses of the German-speaking DH community and situating the outcome of the panel both within and beyond the conference setting of the DHd.

            
It is important to note that the panel does not presume to speak for marginalized groups whom it does not represent. For this reason, it primarily aims to provide a forum for self-reflection, as a first step towards a dialogue that should be extended in the future and should, as its long-term goal, seek to amplify voices that would otherwise not be heard.

            
Details of the panelist statements which will initiate the discussion are as follows (in alphabetical order):

            

            

                
Sarah-Mai Dang, 

                
New Insights and Old Blind Spots: Visualizing Film Historical Research Data

            

            
With the increasing production and use of data in the wake of digitalization, the goal of feminist film historians to increase the visibility of women's work in film history has taken on a new urgency (Dang 2020). Through the production, processing and dissemination of data, blind spots can be maintained or amplified, but also minimized (Wreyford/Cobb 2017; D’Ignazio/Klein 2020). In her presentation, Sarah-Mai Dang will show how digital data visualization can help us to open up research on women in early cinema and thus make their impact in film history more visible. By presenting a case study on the Women Film Pioneers Explorer (
                
https://www.online.uni-marburg.de/women-film-pioneers-explorer
, Dickel et al. 2021), she seeks to discuss how visualized visions about the past can be defined as situated knowledges.
            

            

            

                
Tinghui Duan, 

                
Multilinguality as Challenge

            

            
Multilinguality in DH can refer to the inclusion of multilingual research objects in digital resources. It also refers to the need for multilingual academic exchange (which is often limited to English), especially since many terms are difficult to translate. Both aspects come with major challenges (cf. Fiormonte 2021). Tinghui Duan will address these challenges by using the example of the Romantic Period Poetry Archive (
                
https://t.co/QdLYAaWiCZ
), an “open access digital platform of global Romantic-period poetry”. Since the project itself notes that its "selection process is largely Western/Northern-centric" (e.g. Görner 2021, Matuschek 2021), it is not surprising that not a single Chinese romantic writer was registered originally (
                
https://t.co/o8MkasqQ0A
). While three Chinese writers have now been admitted at the suggestion of Tinghui Duan (for romanticism in Asia cf. Long et al. 2018, Rabut 2014), he will address possibilities of acknowledging languages more equally.
            

            

            

                
Till Grallert, 

                
Neo-Colonial Layers of Invisibility in a Digitised World: Arabic Cultural Production and the Affordances of the Digital

            

            
Against the backdrop of his work on Arabic periodicals from the late nineteenth-century Eastern Mediterranean, Till Grallert will challenge the equation of "digitisation = access" by outlining the layers of inaccessibility inherent to existing digitisation efforts and infrastructures concentrated, for a large part, in the Global North (Grallert 2022, Risam 2018, 2019). These are a) a knowledge gap regarding the material artefacts themselves; b) a digitisation bias rooted in collection and survival biases and the direct costs of digitisation; c) the hegemony of socio-technical infrastructures rooted in Anglo-American neoliberal capitalism and the Western cultural canon (cf. Piron 2018); d) insufficient computational tools for layout and optical character recognition (cf. Wrisley et al. forthcoming); and e) insufficient access to basic digital infrastructures and utilities (cf. Aiyegbusi 2019). 

            

            

                
Jana Keck, 

                
The Working Group Digital Humanities of the Max Weber Foundation and its Role as a Transnational Forum for German DH Scholars

            

            
The Working Group Digital Humanities of the Max Weber Foundation brings together German DH scholars from 11 institutes worldwide with diverse backgrounds in the humanities (cf. Keck/Rohden 2021). The group provides not only a platform to discuss DH methods, theories and projects, but it also allows for the reflection on how the respective cultural, political, social, or economic conditions of the different locations shape the everyday experiences of DH research. Discussions in the group about upcoming trends and topics in DH have revealed a unanimous view: We need more ethics in DH! Jana Keck’s statement will elaborate on the group’s findings that it is not so much the debate about technological innovations or newest tools that brings DH-scholars together worldwide, but rather discussions on ethical frameworks in DH (Proferes 2020). 

            
            

            

                
Julianne Nyhan, 

                
On Hidden and Devalued Labour in the Incunabular Digital Humanities: The Index Thomisticus Project c. 1954–67

            

            
Julianne Nyhan will seek to 'represence' the role of gender and invisible labour in the incunabular DH. Between 1954–1967, the Index Thomisticus project, an influential project in the early years of the field of Humanities Computing, had a workforce that numbered, at its peak, about 65 individuals. Yet many of those who worked in this project, especially the young women handling the project data, remain absent from histories of the development of DH. As her forthcoming book (Nyhan 2023) explores, these labour absences were neither inconsequential nor thoughtless, but can be understood to distill processes of exclusion/inclusion and expressions of social and epistemological hierarchy that would shape not only the emerging field of humanities computing, and in due course DH, but aspects of the wider development of computing too.

            

            

                
Antonio Rojas Castro, 

                
Diversifying the User Experience in Digital Editions

            

            
User experience research and design is very common in libraries, archives, and universities to assess the ease with which the community uses informational resources (Azadbakht, Blair, and Jones 2017; Seale, Hicks, and Nicholson 2022). In the field of DH, researchers have recognized the important role of the graphical user interface in digital editions (Bleier et al. 2018) and how software and tools are shaping the presentation of texts (Alvite-Díez and Rojas Castro 2022). This statement aims to take a critical approach to user experience and to interrogate how we can diversify the targeted users of Proyecto Humboldt Digital (
                
https://habanaberlin.hypotheses.org/
) to broaden the access and usage of our digital editions.
            

        

            

                
Das Forschungsvorhaben

                
In einem Kooperationsprojekt zwischen der Potsdamer Arbeitsstelle des Corpus Vitrearum Medii Aevi (CVMA) Deutschland an der Berlin-Brandenburgischen Akademie der Wissenschaften sowie dem Institut für Kunstwissenschaft der Technischen Universität Berlin wird die digitale Erschließung und Repräsentation der Nikolaikirche in Bad Wilsnack sowie der Quellen zur als “Wilsnackfahrt” bekannten Wallfahrt umgesetzt. Hierbei handelt es sich um ein interdisziplinäres Forschungsvorhaben aus den Bereichen Kunstgeschichte, Mediävistik, Digital Humanities (DH) und Informationswissenschaften. Verschiedene Forschende haben unterschiedliche Bedarfe an die Infrastruktur sowie die erzeugten digitalen Informationen und Dateien. 

                

                    

                    
Abbildung 1: Wilsnack, Kirche St. Nikolai, Außenansicht, Blick von Westen, Foto: Berenike Rensinghoff.

                

                
Die Kirche St. Nikolai in Bad Wilsnack ist auch als Wunderblutkirche bekannt und war seit Auffindung der drei “Wunderbluthostien” im Jahr 1383 ein zentraler Wallfahrtsort (Bednarz et al. 2010, 87–165; Kühne und Ziesack 2005, 9–18). Zahlreiche Quellen verschiedener Art belegen dies, als Stiftungen in Form von Glasmalereien und Ausstattungsstücken des Bauwerks oder als Überlieferungen wie Pilgerzeichen, Testamente und Ablässe. Schriftquellen liegen teilweise nur gedruckt oder nicht erschlossen in Archiven verschiedener nord- und mitteleuropäischer Länder vor und benötigen eine systematische Aufarbeitung. Überlieferungen sind zum Teil nicht sicher belegt und nur durch Verweise auf nicht mehr vorhandene Quellen bekannt. Das Bauwerk war aufgrund der politischen und religiösen Geschehnisse im Laufe der Jahrhunderte verschiedenen baulichen Veränderungen ausgesetzt. Neben umfangreichen Restaurierungsmaßnahmen wurden Ausstattungsstücke verändert, bewegt, entfernt oder gar – insbesondere während der Zeit der Reformation und durch verschiedene Eigentümerwechsel – zerstört (Bednarz et al. 2010, 88–92).

                

                    

                    
Abbildung 2: Wilsnack, Kirche St. Nikolai, Blick in den Chor mit Hochaltarretabel, Foto: Berenike Rensinghoff.

                

            

            

                
Datenbasis

                
Die Quellen der Wallfahrt bilden die Basis für die Datenerfassung und eine daraus erfolgende Repräsentation, z. B. auf Karten oder als 3D-Modell des Bauwerks sowie ausgewählter Ausstattung. Die Forschungsdateninfrastruktur, die aktuell entwickelt wird, muss einerseits mit heterogenen Inhalten der Quellen, andererseits auch mit verschiedenen Dateitypen umgehen. Neben Bilddateien als TIFF oder JPG sowie Regesten der Quellen, die in proprietären Office-Formaten erstellt und in ein menschen- und maschinenlesbares Format wie XML oder JSON konvertiert werden, entstehen ebenfalls 3D-Daten (Punktwolken, digitale Rekonstruktionen) in unterschiedlichen Formaten, z. B. OBJ oder PLY, und Koordinaten als GeoJSON für die Kartendarstellungen. Heterogene Dateitypen und verschiedene Informationsschichten in z. T. ungenauem bzw. ungesichertem Überlieferungsstatus werden zu einem multimodalen Datenmodell zusammengeführt. Das Bauwerk betreffende Informationen werden mit Ereignissen in Verbindung gebracht, um zu erfassen und abzufragen, was bspw. Akteure der damaligen Zeit gesehen haben könnten, als sie die Kirche St. Nikolai zu einem bestimmten Zeitpunkt betraten, welche Ausstattungsstücke wann sichtbar waren oder welche Routen Pilger zurückgelegt haben. Die digitalen Ressourcen werden auf einer von der Software getrennten Dateiebene wiederum mit Metadaten angereichert.

            

            

                
Forschungsdateninfrastruktur

                
Im Bereich des digitalen Kulturerbes gibt es nur wenige standardisierten Lösungen für digitale Infrastrukturen, auf die zurückgegriffen werden kann. Bekannte virtuelle Forschungsumgebungen sind MonArch und WissKI. MonArch ist jedoch für die Zwecke des Vorhabens nicht ausreichend, da es hier primär um 
                    
Building Information Modeling
 für die Erfassung und semantische Annotation bauwerksbezogener Inhalte geht (AriInfoware 2022). WissKI ist eine Forschungsumgebung, die über das CMS Drupal Zugriff auf verschiedene Dateitypen bietet und mittels Datenmodell organisiert ist. Eine Repräsentation von Kartendarstellungen und eine Einbindung von 3D-Viewern ist möglich (Germanisches Nationalmuseum o. J.). Im Bereich des Digitalen Kulturerbes findet vor allem das 
                    
CIDOC Conceptual Reference Model
 (CRM) für die Datenmodellierung Anwendung, da hiermit objektbezogene und ereigniszentrierte Informationen und ihre Eigenschaften modelliert werden können (Bekiari et al. 2022). So kann eine Nachnutzbarkeit bereits vorhandener aber auch der eigenen, zum Großteil noch zu erstellenden Daten gewährleistet werden. Eine Besonderheit stellt im Bereich der Kunstgeschichte und des Kulturerbes die Vermittlung von Beziehungen zwischen Informationen dar, die getroffene Aussagen über Objekte und Orte in Zusammenhang mit Akteuren und Ereignissen in einen Kontext setzen (Burrichter et al. 2021, 111f.). 
                

                
Eine Speicherung der Dateien erfolgt in einer durch das Projekt vorgegebenen Ordnerstruktur (Kontinent/Land/Bundesland/Stadt/Gebäude/Datentyp). Verschiedene Tools greifen auf die digitalen Ressourcen zu und die Datenschicht ist von der Software getrennt (Gerber 2022, 16–28). Für ihre Erschließung mit Metadaten aber auch für die Erfassung des Datenmodells wird derzeit der bereits in den beiden Arbeitsstellen des CVMA Deutschland verwendete CVMA Digitaler Ressourcen Manager (CVMA DRM) (Gerber und Fischer 2021) weiterentwickelt. In erster Linie wurde er für die Erschließung von digitalen Bilddateien konzipiert. Die Inhalte werden online über ein webbasiertes GUI auf Basis von Javascript aufgerufen und verarbeitet. Der Zugriff auf die Daten erfolgt über einen Webserver. Metadaten werden in einer der jeweiligen digitalen Ressource zugehörigen maschinen- und menschenlesbaren JSON-Datei gespeichert und perspektivisch in GitLab versioniert. Das zu annotierende Datenformat ist unerheblich, da sich die JSON-Dateien mit jedem Datentyp verknüpfen lassen, neben Bilddaten z. B. auch Textdateien, Daten der Punktwolken oder Koordinaten. Eine Zuordnung erfolgt über den Dateinamen und den persistenten Identifikator (PID). Die Metadateninformationen haben denselben Dateinamen wie die jeweils verschlagwortete Ressource, die Dateiendung .meta und sie werden in derselben Ordnerstruktur wie die durch sie beschriebenen Dateien abgelegt. Durch die Speicherung als JSON-Dateien werden zunächst nur die Metainformationen und stark herunter skalierte Bilder bzw. Vorschauansichten der Modelle geladen. Die großen, hochaufgelösten Daten sind für eine Anzeige explizit auszuwählen, so dass sich die Ladezeiten erheblich verkürzen. Die Konfiguration der Ansicht und des Metadatenschemas erfolgt ebenfalls über JSON-Dateien.

                

                    

                    
Abbildung 3: Ansicht Graphical User Interface (GUI) des CVMA Digitaler Ressourcen Manager für die Erschließung der Forschungsobjekte, Anja Gerber 2022.

                

                
                

                    

                    
Abbildung 4: Verknüpfung der Forschungsobjekte mit Ressourcen und Metadatendateien, Anja Gerber 2022.

                

                
                
Die erschlossenen Inhalte der Quellen werden durch das Datenmodell organisiert und mittels verschiedener Viewer, u. a. Kompakkt für 3D-Modelle (Universität zu Köln 2018–2022), Digilib für Bilder (digilib Community 2001–2022) und das Chronotopische Tool für Kartendarstellungen (Fischer und Thomas 2021), repräsentiert. In diese Viewer können zudem weitere digitale Ressourcen, z. B. Bilder oder beschreibende Texte, eingebunden und mit den dort dargestellten Daten angezeigt, sowie die Inhalte mit zusätzlichen Informationen außerhalb der Metadaten annotiert werden. Ein einzelnes System müsste aufwendig angepasst oder es muss auf verschiedene Datenbanken zurückgegriffen werden, um die verschiedenen Dateitypen speichern zu können. Daher ist die Entwicklung einer Weboberfläche geplant, über die der Ressourcenmanager zur Erfassung und Recherche, aber auch die Tools zur Anzeige der Präsentationsschicht eingebunden und angezeigt werden. Bisher erfolgt ein Aufruf der jeweiligen Tools über verschiedene Links. Sie befinden sich bereits auf demselben Webserver und greifen auf denselben Datenbestand zu. Für die Regesten und Texte der Quellen muss zum Zeitpunkt des Abstracts noch eine Lösung gefunden werden.

            

            

                
Datenmodellierung

                
Über das Datenmodell werden die sehr heterogenen Informationen und Dateien inhaltlich organisiert und mittels automatisiert vergebener PID miteinander verknüpft. Dieser Prozess erfolgt manuell. Aus den Quellentexten entnommene Angaben zu Akteuren, Datierungen, Ereignissen, Objekten und Orten erhalten konkrete Bezeichnungen und bilden die Gruppe der Forschungsobjekte. Diese werden dann verschiedenen Kategorien des Datenmodells zugewiesen, z. B. St. Nikolai als Bauwerk der Klasse 
                    
E18 Physical Object
, Wilsnack als Stadt der Klasse 
                    
E53 Place
 oder Bischöfe und Pilger als Personen der Klasse 
                    
E39 Actor
. Die Quelle liegt als digitale Ressource vor und wird mit Metadaten angereichert sowie den entsprechenden Forschungsobjekten zugewiesen. Der Quellentyp, wie Ablass, Testament, Urkunde, wird zusammen mit ihrem Fundort in den Metadaten erfasst. Es ist nicht ausreichend, das Datenmodell nur am Bauwerk oder an geographischen Informationen auszurichten, da heterogene Daten miteinander in Verbindung gebracht werden müssen und Informationen zu Bauwerk und Ereignissen beinhalten.
                

                
Derzeit orientiert sich das Modell an der Basisontologie von CIDOC CRM. Im Laufe der folgenden Projektphase wird in verschiedenen Unterontologien geprüft, welche Klassen und Eigenschaften zutreffend sein könnten, um bestimmte Angaben zu präzisieren. Hier erscheinen insbesondere CIDOC CRMba für archäologische Bauwerke (Ronzino et al. 2016), CRMdig für Herkunftsmetadaten (Doerr, Stead und Theodoridou 2016) sowie CRMgeo für räumlich-zeitliche Daten (Hiebel et al. 2015) als geeignet. Die Kategorien für die Erfassung der Forschungsobjekte entsprechen zu diesem Zeitpunkt hauptsächlich den 
                    
High Level Classes
, für die Hierarchisierung des Bauwerks werden bereits untergeordnete Klassen verwendet. Die Modellierung der Beziehungen zwischen den Forschungsobjekten dient deren Kontextualisierung und erfolgt unter Verwendung der den jeweiligen Klassen zugewiesenen Eigenschaften. Aus Gründen der Datenlogik lässt sich nicht jede Klasse mit jeder Eigenschaft verbinden. Eine feinere Granulierung erfolgt während des Fortschreitens des Projekts, da die Entwicklung des Datenmodells im Dialog mit den Erkenntnissen der Forschung erfolgt. Eine zu grobe oder kleinteilige Erfassung führt zu ungenauen oder zu geringen Informationen, die miteinander in Verbindung gebracht werden. Anpassungen werden im Projekt dokumentiert. Es besteht ebenfalls die Möglichkeit, ab Herbst 2022 das in der Weiterentwicklung befindliche Datenmodell für die semantische Annotation von 3D Artefakten der NFDI4Culture (Rossenova 2021) zu testen und für das Wilsnackprojekt anzupassen.
                

                
Die sehr heterogenen Forschungsobjekte erhalten bereits während des Erfassungsprozesses konkrete Bezeichnungen wie “Kirche St. Nikolai”, “Chor”, “Chorfenster I”, “Hostienwunder”, “1383”, “Hostienverbrennung”, “Johann Ellefeldt”, “1552”, u. s. w., so dass eine Zuordnung eindeutig ist. Nicht jedes Ausstattungsstück oder jeder Akteur der Zeit werden erfasst. Gemeinsam in der Projektgruppe erfolgt eine Priorisierung. Gleich benannte Forschungsobjekte, wie Säulen, nicht näher bezeichnete Altäre oder die Wunderbluthostien, werden nummeriert. Eine Dokumentation erfolgt in den Metadaten der zugehörigen digitalen Ressourcen und im Datenmodell. Das Bauwerk wird durch die Kategorien Bauwerk – Gebäudeteile – immobile und mobile Ausstattung hierarchisiert. Den Forschungsobjekten werden alle zugehörigen Informationen und Dateien – egal welchen Typs – zugewiesen, z. B. werden alle Fotografien, Bauberichte und entstehenden Modelle des Chors mit dem Forschungsobjekt “Chor” verknüpft. Jede erfasste Information, egal ob Forschungsobjekt oder Datei, erhält einen PID, so dass eine Eindeutigkeit gewährleistet ist. Die Zuweisung erfolgt händisch und kann korrigiert werden. Einzelscheiben der Montagen oder des Gesamtfensters können gruppiert werden, wenn sie dieselben Bewegungen im Raum durchlaufen. Eine Erfassung erfolgt jedoch für jedes Forschungsobjekt einzeln, da sie verschiedene Ereignisse durchlaufen haben und noch können.

                

                    

                    
Abbildung 5: Wilsnack, Kirche St. Nikolai, Nördliches Querhaus, Fenster n VIII, CVMA Deutschland Potsdam/Berlin-Brandenburgische Akademie der Wissenschaften, Foto: Holger Kupfer.

                

                
                

                    

                    
Abbildung 6: Wilsnack, Kirche St. Nikolai, Modellierung des Fenster nVIII anhand CIDOC CRM, Visualisierung: Gordon Fischer / Daten: Anja Gerber 2022.

                

                
            

            

                
Erschließung der Daten

                
Eine Annotation von digitalen Ressourcen mit Metadaten erfolgt auf der Dateiebene mit einer projekteigenen und sich in der Entwicklung befindlichen Spezifikation, die sich aus Gründen der Nachnutzbarkeit an bekannten und etablierten Standards orientiert. Der Erstentwurf verwendet das Austauschschema LIDO 1.1 (ICOM-CIDOC LIDO Working Group 2021), welches sich u. a. an CIDOC CRM, CDWA Lite (J. Paul Getty Trust 2006) und Spectrum (Collections Trust 2017) orientiert, sowie Elemente aus Dublin Core (DCMI 1995–2022, DCMI 2020). Bei LIDO handelt es sich um einen etablierten Standard für die (Meta-)Datenerfassung im Bereich des Digitalen Kulturerbes (Knaus, Stein und Kailus 2019, 12–15). Dieses Schema ist neben der Erfassung von Grafiken und Fotografien auch für Ausstattungsstücke geeignet (Knaus, Kailus und Stein 2022, 18–21). Die Spezifikation des Projekts wird im Hinblick auf die Annotation der zu erstellenden 3D-Modelle um für diese erforderliche Angaben erweitert. Hier erscheint der Standard CARARE 2.0 als geeignet, da technische Angaben (digitale Herkunftsmetadaten) und beschreibende Informationen für Gebäude, Artefakte und 
                    
born-digital
 Objekte möglich sind (Fernie, Gavrilis und Angelis o. J.). Mit LIDO können Metadaten desselben Typs unter Zuweisung verschiedener Kategorien und Rollen erfasst werden, z. B. ob jemand an der Erstellung einer Ressource beteiligt war oder lediglich auf dieser abgebildet ist. Administrative, beschreibende und technische Metadaten werden abgedeckt. So können die Datei selbst aber auch ihr Erstellungsprozess beschrieben werden. Die Konfiguration des Metadatenschemas erfolgt ebenfalls über eine JSON-Datei. Anpassungen werden dokumentiert und versioniert. Um eine eindeutige Zuordnung zu Akteuren, Ereignissen, Orten oder auch eine zweifelsfreie Klassifizierung der Objekte zu ermöglichen, werden bereits etablierte Normdatenvokabulare, wie GND, Wikidata, Iconclass, Getty Art &amp; Architecture Thesaurus oder Geonames, genutzt und die Identifier erfasst. Zudem gibt es Überlegungen, Normdaten im Rahmen der NFDI4Culture über Wikibase (Rossenova u. a. 2021) zu erfassen und als Linked Open Data in den Wissensgraphen (NFDI4Culture o. J.) einzuspeisen.
                

            

        

            

                
Digital Social Reading, a term proposed by Rebora et al. (2021), is described as "a wide variety of practices related to the activity of reading and using digital technologies and platforms… to share with other people, thoughts and impressions about texts" (Pianzola, forthcoming). This study will dive deep into the world of Indian Digital Social Reading practices, thus filling the gap in current global discourse. Aesthetic experience, central to reading as an 

                
activity

                
, forms the basis of aesthetic judgement of a literary text. Therefore, it is essential that our understanding of 

                
how

                
 reading happens also includes some empirical analysis of its 

                
aesthetic properties

                
- the foundational "units" of aesthetic judgement. Combining these two strands, this study aims 

                
to conceptualise a theoretical framework of aesthetic properties (AP) of reading fiction in the context of Indian Digital Social Readers by employing an embedded mixed-method approach

                
. 

            

            
Literature:

            

                
While most scholarship in DSR is global north-centric, Pianzola et al. (2020) offer a more global perspective in their study of Wattpad as a literary resource. Pianzola's upcoming book is the only textbook-like work organising the different DSR studies in a historical timeline and taxonomies; DSR's role in cognitive, aesthetic, and educational aspects of reading, and its pedagogical properties. The democratising agency of DSR (Sedo 2011; Kellner 2016); statistical interpretation of data on gender biases, gender and genre fixation, authorship and gender identity etc., (Thelwall and Kousha 2017); the impact of tweaking reviews for book sales (Nan Hu and colleagues 2012)- are some of the most prominent works. Holur et al. (2021) reflected upon the manners of reading novels that readers engage in, making a significant contribution to "infinite vocabulary networks". Koolen, Neugarten, and Boot (2022) identified impact categories in English book reviews. 

            

            

                
The Oxford Handbook of Cognitive Literary Studies

                
 (2015) discusses intersections between the literary and cognitive, reflecting on imagination, brain imaging, and its neural networks; imagery as the key element of aesthetic experience; the delicate balance between internal and external cognition giving rise to an intense aesthetic response etc. 

                
The Oxford Handbook of Aesthetics

                
 (2005) is a comprehensive work on philosophical ideas of aesthetics and its properties; arts and their relationship with aesthetics; notions of interpretation etc. In a self-reflexive essay, Matravers and Levinson (2005) defend Levinson's ontology of aesthetic properties while also questioning issues like aesthetic autonomy. More empirical works, such as by Larson et al. (2007), measured the aesthetics of reading based on typographic alterations and textual optimisation that affected the reader's cognitive functions and frowning while reading. DeClerq (2002) observed that existing definitions of aesthetic properties pertain mostly to visual objects, leaving enough scope for inquiry into the other forms of aesthetic expressions, such as music and literature. 

            

            
Hypotheses and research questions:

            

                
Based on the hypotheses that digital space and digital technologies redefine the social reading experience and that the reading experience as a whole can be best understood through an analysis of the aesthetic properties of reading, this project attempts to answer the following questions:

            



                

                    
What parameters determine the aesthetic properties that govern a reader's 

                    
aesthetic judgement of fiction

                    
? (the underlying assumption here is that aesthetic properties differ for fiction and non-fiction)



                        
How are these parameters of AP affected by the affordances created by the digital?

                    

                

                
Is it possible to formulate a framework of aesthetic properties of reading fiction from DSR user data?

            

            
Parameters of AP can be explained better through their dimensions, categorised according to their epistemological qualities, such as cultural, affective, semantic, and communal.

            
Methodology: 

            

                
Owing to the complex and abstract nature of the concepts, I propose an embedded mixed methodology. As the main crux of the study, subjective data will be gathered through semi-structured interviews of Indian digital social readers. Codes generated from the transcripts (using NVivo) will help identify the parameters of AP, which will then be used in the supplementary survey involving bilingual adult Indian readers. This data should help reinforce the proposed framework. Cluster sampling will be used for this purpose. R will be used for data collation and final analysis.

                
The project does not concentrate on a fixed corpus given the generic nature of the questions it asks and India's multilingual readership.

                
 Data collection begins in mid-January 2023, following the completion of basic theorising. 

            

            
Limitations:



                
Does not consider non-fiction works

                
survey method is not the most reliable of all quantitative methods

                

                    
difficulty justifying abstract concepts through empirical approach.

                

            

        

            

                
Approaches to Corpus Homogenization

                
Comparative endeavors in Computational Literary Studies typically require corpora which are both diverse, i.e., including texts in different languages and from different sources, and homogenized, i.e., formal and structural consistent. One way to tackle this issue is to establish upstream internal guidelines, such as the ones developed within the ELTeC initiative (Schöch et al. 2021). 
 In the following, we report on our approach to homogenizing corpora for DraCor. 

                

                
DraCor, based on the concept of Programmable Corpora (Fischer et al. 2019), is an open platform as well as a growing network for hosting, accessing, and analyzing theater plays. DraCor relies on the general TEI model for dramatic texts, with minimal enhancements, and thus facilitates contributions by external scholars who want to onboard their corpora onto its ecosystem. Once integrated, corpora can benefit from the platform’s APIs and services, ranging from the computation of network metrics via various extraction functions to SPARQL queries. 

                
Typically, corpora for DraCor are not built from scratch, but are created either by aggregating formally heterogeneous texts from different sources or by transforming existing corpora. Unlike in ELTeC, the homogenization of texts for DraCor usually does not stand at the beginning of the corpus creation process, but is rather an intervention in existing corpora which are sometimes subject to amendment and growth, hence ›living‹. This approach poses a number of challenges, for which we are currently prototyping several workflows. Here, we present the pipelines for mounting to DraCor two new corpora: the English-language 
                    
EarlyPrint Drama Corpus
 (EPDraCor) and the 
                    
Ukrainian Drama Corpus
 (UDraCor). 

                

            

            

                
Corpus Onboarding

                
From a technical point of view, onboarding corpora onto DraCor is a series of automated and manual transformations of the source data, which depend crucially on the format and markup of the files. Texts from a single, homogeneous collection with pre-existing markup and metadata will require different workflows and pipelines than those coming, for example, from a variety of raw text sources.

                
This heterogeneous point of departure is what shapes our onboarding approach. Consequently, we are developing a modular workflow made up of a set of demand-dependent components. In addition to guideline-based manual revisions (e.g. pre-structuring texts with Markdown), we use XSLT scripts for automated transformations. Edits specific to theater plays, such as the task of speaker identification, are supported by an Oxygen framework; 
 we are furthermore experimenting with task-specific GUI applications based on react.js. 
 The correction and enrichment of metadata, such as the addition of Wikidata ID, is organized semi-automatically via OpenRefine.
                

                
A particular challenge is posed by living corpora. Here, the manual transformations performed during onboarding should be reapplicable in case of edits to the source data. Accordingly, we implemented routines for a ›backward compatibility‹ of the markup: the changes made by us during onboarding can later be applied again to a newer version of the source files. 

                

            

            

                
EPDraCor and UDraCor

                
To develop our workflows and pipelines, two corpora with very different requirements are currently in the process of onboarding. While UDraCor originates from a growing collection of heterogeneous sources, EPDraCor is based on semantically rich TEI files from the Early Print project. 
 The onboarding of EPDraCor starts with enhancing and correcting the original markup in our copy of the source corpus, accompanied by collecting LOD metadata from additional sources. Then, we combine the enhanced sources with their metadata and use XSLT to transform them, so that the TEI fulfills the requirements of the DraCor platform.
                

                
Due to the heterogeneity of the sources, a more case-specific solution must be found for UDraCor in this initial step. Here, the conversion is a semi-automatic procedure with heavy use of string patterns and regex. At the same time, UDraCor takes a community-based corpus-building approach by inviting scholars specializing in Ukrainian studies to work on both technical and content-related tasks. This work on UDraCor once again shows how the technical task of corpus building and community activities are crucially intertwined.

            

        

            

                
Einleitung

                
Negative Auswirkungen schlankheitsidealisierender Medieninhalte auf Körperbild und -zufriedenheit sind hinreichend belegt (z.B. Frederick et al. 2019, 193, 195; Grabe, Ward und Hyde 2008, 469-470). Inhalte, die Untergewicht oder Essstörungen glorifizieren (z.B #thinspiration), sind mittlerweile auf Instagram verboten. Die 
                    
Fitspiration
-Bewegung („fitness“ + „inspiration“) stellt sich selbst als Alternative dar, die Unterstützung auf dem Weg zu einem gesünderen Lebensstil bieten will (Boepple et al. 2016, 133; Holland und Tiggemann 2017, 76). Empirische Untersuchungen belegen zwar ihr Potential, Follower:innen zu mehr Bewegung und gesunder Ernährung zu motivieren (Santarossa et al. 2019, 381; Tiggemann und Zaccardo 2015, 66), weisen aber zugleich negative Auswirkungen auf Körperbild, Selbstzufriedenheit und Stimmung nach (Prichard et al. 2020, 4; Tiggemann und Zaccardo 2015, 64-65). Inhaltsanalysen von #fitspiration-Posts auf Instagram stellen fest, dass diese den Körpertyp „dünn und muskulös“ idealisieren und durch bestimmte Posen oder Kleidung bewusst in Szene setzen – die beworbene Sport- und Ernährungsweise dient also nicht der Förderung von Gesundheit und Wohlbefinden, sondern der „Optimierung“ des eigenen Körpers (Pilgrim und Bohnet-Joschko 2022, 117; Santarossa et al. 2019, 380; Tiggemann und Zaccardo 2018, 1007).
                

                

                    
Bewusst als Gegenströmung bezeichnet sich die 
                    
Bodypositivity
-Bewegung und stellt es als ihr Ziel dar, gegen unrealistische Gewichts- und Körperideale einzutreten und so insbesondere Frauen zu mehr Körperakzeptanz und Selbstliebe zu verhelfen (Cwynar-Horta 2016, 40).
                    
 Studien belegen teilweise positive Auswirkungen von Bodypositivty-Inhalten auf die Körperwertschätzung und -zufriedenheit (Cohen et al. 2019a, 1554-1556; Fioravanti et al. 2021, 13-14), stellen aber auch eine erhöhte Selbst-Objektifizierung fest (Cohen et al. 2019a, 1556-1557). Inhaltsanalysen belegen, dass #bodypositivity-Posts zwar Frauen unterschiedlichen Körperbaus repräsentieren, gleichzeitig aber häufig objektifizierende Merkmale (z.B. freizügige Kleidung, aufreizende Posen) enthalten (Cohen et al. 2019b, 51-52; Lazuka et al. 2020, 89). Außerdem mangele es Bodypositivity in anderen Dimensionen jenseits des Körpergewichts (z.B. Geschlecht, Ethnie, klassische Schönheitsideale) an Variabilität und Vielfalt (Cohen et al. 2019b, 51-52; Cwynar-Horta 2016, 40; Lazuka et al. 2020, 87).
                

                
Es ist daher die Frage zu stellen, ob die Selbstinszenierung von Bodypositivity als Gegenströmung zu Fitspiration (im Sinne von „Selbstliebe“ statt „Selbstoptimierung“) gerechtfertigt ist oder ob beide Bewegungen doch mehr Gemeinsamkeiten als Unterschiede aufweisen. Bisherige Inhaltsanalysen zu Instagram-Bildern betrachten immer entweder nur Fitspiration- oder nur Bodypositivity-Posts, was einen inhaltlichen Vergleich beider Bewegungen erschwert. Ansatz des hier vorgestellten Projekts ist es daher, Bilder beider Hashtags hinsichtlich derselben inhaltlichen Kategorien zu analysieren. Während in früheren Studien Bilder ausschließlich per Hand kodiert wurden, finden zudem erstmals Methoden der automatischen Bildklassifikation Anwendung, um statistische Gegenüberstellungen von Fitspiration und Bodypositivity an einem großen Bilddatensatz vornehmen zu können.

            

            

                
Korpus und händische Annotation

                
Das Korpus umfasst 10000 Bilder, die im Zeitraum März bis April 2022 mit einem der Hashtags #fitspiration oder #bodypositivity auf Instagram gepostet wurden. Eine zufällig gezogene Stichprobe von je 500 Bildern pro Hashtag wurde manuell hinsichtlich verschiedener für einen Vergleich der beiden Bewegungen relevanter inhaltlicher Kategorien kodiert. Betrachtet wurden allgemeine Kategorien zur Art des Beitrags (Bild, Text) und des Abbildungsgegenstands (Person, Lebensmittel) sowie spezifischere Kategorien zur Analyse abgebildeter Personen aus den Bereichen Demographie, körperbezogene Eigenschaften, Kleidung, Freizügigkeit, Art der Pose und Anzeichen einer Objektifizierung. Diese sind in Tabelle 1 mit den Häufigkeiten der einzelnen Ausprägungen unter den Bilddaten aufgeführt. Zur Absicherung der Objektivität der Kodierung wurde ein Teil der Bilder von zwei Personen unabhängig kodiert und die Inter-Koder-Reliabilität bestimmt. Für die einzelnen inhaltlichen Kategorien wurden zwischen 200 und 800 Bilder doppelt kodiert, sodass Cohen’s Kappa Werte über 0.7 für alle Kategorien erzielt werden konnten.

            

            

                
Automatische Bildklassifikation

                
Um den Vergleich von Fitspiration- und Bodypositivity-Bildern auf Instagram auf einer möglichst großen Datenbasis durchführen zu können, wurde versucht, die verbleibenden 9000 Bilder mit an den händisch kodierten Daten trainierten Modellen automatisch zu klassifizieren. Der manuell kodierte Bilddatensatz (N = 1000) wurde dazu im Verhältnis 4:1 in Trainings- und Testdaten aufgeteilt. Implementiert wurden neben Modellen aus dem Bereich des traditionellen 
                    
Machine Learning
 (
                    
Support Vector Machine
 SVM, 
                    
Decision Tree
 DT) als aktuelles Beispiel für 
                    
Deep Learning
-Verfahren auch 
                    
Convolutional Neural Networks
 (CNN), die relevante Bildmerkmale selbst extrahieren können (Guo et al. 2017, 721). 
                

                
Da der Trainingsprozess von CNN-Modellen eigentlich einen weitaus größeren Trainingsdatensatz erfordert (Sheykhmousa et al. 2020, 6309), wurde versucht, die Performanz der Modelle durch Transferlernen zu verbessern. Dabei werden an großen Datensätzen vortrainierte Modelle an wenige annotierte Daten einer neuen Domäne angepasst (
                    
Fine Tuning
, vgl. Boumaraf et al. 2021, 3). Im vorliegenden Projekt wurde das am umfassenden Bilddatensatz 
                    
ImageNet
 trainierte Modell VGG16 (Swasono, Tjandrasa und Fathicah 2019, 178) als Grundlage verwendet, da auf diesem Modlel basierende Transferlernprozesse in der Vergangenheit in verschiedenen Domänen die Klassifikationsergebnisse verbessern konnten (Boumaraf et al. 2021, 23; Dubey und Jain 2020, 5; Swasono, Tjandrasa und Fathicah 2019, 179-181). Für das hier vorgestellte Projekt wurden die letzten vier 
                    
Dense Layers
 aus der Netzwerkarchitektur entfernt und durch ein weiteres 
                    
Pooling Layer
, ein 
                    
Dense Layer
, ein 
                    
Dropout Layer
 (Dropout-Rate 50%) und ein abschließendes 
                    
Dense Layer
 zur finalen Klassenvorhersage (Aktivierungsfunktion Sigmoid) ersetzt. Nur die Gewichte dieser äußeren Schichten wurden während des 
                    
Fine Tunings
 (Adaption an den Instagram-Bilddatensatz) neu trainiert, die vortrainierten Gewichte der tieferen Schichten werden beibehalten. 
                

                
Da für einige inhaltliche Kategorien ein starkes Klassenungleichgewicht im Trainingsdatensatz bestand (z.B. bei der Kategorie „nackte Haut“: Nur 20% der untersuchten Bilder zeigten keine nackten Hautstellen), wurde mit 
                    
Upsampling
 gearbeitet: Mithilfe von SMOTE wurden zusätzliche Daten der unterrepräsentierten Klasse(n) synthetisch erzeugt, um ein Klassengleichgewicht herzustellen (Chawla et al. 2002, 328).
                

                
Tabelle 1 zeigt für die verschiedenen Inhaltskategorien, welche Anteile der Daten pro Klasse durch das für diese Kategorie am besten funktionierende Modell korrekt klassifiziert werden konnten. Teilweise erzielten implementierte SVM-Modelle die besten Ergebnisse, die meisten Kategorien ließen sich aber mit einem der CNN-Modelle am besten klassifizieren. Die auf Transferlernen basierenden CNN-Modelle zeigten in allen Fällen bessere Resultate als die CNNs ohne Transferlernen. Auch das Upsampling der unterrepräsentierte(n) Klasse(n) wirkte sich in einigen Fällen positiv auf die Performanz aus.

                
Wie Tabelle 1 zu entnehmen ist, gelang dennoch nicht für alle Kategorien mit einem der implementierten Modelle die Klassifikation mit zufriedenstellenden Ergebnissen. Probleme zeigten sich insbesondere bei multinominalen Klassifikationsproblemen, also Kategorien, die mehr als zwei Ausprägungen unterscheiden. In solchen Fällen, wie etwa dem Körperbau, der beim händischen Kodieren auf einer 9-stufigen Skala (Pulvers et al. 2014, 1643) eingeordnet worden war, wurden für die automatische Klassifikation mehrere Klassen zusammengelegt; dennoch wurden nur unzureichende Klassifikationsergebnisse erzielt. Auch die automatische Klassifikation von Kategorien mit starkem Klassenungleichgewicht führte zu Schwierigkeiten, etwa bei der Kategorie „Entsprechung klassischer Schönheitsideale“: Bilder von Personen, die klassischen Schönheitsidealen überhaupt nicht entsprechen, waren im händisch annotierten Datensatz selten und wurden von dem implementierten Klassifikator nur unzuverlässig erkannt, wohingegen Bilder der häufigeren Klasse „normale/extreme Entsprechung“ meistens korrekt klassifiziert wurden.

                
Für die Kategorien, die im Testdatensatz mit einem der implementierten Modelle gut klassifiziert werden konnten, wurde das jeweilige Modell auf den Gesamtdatensatz angewandt. Das war insgesamt nur für acht Kategorien der Fall, fünfmal wurde dabei ein CNN-Modell mit Transferlernen eingesetzt, dreimal wurde ein traditionelles 
                    
Machine Learning-
Modell angewandt. Die statistischen Vergleiche zwischen #fitspiration und #bodypositivity konnten in diesen Fällen anhand des automatisch klassifizierten Gesamtdatensatzes (N = 10000) erfolgen, für die übrigen Kategorien wurden die statistischen Tests nur mit den 1000 händisch annotierten Daten durchgeführt.
                

                    

                        

                            
Inhaltliche Kategorie 

                            

                                

                                    
Modell mit den besten Klassifikations-ergebnissen

                                

                            

                            

                                
Anteil richtig klassifizierter Daten pro Klasse (Testdatensatz)

                            

                            

                                
Klassenverteilung händisch annotierter Datensatz (

                                
N

                                
 = 1000)

                            

                            

                                
Klassenverteilung Gesamtdatensatz (

                                
N 

                                
= 10000)

                            

                        

                        

                            

                                

                                    
Beitrag enthält ein Bild 

                                

                            

                            

                                

                                    
CNN mit Transferlernen, mit Upsampling der Klasse „Nein, kein Bild“

                                

                            

                            

                                

                                    
Ja, Bild enthalten: 97%

                                

                                

                                    
Nein, kein Bild enthalten, Beitrag ist rein textbasiert: 93%

                                

                            

                            

                                
Fitspiration:

                                
Ja: 93,0%

                                
Nein: 7,0%

                                
Bodypositivity:

                                
Ja: 92,2%

                                
Nein: 7,8%

                            

                            

                                
Fitspiration:

                                
Ja: 83,8%

                                
Nein: 6,2%

                                
Bodypositivity:

                                
Ja: 81,9%

                                
Nein: 8,1%

                            

                        

                        

                            
Abbildung einer Person 

                            

                                
CNN mit Transferlernen, ohne Upsampling

                            

                            

                                
Ja: 84%

                                
Nein: 54%

                            

                            

                                
Fitspiration:

                                
Ja: 60,0%

                                
Nein: 40,0%

                                
Bodypositivity:

                                
Ja: 65,4%

                                
Nein: 34,6%

                            

                            

                                
Fitspiration:

                                
Ja: 63,5%

                                
Nein: 36,5%

                                
Bodypositivity:

                                
Ja: 63,7%

                                
Nein: 36,3%

                            

                        

                        

                            
Abbildung eines Lebensmittels 

                            

                                
CNN mit Transferlernen, ohne Upsampling

                            

                            

                                
Ja: 55%

                                
Nein: 100%

                            

                            

                                
Fitspiration:

                                
Ja: 4,9%

                                
Nein: 95,1%

                                
Bodypositivity:

                                
Ja: 6,9%

                                
Nein: 93,1%

                            

                            

                                
Fitspiration:

                                
Ja: 2,1%

                                
Nein: 97,9%

                                
Bodypositivity:

                                
Ja: 2,2%

                                
Nein: 97,8%

                            

                        

                        

                            
Geschlecht 

                            

                                
SVM mit Upsampling der Klasse „männlich“

                            

                            

                                
Männlich: 74%

                                
Weiblich: 71%

                            

                            

                                
Fitspiration:

                                
Männlich: 48,0%

                                
Weiblich: 53,0%

                                
Bodypositivity:

                                
Männlich: 19,0%

                                
Weiblich: 81,0%

                            

                            

                                
Fitspiration:

                                
Männlich: 46,9%

                                
Weiblich: 53,1%

                                
Bodypositivity:

                                
Männlich: 38,1%

                                
Weiblich: 61,9%

                            

                        

                        

                            

                                
Ethnie

                            

                            

                                
SVM ohne Upsampling

                            

                            

                                

                                    
Weiß: 79%

                                

                                
Andere: 29%

                            

                            

                                
Fitspiration:

                                

                                    
Weiß: 81,0%

                                

                                
Andere: 19,0%

                                
Bodypositivity:

                                

                                    
Weiß: 82,9%

                                

                                
Andere: 17,1%

                            

                            
/ (keine Modell-anwendung auf Gesamtdatensatz)

                        

                        

                            
Körperbau 

                            

                                
SVM ohne Upsampling

                            

                            

                                
Nicht beurteilbar: 42%

                                

                                    
Skalawert 1-3: 23%

                                    

                                    
Skalawert 4: 31%

                                

                                

                                    
Skalawert 5: 15%

                                

                                

                                    
Skalawert 6-9: 35%

                                

                            

                            

                                
Fitspiration:

                                
1-3: 23,9%

                                
4: 39%

                                
5: 19,7%

                                
6-9: 6,7%

                                
Nicht beurteilbar: 10,7%

                                
Bodypositivity:

                                
1-3: 12,5%

                                
4: 20,2%

                                
5: 20,8%

                                
6-9: 24,1%

                                
Nicht beurteilbar: 22,3%

                            

                            
/ (keine Modell-anwendung auf Gesamtdatensatz)

                        

                        

                            
Muskulosität 

                            

                                
SVM mit Upsampling der Klasse „nicht muskulös“

                            

                            

                                
Nicht muskulös: 0%

                                
Sichtbar muskulös: 38%

                                
Sehr/extrem muskulös: 28%

                                
Nicht beurteilbar: 54%

                            

                            

                                
Fitspiration:

                                

                                    
Nicht muskulös: 1,3%

                                

                                

                                    
Sichtbar muskulös: 44,3%

                                

                                

                                    
Sehr/extrem muskulös: 39,3%

                                

                                

                                    
Nicht beurteilbar: 17,0%

                                

                                
Bodypositivity:

                                

                                    
Nicht muskulös: 13,7%

                                

                                

                                    
Sichtbar muskulös: 30,6%

                                

                                

                                    
Sehr/extrem muskulös: 15,5%

                                

                                

                                    
Nicht beurteilbar: 40,1%

                                

                            

                            
/ (keine Modell-anwendung auf Gesamtdatensatz)

                        

                        

                            
Entsprechung klassischer Schönheitsideale

                            

                                
CNN mit Transferlernen, mit Upsampling der Klasse „überhaupt nicht“

                            

                            

                                
Überhaupt nicht: 16%

                                
Normal / extrem: 81%

                                
Nicht beurteilbar: 44%

                            

                            

                                
Fitspiration:

                                

                                    
Gar nicht: 1,7%

                                

                                

                                    
Normal: 52,7%

                                

                                

                                    
Extrem: 15,7%

                                

                                

                                    
Nicht beurteilbar: 30,0%

                                

                                
Bodypositivity:

                                

                                    
Gar nicht: 5,2%

                                

                                

                                    
Normal: 47,1%

                                

                                

                                    
Extrem: 23,5%

                                

                                

                                    
Nicht beurteilbar: 23,5%

                                

                            

                            
/ (keine Modell-anwendung auf Gesamtdatensatz)

                        

                        

                            
Art der Kleidung

                            

                                
CNN mit Transferlernen, mit Upsampling der Klasse „Sportkleidung“

                            

                            

                                
Sportkleidung: 54%

                                
Sonstige Kleidung: 78%

                            

                            

                                
Fitspiration:

                                

                                    
Unterwäsche: 11,3%

                                

                                

                                    
Sportkleidung: 61,5%

                                

                                

                                    
eng: 11,7%

                                

                                

                                    
normal: 10,1%

                                

                                
weit: 4,1%

                                
Bodypositivity:

                                

                                    
Unterwäsche: 23,2%

                                

                                

                                    
Sportkleidung: 20,5%

                                

                                

                                    
eng: 20,2%

                                

                                

                                    
normal: 24,2%

                                

                                
weit: 6,4%

                            

                            

                                
Fitspiration:

                                
Sportkleidung: 52,7%

                                
Sonstige Kleidung: 47,3%

                                
Bodypositivity:

                                
Sportkleidung: 30,7%

                                
Sonstige Kleidung: 69,3%

                            

                        

                        

                            
Abbildung nackter Haut

                            

                                
SVM mit Upsampling der Klasse „Nein“

                            

                            

                                
Ja: 79%

                                

                                    
Nein: 50%

                                

                            

                            

                                
Fitspiration:

                                
Ja: 85,7%

                                
Nein: 14,3%

                                
Bodypositivity:

                                
Ja: 74,6%

                                
Nein: 25,4%

                            

                            

                                
Fitspiration:

                                
Ja: 63,3%

                                
Nein: 36,7%

                                
Bodypositivity:

                                
Ja: 64,3%

                                
Nein: 35,7%

                            

                        

                        

                            
Abbildung nackter Bauch

                            

                                
DT mit Upsampling der Klasse „Ja“ 

                            

                            

                                
Ja: 58%

                                
Nein: 63%

                            

                            

                                
Fitspiration:

                                
Ja: 44,4%

                                
Nein: 55,6%

                                
Bodypositivity:

                                
Ja: 41,0%

                                
Nein: 59,0%

                            

                            

                                
Fitspiration:

                                
Ja: 38,8%

                                
Nein: 61,2%

                                
Bodypositivity:

                                
Ja: 38,3%

                                
Nein: 61,7%

                            

                        

                        

                            
Person in Bewegung

                            

                                
CNN mit Transferlernen, mit Upsampling der Klasse „Ja“

                            

                            

                                
Ja: 36%

                                
Nein: 91%

                            

                            

                                
Fitspiration:

                                
Ja: 31,3%

                                
Nein: 68,7%

                                
Bodypositivity:

                                
Ja: 13,1%

                                
Nein: 86,9%

                            

                            
/ (keine Modell-anwendung auf Gesamtdatensatz)

                        

                        

                            
Objektifizierung: Fokus auf bestimmtes Körperteil

                            

                                
SVM mit Upsampling der Klasse „Ja“ 

                            

                            

                                
Ja: 44%

                                
Nein: 82%

                            

                            

                                
Fitspiration:

                                
Ja: 24,3%

                                
Nein: 75,7%

                                
Bodypositivity:

                                
Ja: 25,7%

                                
Nein: 74,3%

                            

                            
/ (keine Modell-anwendung auf Gesamtdatensatz)

                        

                        

                            
Objektifizierung: Aufreizende Pose

                            

                                
CNN mit Transferlernen, mit Upsampling der Klasse „Ja“

                            

                            

                                
Ja: 23%

                                
Nein: 95%

                            

                            

                                
Fitspiration:

                                
Ja: 9,0%

                                
Nein: 91,0%

                                
Bodypositivity:

                                
Ja: 25,4%

                                
Nein: 74,6%

                            

                            
/ (keine Modell-anwendung auf Gesamtdatensatz)

                        

                        

                            
Objektifizierung: Gesicht nicht erkenntlich

                            

                                
SVM mit Upsampling der Klasse „Ja“

                            

                            

                                
Ja: 41%

                                
Nein: 72%

                            

                            

                                
Fitspiration:

                                
Ja: 32,7%

                                
Nein: 67,3%

                                
Bodypositivity:

                                
Ja: 30,0%

                                
Nein: 70,0%

                            

                            
/ (keine Modell-anwendung auf Gesamtdatensatz)

                        

                        

                            
Vorher-Nachher-Vergleich

                            

                                
CNN mit Transferlernen, ohne Upsampling

                            

                            

                                
Ja: 90%

                                
Nein: 99%

                            

                            

                                
Fitspiration:

                                
Ja: 8,1%

                                
Nein: 91,9%

                                
Bodypositivity:

                                
Ja: 4,0%

                                
Nein: 96,0%

                            

                            

                                
Fitspiration:

                                
Ja: 2,1%

                                
Nein: 97,9%

                                
Bodypositivity:

                                
Ja: 1,1%

                                
Nein: 98,9%

                            

                        

                        
Tabelle 1: Übersicht der inhaltlichen Kategorien mit jeweils am besten klassifizierenden Modell sowie der Häufigkeitsverteilungen in händisch annotiertem und Gesamtdatensatz

                    

                    
            

            

                
Befunde zum Vergleich der Fitspiration- und Bodypositivity-Bilder

                
Statistische Vergleiche der Fitspiration- und Bodypositivity-Bilder bezüglich der untersuchten Kategorien konnten sowohl Unterschiede als auch Gemeinsamkeiten in der Häufigkeit bestimmter Merkmale nachweisen (siehe auch Tabelle 1). Bilder beider Hashtags zeigen zumeist mindestens eine nackte Körperstelle: Die abgebildeten Personen tragen häufig sehr freizügige Kleidung (bei Fitspiration meist knappe Sportkleidung, bei Bodypositivity eher Unterwäsche/Badebekleidung oder enge Alltagskleidung). Auch weitere Anzeichen einer Objektifizierung (Bildfokus auf ein Körperteil gerichtet, anzügliches Posieren, Gesicht nicht erkenntlich) und die Tatsache, dass Personen hauptsächlich in statischer Pose statt in Bewegung abgebildet sind, belegen für beide Hashtags die eindeutige Fokuslegung auf das äußere Erscheinungsbild. Unterschiede zwischen den Hashtags zeigen sich hinsichtlich körperbezogener Merkmale: Frauen haben auf Fitspiration-Bildern einen signifikant schlankeren Körperbau und mehr Muskelmasse als auf Bodypositivity-Bildern, wo mehr unterschiedliche Körperformen repräsentiert sind. Die Betonung klassischer Schönheitsideale (z.B. reine Haut, glänzende Haare) sowie die Überrepräsentation weißer Personen lässt sich hingegen beiden Bewegungen vorwerfen: Personen mit äußeren „Makeln“ wie Cellulite oder Hautunreinheiten sind unter beiden Hashtags deutlich unterrepräsentiert, ebenso wie nicht-weiße Personen. 

                
Zusammenfassend lässt sich festhalten, dass unter dem Hashtag #fitspiration hauptsächlich schlanke und muskulöse Frauen und Männer in Sportkleidung so posieren, dass ihr durchtrainierter Körper möglichst gut zur Geltung kommt. Bodypositivity hingegen repräsentiert zwar mehr unterschiedliche Körpertypen – ist dafür aber größtenteils auf Bilder von Frauen beschränkt, die klassischen Schönheitsidealen entsprechen. Beide Bewegungen setzen durch freizügige Kleidung, Nacktheit und Objektifizierungen einen starken Fokus auf das äußere Erscheinungsbild. 

            

            

                
Fazit und Ausblick

                
Die durchgeführte Analyse von #fitspiration- und #bodypositivity-Bildern auf Instagram mit einheitlichen inhaltlichen Kategorien ermöglichte erstmals eine konkrete Gegenüberstellung beider Bewegungen hinsichtlich relevanter inhaltlicher Bildmerkmale. Durch die implementierten Modelle zur automatischen Bildklassifikation konnten die Analysen sich zudem zumindest teilweise auf einen großen Bilddatensatz (N = 10000) stützen. Zwar zeigen sich hinsichtlich Körperbau und Muskulosität die erwarteten Unterschiede zwischen den beiden Hashtags, in den meisten anderen Dimensionen unterschieden sich Bilder beider Bewegungen aber nicht signifikant. Die Selbstinszenierung der Bodypositivity-Bewegung als Gegenströmung zu Fitspiration lässt sich daher nicht belegen: Statt für Selbstliebe unabhängig von Äußerlichkeiten einzutreten, ist an Bodypositivity ebenso wie an Fitspiration die Unterrepräsentation bestimmter Personengruppen, die Objektifizierung von insbesondere Frauen, sowie der starke Fokus auf ein makelloses äußeres Erscheinungsbild im Zuge einer (äußerlichen) Selbstoptimierung kritisch hervorzuheben.

                
Die automatische Bildklassifikation gelang nicht für alle betrachteten inhaltlichen Kategorien, weitere Forschung scheint hier notwendig. Besonderes Augenmerk sollte auf den Umgang mit multinominalen Klassifikationsproblemen sowie unbalancierten Klassen gelegt werden. Der erfolgsversprechenden Methodik des Transferlernens mit CNN-Modellen sollte in Zukunft weiter nachgegangen werden, dabei könnte als Grundlage für den Transferlernprozess spezifischer Modelle, die bereits auf konkrete Klassifikationsaufgaben wie das Erkennen von Gesichtern oder Körperteilen hin vortrainiert sind, hilfreich sein. Ein anderer Ansatz könnte sein, neben den Bildern selbst auch weitere Daten andere Modalitäten miteinzubeziehen, um die beiden Hashtags noch umfassender vergleichen zu können. In Frage kämen hier etwa die sprachlichen Informationen aus Beitragstexten, Kommentare anderer User:innen unter dem Beitrag oder Metadaten zur:m Verfasser:in (z.B. Follower:innenzahl).

            

        

            
In dem Vortrag werden Ergebnisse eines Leseexperiments vorgestellt, welches unter anderem darauf abzielte, die Lücke zwischen psychologisch orientierten Lesereaktionsstudien und literaturwissenschaftlich fundierten Rezeptionsstudien (Kavanagh, 2021) zu schließen. Die Stimuli umfassten Passagen aus der beliebten "Harry Potter"-Buchreihe in deutscher Sprache sowie Auszügen aus „Harry Potter“-Fanfictions. In dem Experiment sollten folgende Forschungsfragen beantwortet werden: 



                
Wie beeinflusst der emotionale Gehalt der Texte die Reaktion der Lesenden? 

                
Welche Wirkung hat der Hintergrund der Lesenden: gibt es Unterschiede in der Wahrnehmung, die durch Leseerfahrung und Fandom-Affinität bedingt sind? 

            

            
In der Studie wurde eine Reihe von Messmethoden verwendet, darunter die Messung der Augenbewegungen (inklusive der Pupillengröße) und des Hautleitwerts (GSR) der Teilnehmer:innen. Diese beiden Messmethoden werden am häufigsten als Marker von emotionaler Reaktion in Betracht gezogen. 

            
Die Originaltexte unter den Stimuli umfassten 40 “neutrale” Texte, 40 Texte, die als “furchteinflößend”, und 40 Texte, die als "fröhlich" gekennzeichnet waren (s. Tabelle 1). Diese Textstellen sowie ihre Sentimentmarkierungen wurden aus einer früheren Lesestudie von Hsu (2015) übernommen. Die Liste der Stimuli wurde um Fanfiction-Texte erweitert, die zuvor von 82 Fanfiction-Leser:innen in einer Umfrage ausgewählt wurden, weil sie besonders starke Emotionen bei ihnen ausgelöst hatten. 


                

                    

                        
Stimulus

                        
Sentiment

                    

                    

                        
Als Hagrid Harrys Gesicht sah, strahlte er, ohne die verdutzten Blicke der vorübergehenden Muggel zu bemerken. "Harry!" dröhnte er, und kaum war Harry aus dem Wagen gestiegen, schloss Hagrid ihn auch schon in eine knochenbrechende Umarmung.

                        
Fröhlich

                    

                    

                        
Das Wetter draußen vor den Zugfenstern war so durchwachsen, wie es den ganzen Sommer über gewesen war; sie fuhren streckenweise durch kalten Nebel, dann wieder in schwaches klares Sonnenlicht.

                        
Neutral

                    

                    

                        
Mindestens hundert Dementoren, die vermummten Gesichter ihm zugewandt, standen dort unter ihm. Es war, als würde eiskaltes Wasser in seiner Brust aufsteigen und ihm die Eingeweide abtöten. Und dann hörte er es wieder... Jemand schrie, schrie im Innern seines Kopfes... eine Frau.

                        
Furchteinflößend

                    

                
                
Tabelle 1.


            
Die vielseitige Auswahl der Stimuli in der Studie von Hsu deckte unterschiedliche Aspekte auf, die mit Wirkung von Literatur verbunden sind. Einer davon war für unsere Analysen besonders anregend: es wurde ein Zusammenhang zwischen Immersion und emotionalen Inhalten, "especially negative, arousing and suspenseful ones" (Hsu, Conrad, Jacobs 2014; 1359) festgestellt. Daher interessierten wir uns zunächst dafür, ob die gemessenen Indikatoren für Erregung, Pupillengröße und Hautleitwert (Bradley et al. 2008) vergleichbare Ergebnisse wie andere Studien zur Fiction-Feeling-Hypothese aufzeigen, z. B. dass als "furchteinflößend" markierte Passagen stärkere Reaktionen hervorrufen als diejenigen, die das Label "fröhlich" oder "neutral" tragen (Hsu, Conrad, Jacobs 2014, Eekhof et al. 2021). Diese Reaktion würde in der Klassifikation der Leseemotionen von Miall und Kuiken (2002) in den Bereich der “narrative feelings” fallen, also Gefühlen, die gegenüber literarischen Figuren entwickelt werden bzw. auf eine Resonanz mit der Stimmung und dem Schauplatz eines literarischen Textes hindeuten. Von dieser Art der Emotionen erwartet man, dass sie den Emotionsgehalt des Textes “spiegeln” (Miall, Kuiken, 2002; 224). Dies ist das erste Experiment in einer Reihe von geplanten Studien am LitLab der TU Darmstadt, die als Ziel die Erforschung des Zusammenhangs zwischen Textsentiment und empirischen Untersuchungen von Leseprozessen haben. 

            
Insgesamt haben im Rahmen des aktuellen Experiments 40 deutsche Muttersprachler:innen 150 Textpassagen gelesen (120 Originale, 15 Fanfictions und 15 Badfictions). Anschließend füllten die Teilnehmer:innen einen Fragebogen aus. Entgegen der Vorläuferstudie, wurden keine Fragen zur Immersion gestellt: Die Stimuli waren recht kurz (40-50 Wörter) und wurden in einer zufälligen Reihenfolge präsentiert, was die Immersion behindern würde. Wir erwarteten, dass andere Faktoren das Leseverhalten beeinflussen würden und befragten die 40 Teilnehmer auf drei verschiedene Arten zu ihren Lesegewohnheiten. Da bereits in Experimenten zur Verbindung zwischen Lesen und Theory of Mind die Unterteilung in Gruppen nach Leseerfahrung signifikante Unterschieden aufgedeckt hat (Kidd und Castano 2013, Panero et al., 2016), wollten wir den Einfluss auch bei der emotionalen Reaktion überprüfen.

            
Zunächst wurden Lesegewohnheiten der Teilnehmer:innen mit Hilfe des Reading Habits Questionnaire (Kuijpers et al., 2020) erfasst. Der Fragebogen nimmt die selbst angegebene Lesemenge im Laufe des letzten Jahres auf. Obwohl der Fragebogen vielseitig in seinen Auswertungsmöglichkeiten ist, kann er als Selbstauskunft nur bedingt als zuverlässig eingestuft werden. Intensive Lesephasen und Mehrfachnennungen sind hierbei nur schwer erfassbar. Für die Auswertung wurden Angaben über unterschiedliche Genres summiert und drei fast gleich große Gruppen gebildet: die Teilnehmenden wurden in Vielleser:innen (13 Teilnehmer, Summe der Punktzahlen: 24-80), Durchschnittsleser:innen (13 Teilnehmer, Summe der Punktzahlen: 13-22) und Selten-Leser:innen (14 Teilnehmer, Summe der Punktzahlen: 4-11) eingeteilt.

            
Außerdem haben Proband:innen die deutsche Version des Tests zur Autorenerkennung ausgefüllt (Grolig et al. 2020), ein bewährte Methode um die Kenntnis des Literatursystems oder die langfristige Auseinandersetzung mit Literatur zu erfassen (Panero et al. 2016; Stanovich et al. 1989). Auch hier teilten wir die Proband:innen in drei Gruppen auf: Literaturkenner:innen (14, erkannten 13-38 Autoren richtig), Literatureinsteiger:innen (12, erkannten 1-6 Autoren richtig) und Mittelfeld (14, erkannten 7-12 Autoren richtig). Wie erwartet, gab es einige Überschneidungen zwischen den Gruppierungen, doch die Rangkorrelation zwischen den beiden Angaben war schwach (tau = 0,21).

            
Der dritte Bereich, den wir in Hsus Originalexperiment als nicht ausreichend untersucht betrachteten, war die Einbeziehung des Fandom-Wissens: es wird lediglich erwähnt, dass alle Proband:innen mindestens ein Buch aus der „Harry Potter“-Reihe gelesen haben. Der Fragebogen der aktuellen Studie enthielt zwei Fragen zum Wissen über das Harry-Potter-Fandom, differenziert nach Filmen und Büchern. Wir erwarteten, dass Fans stärker auf die präsentierten Stimuli reagieren würden. Ein Wert von 0 stand für Teilnehmer, die keines der Bücher gelesen und keinen der Filme gesehen haben, während 5 bedeuten würde, dass alle Filme, alle Bücher und zusätzliches Material gelesen wurden. Insgesamt wurden Teilnehmer mit einer Punktzahl von 4 und 5 der Gruppe "Fans" (17) zugeordnet, Teilnehmer mit einer Punktzahl von 0 und 1 galten als "Nicht-Fans" (10) und Teilnehmer mit einer Punktzahl von 2 und 3 wurden dem "Fandom-Mittelfeld" (13) zugerechnet. Die Rangkorrelation zwischen dem Context Score und den beiden anderen Gruppeneinteilungen ist ebenfalls schwach (tau = 0,26 mit den Autorenerkennungsergebnissen; tau = 0,35 mit der selbstberichteten Lesehäufigkeit).

            

            
Datenanalyse

            
Die Hautleitwertdaten wurden mit Brainvision Recorder aufgenommen und mit Hilfe von Ledalab (Benedek 2010) analysiert und exportiert. Ledalab ist eine Software, die die Segmentierung von Hautleitwertdaten sowie eine automatische Ermittlung von Hautleitwertreaktionen durchführt. Dafür werden zwei unterschiedlichen Methoden verwendet: die TTP-Analyse (trough-to-peak), die auf vorgegebenen zeitlichen Kriterien basiert (Bouscein et al. 2012), und die CDA (continuous decomposition analysis), die das Signal zunächst in seine kontinuierlichen (tonischen) und stimulusbezogenen (phasischen) Komponenten unterteilt und dann mit Hilfe eines 
                
Algorithmus
 die Reaktionen identifiziert (für eine genaue Beschreibung und Auswertung der Methoden s. Kuhn et al. 2022). Anschließend wird eine Reihe von Statistiken ausgegeben: die Summe der Amplituden der signifikanten Hautleitwertreaktionen, der durchschnittliche stimulusbezogene Hautleitwert und der durchschnittliche Hautleitwert inklusive der tonischen Komponente. 
            

            
Die Pupillometriedaten wurden aus der Eyetracking-Software SR Research Data Viewer exportiert und normalisiert: für jede Testperson wurde eine Baseline der Pupillengröße ermittelt, die auf der durchschnittlichen Pupillengröße basiert, die zwischen den Trials aufgenommen wurde. Um eine mittlere Veränderung der Pupillengröße zu bestimmen, wurde von der mittleren Pupillengröße pro Trial die Baseline subtrahiert.

            
Bei der Datenverarbeitung der Hautleitwertreaktion und der Pupillometrie-Daten wurde besonders darauf geachtet, ob die Daten die Voraussetzungen für einen ANOVA-Test erfüllten: Unabhängigkeit (die durch das Experimentdesign gegeben war), Normalverteilung und Homogenität der Varianz. Es zeigte sich, dass die Pupillengröße und die Daten zur Anzahl der Hautleitwertpeaks normalverteilt waren, die anderen Hautleitwertdaten jedoch nicht.

            
Die normalverteilten Daten wurden mit einem ANOVA-Test untersucht, während für die nicht normal verteilten Werte der Kruskal-Wallis-Test durchgeführt wurde. Nach den Berechnungen mit ANOVA und dem Kruskal-Wallis-Test wurde eine Auswertung der Effekte mit Epsilon-Quadrat durchgeführt, die zeigte, dass die signifikanten Ergebnisse starke Effekte aufweisen und die meisten Ergebnisse, die sich der Signifikanz näherten, mittlere Effekte zeigten.

        

        

            
Ergebnisse

            
Unsere Ergebnissen zufolge gab es keinen signifikanten Einfluss von Textsentiment auf die Anzahl oder Stärke der Reaktionen, weder beim Hautleitwert noch bei der Pupillengröße. Doch wir konnten einen weiteren signifikanten Faktor ausfindig machen, der bei der Erforschung der Leser:innenreaktionen eine Rolle spielt. Sobald Proband:innen in Gruppen nach den Ergebnissen des Author Recognition Tests eingeteilt wurden, konnte man sehen, dass Literaturkenner:innen signifikant stärkere Reaktionen im Bereich des Hautleitwertes gezeigt haben im Vergleich zu Literatureinsteiger:innen und den Mittelfeld-Proband:innen.

            

            
Tabelle 2: p-Werte des Kruskal-Wallis-Tests bei der Unterteilung in Gruppen nach den Lesegewohnheiten.

                

            

            
Tabelle 2 zeigt die p-Werte des Kruskal-Wallis-Tests. Bei den fettgedruckten Werten wurde der Effekt der Gruppenaufteilung als “mittel” eingestuft, während bei unterstrichenen Werten der Effekt als “stark” bewertet wurde. Wir sehen, dass die durch den Autorenerkennungstest gebildeten Gruppen bei den meisten Werten signifikante Unterschiede in ihren Mittelwerten aufweisen.

            

                
Tabelle 3: p-Werte der ANOVA bei der Unterteilung in Gruppen nach den Lesegewohnheiten.

                

            

            
Tabelle 3 zeigt, dass eine ähnliche Tendenz in den normalverteilten Variablen auffindbar ist: während die Gruppen, die auf der Basis des Fandomscores und des Reading Habit Questionnaire gebildet wurden, keine signifikanten Unterschiede aufzeigen, zeigen die Gruppen der Autorerkennungstests mittlere bis starke Effekte. 

            
Meistens manifestieren sich die signifikanten Effekte in Unterschieden zwischen dem Verhalten der Literaturkenner:innen auf der einen und Literatureinsteiger:innen und dem Mittelfeld auf der anderen Seite, wie in Abbildung 1. Es scheint, als würde die Kenntnis des literarischen Feldes Voraussetzung für häufigere Hautleitwertreaktionen sein.

            

                

                
Abbildung 1. Durchschnittliche Anzahl der Hautleitwertreaktionen bei Literaturkenner:innen, Mittelfeld-Proband:innen und Literaturkenner:innen. 

            

            
Wird eine Unterteilung nach der Leistung im Autorenerkennungstest vorgenommen, so zeigt sich, dass Literaturkenner:innen bei den meisten Werten signifikant größere Reaktionen zeigten: Es gibt eine höhere Anzahl von Peaks, die Summe der Amplituden ist höher und die durchschnittliche Pupillengröße ist größer. Bei den Literaturkenner:innen sind die Reaktionen auf fröhliche Stimuli am höchsten und auf neutrale Stimuli fast immer am niedrigsten. Die Literatureinsteiger:innen hingegen zeigen meist minimale Werte bei fröhlichen Stimuli und am häufigsten höchste Werte bei neutralen Stimuli. Keiner dieser Werte wich signifikant vom Mittelwert ab.

            

            
Diskussion

            
Unsere Ergebnisse zeigen, dass die nach unterschiedlichem Sentiment gelabelten Texte keine signifikanten Unterschiede in der Hautleitwertreaktion und in der Pupillengröße aufzeigen. Die Analysen der Lesegewohnheiten der Proband:innen lassen hingegen darauf schließen, dass diejenigen, die mehr lesen, auch stärkere Reaktionen auf Texte insgesamt aufweisen. 

            
Vor allem die Kenntnis des Literatursystems – wie der Autorenerkennungstest oft interpretiert wird – beeinflusst die körperlichen Reaktionen auf das Lesen in erheblichem Maße. Leser:innen, die mehr Erfahrung mit Literatur haben, reagieren stärker auf literarische Werke und spiegeln dabei den Textsentiment wieder, die ein Text enthält (stärkere Reaktionen auf emotionale Inhalte, schwächere auf neutrale Passagen).

            
Leser mit geringerem Wissen über Literatur scheinen auch auf neutrale Stimuli stark zu reagieren, vielleicht weil sie einen emotionalen Stimulus erwarten und diesen nicht erhalten. Diesen Ergebnissen zufolge ist das Wissen über Literatur für eine andere Art der Reaktion auf Texte verantwortlich. 

            
Entgegen unseren Erwartungen zeigte sich kein signifikanter Einfluss von höherer Kenntnis des Werks, obwohl der Gesamtmittelwert des Hautleitwerts bei fröhlichen Stimuli bei Fans höher war. Möglicherweise ist dies auf eine Kombination aus Nostalgie und narrativen Gefühlen zurückzuführen.

            
Schließlich hat die jüngste Leseaktivität, die mit dem RHQ ermittelt wurde, fast keinen Einfluss auf die physiologischen Reaktionen - nur als zusätzlicher Faktor bei der Berücksichtigung der Pupillengröße. Diese korrelativen Zusammenhänge bieten allerdings noch keine Antwort auf die Frage nach der Kausalität – die Frage, ob Lektüre die emotionale Reaktion trainiert oder ob empfindsame Menschen sich mehr zu Literatur hingezogen fühlen, bleibt offen.

            
Die Aussagekraft der Ergebnisse ist durch einige Schwachstellen eingeschränkt: beispielsweise sind die Proband:innen überwiegend Studierende und können daher nur schwer als absolute Wenigleser:innen bezeichnet werden. Vielleicht ist das der Grund, warum die Daten so selten Unterschiede zwischen Literatureinsteiger:innen und Mittelfeld-Proband:innen aufzeigen.

            
Darüber hinaus hat die Anzahl der Proband:innen eine eher geringe statistische Aussagekraft (40 Teilnehmer), wovon allerdings nur die Analyse der Proband:innengruppen betroffen ist: Für die Analyse des Sentimenteinflusses auf die Leser:innenreaktion wird die statistische Signifikanz durch die große Anzahl an Trials derselben Sentimentklasse wieder angehoben. Die Ergebnisse dienen zum Anlass, über mehrere Studien hinweg den Einfluss der Lesekompetenz zu berücksichtigen.

            
Zuletzt wären Vergleichsstudien mit anderen literarischen Gegenständen interessant, um die Zusammenhänge der hier vorgestellten Variablen über „Harry Potter“ hinaus zu beobachten und weitere Aspekte von literarischen Texten wie Stil und Epoche ebenfalls in ihrer Wirkung zu untersuchen. 

        

            

                
Das seit 2020 von der DFG geförderte Projekt “duerer.online - Virtuelles Forschungsnetzwerk Albrecht Dürer”
 baut eine Forschungsumgebung mit vollständigem Werkverzeichnis der Druckgraphik, Gemälde und Zeichnungen des bedeutenden Renaissance-Künstlers sowie dessen Nachlebens auf. In der bis 2023 laufenden ersten Projektphase steht die Erfassung von Werken im Fokus, die im schriftlichen Nachlass explizit genannt sind. Die Bestände der kooperierenden Sammlungen (Kunstsammlungen der Stadt Nürnberg, Albrecht-Dürer-Haus-Stiftung e.V., Nürnberg, Germanischen Nationalmuseum und Albertina, Wien) dienen als Erschließungsgrundlage. Grundsätzlich können und werden bereits Informationen zu Beständen weiterer Sammlungen erfasst. Ziel ist es nach Projektende der Fachcommunity zu ermöglichen, neue Forschungsergebnisse zu den in der Datenbank aufgenommenen Werken zu ergänzen und Werke bzw. Exemplare hinzuzufügen. Nach Projektende soll es der Fachcommunity ermöglicht werden, neue Forschungsergebnisse zu den in der Datenbank aufgenommenen Werken zu ergänzen und Werke bzw. Exemplare hinzuzufügen. Die Fortführung, Pflege und Sichtbarkeit für die Fachcommunity über die Laufzeit des Projektes hinaus wird durch die Integration in das Angebot des Fachinformationsdienstes Kunst, Fotografie, Design - arthistoricum.net an der UB Heidelberg
 gesichert.
            

            
Im Unterschied zu den bereits im Druck vorliegenden Werkverzeichnissen des Künstlers (wie z.B. Schoch/Mende/Scherbaum 2001-2004 für das graphische Werk, Anzelewsky 1991 für Gemälde und Winkler 1936-1939 für Zeichnungen) werden in der Forschungsumgebung Dürers Werke gattungsübergreifend nach einheitlichen Kriterien erschlossen, ebenso Werke der bis in die heutige Zeit andauernden Rezeption. Dabei versteht sich das Projekt nicht als ein weiteres autoritatives Werkverzeichnis, d.h. es werden keine Werke zu- oder abgeschrieben, sondern offen und transparent historische und aktuelle Diskussionen bezüglich der Autorschaft abgebildet und somit eine Grundlage für weitere Forschungen rund um das Werk Dürers und seiner Nachfolge geschaffen.

            

                
Neben der Erschließung der Werke werden ausgewählte Quellen des schriftlichen Nachlasses transkribiert und ediert. Durch die Auszeichnung mittels TEI
 und der Anreicherung der ausgezeichneten Werke, Personen und Orte mit Normdaten der Gemeinsamen Normdatei (GND) ist eine Durchsuch- und Recherchierbarkeit dieses Materials möglich, die für Forschende eine Neuheit darstellt. Über eine programmierte Pipeline werden zudem Registerdateien erzeugt, die zusätzlich mit Informationen zu Kunstwerken und Personen aus der Datenbank angereichert werden. Ebenso wird im Datensatz zum jeweiligen Werk/Person automatisch die Verlinkung auf den entsprechenden Registereintrag abgelegt, sodass sich in „duerer.online“ Nennungen der Entitäten in Quellen nachvollziehen lassen.
            

            

                
Für das Portal wird die “Wissenschaftliche Kommunikationsinfrastruktur (WissKI)”
 eingesetzt, eine virtuelle Forschungsumgebung, die den Aufbau von Anwendungen im Bereich der Digital Humanities unter Nutzung von semantic web-Standards ermöglicht. Die Einordnung und Speicherung der erhobenen Daten erfolgt ontologiebasiert mittels einer auf CIDOC CRM
 (ISO-Standard 21127) basierenden Anwendungsontologie und unter Nutzung der Gemeinsamen Normdatei (GND), des Getty AAT
 und Iconclass
. Das Projekt nutzt die “Heidelberger Anwendungsontologie für Werkverzeichnisse” (Sobriel 2022) nach, die um Klassen und Eigenschaften erweitert wurde, die zur Dokumentation eines Kunstwerkes und dessen Werkbiographie benötigt werden. So wurden vor allem Properties aufgenommen, die Beziehungen zwischen Werken beschreiben. Die Relationen (z.B. has copy/ is copy after) wurden nach dem Lightweight Information Describing Objects (LIDO) Schema modelliert (Sobriel 2022, 43).
            

            

                
Das mit der Anwendungsontologie umgesetzte Datenmodell berücksichtigt die Erfassung von Unikaten und Werken in mehreren Ausführungen. Das Werkkonzept beschreibt die inhaltliche Entstehung und nimmt alle Informationen auf, die jede Ausführung betreffen und konzeptioneller Natur sind. In der Ebene Ausführung/Exemplar werden spezifische Angaben zu einer Ausführung bzw. einem Exemplar dokumentiert. Das Datenmodell kann somit semantisch korrekt die Rezeption von Originalen darstellen, da diese immer vom Inhalt/Konzept ausgeht.
                
 Außerdem müssen keine Informationen mehrfach erfasst werden und zukünftig können graphische Sammlungen Inhalte zu ihrer speziellen Ausführung dem entsprechenden Werk-Datensatz hinzufügen. Das beschriebene Datenmodell und die darauf beruhende konsequente Erschließung von Werk- und Ausführungs-/Exemplarebene ist in ihrer Durchsuchbarkeit in sammlungsübergreifenden Graphik-Portalen derzeit singulär.
            

            

                
Neben Beziehungen von Werken untereinander werden u.a. Informationen zu Verkaufsereignissen und Ausstellungen erfasst. Durch Verknüpfung mit den jeweils verkauften oder ausgestellten Werken bzw. Ausführungen können historische nicht mehr existierende Sammlungen bzw. die Ausstellungshistorie eines Exemplars/Ausführung sichtbar gemacht werden. Zudem wird, wenn möglich, seitengenau auf die zur Verfügung stehenden Digitalisate von Auktions- und Ausstellungskatalogen verlinkt, sodass der Forschende schnell an die jeweiligen Nachweise gelangt.
            

            
Die Projekte zum Werk von Lucas Cranach d. Ä.
 und Rembrandt van Rijn
 sind dem beschriebenen Portal vergleichbar in ihrer Ausrichtung, doch für das Werk Albrecht Dürers besteht bisher kein Angebot, das gattungs- und sammlungsübergreifend Informationen zur Verfügung stellt und dabei auf anschlussfähige offene Formate gemäß der FAIR-Prinzipien unter Verwendung von Semantic-Web-Standards setzt. Damit steht der Dürer-Forschung ein Instrument zur Verfügung, das nicht nur zukünftige Forschungsergebnisse aufnehmen und sichtbar machen kann, sondern auch alle Informationen über Schnittstellen (SPARQL Endpoint) verfügbar macht, sodass es Forschenden möglich ist, mit eigenen Anwendungen Forschungsfragen weiter zu verfolgen.
            

        

            

                
Einleitung 

                
Eine zentrale Anforderung an die „Offenheit“ in der Wissenschaft, für die Geisteswissenschaften unter dem Label „Open Humanities“ eingeführt, ist die Publikation in offenen Formaten (Kleineberg, Kaden 2017; CfP Dhd 2023
). Bisher orientieren sich geisteswissenschaftliche digitale Publikationen meist an den etablierten Printformaten Monographie, Zeitschriftenartikel und Sammelwerk
; mit 
                    
Open Monograph Press
 und 
                    
Open Journal System
 liegen erprobte Open-Access-Publikationsinfrastrukturen für Buchpublikationen und Zeitschriften vor. „Offenheit“ erfordert jedoch mehr als die freie Zugänglichkeit, sondern betrifft weiterhin die Offenheit der der Publikation zugrundeliegenden Daten (Open Data), des Quellcodes der Publikationsinfrastruktur (Open Source) und eine strukturelle Offenheit der Veröffentlichungen (u.a. Kleineberg, Kaden 2017; CfP Dhd 2023). Angesichts einer sich verändernden Publikationskultur in den Geisteswissenschaften ist es daher nicht mit der Schaffung digitaler Äquivalente zu Printformaten getan, vielmehr sollten Online-Publikationslösungen der Diskussion um alternative Veröffentlichungsformen Rechnung tragen: Zu nennen sind hier kollaborative Autorschaft (Kaden 2016, 4) und diskursive Schreibprozesse mit einer Bewegung „von der Ergebnis- zur Prozesspublikation“ (AG Digitales Publizieren 2021, 13), kleinformatige Veröffentlichungsformen und eine „strukturelle Ausdifferenzierung des Publikationsobjektes“ (Kaden 2016, 19), die semantisch annotiert sind. Entsprechend fordert der Wissenschaftsrat (2022, 54) dass „Voraussetzungen dafür zu schaffen [seien], dass auch neue, innovative Publikationsorte entstehen“ können.
                

                
Mit dem 
                    
Open Encyclopedia System
 (OES)
 existiert eine webbasierte Plattform zur Erstellung, Publikation und Pflege von lemmabasierten Online-Publikationen. Darunter verstehen wir eine Sammlung von wissenschaftlichen Beiträgen zu einem bestimmten Themen- und/oder Methodenbereich, die von unterschiedlichen Autor:innen namentlich gekennzeichnet erstellt werden und deren Publikation durch ein Redaktionsteam betreut wird. OES unterstützt für diese Publikationsform die inkrementelle Erstellung und Verwaltung von Inhalten und Metadaten bis hin zur Einbettung von multimedialen Inhalten, Normdaten und bibliographischen Informationen, die Registererstellung, die Veröffentlichung mehrsprachiger zitierfähiger Artikel sowie vielfältige Formen der Navigation und Suche in und Interaktion mit den publizierten Inhalten, möglichst unter Berücksichtigung der o.g. Facetten von „Offenheit“. OES stellt so zum einen eine Open-Source-Anwendung für eine weitere zentrale Publikationsform der Geisteswissenschaften dar, und bietet zum anderen Lösungsansätze für offene und innovative Formate. Damit grenzt sich OES von bestehenden Publikationsinfrastrukturen für wissenschaftliche Online-Referenzwerke ab, die entweder proprietäre Lösungen oder an Verlagen angesiedelt sind, nicht primär für den wissenschaftlichen Gebrauch (Wikimedia) gedacht oder noch in Entwicklung (OAPEnz) sind.

                

                
OES wurde im Rahmen einer DFG-Förderung (LIS)
 am Center für Digitale Systeme der Freien Universität Berlin als Open Source Software entwickelt.
 Seit 2019 werden mit fachwissenschaftlichen Partner:innen Online-Publikationen mit OES realisiert:

                    
wissenschaftliche Online-Enzyklopädien, Online-Compendien und -Lexika, sowie aktuell „Living Handbooks“ zur (kollaborativen) Verschriftlichung, Diskussion und Präsentation der Leitbegriffe interdisziplinärer Forschungsgruppen
                    
.
 OES wird kontinuierlich weiterentwickelt; Betrieb und Pflege der Software werden durch die Freie Universität Berlin gesichert.
                

            

            

                
OES-Systemarchitektur und -Komponenten

                
Die OES-Software basiert auf dem Open-Source Content-Management-System WordPress
mit übernimmt dessen Systemarchitektur mit der Unterscheidung in Publikations- und Redaktionsumgebung, und ist als WordPress-Plugins mit zugehörigem Themes umgesetzt (vgl. Abb. 1). OES erweitert WordPress um vielfältige Funktionen zur Erstellung, Publikation und Pflege lemmabasierter Publikationen. 
                

                

                    

                    
Abb.1: Aufbau der OES-Software 

                

                
Über die interaktive 
                    
Präsentationsschicht
 („frontend“) ist das Sammelwerk online zugänglich. Für die Präsentation der digitalen Inhalte entsprechend der definierten Datenstrukturen wurde ein WordPress-Theme entwickelt.
 Das OES-Theme erfüllt die gängigen Webstandards und ermöglicht eine barrierefreie und responsive Darstellung. Im 
                    
Redaktionssystem (
„editorial layer“) werden die redaktionellen Prozesse der Inhaltserstellung und Datenpflege sowie die Administration der Präsentationsschicht standordübergreifend und kollaborativ organisiert. Der OES Core-Plugin („OES Core“) enthält die OES-spezifischen WordPress-Erweiterungen und kann mit weiteren WordPress-Plugins kombiniert werden. OES nutzt für das Arbeiten mit strukturierten Daten das Open-Source Plugin Advanced Custom Fields (ACF).
 Die erstellten Inhalte werden in einer relationalen mySQL-
                    
Datenbank
 („database“) verwaltet und können über eine csv-Schnittstelle strukturiert importiert und exportiert werden. WordPress selbst ermöglicht den Export der Datenbank als SQL-Datensatz und über eine REST API in XML/RDF-Formate. Weitere Exportformate können projektabhängig festgelegt und implementiert werden.
                

                
OES unterscheidet standardisierte Datenstrukturen und Funktionalitäten, die im OES-Core und OES-Theme zusammengefasst werden, von projektspezifischen Erweiterungen, die die zusätzlichen Bedarfe einzelner OES-Anwendungen abdecken. 
                    
Jede OES-Instanz implementiert über einen OES Projekt-Plugin („
OES Project“
                    
) ein projektabhängiges Datenmodell, über welches projektspezifische Datenstrukturen, Präsentationen und Interaktionen der digitalen Inhalte definiert werden.
 Über Konfigurationsoptionen in der Redaktionsumgebung („OES-Settings“) lassen sich zahlreiche Aspekte der Präsentation administrieren; für weitere projektspezifische Anpassungen kann in Ergänzung zum OES-Theme ein Child-Theme („OES Project Theme“) erstellt werden. 
                

                

                    

                    
Abb. 2: Exemplarisches Datenmodell für OES

                

                

                    
A
bb. 2 illustriert anhand der für lemmabasierte Publikationen charakteristischen Inhaltstypen die Struktur eines OES-Datenmodells mit Datenobjekten wie Lemma, Verfasser:in und bibliographischer Eintrag, die mit Attributen angereichert werden können, und die über Relationen in Beziehung zu anderen Datenobjekten stehen. Schnittstellen zu externen Datenbeständen wie Normdatenbanken und Zotero
 erlauben eine dynamische Verknüpfung mit diesen strukturierten Informationssammlungen und die Übernahme von Daten. Attribute und Relationen werden bei der Darstellung in der Publikationsumgebung ausgewertet und unterstützen eine dynamische Anzeige der Daten. 
                

            

            

                
Mit OES im Open Access publizieren 

                
OES unterstützt die Open-Access-Publikation von lemmabasierten Werken und reagiert dabei auf die Anforderungen, die an zeitgemäße digitale Publikationen formuliert werden:
 Über die Redaktionsumgebung kann jedes Lemma separat lizensiert werden; OES unterstützt dabei die Auswahl unterschiedlicher CC-Lizenzen. Ebenso können für multimediale Elemente die spezifischen Lizenzierungen und Rechte einzeln ausgewiesen werden. Referenzierbarkeit und Zitierfähigkeit sind durch Angabe einer DOI gegeben; dieses gilt jeweils für die Fassung (Version oder Sprachfassung) eines Artikels, sodass die Persistenz der Referenz garantiert ist. Autor:innen und andere Beitragende können einzeln ausgewiesen und mit ORCID/GND-IDs aufgeführt (vgl. AG Digitales Publizieren 2021, 41) und „verschiedene Autorschafts- und Beiträger*innenrollen“ (ebd., 18) sichtbar gemacht werden. Eine standardisierte Zitierweise als obligatorische Angabe wird durch eine automatische Generierung nach einem durch die Redaktion definierten Muster
 garantiert (vgl. Abb. 4). Neben offener Zugänglichkeit und stabilem Nachweis ist die menschliche und maschinelle Weiternutzung der Inhalte eine zentrale Anforderung an offene Publikationen Die Bereitstellung der XML/TEI-Kodierung der generischen Textstruktur und der typischen Entitäten im Frontend ist in Vorbereitung. Ein PDF-Download kann über das Frontend bereitgestellt werden (vgl. Abb. 3).
                

                

                    

                    
Abb. 3: Ansicht eines Artikelheaders am Beispiel des Compendium Heroicum des SFB 948

                    

                

            

            

                
Offene Inhalte und Daten erstellen und pflegen

                
Die OES-Redaktionsumgebung umfasst Funktionen zur Inhaltserstellung und Datenpflege, die kollaborativ von einem Redaktionsteam wahrgenommen werden. Zentrales Datenobjekt ist das Lemma („article“, Abb. 2), welches mit Inhaltsverzeichnis, Verfasser:in, Querverweisen und externen Verlinkungen, Einzelnachweisen in Form von Endnoten, bibliographischen Angaben, Zitiervorschlag und inhaltlichen und strukturellen Metadaten erstellt und publiziert werden kann (vgl. Abb. 3 und 4). OES unterstützt den Schreibprozess durch den um OES-spezifische Funktionalitäten erweiterten Gutenberg-Editor
 und durch an die OES-Inhaltstypen angepassten WordPress-Formulare für die Erstellung und Pflege der Vokabulare. Neben Textblöcken können multimediale Elemente einfach eingebunden und Querverweise innerhalb des Sammelwerks und zu externen Datenbeständen integriert werden. Für bibliographische Daten ist ein Import aus dem Literaturverwaltungsprogramm Zotero oder eine dynamische Anbindung über das Zotpress-Plugin
 möglich. Die Qualitätssicherung der Inhalte erfolgt in den aktuellen Anwendungen durch die Redaktion; die Review-Prozesse werden i.d.R. außerhalb von OES organisiert.

                

                
OES geht über das an Printformaten orientierte Konstrukt einer „Ergebnispublikation“ hinaus und ermöglich die Umsetzung dynamischer und kollaborativer Inhaltserstellung im Sinne einer Prozesspublikation (u.a. AG Digitales Publizieren 2021): Lemmata sind in OES veränderbar und können nach Erstveröffentlichung weitergeschrieben werden. Sie existieren in verschiedenen Text- und Sprachfassungen, den sog. Versionen, die über ein „Stammlemma“ miteinander in Beziehung stehen, und die Inhalte und strukturierte Daten teilen können. Dieser Abkehr einer reinen Ergebnispublikation geht mit der Herausforderung einher, Zitierfähigkeit und Referenzierbarkeit zu erhalten. OES sieht deshalb vor, dass alle Versionen mit einer eigenen DOI versehen werden können. Die aktuellste Version kann wie in Abb. 3 als Volltext, weitere als Liste angezeigt werden.

                
Inkrementelles und dynamisches Publizieren kann noch weitergedacht werden als eine „strukturelle Ausdifferenzierung des Publikationsobjektes in unterschiedlich verarbeitbare und aktualisierbare Teile“ (Kaden 2016, 19). OES bietet erste Lösungen, indem es den modularen Aufbau von Lemmata ermöglicht,
 und so eine dynamische Bündelung und Ausgabe von Inhalten im Frontend unterstützt. Im Rahmen verschiedener Anwendungen
 werden aktuell strukturierte Formate und modulare Ansätze ausgearbeitet, um das „Dokument selbst als Einheit zu öffnen und zu verflüssigen“ (Kaden 2016, 19). Zu betrachten ist hierbei die Ausweitung des Autor:innenbegriffs, da diese Form der Publikation i.d.R. mit geteilter Autorschaft einhergeht, die neue Zitationspraktiken erfordern.
                

                

                    

                    
Anzeige von Zitierweise und Metadaten am Beispiel des Compendium Heroicum des SFB 948

                    

                

                
Auf der Präsentationsebene werden den Nutzenden vielfältige Optionen zur Erschließung der erstellten Inhalte angeboten: Die Inhalte können neben der für Referenzwerke typischen alphabetischen Listenansicht über thematische Rubriken, Register sowie über eine Volltextsuche mit Filteroptionen (facettierte Suche) aufgerufen werden. Für einzelne OES-Anwendungen wurden Visualisierungen wie Karten und Zeitstrahl zur Exploration der Inhalte entwickelt.
 Die Unterschiede zwischen den Anwendungen manifestieren sich aufgrund spezifischer Anforderungen der Zielgruppe und der Daten vor allem im Frontend, und erfordern projektspezifische Anpassungen (über OES-Konfigurationsoptionen) bzw. Erweiterungen (projektspezifische OES-Plugins und Themes).
                

            

            

                
Datenbestände verknüpfen und verfügbar machen 

                
Wie oben beschrieben werden die in OES gemäß dem Datenmodell erstellten Forschungsdaten über normierte Schnittstellen bereitgestellt. Gleichzeitig greift OES aber auch zum Zwecke der strukturierten Beschreibung, der eindeutigen Referenzierung, der Vernetzung der Inhalte sowie einer verbesserten Auffindbarkeit (u.a. Wissenschaftsrat 2022, 9; AG Digitales Publizieren 2021, 14) der OES-Datenbestände auf Normdateien und Verbundkataloge zu. OES unterstützt die Suche in und Datenübernahme aus ausgewählten Normdateien: Über die von lobid bereitgestellte API
 werden GND-Daten abgefragt, ortsgebundene Normdaten werden direkt vom Ortsnamensdienst Geonames
 abgerufen und die Subject Headings von der Library of Congress
. Eine Anbindung von Wikidata und der Schlagwortnormdatei Rameau befindet sich in Umsetzung. Eine in die Redaktionsumgebung integrierte Auswahlmaske ermöglicht die Auswahl von GND- bzw. LCSH-Schlagworten und deren Zuweisung zu Lemmata sowie die Generierung von „shortlinks“ zu GND- und Geonames-Kennungen, mit denen Named Entities in den Volltexten verknüpft werden können. Diese Verweise ermöglichen im Frontend die dynamische Anzeige von ausgewählten Informationen aus dem verknüpften Normdatensatz. Weiterhin können über die LOD-Auswahlmaske für strukturierte Datenobjekte semi-automatisch Angaben aus den Normdatenbanken in den OES-Datenbestand übernommen werden. Weitere Verknüpfungsmöglichkeiten ergeben sich durch die bereits erwähnte persistente Identifizierung der Autor:innen mit ORCID und GND-ID sowie die dynamische Verknüpfung mit bibliographischen Einträgen in Zotero-Bibliotheken.
                

            

            

                
Diskussion und Ausblick 

                
Das 
                    
Open Encyclopedia System
 fügt den Angeboten für wissenschaftliches digitales Publizieren eine weitere Open-Access-Lösung hinzu. Dabei bildet OES nicht primär bestehende Printformate nach, sondern strebt innovative und offene Publikationsformen an. Während hinsichtlich Open-Access-Veröffentlichungen und offene Daten tragfähige Lösungen entstanden sind, sind im Kontext offener Formate, sowohl inhaltlich als auch strukturell, zwar erste Lösungsansätze benannt, zu deren weiterer Ausgestaltung bedarf es jedoch noch konzeptioneller Überlegungen: Wie definiert sich Autorschaft bei modularen Publikationen? Wie kann in modularen und „fluiden“ Formaten punktgenau zitiert werden? Bis auf welche Strukturebene soll verschlagwortet werden? Welche Rollen und Beitragsmodi müssen ausgewiesen werden? Über welche Verfahren kann die Qualitätssicherung erfolgen? Diese Diskussion wird im OES-Kontext verstärkt im Rahmen der in interdisziplinären Forschungsverbünden verorteten OES-Anwendungen wie dem LHTC und dem Organon-Lexikon fortgeführt, die mit „lebhaften“ und modularen Publikationsformen experimentieren. Durch solche Diskussionen und die praktische Ausgestaltung weiterer OES-Anwendungen wird die Entwicklung des OES-Frameworks bzw. -Plugins auch in generischer Sicht vorangetrieben.
                

            

        

            
Strukturierte Daten, insbesondere Wissensgraphen, gewinnen in vielen Forschungsfeldern zunehmend an Bedeutung. Sie bieten eine kondensierte Sicht auf menschliches Wissen, sei es allgemein oder domänenspezifisch. Unter anderem können sie damit in Forschungsprojekten helfen, Textkorpora mit zusätzlichen Informationen anzureichern oder das Beantworten spezifischer Fragestellungen durch Inferenzbildungen erst möglich zu machen. Für den Erstellungsprozess eines Wissensgraphen gibt es allerdings keinen universal gültigen Lösungsweg (Kejriwal 2019, 4).

            
In diesem Beitrag geht es speziell um die Herausforderungen, einen Wissensgraphen aus historischen Enzyklopädien zu erstellen. Als Beispiel dafür dient das DFG-geförderte Projekt EncycNet.
 Es soll hierbei nicht die Präsentation von Methoden und deren Ergebnisse im Vordergrund stehen, sondern es sollen eher die Besonderheiten des Vorhabens und die damit einhergehenden konzeptuellen Überlegungen, die letztendlich auch bei der Auswahl der Methoden eine Rolle spielen, genauer darstellt werden. Im Folgenden soll deshalb zunächst ein kurzer Überblick über Wissensgraphen und deren Erstellung gegeben werden. Den Hauptteil leitet dann eine kurze Einführung zu EncycNet ein und schließlich werden die drei Herausforderungen des Projekts diskutiert.
            

            

                
Forschungskontext

                
Um zu verstehen, was die Umwandlung von verschiedenen Quellen in einen Wissensgraph bedeutet, sollte zunächst der Begriff „Wissensgraph” geklärt werden. Es gibt zwei verschiedene Definitionen des Begriffs; eine traditionelle und eine moderne Verwendung. Traditionell wird davon ausgegangen, dass ein Wissensgraph nur Weltwissen enthält – ganz konkret nur Named Entities. Dabei wird auch erwartet, dass es eine schwergewichtige Ontologie zu dem Wissensgraph gibt. Geprägt ist diese traditionelle Sichtweise durch das erste Aufkommen des Begriffs durch den Google Knowledge Graph, also Wikidata, welcher genau diese Eigenschaften besitzt. Gemäß dieser Definition sind also WordNet oder GermaNet auch keine Wissensgraphen, sondern semantische Netzwerke. Die moderne Definition ist dem gegenüber etwas weniger streng; hier können Wissensgraphen jegliche Art von Wissen abbilden und die Ontologie darf ebenso auch leichtgewichtig sein.

                
Die Erstellung von Wissensgraphen lässt sich in drei Schritten zusammenfassen: 1) der Aufbau einer Ontologie, 2) die Erkennung von Entitäten und deren Alignierung und 3) die Relationsextraktion. Zusammengefasst aus der neuesten Forschung in dem Feld (aus Chaves-Fraga et al. 2021 u. 2022) lässt sich sagen, dass hier hauptsächlich das Mapping von Datenstrukturen im Vordergrund steht bzw. das Vereinheitlichen verschieden strukturierter Daten, um einen Graphen zu erstellen. Das bedeutet: Für alle drei Schritte kann auf bereits strukturierte Information zurückgegriffen werden und die Re-Organisation ist Kern der Aufgabe (Schröder et al. 2021, Wu et al. 2020). Daneben stehen häufig auch domänenspezifische Anforderungen an den Graphen im Vordergrund, so wie zum Beispiel bei der Erstellung des Open Drug Knowledge Graphs (Mann et al. 2021).

                
Ein Teilbereich der Forschung wiederum beschäftigt sich auch mit dem Erstellen von Graphen aus Fließtext, wobei hier die traditionelle Sichtweise auf Wissensgraphen, also die Repräsentation von realen Entitäten, dominiert. Typische Methoden aus dem Bereich sind daher z.B. Named Entity Recognition und Entity Linking (Kejriwal 2019, 12, 33). Dies gilt auch für das Feld der Digital Humanities, eben besonders für das Beschreiben kultureller Objekte, so wie zum Beispiel dem Modellieren von Erzählorten in Romanen (Hinzmann et al. 2022) oder dem Modellieren von Künstlern und Werken aus kunsthistorischen Texten (Jain et al. 2022). Daneben können auch syntaktische Marker statt Named Entities die Graphmodellierung stützen (Perak 2020). Häufig wird auch auf strukturierte Daten, z.B. Wissensgraphen aus derselben Domäne, zurückgegriffen, um eine Basis für die Alignierung der Konzepte, Relationsauswahl und Ontologie zu schaffen (Jain et al. 2022, Clancy et al. 2019).

            

            

                
EncycNet: Herausforderungen und Chancen

                
Das Ziel von EncycNet ist es, einen Wissensgraphen aus sechs historischen Enzyklopädien (genauer: Konversationslexika, siehe Tabelle 1) zu erstellen.
 Der Graph soll ein lexikalisch-semantisches Netzwerk sein, welcher einerseits die Einträge über alle Enzyklopädien hinweg aligniert und disambiguiert. Zum anderen müssen die relevanten Relationen aus den Einträgen enthalten sein. So sollen beispielsweise Themenbereiche zu Stichwörtern verzeichnet sein („Operation“ / „Mathematik“ und „Operation“ / „Medizin“), aber auch typische Informationen über Named Entities wie Geburtsdatum und Geburtsort von bekannten Personen. Der Graph soll zunächst eine Vogelperspektive auf die Daten bieten; insbesondere sollen so Bedeutungsverschiebungen eines Konzepts im Verlauf der Zeit (z.B. durch Veränderung der nächsten Nachbarn) sichtbar gemacht werden. Ebenfalls kann mithilfe von Inferenzen der Graph genauer analysiert werden: Etwa die Verwandtschaftsgrade von Konzepten über Pfade, Zentralität von Konzepten für eine bestimmte Zeit oder die Bildung von Communities. Darüber hinaus können die Graphdaten dazu dienen, Textkorpora anzureichern oder historische Evaluationsdaten bereitzustellen. 

                        

                            

                                
Lexikon

                                
Anzahl Einträge

                                
Anzahl Tokens

                            

                            

                                
Brockhaus Conversations-Lexikon oder kurzgefaßtes Handwörterbuch (1809)

                                
6.960

                                
1.186.000

                            

                            

                                
Brockhaus Bilder-Conversations-Lexikon (1837)

                                
7.049

                                
2.604.000

                            

                            

                                
Brockhaus Kleines Konversations-Lexikon (1911)

                                
82.780

                                
2.434.000

                            

                            

                                
Damen Conversations Lexikon (1834)

                                
7.099

                                
1.461.000

                            

                            

                                
Herders Conversations-Lexikon (1854)

                                
39.755

                                
2.256.000

                            

                            

                                
Meyers Großes Konversations-Lexikon (1905)

                                
156.264

                                
17.437.000

                            

                            
Tabelle 1: Auflistung der Konversationslexika für die Grundlage des Wissensgraphen EncycNet

                        


Für die Erstellung des Graphen bedeutet der dargestellte Forschungskontext, dass ein solches Vorhaben verschiedene Forschungsnischen in sich vereint. Zum einen bedient sich EncycNet der modernen Definition für Wissensgraphen. Da in den historischen Enzyklopädien nicht nur Personen und Orte zu finden sind, sondern auch Objekte und Phänomene aller Art, muss der resultierende Wissensgraph diese Dinge auch abbilden können. Zum anderen liegen die Enzyklopädien nur semi-strukturiert (TEI-XML) vor.
 – zum Großteil handelt es sich deshalb um eine Grapherstellung aus Fließtext. Und letztlich, durch die Diversität, die mit einem historischen Korpus einhergeht, muss die Erstellung zusätzlich auf heterogene Fließtextdaten abgestimmt sein. Auf diese drei Herausforderungen, die das Enzyklopädienkorpus mit sich bringt, soll im Folgenden genauer eingegangen werden.
                

                

                    
Domänenübergreifendes Wissen

                    
In den Enzyklopädien wird jegliche Art von Wissen über unterschiedlichste Wissensdomänen äußerst detailreich abgebildet. Insbesondere dann, wenn Einträge deutlich länger als nur eine Definition sind, findet sich dort einiges an Wissen, welches sich idealerweise auch in dem resultierenden Wissensgraphen widerspiegeln sollte. Allerdings sind solche längeren Einträge auch weniger standardisiert als die kürzeren, einfachen Begriffsdefinitionen in den Enzyklopädien. Trotzdem gibt es auch in längeren Einträgen bereits einige vorstrukturierte Elemente, gekennzeichnet durch die XML Annotation, die sich auf den ersten Blick zum extrahieren anbieten. Dazu gehören beispielsweise Hierarchien oder Aufzählungen, Vers, oder auch semi-strukturierte Formen im Fließtext wie z.B. Gleichungen, Angaben von Einheiten oder Ähnliches (einige Beispiele in Abbildung 1). Allerdings sind diese Elemente dann fast immer domänenspezifisch und sind damit nur in den wenigsten Einträgen zu finden.

                    

                        

                            

                            
Abbildung 1: Beispiele für strukturierte bis semi-strukturierte Inhalte in den Einträgen der Enzyklopädien.

                        

                    

                    
Um ein möglichst generisches Bild von dem Inhalt der Enzyklopädien zu erhalten, können die Einträge zunächst in generische Klassen unterteilt werden; für EncycNet ergaben sich die Klassen Personen, Orte, Objekte und Abstrakta. Diese Klassen wurden nach dem Vorbild von Wikidata (Personen und Orte als 2 Hauptkategorien) und WordNet („physical entity“ und „abstract entity“ als direkte Hyponyme des Wurzelelements „entity“). Mehrere Artikel aus diesen Klassen können dann gesichtet werden und in Abstimmung mit Wikidata oder WordNet die wichtigsten Informationen oder thematische Segmente innerhalb der Einträge identifiziert werden.

                    
So ergeben sich drei Gütekriterien für die regelbasierte Extraktion von strukturiertem Wissen aus historischen Enzyklopädien: 1) Wie generisch ist die Information; also auf wie viele Klassen und auf wie viele Einträge trifft sie insgesamt zu, 2) Wie stark ist die Information vorstrukturiert, also wie präzise kann eine Regel für die Information gefunden werden und 3) Wie relevant ist die Information im Hinblick auf die Ziele, die der Wissensgraph verfolgt? Recht generische Informationen sind beispielsweise Synonyme und Übersetzungen; in den meisten Einträgen werden diese direkt nach der Nennung des Konzepts aufgelistet und sind damit auch vergleichsweise einfach zu extrahieren. Die Beispiele aus Abbildung 1 sind das Gegenteil: Zwar sind sie alle eher einfach zu extrahieren durch die vorstrukturierte Form, allerdings sind sie auch in nur wenigen Einträgen vorhanden und insbesondere Zahlen und Einheiten sind für das Ziel das EncycNet verfolgt, nämlich ein semantisches Netzwerk zu bilden, eher uninteressant. Es empfiehlt sich daher, zunächst alle möglichen Informationen nach diesen Kriterien zu sortieren und dann erst mit der Extraktion zu beginnen, um eben nicht nur Detailwissen zu extrahieren bzw. bestimmte Domänen zu bevorzugen.

                    
Die domänenübergreifende Perspektive wirkt sich auch auf die Auswahl der Ontologie aus. Zusammen mit dem Ziel, nicht nur Entitäten sondern auch lexikalischen Wissen semantisch zu modellieren, schließt dies einige Standards aus. OntoLex (Cimiano et al. 2016) beispielsweise ist gerade dafür gedacht, lexikalisches Wissen aus Nachschlagewerken in einen Graphen umzuwandeln. Allerdings liegt hier der Fokus auf morphologischen statt semantischen Eigenschaften (Wortart, Genus, etc.), welche in Lexika nicht unbedingt aufgeführt werden. Die Struktur des Lexikons wird außerdem explizit beibehalten. So werden beispielsweise Referenzen auf andere Artikel als Kante „reference“ eingepflegt und nicht weiter typisiert. Daneben gibt es CIDOC-CRM (Doerr 2005), eine Standard-Ontologie zum Beschreiben von kulturellen Objekten, und Faktoide (Bradley und Short 2005) zur Abbildung von spezifischen Stellen in einer Quelle über Personen. Bei beiden Modellierungsarten sind Objekte und Eigenschaften eher auf Named Enitites und weniger auf Lexeme ausgelegt. Alle drei Standards sind gut geeignet für Teilbereiche der Enzyklopädien, so wie zum Beispiel Faktoide für biographische Einträge. Die möglichst vollständige Abbildung aller Inhalte die EncycNet anstrebt, auch hauptsächlich von lexikalischen Eigenschaften (z.B. Synonyme oder Hyperonyme), ist aber nicht möglich.

                    
Synonyme und Hyperonyme gehören bei der Bildung eines lexikalisch-semantischen Netzwerks zu den Grundbausteinen; sind also besonders wichtig für Punkt 3. Synonyme werden benötigt, um Synsets nach dem Vorbild von WordNet zu bilden (bzw. das Verknüpfen gleicher Konzepte) und Hyperonyme für den Aufbau einer Taxonomie. Allerdings ist die Extraktion einer vollständigen Taxonomie, welche alle Domänen umfasst, nur aus dem Enzyklopädienkorpus weitestgehend unrealistisch, weswegen auf zusätzliches Material zurückgegriffen werden muss. Als bislang größte Online-Enzyklopädie kann Wikidata / Wikipedia, welche inzwischen nicht nur Entitäten-bezogenes Wissen sondern über die Integration von WordNet auch über lexikalisches Wissen verfügt, als Schnittstelle genutzt werden. Sie liefert einerseits die Taxonomie, aber auch andererseits über Wikipedia zusätzliches Textmaterial zu den Konzepten, welches ebenfalls für die Alignierung der Einträge über verschiedene Enzyklopädien hinweg von nutzen sein kann (Hagen et al. 2022).

                

                

                    
Heterogene Daten

                    
Der Wissensgraph, der durch EncycNet entstehen soll, fasst das Wissen von 6 allgemeinen Enzyklopädien zusammen. Alle diese Enzyklopädien unterscheiden sich hinsichtlich der Auswahl und Organisation der Konzepte, der inneren Struktur der Einträge, Umfang und Auslegung der Definitionen und dem Stil. Jede dieser Eigenheiten müssen für die Informationsextraktion berücksichtigt werden, was bedeutet, dass die Relationen für alle Enzyklopädien neu beurteilt werden müssen. Pragmatisch betrachtet heißt dies auch, dass die Informationsdichte im resultierenden Graph abnimmt, da weniger Relationen in Betracht genommen werden können. Insbesondere auf mögliche Inferenzbildungen durch den Graph wirkt sich das negativ aus.

                    
Aus diesem Grund sollten generische Methoden für die Informationsextraktion hinzugezogen werden, sodass der Graph an Informationsdichte gewinnt. Hierbei kann auf typische Methoden in den Digital Humanities zurückgegriffen werden: Topic Modeling zum Identifizieren von übergreifenden Wissensbereichen, TF-IDF für distinktive Terme eines Eintrags, Komposita des Konzepts als verwandte Konzepte, Named Entity Recognition, oder spezifisch für Enzyklopädien die Extraktion von Referenzen auf andere Einträge. Nachteil dieser generischen Extraktion ist allerdings, dass das Mapping der extrahierten Terme auf eine Relation sich schwieriger gestaltet bzw. viele der Terme eine unspezifische Relation zum Konzept erhalten (z.B. in GermaNet „related_to“).

                    
Auch für die Alignierung ergeben sich praktische Probleme bei der Grapherstellung, denn in den Enzyklopädien können die Konzepte unterschiedlich organisiert sein. Dies reicht von Betitelungskonventionen der Einträge (z.B. „Der Adler“ / „Adler,“ oder „William Shakespeare“ / „Shakespeare“ / „Shakespeare, William“) bis hin zu der Möglichkeit, dass es Einträge gibt, die mehrere Konzepte zusammenfassen. In Meyer (1905) werden beispielsweise manche Entitäten thematisch gruppiert. So gibt es zum Beispiel genau zwei Einträge zu 
                        
Alexander
: einer gruppiert Fürsten und der andere griechische Schriftsteller mit dem Vornamen, die auch jeweils nochmal genauer beschrieben werden. In Herder (1854) dagegen gibt es drei Einträge: einen, der den Vornamen ohne Personenzuordnung nennt, einen zu Alexander I. (welcher auch Alexander II. umfasst) und einen zu Alexander III. Hier gilt es also zu klären, inwieweit die Konzepte getrennt werden können. In Meyer sind die Konzepte deutlich durch Paragraphen gekennzeichnet, in Herder werden sie sprachlich vermischt. Zusätzlich können allerdings, selbst wenn die Konzepte isoliert sind, diese auch unterschiedlich ausgelegt werden, auch historisch bedingt.
                    

                

                

                    
Historisches Wissen

                    
Die Historizität der Daten wirkt sich damit ebenso auf die Alignierung der Konzepte aus. Auf der einen Seite werden teils archaische Begriffe oder Schreibweisen für Konzepte verwendet (z.B. „Irrenanstalt,“ jetzt „psychiatrische Klinik“). Durch eine orthographische Normalisierung und durch die Verwendung von Wikipedia für die Alignierung, welche zum Teil auch archaische Begriffe umfasst und auflösen kann, kann diesem Problem entgegengekommen werden. Auf der anderen Seite können sich aber auch die Definitionen von Begriffen im Laufe der Zeit stark verändert haben, etwa weil sich ein Konzept in ein anderes verwandelt hat oder weil Konzepte zu unterschiedlichen Zeiten anders ausgelegt werden.

                    
So wird beispielsweise in Meyer (1905) die 
                        
Exploration
 beschrieben als die physische Untersuchung eines Kranken durch einen Arzt, während sie in Wikipedia mit 
                        
Anamnese
 gleichgestellt wird, also der Erfragung von Informationen im Rahmen einer Erkrankung. Ein weiteres Beispiel findet sich in Herder (1854): Der Begriff 
                        
Proportionalität
 wird beschrieben durch „Harmonie der Größenverhältnisse” und „Proportionslehre der menschlichen Gestalt,” während für 
                        
Proportion
 die Verhältnisgleichung in der Mathematik genannt wird; also was die Menschen heute eigentlich unter Proportionalität verstehen würden.
                    

                    
Diese Beispiele zeigen eine notwendige Modellierungsentscheidung für einen historischen Wissensgraphen auf: Sollen Konzepte, welche sich grundlegend verändert haben trotzdem miteinander aligniert werden und die Alignierung macht somit den semantischen Wandel sichtbar? Oder sollten nur Konzepte, welche tatsächlich semantische Äquivalente sind aligniert werden, da die Definitionen so unterschiedlich ausfallen können? Je nach Ziel, den der resultierende Graph verfolgt, kann diese Entscheidung unterschiedlich ausfallen.

                    
Letztlich wirkt sich das historische Wissen auch auf den Aufbau der Taxonomie aus. Um eine vollständige Taxonomie automatisch zu generieren, kann auf bestehendes strukturiertes Wissen (wie Wikidata) zurückgegriffen werden. Allerdings besteht hierbei die Gefahr, historisches Wissen zu ignorieren oder zu überschreiben. In den Enzyklopädien wird beispielsweise der Begriff 
                        
Hexe
 als „Unholdin,” „Weib” oder „weissagende Frau” beschrieben, während in Wikidata „Magier” verwendet wird. Ein anderes Beispiel ist 
                        
Hierarchie
, welche in Wikidata als „Struktur“ oder „System“ eingeordnet wird, jedoch in den Enzyklopädien mit „Priesterherrschaft,” „Macht” oder „alle Rechte der Römischen Päpste über die gesammte Christenheit” beschrieben wird.
                    

                

            

            

                
Zusammenfassung

                
Das Bilden eines historischen Wissensgraphen aus Enzyklopädien kombiniert zwei Forschungsnischen aus dem Forschungsfeld. Einerseits geht es hier um die Abbildung von lexikalisch-semantischem Wissen und nicht um nur um Entitätenwissen wie z.B. Wikidata, und andererseits stellt heterogener Fließtext die Datengrundlage für den Graphen dar. Beides sind Voraussetzungen, die sich selten in der aktuellen Forschung zur Erstellung von Wissensgraphen wiederfinden. Der Aufbau von EncycNet wird geleitet von praktischen Anforderungen an den Graphen, welche sich aus den vordefinierten Zielen ergeben. Dabei geht es um die Einschätzung, welche Informationen in den Enzyklopädien sich für eine Relationsextraktion anbieten, aber gleichzeitig soll möglichst viel Wissen mit dem Graphen abgedeckt werden. Zusätzlich, insbesondere um Inferenzen zu ermöglichen, muss der resultierende Graph eine möglichst große Informationsdichte in Tiefe (Taxonomie) und Breite (generische Relationen) aufweisen. Es ergibt sich daraus eine Bottom-up Strategie (schematisch zusammengefasst in Abbildung 2).

                

                    

                        

                        
Abbildung 2: Schematische Darstellung der Erstellung des Graphen: gezielte Extraktion von Relationen aus den Enzyklopädien (links), Aufbau der Taxonomie (Mitte) und Verdichtung durch ungezielte, generische Relationsextraktion (rechts).

                    
Für EncycNet werden aktuell Alignierung sowie Relationsextraktion fertiggestellt. Da die größtmögliche Abdeckung für beide Aufgaben erzielt werden soll, werden die Methoden diesbezüglich kontinuierlich optimiert. Noch ausstehend ist die Evaluierung des extrahierten Wissens mit Golddaten. Final sollen dann über die Evaluierung die Relationen und die Alignierung mit Gewichten ausgestattet werden, welche die Konfidenz angeben. Im Frühjahr 2024 sollen dann die Daten in RDF* zur Verfügung gestellt werden.
                

                
Im Vordergrund dieses Beitrags sollten damit jene Entscheidungsfindungen stehen, die in Methoden-orientierten Beiträgen sonst meist nur am Rande erwähnt werden. Dabei wurde sich auf EncycNet bezogen, jedoch können die hier aufgeführten Ideen genauso für Projekte, die auf ähnliche Herausforderungen bei dem Aufbau eines Wissensgraphen stoßen, interessant sein.

            

        

            

                
Motivation

                
Dieses Poster stellt eine Datenbank vor, die sich  auf die thesaurierende Erschließung sämtlicher Konfessionsaspekte in den deutschen Poesielehrbüchern der Barockzeit richtet. Das barocke Poetikparadigma, das sich zeitlich von Opitz’ 
                    
Buch von der Poeterey
 (1624) bis zu Gottscheds 
                    
Critischer Dichtkunst 
(1730) erstreckt (Wesche 2004, 164), wird damit erstmals systematisch im Hinblick auf Fragen der Konfessionalität beforscht, die gerade für das 17. und 18. Jahrhundert von zentraler historischer Bedeutung sind. Das Projekt hat sich zum Ziel gesetzt, die konfessionsgeschichtlichen Inhalte von insgesamt 54 historisch einschlägigen Poetiken offen zugänglich und durchsuchbar zu machen. Damit fällt es unter das Konferenzthema ‚Open Data‘.
                

                
Die Datenbank entsteht im Rahmen des Teilprojekts “Uneindeutige Barockdichtung. Poetische und konfessionelle Ambiguität in Schlesien als kulturdynamische Faktoren einer neuen deutschen Dichtkunst (1620 bis 1742)” der DFG-Forschungsgruppe “Ambiguität und Unterscheidung. Historisch-kulturelle Dynamiken”, das Prof. Wesche gegenwärtig an der Universität Göttingen leitet. Die Wikibase ist zu finden unter 
                    

                        
http://barockpoetik.de

                    

                

            

            

                
Datenmodellierung

                
Im Fokus steht, die Modellierung der Daten so vorzunehmen, dass die Inhalte dynamisch abrufbar sind. Dies wurde umgesetzt durch eine dedizierte Wikibase, die unsere spezifischen Inhalte im Stil von Wikidata darstellt und per Volltextsuche und einem SPARQL Query Interface zugänglich macht. Die Daten sind dabei als ‚Items‘ und ‚Properties‘ organisiert, wobei Items als Knoten und Properties als Kanten in einem Graph verstanden werden können, was es uns erlaubt beliebige Beziehungen im Graphen darzustellen und zu suchen.

                
Zentrale Elemente der Datenbank sind Autoren und ihre Werke, welche als Items gespeichert wurden. Siehe etwa das Item:Q29 (
                    

                        
http://barockpoetik.de/mediawiki/index.php/Item:Q29

                    
), welches den Eintrag für den Autor Martin Opitz festlegt, und dabei diverse Statements definiert, die für die Autoren relevant sind (wie etwa das verfasste Werk, seinen Vor- und Nachnamen, das Geburts- und Todesjahr) und auf einen Eintrag in der GND verweist (Deutsche Nationalbibliothek). Autoren, die in der Wikipedia geführt sind, werden ebenfalls mit einem Link dorthin ausgestattet.
                

                
Das assoziierte Werk von Opitz, 
                    
Das Buch von der Deutschen Poeterey
, ist hingegen unter Item:Q82 abgelegt (
                    

                        
http://barockpoetik.de/mediawiki/index.php/Item:Q82

                    
). Neben relevanten Metadaten (Publikationsjahr und -ort, Autor, Digitalisat), enthalten Einträge zu Werken Informationen zu bibliographischen Angaben, zur Sekundärliteratur sowie die werkeigenen Kapitelüberschriften. Hinzu kommen als Kernstück der Arbeit Textstellen, die konfessionsgeschichtlich relevant sind. Die Verschlagwortung folgt hier innerhalb der im Projekt entwickelten Systematik ‚Dichtung/Theologie‘, ‚Inspiration‘, ‚Sprachgenealogien‘, ‚Themen/Gattungen‘, ‚Autoritäten‘, ‚Widmungen/Adressaten‘ und ‚Exempelpolitik‘.
                

                
Einzelne Textstellen sind dabei als Items angelegt, um sie mehrfach verschlagworten zu können (wobei eine Textstelle unter verschiedenen Schlagworten geführt werden kann). Zudem besitzt jede Textstelle eine Angabe zu ihrer Fundstelle, um sie im Digitalisat ausfindig zu machen.

                
 Die Volltextsuche erlaubt es, nach bestimmten Inhalten zu suchen. Etwa ergibt eine Suche nach dem Wort ‘Mensch’ alle Textstellen, in denen dieses Wort vorkommt, oder eine Suche nach ‘[aq]’ liefert alle Textstellen mit einer Auszeichnung für die Schriftart ‘Antiqua’, welche in frühneuzeitlichen Drucken konventionell für fremdsprachliches Material verwendet wird. Diese Textstellen können wiederum durch eine ‘Inverse Suche’ dem jeweiligen Werk zugeordnet werden.

                
Der SPARQL Endpoint erlaubt es uns, beliebige Daten zu extrahieren, wie etwa eine Übersicht der Autoren mit ihren Werken, oder zum Beispiel alle Werke, die ‘Exempelpolitik’ enthalten, mit ihren Autoren, und den entsprechenden Textstellen, sortiert (oder gefiltert) nach Publikationsjahr. 

            

            

                
Fazit

                
Die Datenbank leistet im Bereich der Geschichte der Poetik als einem zentralen Forschungsgebiet der germanistischen Literaturwissenschaft einen substantiellen Beitrag zur Exploration aktueller Methoden der Linked Open Data (Chiarcos et al., 2022; Sturgeon, 2022) und der Nutzung von Wikidata in den Digital Humanities (Zhao, 2022) in einem (frühneuzeit-)historischen Forschungsgebiet, indem es nicht nur um die Digitalisierung und (forschungs-)öffentliche Bereitstellung von Textdaten geht, sondern diese zugleich mit einem auf Fragen der Konfessionalität gerichteten Erkenntnisinteresse digital aufbereitet und empirisch ausgewertet werden. Übergeordnetes Forschungsziel ist es dabei, von der Universität Göttingen aus ein erweiterbares Portal „Barockpoetik digital“ zu etablieren, in dem die Forschungsdaten zum Thema zentral gebündelt und verfügbar gehalten werden. Als konkrete Erweiterungsperspektive erschließt das Team derzeit im Rahmen des DFG-Schwerpunkprogramms „Übersetzungskulturen der Frühen Neuzeit“ sämtliche Aspekte, die im Poetikkorpus translationsgeschichtlich relevant sind.

            

        

            
Die Digitalisierung unseres Kulturguts hat zu riesigen Sammlungen geführt, welche über Suchsysteme öffentlich verfügbar sind. Im Sinne der “Open Culture” ist Verfügbarkeit aber nicht genug (Walker 2022), da die weiße Suchmaske für nicht-Expert:inn:en eine signifikante Hürde für den Zugriff darstellt (Belkin, Oddy, and Brooks 1982; Whitelaw 2015). Nicht-Expert:inn:en fehlt oft das notwendige Wissen und das spezifisches Suchziel (Mayr et al. 2016), um erfolgreich Suchschlüsselwörter für die Suche zu entwickeln. Für digitale Sammlungen ist daher eine Bounce-rate von über 50% normal (Hall et al. 2012; Walsh et al. 2020).

            
Rich Prospect Browsing (Ruecker, Radzikowska, and Sinclair 2011) und Generous Interfaces (Whitelaw 2015) bieten den weniger erfahrenen Nutzer:inne:n Interfaces an, die einen sanfteren Einstieg in digitale Sammlungen ermöglichen. Beide Ansätze versuchen einen Überblick über die Sammlung zu geben, bevor sie zu einzelnen Objekten hineinzoomen (Shneiderman et al. 1998).

            
Bestehende Ansätze wie das Coins interface (Gortana et al. 2018), Curator Table interface (Google Arts &amp; Culture 2022), oder das Museum of the World (The British Museum and Google Cultural Institute 2022) nutzen Visualisierungen um einen Überblick über die gesamte Sammlung zu geben, die dann erforscht werden kann. Die Visualisierung bieten jedoch wenig bis keine Hinweise darauf gibt, welche Themen die Sammlung abdeckt und wo in der Visualisierung diese zu finden sind. Wegen der Größe der Sammlungen sind die einzelnen Objekte in der Visualisierung am Anfang auch oft wenig größer als ein paar farbige Pixel.

            
In einem physischen Museum gibt die Gebäude- und Raumstruktur einen Rahmen um das Museum zu erforschen. Eine derartige Struktur ist auch im digitalen Museum notwendig, fehlt aber in den meisten digitalen Sammlungen und den Kulturorganisationen fehlt die Kapazität um eine zu entwickeln. In Hall and Walsh (2021) haben wir mit der Digital Museum Map einen automatischen Kuratierungsalgorithmus entwickelt, der, basierende auf den Objektmetadaten und einer Reihe von Heuristiken, eine navigierbare Struktur entwirft, die aus hierarchisch strukturierten Gruppen von zwischen 25 und 100 Objekten besteht. Die Qualität der Struktur ist niedriger als bei einer händischen Kuratierung, erlaubt es aber große Sammlungen schnell zu kuratieren. Der Algorithmus arrangiert die navigierbare Struktur dann in Räume, welche in Stockwerken organisiert sind. Dazu wird eine greedy Heuristik genutzt, um die automatisch kuratierte Struktur, basierend auf thematischen Ähnlichkeiten, koherent im Stockwerksgrundriss zu positionieren. Der Grundriss wird vom Museum bereitgestellt und erlaubt es das Layout den Museumsinteressen anzupassen.

            

                
Das Digitale Museum

                
Das Digitale Museum nutzt auf der Landeseite (Abb. 1) keine Visualisierung um die gesamte Sammlung anzuzeigen, sondern bietet vier verschiedene Einstiegspunkte in die Sammlung an: ein Objekt des Tages, eine Auswahl der größten Teilsammlungen, eine zufällige Auswahl an Objekten, und ein Link in die gesamte Sammlung. Folgt der/die Nutzer:in dem Link zu einer Teilsammlung oder in die gesamte Sammlung, dann wird das Navigationsinterface in Abbildung 2 angezeigt. Links können die einzelnen Stockwerke ausgewählt werden und rechts werden die Räume auf dem Museumsgrundriss angezeigt. Nach der Auswahl eines Raumes, werden die Objekte des Raumes angezeigt (Abb. 3). Auf diese Weise kann das gesamte Museum durch Browsing erkundet werden.


                    

                        

                        
Abbildung 1: Die Landeseite bietet Einstiegspunkte in das Digitale Museum: auf der Objektebene, auf der Teilsammlungsebene, und auf der Ebene der gesammten Sammlung. Alle Bilder © Victoria &amp; Albert Museum, 2022.

                    


                    

                        

                        
Abbildung 2: Das Stockwerksinterface erlaubt freies Entdecken im Digitalen Museum und kann mit anderen Informationen überlagert werden, wie hier mit Suchergebnissen.

                    

                
Ein Vorteil des digitalen Mediums ist, dass zusätzliche Informationen über die Stockwerksvisualisierung gelegt werden können. Browsing wird zwar bevorzugt, aber eine Suchfunktion ist auch vorhanden. Sucht der/die Nutzer:in nach etwas, werden die Stockwerke und Räume, die passende Objekte beinhalten, visuell hervorgehoben. Ebenso werden in der Raumansicht die passenden Objekte hervorgehoben. Dadurch kann gezielt gesucht werden, ohne den Kontext, in dem die Objekte stehen, zu verlieren.


                    

                        

                        
Abbildung 3: Ein Raum im Museum, links in der Desktopansicht, rechts die mobile Version.

                    


            

            

                
Software und Ausblick

                
Die Digital Museum Map bietet einen einfachen Einstieg, um Sammlungen der interessierten Öffentlichkeit in einem Digitalen Museum zugänglich zu machen. Daher ist sie konfigurierbar und als Open-Source Software verfügbar (https://github.com/scmmmh/museum-map/). Zur Zeit bestehen Demoversionen der Digital Museum Map für einen Teil der 
                    
Victoria &amp; Albert
 Sammlung (https://va.museum-map.research.room3b.eu/) und für die 
                    
People Past and Present
 Sammlung der Stadt Durham (https://ppp.museum-map.research.room3b.eu/).
                

                
Weiterentwicklungen werden sich auf zwei Bereiche konzentrieren. Empfehlungen für ähnliche Objekte, um eine horizontale Navigation zwischen Objekten zu ermöglichen. Eine offene Frage ist was für Empfehlungen Nutzer:innen wollen: mehr ähnliche Objekte oder eine diversere Empfehlung. Der zweite Bereich ist die Integration digitaler Museumsführer, die von Kurator:inn:en und auch Nutzer:inne:n erstellt werden und einen alternativen Zugang bieten.

            

        

            

                
Relevante Ereignisse in Erzähltexten

                
Welche Ereignisse in Erzähltexten sind besonders relevant? Diese Frage wird in der Literaturwissenschaft im Kontext von verschiedenen Konzepten verhandelt. So können relevante Ereignisse identifiziert werden, indem man die für die Textinterpretation als besonders wichtig erachteten Stellen (so genannte “Schlüsselstellen”) betrachtet (Arnold &amp; Fiechter, 2022). Auf die Rezeption orientiert sind ebenfalls die empirische Leser:innenforschung (Groeben 1977; Miall &amp; Kuiken 2001) oder die Rezeptionsästhetik (Iser 1976). Steht hingegen der Text im Fokus, kann die Frage nach der Wichtigkeit von Ereignissen in Bezug auf Ereignishaftigkeit oder die so genannte Erzählwürdigkeit untersucht werden (z. B. Hühn 2014; Baroni 2012). Allen Ansätzen gemeinsam ist, dass sie bestimmte Qualitäten von Texten bzw. Textbestandteilen betrachten.

                

                    

                        

                        
Abb. 1: Narrativitätskurve für Kafkas Die Verwandlung (Vauth et al., 2021)

                    
In unserem EvENT-Projekt wurde bereits die Identifikation und Klassifikation von Ereignissen anhand von textuellen Merkmalen vorgenommen. Den hierbei entwickelten vier Ereignistypen haben wir Werte entsprechend ihrer Ereignishaftigkeit zugewiesen und darauf basierend Narrativitätskurven erzeugt, die den Verlauf der Ereignishaftigkeit über einen Text abbilden (siehe Abb. 1).

                

                
Im Fortgang des Projektes wollen wir überprüfen, inwiefern zwischen diesen grundlegenden Ereignistypen, die auch unter dem Konzept “Event I” subsumiert werden können, und besonders erzählwürdigen Ereignissen bzw. so genannten Events II, eine Verbindung besteht.
 Nimmt man an, dass ein Event II ein Event I mit weiteren Qualitäten ist, so ist die Bestimmung von Events II insofern komplexer, als sie zusätzliches Wissen erfordert – etwa um Regelmäßigkeiten und Auffälligkeiten in der fiktionalen Welt, aber auch um den Kontext. Mit anderen Worten, wir wollen die
                    
 

                    
von uns im Vorlauf vorgenommene 

                    
textoberflächenbasierte Detektion von Ereignissen 

                    
dahingehend prüfen 

                    
inwiefern mit ihr auch jene Textstellen erfasst werden, die in Handlungszusammenfassungen als besonders handlungsrelevant bzw. erzählwürdig gelten. 

                

                
Was bereits mit unseren bestehenden Daten und ohne die weitere Operationalisierung des Event II-Konzepts möglich ist, ist der Abgleich unserer Annotationen mit als besonders handlungsrelevant markierten Textstellen. Diesen Vergleich stellen wir im vorliegenden Beitrag an, indem wir unsere Annotationen von Ereignissen in literarischen Texten mit Zusammenfassungen der entsprechenden Texte abgleichen. 

            

            

                
Semiprofessionelle, professionelle und nutzer:innengenerierte Handlungszusammenfassungen 

                
Wir gehen davon aus, dass Textstellen durch ihre Erwähnung in Zusammenfassungen als für die Handlung wichtig markiert werden. Für den Abgleich dieser Textstellen mit unseren Narrativitätsverläufen nutzen wir drei Typen von Zusammenfassungen: (1) semiprofessionelle Zusammenfassungen, die Studierende der Literaturwissenschaft verfasst haben, (2) professionelle Zusammenfassungen aus Kindlers Literatur Lexikon und (3) nutzer:innengenerierte Zusammenfassungen der Online-Enzyklopädie Wikipedia. Die Verwendung dieser verschiedenen Zusammenfassungstypen zielt darauf ab, zu analysieren, welche Aspekte bzw. Textstellen für die jeweiligen Zusammenfassungstypen relevant sind und dadurch Rückschlüsse auf ihre Qualität zu ziehen.

                
Die (1) Zusammenfassungen der Studierenden waren Teil der Studienleistungen in einem Seminar. Sie wurden als explizit auf die Handlung bezogene Zusammenfassungen verfasst, die eine maximale Länge von 20 Sätzen haben durften. Außerdem wurden die Studierenden aufgefordert, keine Hilfsmittel (wie Zusammenfassungen auf Wikipedia oder aus Literaturlexika) zu nutzen. Für die vier genutzten Primärtexte 
                    
Das Erdbeben in Chili 
(Heinrich von Kleist, 1807), 
                    
Die Judenbuche
 (Annette von Droste-Hülshoff, 1842), 
                    
Krambambuli
 (Marie von Ebner-Eschenbach, 1896) und 
                    
Die Verwandlung
 (Franz Kafka, 1915) haben wir jeweils 11, 9, 11 und 10 Zusammenfassungen.
                

                
Aus den Beiträgen des (2) Kindler-Literaturlexikons und von (3) Wikipedia wurden nur jene Passagen verwendet, die sich auf die Handlung in den Primärtexten beziehen. Passagen, die sich der Autor:in, Rezeption oder Interpretation widmen, wurden nicht berücksichtigt. Durch eine kollaborative Annotation der einzelnen Sätze wurde deren Bezug auf die Handlung des Textes annotiert und ein entsprechender Goldstandard erstellt, der den weiteren Analysen zugrunde liegt. So wurde sichergestellt, dass alle drei Zusammenfassungstypen handlungsorientiert sind. Die Zusammenfassungen wurden dahingehend annotiert, dass jeder Satz der Zusammenfassung mit einer Referenz auf alle Spannen des Primärtextes versehen wurde, auf die er Bezug nimmt.
 Als logische Einheiten wurden entsprechend die bereits vorliegenden Annotationen aller Verbalphrasen in Bezug auf die vier Ereignistypen genutzt.
 Dabei wurden konkret jeweils eine oder mehrer Spannen im Originaltext als zugehörig annotiert, die Sequenzen relevanter Ereignisse enthalten; ein Satz der Zusammenfassung kann also mehrere Passagen im Originaltext referenzieren.
                

            

            

                
Evaluation der Zusammenfassungen

                
Um die Qualität der Zusammenfassungen und mögliche Unterschiede zwischen den drei Zusammenfassungstypen zu analysieren, evaluieren wir deren Ähnlichkeit. Dazu nutzen wir drei Metriken, mit denen implizit drei unterschiedliche Auffassungen von Ähnlichkeit verbunden sind: Eine weitgehend lexikalische (N-Gramme), eine Metrik auf Basis distributioneller Semantik (Word Embeddings) und eine, die sich weitgehend von der sprachlichen Struktur löst und inhaltsbezogene Vergleiche vornimmt (adaptierte Pyramiden-Methode). 

                
Wir nehmen zunächst an, dass die semiprofessionellen Zusammenfassungen durchweg handlungsbezogen sind. Deshalb vergleichen wir jeweils eine semiprofessionelle Zusammenfassung mit allen anderen semiprofessionellen und jede Zusammenfassung aller anderen Typen mit allen semiprofessionellen.

                

                    
N-Gramm-basierte Ähnlichkeit

                    
Als erstes berechnen wir BLEU- (Papineni et al., 2002) und ROUGE-Scores (Lin et al., 2004), die Ähnlichkeiten unter Zusammenfassungen als N-Gramm-Ähnlichkeit abbilden.

                    
Wir gewichten BLEU-{1,2,3} und ROUGE-{1,2,3} jeweils gleich und quantifizieren so die Überlappung von 1-, 2- und 3-Grammen zwischen den unterschiedlichen Texten und geben für BLEU die Precision und ROUGE den F1-Score an.

                    
Anhand der Scores in Tab. 1 und Tab. 2 wird ersichtlich, dass die semiprofessionellen Zusammenfassungen nahezu durchgehend die höchsten Ähnlichkeitswerte aufweisen.

                    

                        

                            

                            
Erdbeben

                            
Judenbuche

                            
Krambambuli

                            
Verwandlung

                        

                        

                            
semiprofessionell

                            
0,27

                            
0,23

                            
0,24

                            
0,22

                        

                        

                            
professionell

                            
0,15

                            
0,2

                            
0,19

                            
0,09

                        

                        

                            
nutzer:innengeneriert

                            
0,14

                            
0,22

                            
0,24

                            
0,1

                        

                        
Tab. 1: BLEU-{1,2,3} Scores für Ähnlichkeiten. Für die semiprofessionellen Zusammenfassungen wird der Mittelwert angegeben.

                    

                    

                        

                            

                            
Erdbeben

                            
Judenbuche

                            
Krambambuli

                            
Verwandlung

                        

                        

                            
semiprofessionell

                            
0,51

                            
0,56

                            
0,6

                            
0,47

                        

                        

                            
professionell

                            
0,58

                            
0,53

                            
0,63

                            
0,43

                        

                        

                            
nutzer:innengeneriert

                            
0,34

                            
0,49

                            
0,55

                            
0,31

                        

                        
Tab. 2: ROUGE-{1,2,3} F-Scores. Für die semiprofessionellen Zusammenfassungen wird der Mittelwert angegeben.

                    

                

                

                    
Word-embedding-basierte Ähnlichkeit

                    
N-Gramm basierte Metriken haben den Nachteil, dass kleine Unterschiede in der Wortwahl zu einer deutlich geringeren Ähnlichkeit führen können. Um Vergleiche stärker auf die Semantik zu fokussieren, wurde mit BERTScore (Zhang et al., 2020) eine embedding-basierte Methode etabliert. Wenden wir diese auf unsere Texte an, zeigt sich ein deutlich geringerer Unterschied der Zusammenfassungstypen (siehe Tab. 3). Dies weist darauf hin, dass Unterschiede in der N-Gramm-basierten Bewertung zu einem großen Teil auf Unterschiede in der Wortwahl zurückzuführen sind. 

                    

                        

                            

                            
Erdbeben

                            
Judenbuche

                            
Krambambuli

                            
Verwandlung

                        

                        

                            
semiprofessionell

                            
0,74

                            
0,71

                            
0,74

                            
0,72

                        

                        

                            
professionell

                            
0,69

                            
0,71

                            
0,74

                            
0,69

                        

                        

                            
nutzer:innengeneriert

                            
0,72

                            
0,68

                            
0,74

                            
0,72

                        

                        
Tab. 3: BERTScore F-Werte. Für die semiprofessionellen Zusammenfassungen wird der Mittelwert angegeben.

                    

                

                

                    
Inhaltsbasierte Ähnlichkeit

                    
Für den letzten Vergleich der Zusammenfassungen adaptieren wir die Pyramiden-Methode, die für die automatische Evaluation maschinell generierter Zusammenfassungen entwickelt wurde (Nenkova et al., 2004). Die Zusammenfassungen werden auf Basis von sogenannten Summary Content Units (SCU) mit Referenzzusammenfassungen verglichen. Eine SCU repräsentiert dabei eine semantische, inhaltliche Aussage aus dem Zusammengefassten. Die namensgebende Pyramide repräsentiert dabei das Vorkommen der unterschiedlichen SCUs in der Menge der Referenzzusammenfassungen, wobei die Höhe der Pyramide n der Anzahl der Referenzzusammenfassungen entspricht. Dabei ist in der Regel eine Verteilung zu beobachten, die tatsächlich eine Pyramide aufbaut: eine SCU taucht in allen 
                        
n
 Texten auf und bildet die Spitze, einige wenige SCUs tauchen in 
                        
n-1
 Texten auf usw. bis in der letzten Stufe SCUs auftauchen, die nur in einer Zusammenfassung vorkommen. Eine zu evaluierende Zusammenfassung sollte nun, um ihren Pyramiden-Score zu maximieren, SCUs aus hohen Schichten der Pyramide enthalten. Dabei erhält die oberste Schicht das Gewicht 
                        
n
, sodass jede SCU aus dieser Schicht n Punkte gibt. Der Score einer Zusammenfassung wird als Anteil der tatsächlichen Punkte an denen der optimalen Zusammenfassung der gleichen Länge (in SCUs) angegeben. Entsprechend sind die Pyramiden-Scores reelle Zahlen im Intervall 0 bis 1, wobei 1 eine perfekte Zusammenfassung beschreibt.
                    

                    
Wir passen die Pyramiden-Methode in zwei Punkten an unsere Fragestellung an. Zum einen haben wir keine Referenztexte, sondern benutzen das Verfahren zum Vergleich verschiedener Zusammenfassungen. Zum anderen enthalten unsere Daten keine SCUs, diese werden deshalb über Textspannen approximiert. Dafür nehmen wir zunächst an, dass jede Spanne des Textes eine Menge von SCUs enthält, die insofern eindeutig ist, als sie keine Schnittmenge mit den SCUs anderer, disjunkter Textabschnitte hat. Insofern kann jede Textspanne auf eine oder mehrere SCUs abgebildet werden. Textspannen werden derart in Unterspannen zerlegt, dass Spannen sich nur überlagern, wenn sie identisch sind. Somit erhalten wir Spannen, die gemäß unserer Annahme semantisch eindeutig sind (siehe Abb. 2). Eine Textspanne kann nach unseren Annahmen mehrere SCUs enthalten die wir als eine behandeln, dies entspricht einer Ereignis Modellierung in gröberer Granularität.

                    

                        

                            

                            
Abb. 2: Segmentierung von zwei Annotationsspannen in semantisch eindeutige SCUs. Spannen werden derart zerlegt, dass sich nur noch gleiche Spannen überhaupt überschneiden.

                        

                    

                    
Die Auswertung der Zusammenfassungen mit der Pyramiden-Methode ist in Tab. 4 zu sehen. Nahezu alle semiprofessionellen Zusammenfassungen liegen dabei über dem Wert 0,70 (siehe Abb. 3), während die anderen Zusammenfassungen im Vergleich zum Mittelwert schlechter abschneiden (siehe Tab. 4).

                    

                        

                            

                            
Erdbeben

                            
Judenbuche

                            
Krambambuli

                            
Verwandlung

                        

                        

                            
semiprofessionell

                            
0,85

                            
0,80

                            
0,84

                            
0,80

                        

                        

                            
professionell

                            
0,73

                            
0,52

                            
0,75

                            
0,55

                        

                        

                            
nutzer:innengeneriert

                            
0,71

                            
0,70

                            
0,81

                            
0,62

                        

                        
Tab. 4: Pyramiden-Scores für unterschiedliche Zusammenfassungen. Für die semiprofessionellen Zusammenfassungen wird der Mittelwert angegeben.

                    

                    

                        

                        

                            

                            
Abb. 3: Verteilung der Pyramiden-Scores für die semiprofessionellen Zusammenfassungen.

                        

                    

                

                

                    
Ähnlichkeiten zwischen den Zusammenfassungen

                    
Insgesamt wird so deutlich, dass eine Betrachtung auf Pyramiden-Ebene Unterschiede offenbart, die zwar denen der N-Gramm-Methoden ähnlich, jedoch nicht grundsätzlich anhand oberflächlichen maschinellen Textauswertungen (z.B. BERTScore) festzumachen sind. 

                

            

            

                
Narrativität und Handlung

                
Wir wollen nun evaluieren, wie Erzählwürdigkeit, repräsentiert durch die Handlungszusammenfassungen, und Narrativität, repräsentiert durch unsere Narrativitätsgraphen, zusammenhängen. Wir überprüfen dafür, ob der Teil des Originaltextes, auf den sich die Zusammenfassungen beziehen, einen großen Narrativitätswert aufweist. Als erste Analyse berechnen wir dazu den Narrativitätswert der in der Zusammenfassung referenzierten Passagen. Wir setzen diesen ins Verhältnis zum erwarteten Gesamtscore, gegeben der Länge der in der Zusammenfassung enthaltenen Textstellen (in Ereignissen).
 Somit ergibt sich im Mittel ein Wert von 1,0 bei zufälliger Auswahl der Passagen. Ein Wert > 1,0, hingegen heißt, dass in der Zusammenfassung referenzierte Passagen mehr Narrativität aufweisen als nicht referenzierte Passagen. Dies ist tatsächlich für alle Zusammenfassungstypen der Fall (siehe Tab. 5).
                

                
Auch ein Vergleich von Ereignissen, die in Zusammenfassungen genannt werden, mit jenen die es nicht werden, bestätigt dies anhand der Narrativitätswerte: im Mittel 3,13 für genannte und 2,86 für nicht genannte Ereignisse.

                

                    

                        

                        
Erdbeben

                        
Judenbuche

                        
Krambambuli

                        
Verwandlung

                    

                    

                        
semiprofessionell

                        
1,04±0,06

                        
1,02±0,09

                        
1,04 ±0,07

                        
1,06±0,10

                    

                    

                        
professionell

                        
1,00

                        
1,03

                        
1,02

                        
1,05

                    

                    

                        
nutzer:innengeneriert

                        
1,06

                        
1,12

                        
1,07

                        
1,08

                    

                    
Tab. 5: Faktoren des erwarteten Narrativitätswerts. Für die semiprofessionellen Zusammenfassungen wird der Mittelwert (inklusive der Standardabweichung) angegeben.

                

                
Für den Vergleich der Ausschläge der Narrativtiätskurven verwenden wir den Gipfelprominenzfaktor. Dabei handelt es sich um ein Maß, welches die Wichtigkeit eines Ausschlags und damit seinen Wert im Vergleich zum umliegenden Kurvenverlauf quantifiziert. Für diesen Vergleich werden alle lokalen Maxima in der cosinusgeglätteten Narrativitätskurve (window size=50) berücksichtigt und für jedes lokale Maximum wird die Gipfelprominenz berechnet.
 Jedes Ereignis erhält nun, wenn es ein lokales Maximum darstellt, den Wert der Gipfelprominenz, andernfalls den Wert 0. Nun wird wie oben verfahren und der erwartete Prominenzwert mit dem tatsächlichen verglichen. Es wird der erwartete Wert, also die durchschnittliche Gipfelprominenz des Graphen, ins Verhältnis zur tatsächlich vorgefundenen Gipfelprominenz des betrachteten Segments gesetzt. Lokale Maxima sind durch das Smoothing relativ selten, dementsprechend ist die Streuung der Werte deutlich größer. Dies erschwert die Interpretation der Werte. Interessant aber ist, dass in einigen Fällen der Faktor deutlich über 1 liegt (vgl. Tab. 7), wobei dies für die nutzer:innengenerierten Zusammenfassungen durchweg der Fall ist. Abb. 4 veranschaulicht trotz der starken Varianz erkennbare Unterschiede zwischen den Originaltexten.
                

                

                    

                        

                        
Erdbeben

                        
Judenbuche

                        
Krambambuli

                        
Verwandlung

                    

                    

                        
semiprofessionell

                        
0,91±0,52

                        
0,68±0,65

                        
0,90±0,57

                        
1,45±0,78

                    

                    

                        
professionell

                        
0,09

                        
1,15

                        
1,42

                        
1,38

                    

                    

                        
nutzer:innengeneriert

                        
1,23

                        
1,75

                        
1,28

                        
1,1

                    

                    
Tab. 6: Gipfelprominenzfaktoren

                

                

                    

                    

                        

                        
Abb. 4: Gipfelprominenzfaktoren der semiprofessionellen Zusammenfassungen

                    

                

            

            

                
Narrativität und Handlung: Ein kurzes Fazit

                
Die vorgestellten Ergebnisse deuten darauf hin, dass die Nutzung von Zusammenfassungen für die weitere Arbeit mit Ereignissen und Ereignishaftigkeit produktiv ist. Hervorzuheben ist, dass wir einen Relevanzzusammenhang zwischen Ereignissen und Handlung nachweisen konnten, der auf einer Operationalisierung ersterer anhand der sprachlichen Oberfläche aufbaut. Damit kann unser Ereigniskonzept mit reduziertem Handlungsbezug anhand von handlungsbezogenen Informationen aus Zusammenfassungen weiterentwickelt werden. Die bereits umgesetzte, vergleichsweise erfolgreiche Automatisierung der Ereigniserkennung und damit der Narrativitätsverläufe wird nun in Bezug auf die handlungsbezogene Relevanz von Ereignissen erweiterbar, ohne dass Handlungsinformationen mühevoll manuell für die einzelnen Ereignisse bestimmt werden müssen. Dafür erscheint es vielversprechend, den Handlungsbezug von Zusammenfassungen weiter zu evaluieren und dabei ein Verfahren zu entwickeln, das besonders relevante Stellen identifizieren kann.

            

            

                
Danksagung

                
Dieser Beitrag entstand im von der DFG im Schwerpunktprogramm Computational Literary Studies (SPP 2207) geförderten Projekt „Evelautaing Events in Narrative Theory“ (EvENT).

            

        

            

                
Historischer Hintergrund

            

                
Die Geschichte Preußens und seiner Armee wurde bereits intensiver erforscht. Das Kriegsspiel, das mit dieser verbunden ist, bisher jedoch kaum. Um ein Gesamtbild für diese Zeit und ihre Zusammenhänge erhalten zu können, ist eine kritische Auseinandersetzung mit dem Kriegsspiel ebenfalls nötig. Das 1824 vom preußischen Artillerieoffizier Georg Heinrich von Reisswitz ursprünglich der Armee vorgestellte
                    
 Preußische Kriegsspiel
 war das erste offiziell eingeführte professionelle Kriegsspiel. Es handelt sich um eine kartenbasierte Simulation bei der Truppenfiguren maßstabsgetreu dargestellt sind. Der Würfel dient zur Ermittlung von Gefechtsausgängen und anderen nicht durch die Anleitungen festgelegten Entscheidungen. Die damaligen Offiziere versuchten militärische Auseinandersetzungen realitätsnah zu simulieren (Wintjes 2017, Henning 2021b). Bis zu Beginn des 20. Jahrhunderts wurde das Kriegsspiel von unterschiedlichen Autoren weiterentwickelt, so dass heute 20 Regelwerke vorliegen (Henning 2021a; 2021b).
                

                
 

            


            

                
Forschungsstand

            

                
Aus einem vorangegangenen Projekt liegen Faksimiles von 18 Originalen vor. Diese wurden mit 
                    
OCR4all 
(Reul et al. 2019) erfasst. Da die Faksimiles in unterschiedlichen Frakturschrifttypen vorliegen, musste das OCR-Modell jeweils neu trainiert und für eine erste statistische Analyse einzeln vorverarbeitet werden. Aufgrund des Umfangs und der unterschiedlichen Bildqualität kann die Digitalisierung und Standardisierung des Korpus als zentrale Aufgabe betrachtet werden, bevor die eigentlichen Analysen stattfinden können. Die manuelle Nachkorrektur der Texte und die einheitliche Auszeichnung nach TEI ist ein laufendes Projekt. Weiterhin ist bekannt, dass diverse Übersetzungen der Regelwerke veröffentlicht wurden, allerdings sind diese noch gänzlich unerforscht und bisher nicht im Korpus integriert (Henning 2021a; 2021b, Wintjes 2022).
                

                
 

            

            

                
Teil der bisherigen Analyse war die Extraktion von distinktiven Merkmalen auf Textebene. Diese wurden im Rahmen meiner Masterthesis bereits erstellt und veröffentlicht (Henning, 2021a; Henning, 2021b). So konnte mit Hilfe einer Zeta-Analyse gezeigt werden, dass eine Zuordnung der einzelnen Texte zur Textsorte 
                    
Preußisches Kriegsspiel
 basierend auf distinktiven Wörtern möglich sein kann. Eine auf BERTs (Devlin et al. 2019) Word Embedding (WE) folgende Support Vector Machine (SVM) zeigt, dass eine automatische Textklassifzierung in ‘Regelwerke’ und ‘militärtheoretische Literatur’ – als Vergleichskorpus – ebenfalls aussichtsreich ist. Auch die Untersuchung der 
                    
most

                    
 frequent 

                    
words
 (MFW), 
                    
named

                    
entity

                    
recognition
 (NER) und 
                    
part

                    
of

                    
speech
 (POS) helfen Merkmale des Korpus, wie zum Beispiel die Verwendung der Truppengattungen und des Würfels, sowie den allgemeinen sprachlichen Aufbau (z.B. Wort-/Satzlänge, Verteilung von Substantiven/Adjektiven/Verben) der Textkategorie
                    
 Preußisches Kriegsspiel 
nachzuweisen. Die nach zeitlichen Aspekten gebildeten Teilkorpora sowie das gebildete Vergleichskorpus können ebenfalls anhand der Merkmale unterschieden werden (Versuchsaufbauten: Henning 2021a; 2021b). Dadurch konnte die Hypothese gestärkt werden, dass sich die untersuchten Regelwerke und gebildeten Teilkorpora nicht nur gemeinsam gegen ein Vergleichskorpus abgrenzen lassen, sondern auch untereinander, wodurch eine historische Entwicklung deutlich wird. Durch distinktive Merkmale wie die Verwendung des Würfels als Zufallskomponente und die Veränderung der Gewichtung der Truppengattungen, sind drei Phasen innerhalb der Kriegsspielgeschichte definierbar (Henning 2021a; 2021b). 
                

                
 

            

            

                
Sieben Regelwerke, entstanden zwischen 1862 und 1874, der Autoren Wilhelm von Tschischwitz (1862, 1866, 1870, 1874) und Thilo von Trotha (1867, 1970, 1874) wurden bereits einzeln

                
 

                

                
intensiver untersucht und jeweils zu einer Ausgabe zusammengeführt. Tschischwitz’ Werke

                
 

                

                
bereits veröffentlicht (Wintjes, 2019), Trothas’ liegen in einer bisher unveröffentlichten Abschlussarbeit vor (Henning, 2018). Dadurch ist eine tiefere Analyse des

                
 

                

                
gesamten Korpus angestoßen, sowie die Forschung in der praktischen Durchführung einer 

                
 

                

                
Simulation, mit Studenten oder an einer Führungsakademie, ermöglicht worden.

                
 

            
 


            

                
Geplantes Promotionsprojekt

            

                
Kernziel des Projekts ist die öffentliche Bereitstellung eines Regelwerkskorpus, inklusive dazugehöriger tabellarischer Beiwerke sowie Übersetzungen. Mit Hilfe von Methoden des 
                    
natural

                    
language

                    
processing
 (NLP) sollen inhaltliche und sprachliche Unterschiede und Entwicklungen im Laufe der Entstehungsgeschichte der Kriegsspiele aufgezeigt werden. Ziel ist eine diachrone und vergleichende Analyse der Gattung der Regelwerke sowie das Erstellen einer Ontologie für Regelwerke und nicht-literarischer Texte über das Kriegsspiel hinaus. Durch die Erstellung einer digitalen Sammlung und Ontologie, die auf vollständig annotierten und aufbereiteten Texten basiert, kann in einem Webtool eine direkte visuelle Vergleichbarkeit zwischen individuell gewählten Regelwerken geschaffen werden.  Texte, Faksimiles, Analyseergebnisse und weitere für das preußische Kriegsspiel relevante Materialen sollen zentral gesammelt und für die Forschung sowie die interessierte Öffentlichkeit bereitgestellt werden. Beispielsweise verändert sich die Nutzung eines Würfels als zufallsgebende Instanz zur dritten Phase der Kriegsspiele. Auch die Veränderung bei der Verwendung der Truppengattungen, kann so verdeutlicht werden. Dies kann durch Textverweise und Hervorhebungen sowohl im Original, in einer aufbereiteten Version als auch als Graphik kenntlich gemacht werden. Visualisierungen über sprachliche und formale Veränderungen bieten zudem neue Zugänge zu dem Forschungsgegenstand.
                

            

            

                

                
Abbildung 1: Veränderung der Truppengattungen in den Regelwerken

            

            

        

            
Gegenstand meines Beitrags ist die Erörterung von Anforderungen an eine offene Datensammlung zur digitalen Philosophie und Philosophiegeschichte, insbesondere im Blick auf zwei Fragen. Einerseits ist aus der Sicht der digitalen Geisteswissenschaft zu fragen, welchen Kriterien eine solche Datensammlung genügen sollte, um valide Schlussfolgerungen zu erlauben. Dies betrifft sozusagen ihre "formale" oder "methodische" Seite. Andererseits ist aus der Sicht des Faches Philosophie zu fragen, welche Arten von Daten überhaupt für das Fach relevante Einsichten ermöglichen können. Dies betrifft die inhaltliche oder "materiale" Seite. Hier können beide Fragen natürlich nur exemplarisch erörtert werden, ihre Wichtigkeit für die Projektierung einer Datensammlung zur digitalen Philosophie und Philosophiegeschichte sollte aber auf der Hand liegen.

            

            

                
Zum Begriff der Datensammlung

                
Wir sprechen im folgenden von „Datensammlungen“,
 weil die digitale Philosophie und Philosophiegeschichte, wie genauer zu zeigen wird, nicht nur Textdaten, sondern auch Metadaten über Text zu ihren Forschungsgegenständen zählt. Zudem kann die Philosophie des 20. Jahrhunderts und ihre Geschichte in reproduzierbarer Weise aus urheberrechtlichen Gründen weit überwiegend nur mittels „abgeleiteter Textformate“ (Schöch u. a. 2020) analysiert werden. Die Heterogenität dieser Datenformate legt es nahe, den generischen Begriff der Datensammlung anstatt spezifischerer Termini wie "digitale Textsammlung", "Corpus", "Kanon" (siehe dazu (Henny-Krahmer und Neuber 2017)) in Anschlag zu bringen.
                

            

            

                
Untersuchungsgegenstand

                
Analysieren möchte ich im folgenden zwei jüngere Studien zur digitalen Analyse von Philosophie im 20. bzw. 21. Jahrhundert (Malaterre u. a. 2021; Noichl 2021).
 Gegenstand sind zum einen die Philosophie insgesamt, deren Struktur anhand von Kozitationsanalysen erhellt werden soll,
 zum andern eine Teildisziplin der Philosophie, die Wissenschaftstheorie, deren diachrone Entwicklung anhand von topic models von acht einschlägigen Zeitschriften sichtbar gemacht werden soll.
 Die zugrundeliegenden Datensammlungen enthalten also einmal ausschließlich Metadaten und einmal Volltexte von Zeitschriftenartikeln mit korrespondierenden Metadaten, v. a. Erscheinungsjahr und die publizierende Zeitschrift. Damit ist schon eine erste Anforderung an eine offene Datensammlung zur digitalen Philosophie und Philosophiegeschichte benannt: Sie sollte nicht nur Volltexte, sondern auch Metadaten einschließen, selbst wenn die den Metadaten korrespondierenden Textträger nicht oder noch nicht digitalisiert sein sollten, sondern, wie bei Noichl, nur Angaben zu den erfassten Texten (wie der Aufsatztitel oder Abstract) sowie die Bibliographie zitierter Werke in die Datensammlung aufgenommen werden, da die eigentlichen Aufsätze selbst noch urheberrechtlich geschützt sind.
                

            

            

                
Zur Erstellung von Datensammlungen in der Philosophie: zwei Beispiele

                
Erster Schritt der Erstellung einer Datensammlung und demnach auch ihrer Bewertung ist nach Schöch 2017, 224 die Angabe, wie Gegenstand und Umfang eingregrenzt werden. Malaterre u. a. 2021, 2885 geben den Umfang ihrer Datensammlung mit 15897 englischsprachigen Aufsätzen an, die zwischen 1934 und 2017 in acht wissenschaftsphilosophischen Zeitschriften veröffentlicht worden sind. Die Volltexte wurden von JSTOR zur Verfügung gestellt. Dass die Vollständigkeit der Digitalisierung und die Korrektheit zugrundegelegten Metadaten mit Hilfe von weiteren Quellen überprüft wurden, ist nicht ersichtlich. Noichl 2021, 5092 nutzt als Ausgangspunkt der Erstellung der zugrundeliegenden Datensammlung die Fachbibliographie „Philpapers“ (Bourget und Chalmers o. J.). Die dort verzeichneten 1.782.816 Aufsätzen werden in zwei Schritten auf eine Datensammlung von insgesamt 68.152 Aufsätzen reduziert, indem zunächst Zeitschriften ausgeschlossen werden, die nach Meinung des Autors nicht zum fachlichen Kern der Philosophie zu zählen sind, allerdings ohne die Anzahl der damit ausgeschiedenen Aufsätze anzugeben. Für die verbliebenen Zeitschriften werden die in der Zitationsdatenbank „Web of Science“ enthaltenen Texte abgefragt. Aufsätze, die nicht mindestens viermal in anderen in „Web of Science“ enthaltenen Aufsätzen zitiert werden, werden ausgeschlossen. Es verbleiben 87.720 Datensätze. Aus dieser Teilmenge werden alle Datensätze entfernt, die nicht mindestens drei Zitationen enthalten, die auch in anderen Aufsätzen angeführt werden. Damit umfasst die zu analysierende Datensammlung am Ende Metadatensätze zu 68.152 Aufsätzen. Die zeitliche Erstreckung des erfassten Schrifttums wird nicht angegeben. 

                
Festzuhalten ist zunächst, dass in beiden Analysen die verwendeten Datensammlungen nicht offen sind, die Ergebnisse somit nicht ohne weiteres überprüft bzw. reproduziert werden können. Dass als grundlegende Anforderung für die hier zu projektierende Datensammlung die Erfüllung der FAIR-Prinzipien zugrundezulegen ist, versteht sich eigentlich von selbst, soll aber hier dennoch ausdrücklich hervorgehoben werden.

                
Schöch 2017, 225 f unterscheidet weiter drei Modi der Festlegung von Datensammlungen: repräsentative Zufallsstichproben, "balancierte Sammlungen", in denen versucht wird, Objekte so auszuwählen, dass Kombinationen aller für die jeweilige Forschungsfrage einschlägigen Merkmale in der Sammlung vorhanden sind, sowie schließlich das Verfahren der ‚opportunistischen Auswahl‘, die die Verfügbarkeit von Daten als wesentliches Auswahlkriterium an erste Stelle setzt. 

                
Repräsentativität im statistischen Sinne setzt Bestimmung einer ‚Grundgesamtheit‘ voraus. Für Noichls Ziel, die ‚gegenwärtige Philosophie‘ als solche abzubilden ist eine solche Grundgesamtheit kaum konstituierbar. Selbst die auf Philpapers verzeichneten mehr als eine Million Aufsätze sind nicht als eine solche zu betrachten: Philosophie wird schließlich auch in Buchform publiziert. Auch aus inhaltlicher Sicht ist es zudem fraglich, ob die für die Bestimmung einer solchen Grundgesamtheit erforderliche Definition der Philosophie als Disziplin überhaupt möglich ist. Weitere Hindernisse für die Bestimmung der Grundgesamtheit selbst einer philosophischen Teildisziplin wie der Wissenschaftsphilosophie sind ebenfalls zu bedenken: selbst wenn es gelingen würde, ein solches Feld in operationalisierbarer Form einzugrenzen, müsste es auch in zeitlicher Hinsicht in einleuchtender Weise abgegrenzt werden. Dass der Beginn der Wissenschaftsphilosophie auf das Jahr 1934 festgelegt werden kann, ist aus fachlicher Sicht eine mit guten Gründen bezweifelbare Annahme.

                
Ob die von Malaterre et al. und Noichl vorgelegten Datensammlungen als ‚balanciert‘ gelten können, ist wohl ebenfalls eine strittige Frage. Wie man sie beantwortet, hängt davon ab, welche Merkmale als wesentlich für die behandelte Forschungsfrage anzusehen sind. Malaterre et al. gehen davon aus, dass nicht auf Englisch verfasste Texte für ihre Analyse vernachlässigbar sind bzw. der für deren Modellierung erforderliche Aufwand nicht notwendig ist.
 Die Sprache, in der ein Aufsatz abgefasst ist, wird also nicht als wesentliches Merkmal aufgefasst, sondern kann für die Untersuchung vernachlässigt werden. Noichls Daten lassen die diachrone Dimension außer acht, hier gilt also das Veröffentlichungsdatum sowohl des zitierenden wie des zitierten Textes nicht als wesentliches Merkmal, das für die Balance der zugrundegelegten Datensammlung erforderlich wäre.
                

            

            

                
Schlussfolgerungen

                
Aus diesen Befunden sind meines Erachtens zwei Schlussfolgerungen für die Ausgestaltung einer offenen Datensammlung für die Philosophie und Philosophiegeschichte zu ziehen: erstens sollte man sich wohl von dem Anspruch, mit einer Datensammlung die Disziplin als solche abzubilden, verabschieden. Ziel sollte vielmehr die Zusammenführung von Teildatensammlungen sein, die die Abhängigkeit von den sie leitenden Forschungsfragen offenlegen und damit auch das Gebiet bestimmen, innerhalb dessen aus den in ihnen enthaltenen Datensätzen valide Schlüsse gezogen werden können. Zweitens bedarf die Eingrenzung auf nur einen sprachlich-kulturellen Zusammenhang der forschungsbasierten Begründung und sollte nicht allein pragmatisch motiviert sein.

                
Nicht nur in methodischer, sondern auch in inhaltlicher Hinsicht kann man aus beiden Arbeiten jedoch wertvolle Hinweise erhalten, in welchen Hinsichten die Ergebnisse digitalbasierter Forschungen für die Philosophie relevant sein können. Dies betrifft zunächst die Unterteilung der akademischen Philosophie in Teildisziplinen, d. h. den Prozess ihrer Spezialisierung. Malaterre et al. legen eine solche Teildisziplin als ‚Analyseeinheit‘ zugrunde, nämlich die Wissenschaftsphilosophie. Noichl untersucht die Auffächerung der Philosophie in solche Spezialdisziplinen und -diskurse. Die Organisation von Forschungsdatensammlungen zur Philosophie und Philosophiegeschichte wird sich also auch an solchen Einheiten zu orientieren haben. 

                
Zugleich wird die Frage zu beantworten sein, ob, und wenn ja in welchem Sinne, sich solche subdisziplinären Einheiten von gesamtphilosophischen Traditionen abgrenzen lassen. Noichl diskutiert z. B. auch die Unterscheidung zwischen ‚analytischen‘ und ‚kontinentalen‘ Traditionen der Philosophie. ‚Kontinentale‘ Traditionen wie die der Phänomenologie werden sich jedoch kaum als Teildisziplinen definieren, sondern eher als Teiltraditionen der Philosophie. Noichls Analyse spiegelt dies, da zur Abgrenzung der kontinentalen Philosophie von anderen Teilbereichen ein Kanon zitierter Autor:innen herangezogen wird (Noichl 2021, 5094).

                
Mit dem von Noichl gewählten Werkzeug der Kozitationsanalyse lassen sich beide Dimensionen kaum voneinander abgrenzen. So wie Teildisziplinen Standardtexte zitieren, werden auch in philosophischen Traditionen gemeinsame Referenztexte als Bezugspunkte genutzt. Hier wird an einer inhaltlichen Analyse kein Weg vorbeiführen. Dass topic modeling in dieser Hinsicht eine hilfreiche Methode sein kann, zeigen Malaterre et al.: sie ermöglicht die Erschließung von Begriffskonstellationen und deren diachronen Verlauf. Während also die Identifikation von Traditionen und Schulbildungen wohl über die Betrachtung von Autor:innen und ihren Generationskohorten möglich sein dürfte, also durch Rekonstruktion von Kanonisierungs- und Dekanonisierungsprozessen von Personen, erlauben Verfahren der skalierten Erschließung von Inhalten Einblicke in die Kanonisierung und Dekanonisierung von Begriffen und deren Konstellationen.

            

            

                
Fazit: Anforderungen an philosophische Datensammlungen

                
Abschließend sollen die in diesem Beitrag entwickelten Anforderungen an eine Datensammlung digitaler Philosophie und Philosophiegeschichte kurz zusammengefasst werden. Digitale Philosophie benötigt Metadaten und Textdaten, die auffindbar, zugänglich, interoperabel und reproduzibel sind. Sowohl Textdaten als auch Metadaten sollten je nach Provenienz zumindest stichprobenhaft auf ihre Qualität hin überprüft werden. Die aus ihnen zu konstituierende Datensammlung sollte modular aufgebaut sein, um unterschiedlichen Forschungszielen, die den Teildatensammlungen zugrundeliegen, gerecht werden zu können. Datenquellen sind auf mögliche Verzerrungen und Auslassungen hin zu untersuchen. Diese sind, so weit sie sich pragmatisch aus dem zugrundeliegenden Forschungsziel der Teildatensammlung ergeben, zumindest explizit zu machen. Ein wichtiger Aspekt ist hierbei das Streben nach Multilingualität, um die globale Dimension der Philosophie angemessen abbilden zu können. Datensammlungen können dabei sowohl entlang von Teildisziplinen der Philosophie wie auch von Autor:innen, Epochen oder Traditionszusammenhängen konzipiert werden. Sie sollten es aber immer ermöglichen, auch die Entwicklung von Begriffen und Begriffskonstellationen - eines wesentlichen Mediums des Philosophierens - nachzuvollziehen. 

            

        

            

                
Hintergrund

                
Im Jahr 2012 entdecken Ryan Heuser und Long Le-Khac, dass eine Reihe von semantisch verwandten Wörtern in englischsprachigen Romanen über das 19. Jahrhundert hinweg immer häufiger verwendet wird (vgl. Heuser/Le-Khac 2012). Diese Wörter sind tendenziell konkret.
 Sie bezeichnen Körperteile wie 
                    
finger
 oder 
                    
hair
 oder Farben wie 
                    
red
 oder 
                    
scarlet
. Ted Underwood zeigt in einer späteren Untersuchung, dass dieser Trend bereits 1760 einsetzt. Er weist angesichts des mehrere Epochen umfassenden Anstiegs auf eine Lücke im bisherigen literaturgeschichtlichen Wissen hin.
                

                
Eine Masterarbeit, die dem Poster zugrunde liegt, verfolgte das Ziel, die Beobachtungen von Heuser, Le-Khac und Underwood zunächst versuchsweise für die deutschsprachige Literatur nachzuvollziehen und sodann eine erste Eingrenzung derjenigen Bereiche zu leisten, die von der Entwicklung betroffen sind. Die Ergebnisse sollen hier vorgestellt werden.

            

            

                
Korpus und Methode

                
Das Korpus basiert auf den bei TextGrid und Projekt Gutenberg digital zur Verfügung gestellten Texten (vgl. Neuroth u.a. 2015; Reuters o.J.). Es enthält 1147 zwischen 1760 und 1920 erschiene Romane. Diese sind jedoch nicht gleichmäßig über den Zeitraum verteilt (s. Figure 1). Die Unausgewogenheit soll in anschließenden Arbeiten angegangen werden.

                

                    

                    
Figure 1: Übersicht über die im Korpus enthaltenen Romane pro Jahr.

                

                
Um die Wortfrequenzen zu ermitteln, wurden zunächst Wortlisten erstellt. Als Heuristik diente eine sehr grobe Einteilung in drei Gruppen: 1) Informationen zu Figuren, 2) Informationen zu Räumen und 3) Informationen zu Beschaffenheit und Material. Auf Basis dieser Unterscheidung wurde eine Liste von 31 Wortfeldern erstellt.
 Anschließend wurden auf Basis von GermaNet Listen von zu diesen Wortfeldern gehörenden Wörtern erstellt (vgl. Henrich/Hinrichs 2010). Ein Beispiel für ein solches Wortfeld sind Farbwörter. Ausgehend vom Knoten 
                    
farbspezifisch
 wurden dessen Hyponyme – zum Beispiel 
                    
grün
 – und wiederum dessen Hyponyme – beispielsweise 
                    
dunkelgrün
 und 
                    
blassgrün
 – extrahiert.
                

                
Um der historischen Sprachstufe und der Domäne Roman gerecht zu werden, wurden die Wortlisten anschließend mit einem Word-Embedding-Modell erweitert. Dafür wurde ein auf CommonCrawl trainiertes Fasttext-Modell auf dem Roman-Korpus weitertrainiert (vgl. Bojanowski 2016). Aufgrund guter Performance in ähnlichen Tasks schien ein solches Fasttext-Modell ausreichend (vgl. Ehrmanntraut u.a. 2021). Um die Wortlisten zu erweitern, wurden zu den extrahierten Wörtern abhängig von der Länge der Liste die zwei bis zwanzig nächsten Nachbarn ermittelt und, sofern nicht schon vorhanden, der Liste angehängt. Neben Wörtern wie 
                    
grün
 oder 
                    
dunkelgrün 
enthielt die Liste nun auch sehr spezifische wie 
                    
fenchelgrün
. Anschließend wurden die Listen von Hand bereinigt. 
                

                
Die Romane wurden mit dem Python-Paket spaCy lemmatisiert und für jedes Wortfeld die zugehörigen Wortfrequenzen berechnet (vgl. Honnibal/Montani 2017). 

            

            

                
Ergebnisse

                
Nimmt man die Wortfrequenzen aller 31 konkreten Wortfelder zusammen, ergibt sich ein signifikanter Anstieg (Mann-Kendall-Test, α=0,01, p=2,22e-16). In Figure 2 repräsentiert jeder Punkt einen Roman, die y-Achse gibt jeweils die Wortfrequenz an. Im Korpus gibt es also einen ähnlichen Trend wie in den englischsprachigen Texten.

                

                    

                    
Figure 2: Frequenzen der konkreten Wörter pro Roman und Jahr.

                

                
Insbesondere bei den Wortfeldern, die Figuren, Gebäude und Innenräume beschreiben, gibt es signifikante Anstiege. Bei den Wortfeldern, die der Darstellung von Naturräumen (z.B. die Wortfelder 
                    
Gartenanlage, Gewächs, Bewaldung
) dienen dürften, gibt es dagegen keine signifikanten Trends. Figure 3 gibt einen Überblick über die Ergebnisse zu den untersuchten Wortfeldern (α=0,01).
                

                
                    

                        

                            

                            
Anstieg

                            
Rückgang

                            
Kein Trend

                        

                        

                            
1) Räume

                            

                                
Bau/Gebäude

                                
Gebäudeteil

                                
Zimmer

                                
Einrichtungsgegenstand/

                                
Möbel

                                
Heimtextilie

                                
Haushaltsgegenstand/

                                
Haushaltsprodukt

                                
Dekorationsgegenstand/

                                
Ziergegenstand

                                
Gras/Grünfläche

                            

                            
Pflanzenteil

                            

                                
Gartenanlage/ Grünanlage

                                
Weg

                                
Gewächs/Pflanze

                                
Bewaldung/Wald

                                
Kunstobjekt

                                
Bild

                            

                        

                        

                            
2) Figuren

                            

                                
Körperteil

                                
Bekleidung/Kleidung

                                
Bekleidungsteil/

                                
Kleidungsteil

                                
Aufmachung/Outfit

                                
Aussehensspezifisch

                                
Tasche

                            

                            

                            

                                
Geschmeide/Schmuck

                            

                        

                        

                            

                                
3) Beschaffen-heit

                            

                            

                                
Gewebe/Stoff/Textil

                                
Farbspezifisch

                                
Helligkeitsspezifisch

                                
Oberflächenspezifisch

                                
Muster/Musterung

                                
Formspezifisch

                                
Geruch

                            

                            

                            

                                
Ornament/Verzierung

                                
Holz

                            

                        

                    
Figure 3: Übersicht über die Trends für alle untersuchten Wortfelder.

                    

                
            

            

                
Fazit

                
Die Ergebnisse der Arbeit deuten auf eine grundlegende Veränderung im untersuchten Korpus hin, die von der germanistischen Literaturgeschichte bislang nicht wahrgenommen wurde: Die Art und Weise, in der Romane ihre fiktive Welt physisch gestalten, wandelt sich über einen mehrere Epochen umfassenden Zeitraum hinweg erheblich. Zudem ermöglichen die Ergebnisse der Arbeit eine erste Differenzierung. Besonders betroffen scheinen Informationen über das physische Erscheinungsbild von Figuren und Orten, an denen Figuren leben. Eine anschließende Arbeit soll diese Ergebnisse konkretisieren.

            

        

            

                
Einleitung

                
Nicht nur in Kultur- und Gedächtnisinstitutionen, auch in DH-Projekten ist derzeit eine Zunahme des Linked Open Data-Paradigmas sichtbar. Wie können Daten im Sinne von „Open Data, Open Cultures“ offen, gut zugänglich, interoperabel vernetzt, maschinenlesbar und langfristig verfügbar dargeboten werden? Im Projekt „
                    
Mining and Modeling Text
“ haben wir uns für die offene und kostenlose Software Wikibase entschieden, die einen eigenen 
                    
SPARQL-Endpoint
 beinhaltet und neben Wikidata von einer wachsenden Anzahl an Forschungsprojekten verwendet wird.

                

                
Der Workshop setzt es sich zum Ziel, theoretisches und praktisches Wissen zur Modellierung geisteswissenschaftlichen und speziell literaturgeschichtlichen Wissens in Form von Linked Open Data (LOD) zu vermitteln, Einblick in die Syntax der Abfragesprache SPARQL zu geben und den Mehrwert der Aufbereitung von Daten als Wissensgraphen in Anwendungsszenarien aufzuzeigen. Dabei liegt der Schwerpunkt auf der Vermittlung von SPARQL in theoretischen und praktischen Sessions. Teilnehmende sollen die Kompetenz erlangen, die Struktur von SPARQL zu verstehen und eigenständig Queries zu schreiben.

                
                    

                        

                    
Abb. 1: Query und Ergebnisvisualisierung: Welche thematischen Konzepte sind im französischen Roman 1751-1800 vertreten? Beispiel: 
                    
.

                

                
            

            

                
Linked Open Data für die Geisteswissenschaften

                
Es ist zu beobachten, dass es ein zunehmendes Interesse in der DH-Community gibt, die eigenen Daten in Form von LOD zu veröffentlichen und mit dem Semantic Web zu vernetzen oder die aktuellen Entwicklungen zu reflektieren (Hogan et al. 2021; Ikonić Nešić et al. 2021; Thornton et al. 2021; Alves 2022; Dörpinghaus 2022; Ohmukai / Yamada 2022; Zhao 2022). Auch das Projekt „
                    
Mining and Modeling Text
“ hat es sich zum Ziel gesetzt, Daten aus unterschiedlichen Informationsquellen zu aggregieren und im Sinne des LOD-Paradigmas mit weiteren Ressourcen zu verknüpfen (Schöch et al. 2022). Der Mehrwert der aufwändigen Erschließung und Modellierung der Daten wird erst in den vielfältigen und flexiblen Abfragemöglichkeiten deutlich und ist demnach nicht loszulösen von SPARQL.
                

                
SPARQL (SPARQL Protocol and RDF Query Language) ist eine 2008 vom W3C veröffentlichte, graphenbasierte Abfragesprache für RDF (Resource Description Framework). RDF ist ein Datenmodell, mit dem sich Ressourcen im World Wide Web darstellen lassen. Es ist der zentrale Standard des W3C, der semantische Daten in der charakteristischen Tripel-Struktur bestehend aus ‚Subjekt – Prädikat – Objekt‘ repräsentiert. Ausgehend von einem einzelnen solchen Tripel wird die Struktur eines Knowledge Graphen im Workshop entfaltet und die „Übersetzung“ von Forschungsfragen in natürlicher Sprache in die SPARQL-Syntax erläutert. 

                
                    

                        

                    
Abb. 2: Tripel-Struktur bestehend aus ‚Subjekt – Prädikat – Objekt‘, hier ein Beispiel zu einem literarischen Werk (Candide) aus der MiMoTextBase: 
                    
.

                
                
Die Abfragesprache SPARQL setzt sich aus mehreren Bausteinen zusammen: 
                    
pattern matching
 (Filtern des Datenbestands), 
                    
solution modifier
 (Bearbeitung der Zwischenergebnisse) &amp; 
                    
output
 (Ausgabe als Tabelle oder Graph; Arenas et al. 2010). SPARQL ermöglicht es User:innen, durch Rekombination von Datensätzen neue Muster in den Daten zu erkennen und hypothesengeleitete Abfragen zu formulieren. Die Stärken dieser Abfragesprache und der Strukturierung von Wissen als RDF Triplestore sollen im Workshop in Anwendungsbeispielen gezeigt werden.
                

                
SPARQL-Abfragen werden häufig innerhalb eines einzelnen Knowledge Graphen gestellt. Es besteht jedoch auch die Möglichkeit, über mehrere Knowledge Graphen hinweg Abfragen zu stellen, sogenannte 
                    
federated queries 
(Prud’hommeaux / Buil-Aranda 2013). Hier kommt das volle Potential von LOD zum Vorschein, denn so lässt sich Erkenntnisgewinn aus der Kombination mehrerer Graphen ziehen, ohne durch Replikationen von Datensätzen unnötige Redundanzen zu erzeugen. Im Workshop werden 
                    
federated queries 
mit Wikidata erlernt und es wird gezeigt, inwieweit Nutzen aus diesen gezogen werden kann.
                

                

                    

                        

                    
Abb. 3: 
                    
Federated queries
 in SPARQL erlauben es, eigene Daten und externe Datenbestände (z. B. Wikidata) gleichzeitig abzufragen. Beispiel: 
                    
.

                

            

            

                
Workshop-Konzept

                
Der Workshop vermittelt Grundlagenwissen und Möglichkeiten, die das LOD-Paradigma bietet. Der im Projekt erstellte multilinguale Wissensgraph MiMoTextBase zur Domäne der französischen Literatur des 18. Jahrhunderts soll dabei als Anschauungsbeispiel dienen.

                

                    
Lernziele

                    
Der Workshop möchte praktisches Wissen vermitteln: Wie schreibt man SPARQL-Queries? Welchen Mehrwert kann ein Knowledge Graph für literaturgeschichtliche Fragen im Besonderen und die Geisteswissenschaften im Allgemeinen bieten?

                    
In dem halbtägigen Workshop wird der Wissensgraph 
                        
MiMoTextBase
 (Hinzmann et al. 2022a) des Projekts „Mining and Modeling Text“ vorgestellt und es werden Anwendungsszenarien formuliert und visualisiert. Dazu werden Grundlagen der Abfragesprache SPARQL erlernt und eigene Queries formuliert. Wir arbeiten beispielhaft auf dem projektinternen Knowledge Graphen sowie auf Wikidata und werden über 
                        
federated queries
 die Verbindung mehrerer Wissensgraphen demonstrieren.
                    

                    
Konkrete Lernziele sind: Erwerb von Grundlagenwissen zu Semantic Web und RDF, LOD, Wikidata Graph; vertiefte Kenntnisse zu SPARQL und die praktische Fähigkeit, eigene SPARQL-Queries zu formulieren; Kennenlernen der Software Wikibase und Exploration der Visualisierungsmöglichkeiten des SPARQL-Endpoints. 

                

                

                    
Zielpublikum und Anforderungen

                    
Der Workshop wendet sich an digitale Geisteswissenschaftler:innen mit Interesse an LOD und SPARQL. Spezielle Vorkenntnisse sind nicht notwendig. Teilnehmende benötigen einen Laptop.

                

                

                    
Struktur / Ablauf

                    
Der Workshop setzt sich aus aufeinander aufbauenden Sessions zusammen, die jeweils Input-Phasen und Übungsphasen verbinden. Es wird vorab eine ausführliche Tutorial-Seite (inklusive Verlinkung auf weitere hilfreiche Ressourcen zum SPARQL-Lernen) zur Verfügung gestellt, die den Teilnehmenden (und allen weiteren Interessierten) in der Vorbereitung sowie zur Vertiefung nützlich sein kann (Hinzmann et al. 2022b).

                    
Im Zentrum des Workshops stehen drei Blöcke mit jeweils unterschiedlichem Schwerpunkt, in denen das Formulieren von SPARQL-Queries geübt wird (vgl. für Details den Ablauf im Appendix). Auch Teilnehmende ohne Vorkenntnisse werden schrittweise an zunehmend komplexere Queries herangeführt. Der Schwierigkeitsgrad wächst innerhalb der einzelnen Blöcke, wobei der Fokus auf dem eigenständigen Formulieren sowie Anpassen von Beispiel-Queries und dem Klären aller dabei auftretenden Fragen liegen wird.

                    
1. Im ersten Teil liegt der Fokus auf Abfragen zu literarischen Werken. Im Hinblick auf SPARQL geht es hier zunächst um die zentralen Grundlagen wie das Schreiben einfacher 
                        
triple patterns
 und Möglichkeiten der Kombination mehrerer 
                        
triple patterns
 zu zunehmend komplexeren Queries. Der Mehrwert, der sich aus solchen Kombinationsmöglichkeiten ergibt, wird mit dem durch die MiMoTextBase gegebenen Fokus auf den französischen Aufklärungsroman besonders deutlich.
                    

                    
2. Im zweiten Teil widmen wir uns Wikidata als größtem öffentlichen Wissensgraphen, der sich zugleich als ‚Hub‘ begreifen lässt (Neubert 2017), und fokussieren Autor:innen als Entitäten. Autor:innen sind in allen geisteswissenschaftlichen Disziplinen relevant und ein wichtiges Scharnier zwischen verschiedenen Wissensgraphen. Bezogen auf die SPARQL-Syntax gehen wir einen Schritt weiter und integrieren Funktionen wie OPTIONAL und FILTER, um das Spektrum der Abfragemöglichkeiten zu erweitern. Ein Einstieg wird hier mit Queries zu Literat:innen der MiMoText-Domäne gemacht. Im nächsten Schritt können die Teilnehmenden die Daten von Autor:innen in ihrer jeweiligen Domäne in Wikidata explorieren.

                    
3. Der dritte Teil verknüpft die beiden vorigen Teile auf mehreren Ebenen. Der Schwerpunkt liegt auf 
                        
federated queries
, wobei wir uns auf Abfragen, die sich über die MiMoTextBase und Wikidata erstrecken, konzentrieren werden. Die genauere Betrachtung von Autor:innen des 2. Teils wird hier fortgesetzt und vertieft. In diesem abschließenden Teil wird der Mehrwert von Standards und geteilten Datenmodellen (Ontologien bzw. 
                        
entity schemata
) sowie die Verknüpfung von Ressourcen besonders deutlich.
 Alle Autor:innen der MiMoTextBase, für die es auch Wikidata-Items gibt, können mit diesen über die Property 
                        
exact match
 verknüpft werden, wodurch zusätzliche Informationen bereitstehen und diverse Abfragemöglichkeiten eröffnet werden.
 Die Wikibase-Infrastruktur bietet außerdem vielfältige Explorationsmöglichkeiten, die beispielhaft eingeführt werden (
                        
marker cluster
 für Geo-Daten, Timelines für Geburtsdatum u. ä.).
                    

                    
Es soll in der abschließenden Diskussion auch Raum sein, einen kritischen Blick auf Entwicklungen im Bereich des Semantic Web zu werfen, beispielsweise die Frage, welche Monopolisierungskräfte und Marktkräfte Einfluss nehmen (van Hooland / Verborgh 2014, 247–48; Singhal 2012). Zum Abschluss werden die wichtigsten Anwendungsmöglichkeiten und Fragen zusammengetragen und weiterführende Ressourcen (DuCharme 2013; van Hooland / Verborgh 2014; Lincoln 2015; Blaney 2017) sowie bei Interesse Möglichkeiten der Kooperation thematisiert.

                

            

            

                
Appendix 

                

                    
Ablauf (4 Stunden)

                    

                        

                            
10 Min.

                            
Begrüßung (Vorstellung evtl. über Mentimeter) 

                        

                        

                            
20 Min. 

                            
Einleitung: Input zu Semantic Web, RDF, LOD für die Literaturgeschichte am Beispiel des Projekts Mining and Modeling Text. Wikidata &amp; Wikibase Ecosystem, Mehrsprachigkeit des Graphen.

                        

                        

                            

                            
SPARQL Teil 1 (MiMoTextBase)

                        

                        

                            
20 Min.

                            
(a) Input zu SPARQL-Grundlagen (interaktive Phase), SPARQL-Syntax, Möglichkeiten der Datenvisualisierung in Wikibase, Debugging &amp; Help.

                        

                        

                            
35 Min.

                            
(b) Praxis-Teil: Anpassen vorhandener und Formulieren einfacher, eigener SPARQL-Queries auf der MiMoTextBase (Breakout Session bzw. Gruppenarbeit).

                        

                        

                            
15 Min.

                            
Pause

                        

                        

                            

                            
SPARQL Teil 2 (Wikidata)

                        

                        

                            
20 Min.

                            
(a) Input: Erweiterte Elemente der SPARQL-Syntax wie OPTIONAL und FILTER; Datenmodell für Autor:innen auf Wikidata.

                        

                        

                            
35 Min.

                            
(b) Praxis: Formulieren etwas komplexerer Queries auf Wikidata. 

                        

                        

                            
15 Min.

                            
Pause

                        

                        

                            

                            
SPARQL Teil 3 (
                                
Federated queries
)
                            

                        

                        

                            
20 Min.

                            
(a) Input: Fortgeschrittene SPARQL-Queries: 
                                
Federated queries
, 
                                
prefixes
 definieren, 
                                
marker cluster
 etc.
                            

                        

                        

                            
35 Min.

                            
(b) Praxis: 
                                
Federated queries
 etc. anwenden.
                            

                        

                        

                            
15 Min.

                            
Abschließende Diskussion, Empfehlung weiterführender Ressourcen zur Vertiefung des Gelernten.

                        

                    

                

                

                    
Organisatorisches

                    
Maximale Zahl der Teilnehmenden: 25. Wir benötigen einen Raum mit WLAN und Beamer und bieten gern ein Hybrid-Szenario an. 

                

                

                    
Fördernachweis

                    
„Mining and Modeling Text“ (Universität Trier, Trier Center for Digital Humanities) wird von der Forschungsinitiative des Landes Rheinland-Pfalz 2019-2023 gefördert. 

                

                

                    
Beitragende

                    
Der Workshop wird von Mitarbeiter:innen des LOD-Projekts „Mining and Modeling Text“ durchgeführt. Das interdisziplinäre Projekt verfügt über einen eigenen SPARQL-Endpoint und wurde in Wikibase implementiert. 

                    
Maria Hinzmann; hinzmannm@uni-trier.de; Trier Center for Digital Humanities, Universität Trier | Historisches Seminar: Digital Humanities, Bergische Universität Wuppertal; Forschungsinteressen: Datenmodellierung, LOD, Textanalyseverfahren.

                    
Anne Klee; klee@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: Digitale Textverarbeitung; Digitale Lexikographie.

                    
Johanna Konstanciak; konstanciak@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: Digitale Textverarbeitung; XML/Web-Technologien.

                    
Julia Röttgermann; roettger@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: LOD, Textmining-Verfahren wie Topic Modeling, NER und Sentiment Analysis.

                    
Christof Schöch; schoech@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: Computational Literary Studies.

                    
Moritz Steffes; steffesm@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: Softwaresysteme, Semantic Web Technologien, Forschungsinfrastrukturen.

                

            

        

            
Automatisierte Handschriftenerkennung fokussierte in der Vergangenheit vorwiegend auf Training und Erkennung einzelner Handschriften, die individuell trainiert wurden. Typischerweise finden sich in den Geisteswissenschaften jedoch Datensätze, die von mehreren Händen und häufig auch über längere Zeiträume verschriftlicht wurden. Aussagen zu Erkennalgorithmen müssen entsprechende Voraussetzungen ernst nehmen und nicht nur die Fähigkeit nachweisen, einzelne Hände mit hoher Qualität zu erkennen, sondern auch mit ähnlichen jedoch nicht identischen Handschriften umzugehen.

            
Ein umfangreicher frühneuzeitlicher Briefwechsel aus dem 16. Jahrhundert, der hauptsächlich in zwei Sprachen (Latein und Frühneuhochdeutsch) vorliegt und mehrere hundert Hände umfasst, ist diesbezüglich ein interessanter Vergleichsfall, um bestehende Plattformen mit neuen Möglichkeiten der 
                
Data Augmentation
 und dem Einbezug von Transformer-basierten neuronalen Netzen zu vergleichen. Wir nutzen dafür die im Rahmen des Projekts 
                
Bullinger Digital
 erarbeiteten Daten, um Aussagen über Erkennqualität sowie Potenziale in der automatischen Erkennung zu machen. Für die Evaluation verwenden wir zwei spezifisch erstellte Testsets. Diese widerspiegeln die Tatsache, dass von einigen (wenigen) Personen eine Vielzahl von Briefen und von diversen Personen nur wenige Schreiben im Datensatz vorhanden sind (Abbildung 1). Die beiden Testsets sind zwar ähnlich groß (
                
Viel-
 (1’235 Zeilen) als auch 
                
Wenigschreiber
 (1’013 Zeilen)), leisten aber Aussagen zu sehr unterschiedlichen Größenordnungen, da von den Vielschreibern eine weit umfangreichere Masse erkannt werden wird, aber auch mehr Material zum Training von Modellen zur Verfügung steht. Bei der Evaluation können wir aufgrund der Aufteilung aber präzisere Voraussagen machen, welche Datenmassen mit welcher Qualität erkannt werden. Weiter lässt sich abschätzen, welche Erkennungsform sich für welche (Trainings-)Datenmenge eignet.
            

            

                

                
Abbildung 1. Verteilung der Top-20-Autoren, welche Briefe an Bullinger schrieben (Anzahl Briefe auf y-Achse).

            

            
Im Folgenden testen wir drei Ansätze, die Aufschlüsse zum Stand und möglichen Entwicklungen im Bereich der Handschriftenerkennung versprechen, am Korpus:



                

                    
Transkribus
 mit seinen zwei Engines (HTR+ (Strauss et al. 2018) und PyLaia (Puigcerver 2017)) testet eine etablierte Methode am Datensatz.
                

                

                    
Data Augmentation
 soll die Erkennung mit der State-of-the-Art-Engine HTR-Flor (de Sousa Neto et al. 2020) verbessern.
                

                
Neue Transformer-basierte Modelle (Li et al. 2021) sollen auf ihre Tauglichkeit für historische Daten geprüft werden.

            

            
Seit 1974 erscheinen in regelmäßigen Abständen Bände der Heinrich-Bullinger-Briefwechsel-Edition, erarbeitet am Institut für Schweizerische Reformationsgeschichte (IRG) der Universität Zürich. Der letzte Band stammt von 2022 und weitere sind aktuell in Arbeit (Bullinger 2022). Die Edition bedient diverse Nutzer*innengruppen, insbesondere Theolog*innen und die Geschichtswissenschaften, was sich etwa an den Editionsgrundsätzen (Bullinger 1973) zeigt, z. B. stillschweigende Auflösung von Abkürzungen, Identifikation von Personen und Orten etc.

            
Um die Vielzahl der nicht transkribierten Briefe schneller maschinell zu verarbeiten, arbeiten wir im Rahmen des Projekts 
                
Bullinger Digital
 an der automatisierten Aufbereitung der Dokumente. Die Texte der bereits edierten Briefe sowie der am IRG provisorisch transkribierten Briefe haben wir mittels des 
                
Text-to-Image-Verfahrens
 (Leifert, Labahn, and Sánchez 2020) mit den Bildvorlagen automatisiert aligniert. Damit steht nun ein Korpus von 1’297’908 Token für Training und Validierung zur Verfügung.

            

            
Bei der Zuordnung der Zeilen wurde mit einem 
                
Threshold
 agiert, der dafür sorgte, dass nur transkribierte Textteile zugeordnet wurden, bei denen eine relativ hohe Wahrscheinlichkeit der Übereinstimmung errechnet wurde. Dadurch wurden zwar ca. 25-30% aller Zeilen nicht zugeordnet, Fehler in der Layouterkennung und nicht identifizierte Streichungen und Ähnliches führten aber nicht zu falschen Zuordnungen, die wiederum negativen Einfluss auf das Training von Texterkennungsmodellen hätte.
            

            

                
Texterkennung mit etabliertem Framework und Plattform: Transkribus

                
2015 wurde die Plattform 
                    
Transkribus
 gelauncht und danach von 2016 bis 2019 
                    
Deep-Learning-
basierte Erweiterungen, insbesondere Layout- und Texterkennungsengines (Muehlberger et al. 2019) implementiert. Die Plattform wird von einer Kooperative betrieben und für diverse Zwecke in der Forschung und durch Erinnerungsinstitutionen eingesetzt. Training von Handschriftenmodellen und Erkennung neuer Seiten erfolgen über ein GUI.
                

                
Basierend auf den oben erwähnten Trainings- und Testdaten erzeugten wir mehrere Handschriftenmodelle, dabei testeten wir jeweils beide verfügbaren Engines (HTR+ und PyLaia), um Unterschiede in der Qualität aufzuzeigen. Wir übernahmen die Layouterkennung unverändert. 

                
Die in Transkribus trainierten Modelle (Tabelle 1) zeigen gute Ergebnisse. Gerade für Vielschreiber wird trotz vieler unterschiedlicher Hände insbesondere mit HTR+ eine Erkennung unter 8% 
                    
Character Error Rate
 erreicht.
 Die Erkennung der Wenigschreiber ist dagegen etwas fehlerbehafteter. Alle Modelle wurden “austrainiert”, ohne dass ein problematisches 
                    
Overfitting
 beobachtet wurde (Hodel 2020).
                

                
Eine signifikante Verbesserung erreichten wir durch die Nutzung von grossen Modellen als 
                    
Basemodels
. Mit einem vortrainierten Modell basierend auf 5’820’990 Wörtern (im Sinne von 
                    
Tokens
) in lateinischer Schrift (keine Kurrentschrift), kann die Erkennqualität weiter verbessert werden.
                

                

                    

                        

                        

                        
CER

                    

                    

                        

                        

                        
Vielschreiber

                        
Wenigschreiber

                    

                    

                        
Trainingsdaten

                        

                            
base model

                        

                        
HTR+

                        
PyLaia

                        
HTR+

                        
PyLaia

                    

                    

                        
multilingual

                        
-

                        
7.26

                        
9.9

                        
10.06

                        
12.7

                    

                    

                        

                        
Latin

                        
6.79

                        
-

                        
9.51

                        
-

                    

                    
Tabelle 1. Resultate der sprachunabhängig trainierten Modelle in Transkribus mit HTR+ und PyLaia. HTR+: 500 Epochen, PyLaia: 250 Epochen.

                

            

            

                
Data Augmentation zur Erweiterung des Trainingsmaterials

                
Um für einen gegebenen Schreibstil ein zuverlässiges Erkennungssystem zu trainieren, braucht es eine große Anzahl annotierter Textzeilen. Es ist deshalb schwierig, seltene Hände automatisch zu transkribieren, da der entsprechende Schreibstil nicht oder nur ungenügend in den Trainingsdaten repräsentiert ist. 
                    
Data Augmentation
 kann verwendet werden, um die Trainingsdaten automatisch mit weiteren Beispielen anzureichern und so den Lernprozess zu unterstützen. In unserer Arbeit verfolgen wir dazu einen vielversprechenden Ansatz: Wir lernen die Schreibstile von seltenen Händen mittels 
                    
Generative Adversarial Networks (GANs)
, um anschließend beliebige Texte zu synthetisieren und den Trainingsdaten als zusätzliche Lernbeispiele hinzuzufügen.
                

                
Für die Synthetisierung wird 
                    
lineGen
 (Davis et al. 2020) eingesetzt, ein kürzlich vorgeschlagenes GAN-Netzwerk, welches auf ganzen Textzeilen arbeitet und drei Zielfunktionen integriert: den 
                    
Adversarial Loss
 (Unterscheidung zwischen echten und synthetischen Bildern), den 
                    
Perceptual Loss 
(visuelle Qualität der generierten Bilder) und den 
                    
CTC Loss
 (Qualität der automatischen Transkription). Das trainierte lineGen-System erlaubt es, einen beliebigen Text mit dem gelernten Stil zu synthetisieren, d. h. aus dem Text ein Textzeilenbild zu generieren. Abbildung 2 zeigt Beispiele von generierten Textzeilenbildern, wenn lineGen auf rund 17’000 Textzeilenbildern unterschiedlich lange und unter Verwendung der Standard-Hyperparameter mit verschiedenen Schreibstilen trainiert wird und im Anschluss ein englischer Text synthetisiert wird. Das GAN konvergiert auf einen Schreibstil, der ähnlich leserlich ist wie echte Beispiele aus den Trainingsdaten.

                

                

                    

                    
Abbildung 2. Beispiele von synthetischen Textzeilenbildern basierend auf lineGen über unterschiedliche Anzahl Epochen.

                

                
Der Einfluss der synthetischen Lernbeispiele auf die Erkennungsrate wurde in zwei Szenarien untersucht, welche eine kleine Trainingsmenge vorsehen, wie es für Wenigschreiber typisch ist: In Szenario A gehen wir von 1’000 Trainingszeilen aus und in Szenario B von 200 Trainingszeilen. Als Erkennungssystem wird HTR-Flor (de Sousa Neto et al. 2020) eingesetzt, eines der besten Systeme nach aktuellem Stand der Technik, und für die Auswertung werden rund 1’000 zufällig ausgewählte Textzeilen als Testdaten verwendet. Abbildung 3 zeigt den Trainingsverlauf für Szenario A unter Verwendung der Standard-Hyperparameter für HTR-Flor. Die CER auf den Testdaten erreicht 26.8% für das Training mit 1’000 echten Textzeilen. Wenn mit 1’000 synthetischen Textzeilen trainiert wird, scheitert das Lernen. Eine mögliche Interpretation ist, dass die synthetische Schrift nicht genügend natürliche Variation beinhaltet, welche fürs Trainieren nötig ist. Wenn aber die 1’000 echten mit den 1’000 synthetischen Textzeilen kombiniert werden, kann die CER signifikant auf 24.1% verbessert werden. Abbildung 4 zeigt den Trainingsverlauf für Szenario B. Hier scheitert das Lernen sowohl mit 200 echten als auch mit 200 synthetischen Textzeilen. Hingegen führt die Kombination von echten und synthetischen Trainingszeilen erneut zu einer signifikant verbesserten CER von 47.7%.

                

                

                
Abbildung 3. Training von HTR-Flor während 75 Epochen im Szenario A (1‘000 Trainingszeilen).

                

                

                

                
Abbildung 4. Training von HTR-Flor während 75 Epochen im Szenario B (200 Trainingszeilen).

                

                
Diese initialen experimentellen Resultate sind vielversprechend und legen die Vermutung nahe, dass GAN-basierte synthetische Lernbeispiele dabei helfen können, die Erkennung für Wenigschreiber zu verbessern. In einem nächsten Schritt planen wir umfangreichere Experimente, welche darauf abzielen, den Stil von Wenigschreibern zu lernen oder den Stil von Gruppen ähnlicher Schriftbilder. Eine wichtige Fragestellung dabei wird sein, wie viele Lernbeispiele nötig sind, um einen Stil zuverlässig lernen zu können. Anschließend planen wir, ein 
                    
Transfer Learning
 durchzuführen, ausgehend von einem gut trainierten Grundsystem für Vielschreiber, welches mit Hilfe der synthetischen Lernbeispiele an die Wenigschreiber angepasst wird.
                

            

            

                
Nutzung einer Transformer-basierten Architektur

                
Transformer-basierte Architekturen (Vaswani et al. 2017) haben das Feld der natürlichen Sprachverarbeitung in Sachen Sprachmodellierung revolutioniert und zu einem Paradigmenwechsel beim Trainieren von Systemen für unterschiedlichste Anwendungen geführt. Transformer-Modelle wie BERT (Devlin et al. 2018) und deren Weiterenwicklungen, welche auf großen Mengen an Sprachdaten trainiert wurden, sind als starke Transfer-Lerner (Ruder et al. 2019) bekannt und erlauben einen vielfältigen Einsatz beim Fine-Tuning für spezifische Aufgaben (
                    
Natural Language Understanding
, Fragenbeantwortung, Wortarten-Klassifikation). Transformer können auch für Bildverarbeitung genutzt werden (Dosovitskiy et al. 2021; Touvron et al. 2021), was die Entwicklung von BERT-ähnlichen und auf großen Bildmengen vortrainierten Modellen zur Folge hatte (Bao, Dong, and Wei 2021).
                

                
Für unser Projekt nutzen wir als Grundlage 
                    
TrOCR
 (Li et al. 2021), welchem ein 
                    
Encoder-Decoder
-Struktur zugrunde liegt. Bildseitig beruht der Encoder auf der BEiT-Architektur (Bao, Dong, and Wei 2021), welche für die 
                    
Feature
-Extraktion aus den Bildern verantwortlich ist, während der 
                    
Decoder
 die Bildinformation mit Hilfe eines RoBERTa-Sprachmodells (Liu et al. 2019) in eine 
                    
Subword
-Folge “übersetzt”. Li et al. benutzten in zwei 
                    
Pre-Training
-Phasen über 700 Millionen an synthetisch erzeugten und echten Textzeilenbilder mit dem dazugehörigen Text in englischer Sprache, die danach mit dem IAM-Datenset (Marti and Bunke 2002) einem 
                    
finetuning
 unterzogen wurden. Die CER von 2.89% liegt nur 0.14 Prozentpunkte hinter einem klassischen Ansatz (Diaz et al. 2021).
                

                
Im Gegensatz zu den beiden anderen Projektteilen wurde für die TrOCR-Anwendung die Zeilen nach Sprachen unterschieden. Dies Sprachverteilung und einige weitere Kennzahlen können Tabelle 2 entnommen werden.

                

                    

                        

                        
# Zeilen

                        
%

                        
# Worte

                        
# Wörter / Zeile

                        
# Buchstaben

                        
# Buchstaben / Zeile

                    

                    

                        
Latein

                        
134’236

                        
81.02

                        
1’073’106

                        
7.99

                        
7’314’648

                        
54.49

                    

                    

                        
FNHD

                        
31’437

                        
18.98

                        
253’372

                        
8.06

                        
1’547’725

                        
49.23

                    

                    

                        
Total

                        
165’673

                        

                        
1’326’478

                        
8.01

                        
8’862’373

                        
53.49

                    
                
                    
Tabelle 2. Kennzahlen der extrahierten Zeilen aus dem Material der Bullinger-Briefe.

                    

                
Wir untersuchen die Eignung von TrOCR für die Texterkennung auf historischen Daten, indem wir die Anzahl Epochen, die für das Fine-Tuning aufgewendet werden, variieren und multi- wie auch monolinguale Modelle trainieren. Alle Modelle führen während eines Drittels der Epochenzahl ein 
                    
Warm-Up
 durch.
 Für die Evaluation haben wir auch die Testsets nach Sprachen aufgeteilt. Tabelle 3 fasst die Ergebnisse zusammen.
                

                

                    

                        

                        

                        
CER

                    

                    

                        

                        

                        
Vielschreiber

                        
Wenigschreiber

                    

                    

                        
Trainingsdaten

                        
# Epochen

                        

                            
multiling.

                        

                        
Latein

                        
FNHD

                        

                            
multiling.

                        

                        
Latein

                        
FNHD

                    

                    

                        
multilingual

                        
1

                        
9.53

                        
9.49

                        
9.7

                        
11.58

                        
11.07

                        
12.79

                    

                    

                        

                        
2

                        
8.38

                        
8.51

                        
7.71

                        
10.46

                        
10.18

                        
11.11

                    

                    

                        

                        
3

                        
7.81

                        
8.07

                        
6.5

                        
9.95

                        
9.61

                        
10.74

                    

                    

                        

                        
4

                        
7.21

                        
7.61

                        
5.27

                        
9.68

                        
9.31

                        
10.55

                    

                    

                        

                        
5

                        
7.31

                        
7.85

                        
5.25

                        
9.69

                        
9.54

                        
10.25

                    

                    

                        

                        
6

                        
7.41

                        
7.89

                        
5.09

                        
9.61

                        
9.39

                        
10.11

                    

                    

                        
Latein

                        
5

                        
11.09

                        
7.99

                        
26.21

                        
16.1

                        
9.69

                        
31.21

                    

                    

                        
FNHD

                        
5

                        
28.41

                        
33.02

                        
6.06

                        
27.89

                        
34.83

                        
11.43

                    

                    
Tabelle 3. Resultate der Modelle trainiert auf verschiedenen Sprachzusammenstellungen und Anzahl Epochen (fett = beste Performance in einer Spalte).

                

                
                
Aufgrund der Resultate entschieden wir uns, die monolingualen Modelle auf 5 Epochen zu trainieren. Auf die frühneuhochdeutschen Zeilen der Vielschreiber angewandt liefert das monolinguale Modell mit einer CER von 6.06% entgegen der Erwartungen nicht das beste Resultat. Dieses erreichte ein multilinguales Modell (trainiert über 6 Epochen) mit einer CER von 5.09%, also knapp einem Prozentpunkt Unterschied. Die größere Menge an Bilddaten beeinflusst während des Fine-Tunings merklich die Performanz.

                
Unsere Experimente zeigen, dass Transformer-basierte HTR für historische Daten CERs in akzeptablen Bereichen liefert, ohne dass TrOCR bis zu unserem Fine- Latein oder FNHD gesehen hätte. Trotz der im Vergleich zum Pre-Training kleinen lateinischen und frühneuhochdeutschen Textmenge lernt TrOCR die Dekodierung von neuen Sprachen zuverlässig. In Zeilen, in welchen die Sprache hingegen ändert (Code-Switching), was im Bullinger-Briefwechsel relativ häufig passiert (Volk et al. 2022), reagiert TrOCR zu spät für den Sprachwechsel (Abbildung 5). Solche Phänomene werden wir mit flexibleren Modellen abzufangen versuchen.

                

                    

                    
Abbildung 5. Performanz verschiedener TrOCR-Modelle auf einer Zeile, in welcher Sprachwechsel vorliegt. Der Trennstrich markiert die Sprachgrenze, die im multilingualen Modell zu spät und in monolingualen gar nicht erkannt wird.

                    

                
            

            

                
Schlüsse

                
Die Arbeit mit dem Bullinger-Korpus ist aufschlussreich in unterschiedlicher Hinsicht. Zentral für diese Arbeit sind drei 
                    
Schlussfolgerungen
:
                



                    
Wir stellen eine Harmonisierung unterschiedlicher (
                        
Deep-Learning-
basierter) Systeme mit Bezug zur Erkennqualität von Handschriften fest. Unabhängig davon, ob etablierte Plattformen oder neue Systeme wie TrOCR genutzt werden.
                    

                    
Der Gebrauch von 
                        
Data-Augmentation-
Techniken verspricht einen Gewinn, der aktuell noch weiter auszuloten ist, im Grundsatz aber schon zu Verbesserungen führt. 
                    

                    
Aktuell haben etablierte Plattformen den Vorteil, dass grosse 
                        
Basemodels
 genutzt werden können, die noch leichte Vorteile mitbringen, bei grossen Projekten wie Bullinger Digital aber noch anhand der Fehlermuster ausgelotet werden müssen.
                    

                

            

        

            
Die Frage nach einer spezifisch deutschen Verantwortung für historisches Unrecht und die Kritik an der Realisierbarkeit und Verhältnismäßigkeit von „Wiedergutmachung“ an Opfern der Verfolgung, Entrechtung und Enteignung durch den nationalsozialistischen Unrechtsstaat in der Nachkriegszeit, beschäftigte in den vergangenen Jahrzehnten vor allem Forscher_innen aus den Bereichen der Zeitgeschichte, der Philosophie und der Rechtswissenschaften. In der kunsthistorischen Forschung haben die 
                
Washington Principles on Nazi-Confiscated Art
 von 1998 ein Umdenken herbeigeführt. In der Folge hat sich die Provenienzforschung zunächst mit der Koordinierungsstelle für Kulturgutverluste in Magdeburg und der 2008 eingerichteten Arbeitsstelle für Provenienzrecherche/-forschung beim Institut für Museumsforschung, Stiftung Preußischer Kulturbesitz in Berlin – seit 2015 zusammengeschlossen zum Deutschen Zentrum für Kulturgutverluste (DZK) in Magdeburg – institutionalisiert. Die Provenienzforschung stellt inzwischen eines der virulentesten geisteswissenschaftlichen Forschungsfelder dar, welches von großem öffentlichem und medialem Interesse begleitet wird und inzwischen an einigen, v.a. kunsthistorischen Instituten zum Ausbildungs- bzw. zum Lehrangebot gehört. Seit 2015 wurden an fünf Universitäten in Bonn, Hamburg, München, Berlin und Lüneburg (Junior-)Professuren dieser Denomination eingerichtet, auch an anderen Universitäten werden Seminare oder spezialisierte Masterstudiengänge angeboten.
            

            
Dabei produziert die Provenienzforschung in erheblichem Maße objekt- und personenbezogene Daten: Die seit 2020 vom DZK betriebene Forschungsdatenbank PROVEANA bündelt Entitäten aus 467 von der Stiftung geförderten Projekte, d.h. Daten zu Akteur_innen, Objekten, Archivbeständen und Sekundärquellen, die den Forschenden aber ebenso betroffenen Nachfahren von Verfolgten oder aber Interessierten zur Verfügung zu stehen, darin enthalten auch die Such- und Fundmeldungen der seit 2000 betriebenen Datenbank LostArt.
 Parallel entstanden in den vergangenen zwei Jahrzehnten weitere unabhängige Datenbankprojekte, etwa die am Deutschen Historischen Museum in Berlin angesiedelte Datenbank zum Central Collecting Point (CCP) in München, welcher unter der Leitung amerikanischen Militärbehörden in der in der unmittelbaren Nachkriegszeit Wesentliches für die Rückführung des im Nationalsozialismus enteigneten Kulturguts geleistet hat
, oder die Datenbank zum Kulturgutraub in Frankreich durch den Einsatzstabs Reichsleiter Rosenberg (ERR).

            

            
Auch innerhalb der vom DZK geförderten Projekte werden Provenienzdaten dokumentiert, dies in vielfältiger Form, seien es Excelsheets, kleinere selbstgestrickte Datenbanklösungen oder auch zusätzliche Provenienzfelder, die in bestehende Museumsdatenbanken integriert wurden. Diese hausintern erzeugten Daten bleiben aber in der Regel nicht projekt- oder länderübergreifend nutzbar (s. Hopp 2018). Die Frage der Langzeitarchivierung dieser lokal gehaltenen Projektdaten ist vielfach noch gar nicht angegangen worden. Das mag auch mit daran liegen, dass wir es mit einer föderalen Arbeitsstruktur, mangelnden personellen Kontinuitäten (bedingt durch Drittmittelförderungen und befristete Verträge) sowie Heterogenitäten von Datenmodellierungen auf der räumlichen wie auf der zeitlichen Achse zu tun haben, was die Koordination des Datenrückflusses an Datenzentren zeitaufwendig und schwierig macht. 

            
Ein Blick auf die Provenienzforschung in Museen zeigt, wie vielfältig Provenienzdaten zu einem Objekt sein können. Es handelt sich um Metadaten zu Erwerbsumständen (Zugangsdaten, Rückseitenbefunde, etc.) aber auch um solche, die aus externen Quellen und Überlieferungen in privaten oder öffentlichen Archiven (Beschlagnahme-, Wiedergutmachungsakten, etc.) stammen. Hinzu kommen Daten aus Forschungseinrichtungen, die sich mit den Mechanismen der Verlagerung von Kulturgütern und Akteursnetzwerken sowie dem Kunstmarkt befassen.
 Die Herausforderung besteht darin, diese Erkenntnisse und Informationen zu bündeln, wobei die erhobenen Daten immer auch eine Rückbindung an die jeweiligen archivalischen Belege benötigen, die ähnlich vielfältig wie die durch sie beschriebenen Objekte sind.
            

            
Wie ist es nun um die Zugänglichkeit zur Archivmaterialien bestellt? Die Archive selbst leisten seit Jahren wichtige Unterstützung für die Provenienzforschung. Die Zielvorgaben von Politik und Forschung waren dabei in der Regel bislang Quellen oder archivalische Bestände schnell und/oder unkompliziert als Digitalisate oder Datenbanken zur Verfügung zu stellen, um die direkte Zugänglichkeit für die Forschung zu gewährleisten, wobei auf spezielle bzw. individuelle Abfragemöglichkeiten weniger Wert gelegt wurde. Aufgrund der schieren Menge und Fülle des vorhandenen Materials können die bisherigen Online-Angebote bzw. oben genannte Datenbankprojekte zwar einen wichtigen aber eben auch nur sehr kleinen Ausschnitt der Informationen zum NS-Kunst- und Kulturgutraub wiedergeben. Zudem scheinen bei den digitalen Zugriffsmöglichkeiten auf die für die Provenienzforschung relevanten Archivbestände, große Unterschiede auf. 

            
Das Transparenzgebot der Washingtoner Prinzipien von 1998 steht dabei noch immer im Widerspruch zu den archivrechtlichen Bestimmungen einzelner Einrichtungen, ganz gleich ob auf bundes-, landes- oder kommunaler Ebene. Während die Ende 2016 in Kraft getretene DGSVO mit dem in ihr enthaltenen Erwägungsgrund 158 einen transparenten Umgang mit Daten empfiehlt, die „im Zusammenhang mit dem politischen Verhalten unter ehemaligen totalitären Regimen, Völkermord, Verbrechen gegen die Menschlichkeit, insbesondere dem Holocaust, und Kriegsverbrechen“ stehen,
 bleibt sie bezogen auf die praktische Umsetzung der Verarbeitung und Publikation der Daten weiterhin auslegbar, bzw. wird in der Praxis in Archiven und Verwaltungen sehr viel enger gefasst. Dabei fehlt häufig einen saubere Trennung von Daten die DSGVO-relevant sind, von solchen, die in keinem Bezug zu lebenden Personen stehen.

            

            
Kleinere, regionale Forschungsverbünde arbeiten inzwischen mit digitalen Repositorien oder Portalen zur Bereitstellung von Quellenmaterial speziell für die Provenienzrecherche, doch stehen diese in der Regel nur einem eingeschränkten Kreis an Nutzer_innen zur Verfügung, da neben der rechtlichen Lage auch viele moralisch-ethische Fragen im Umgang mit den erarbeiteten Informationen offen bleiben, so etwa privaten Informationen zu den Geschädigten (Familiendokumente, etc.). So erfüllen die existierenden Lösungsansätze bis heute kaum die Anforderungen an umfassende Transparenz im Umgang mit (Meta-)Daten zur Herkunft der in deutschen Einrichtungen verwahrten kulturellen Objekte, ein Problem, das auch auf die in jüngerer Zeit begründeten Netzwerke zum Umgang mit Objekten aus kolonialen Kontexten – hier sei exemplarisch auf das künftige CCC-Portal der Deutschen Digitalen Bibliothek verwiesen – übertragen werden kann.

            

            
Allerdings gibt es aktuell verschiedene Ansätze zur Aufarbeitung von Archivbeständen stärker maschinell gestützten Verfahren einzusetzen, die über Volltexterschließung, Natural Language Processing (NLP) oder der Named Entity Recognition (NER) Dokumente zugänglich machen.
 Doch auch hier bliebt fraglich wie z.B. ein bundesweit angelegtes Projekt zur Digitalisierung der Wiedergutmachungsakten die zahlreichen archiv- und personenschutzrechtlichen Beschränkungen und Fristen umgehen wird bzw. wie transparent die Ergebnisse schließlich publiziert werden können, womit alle Digital-Projekte in diesem sensiblen Bereich in rechtlichen Grauzonen agieren. Bedarf es nicht sogar gerade in diesem Fall eines speziellen Schutzes von Daten zu Opfern der NS-Verfolgung? Die Akten, mit denen die Provenienzforschung arbeitet, sind Zeugnisse eines totalitären Unrechtsregimes. Neben den Akten der Finanzämter, die Auskunft über Vermögenswerte und fiskalische Verfolgung geben, erlauben Auflistungen des vor der Emigration in den Speditionen deponierten Hausrats unmittelbare Einblicke in Hausstand und Familienleben. Schließlich finden sich in Entschädigungsverfahren nicht selten Zeugnisse, die Foltermethoden benennen, die die Geschädigten über sich ergehen lassen mussten, ebenso sowie die davongetragenen medizinischen Spätfolgen. Dürfen wir heute über dieses Wissen, diese Daten frei verfügen?
            

            
Doch gerade für Forschende ist der Ansatz z.B. über die maschinelle Erschließung von möglichst vielen Dokumententexten die für die Provenienzrecherche wichtige objekt- oder personenbezogenen Daten schnell herauszufiltern natürlich essentiell, da auf Basis der unüberblickbaren Menge der europaweit verstreuten Quellen zum Kunst- und Kulturgutraub der Nationalsozialisten einzelfallbezogene Prüfungen und Sondierungen oft nicht effizient und nachhaltig bearbeitet werden können. Zwar ist es denkbar, maschinelle Verfahren der Texterschließung anzuwenden, aber es braucht zusätzliche Methoden, sensible Inhalte zu filtern und Teile von Beständen – wie bislang auch analog gehandhabt – nur auf begründeten Antrag zur Verfügung zu stellen. 

            
Gleichzeitig steht die digitale Provenienzforschung vor der Herausforderung eine Basis für die Etablierung effizienter Modelle zur standardisierten Erfassung von eindeutigen bzw. uneindeutigen Provenienzdaten zu schaffen und Datenkompetenzen auszubilden. Denn neben der Recherche von Objektbiografien, Eigentumsübergängen und -verlusten, 

            
hat Provenienzforschung auch das Ziel den heutigen Anspruchsberechtigten und der interessierten Öffentlichkeit gerecht zu werden – ergo zu dokumentieren, aufzuklären, zu informieren und damit auch Daten – weltweit – auffindbar zu machen. Bereits im Sommersemester 2020 sind Studierende am Fachgebiet Digitale Provenienzforschung der TU Berlin in einem Seminar der Frage nachgegangen, ob und inwiefern sich Nachfahren der im NS-Regime rassisch oder anderweitig Verfolgten über laufende Projekte und bestehende Online-Datenangebote im Bereich der Provenienzforschung in Deutschland informieren können. Im Zentrum stand die Frage, ob die bereits bestehenden Datenbanken/-tools auch für ein nicht spezialisiertes Publikum zugänglich, ob sie auffindbar, transparent und verständlich sind, ob es Sprachhürden gibt und wie viel Vorwissen erforderlich ist, um die in ihnen enthaltenen Informationen richtig zu interpretieren. In Gesprächen mit heute international ansässigen Nachfahren, kristallisierte sich heraus, dass das Online-Angebot nicht nur unübersichtlich ist, sondern wesentlichen Adressat*innen bzw. Interessentengruppen weitestgehend verschlossen bleibt. Die aktuellen Angebote produzieren folglich nicht nur Expertenwissen, sondern auch Ausschlüsse der eigentlich Betroffenen. 

            
Vor diesem Hintergrund haben wir uns seit Herbst 2020 intensiv mit Optimierungsprozessen im Umgang und bei der Erschließung von öffentlich verfügbaren Quellenbeständen zur Provenienzforschung befasst. Unsere Verfahren zielten dabei unter anderem auf den heute im Bundesarchiv Koblenz befindlichen, komplett digitalisierten aber bisher nicht effizient zu durchsuchenden Bestand B323 der in den Nachkriegsjahren agierenden ehem. Treuhandverwaltung von Kulturgut beim Oberfinanzpräsidium in München ab, der wesentliche Original-Quellen zum NS-Kunstraub sowie zu den alliierten Rückführungs-Bemühungen enthält und der in seiner sehr gemischten Zusammensetzung einen optimalen Testbestand lieferte. Ausgangspunkt des Pilotprojekts waren zunächst Metadaten aus der Archiverfassung,
 die wir als Graph aufbereitet haben (Bussche, Hopp 2022a und 2022b). Hierbei orientieren wir uns an bisherigen Bemühungen, den Nutzen von Linked-Data- und Semantic-Web-Technologien für archivarische Sammlungen zu untersuchen (Ferris 2014, Gracy 2015, Niu 2016). Um diese zu einer verlässlichen Ressource für die Provenienzforschung zu entwickeln, wurden alle über die Rechercheplattform des Bundesarchivs zur Verfügung stehenden Digitalisate mittlels OCR erfasst und die Daten in einer Suchmaschine zur Verfügung gestellt. Bereits auf dieser ersten Grundlage zeichnete sich ab, dass die Erschließung über Volltexte die Anforderungen der Provenienzforschung wesentlich besser abbilden kann, als das bisherige Online-Angebot mit Erschließungen zu Einzelakten über die Bestandbeschreibungen, denn der Praxis geht es häufig darum die Dokumente mit Erwähnungen bestimmter Personen, Werke oder Institutionen erst einmal aufzufinden.
            

            
Die bisher in unserem experimentellen Projekt verwendeten Verfahren machine learning basierter Clouddienste, wie etwa 
                
azure OCR
 oder 
                
Google NER
 ermöglichten es uns auch mit geringen personellen Kapazitäten durchaus umfangreiche Aktenbestände zu verarbeiten, wenngleich die Nutzung der genannten Dienste in öffentlichen Einrichtungen problematisch wäre. Wesentliche Hindernisse für eine Zugänglichmachung von Provenienzdaten beginnen also nicht erst bei der Frage der Online-Stellung, sondern schon sehr viel früher bei der Verarbeitung der Daten. Die von verschiedenen Institutionen unterschiedlich strikt gehandhabten Regeln hierzu blieben ebenso uneinheitlich wie die Vorgaben zum Datenschutz selbst, wobei Empfehlungen für „rechtskonforme“ Ersatzprodukte fehlen. 
            

            
Die Möglichkeiten der maschinellen Verarbeitung der bisher erzeugten Daten zum Bestand B323 gehen allerdings weiter (Stork 2021, Moss et al. 2018, Krenn 2019). Zum gegenwärtigen Zeitpunkt geht es uns darum, die Qualität der bislang eingesetzten Verfahren zu evaluieren und an einer künftigen verbesserten Verarbeitungspipeline zu arbeiten.

            
Welche qualitativen Vorteile bringt eine Vorverarbeitung der Scans (z.B. Ränder entfernen)?

            
Wo müssen Elemente eines Digitalisats segmentiert werden um die Auslesung der Daten bzw. Texte zu optimieren? Das betrifft vor allem kleine Notizen und Belege oder ausgeschnittene Bilder aus Mikrofilmen, die auf DinA4 Seiten montiert wurden.

            
Wo kann hingegen Layouterkennung eingesetzt werden (z.B. bei Listen, Karteikarten, Korrespondenzen)? 

            
Welche Möglichkeiten haben wir Dokumente auch inhaltlich erkennen zu lassen, um z.B. eine Filterung nach Rechnungen oder Transportlisten vorzunehmen? Welche Algorithmen stehen für buchhalterische Dokumente wie Quittungen oder Rechnungen zur Verfügung, um deren Inhalte strukturiert erkennen zu lassen?

            
Wie können wir Texte über Entitäten erschließen?

            
Für die Erschließung von Dokumentenbeständen über Entitäten gibt es bereits Vorbilder: so wurden Dokumente aus der Zeit der deutschen Besatzung in den Niederlanden beispielsweise im Projekt Oorlogsbronnen aufbereitet (Borggräfe et al. 2020).
 Ein weiteres bemerkenswertes Beispiel aus dem Bereich der Provenienzforschung ist der Archivführer zur deutschen Kolonialgeschichte, der archivische Sammlungen katalogisiert und mit Wikidata-Elementen verknüpft und somit neben dem niedrigschwelligen Einstieg in die Recherchen auch die weitere Bearbeitung der Entitäten und den Einbezug von Expert_innenwissen erlaubt, um die Qualität der Erkennung und Disambiguierung zu verbessern (Jung 2019).
 Die beiden Beispiele liefern damit Entitäten, über die Texte mit der entsprechenden Qualität maschinell erschlossen werden können. 
            

            
Das bedeutet im Folgeschluss, dass es immer wichtiger wird, dass strukturierte Daten auch als offene Daten zur Verfügung stehen. Plattformen wie PROVEANA haben die dazu notwendige Struktur und Qualität, bieten aber über Einzelrecherchen hinaus keinen Zugang zu den Rohdaten oder API an. 

            
Die Anwendung von Semantic-Web-Technologien in der Provenienzforschung benötigt unserer Meinung nach: einen offenen Diskurs zum standardisiertn und FAIRen Umgang mit den Digitalisaten und Forschungsdaten an allen Provenienzforschung betreibenden Einrichtungen, Verfahren zur effizienteren Aufbereitung von Dokumenten, um die nötige Qualität der Dokumentenverarbeitung (OCR, Layouterkennung usw.) sicherstellen und Methoden um Unterscheidungen zwischen den für die Forschung offenzulegenden Daten von den sensiblen Informationen vorzunehmen.

        

            

                
Latente und explizite Algorithmizität

                
In den Geisteswissenschaften werden immer wieder tradierte Strukturen der Wissensproduktion hinterfragt und neu geordnet – so auch vor dem Hintergrund der Digital Humanities, die die traditionellen Geisteswissenschaften mit digitalen Forschungsmethoden teils unterstützen, teils konfrontieren. Neben Reflexionen über Medialität und Sozialität geisteswissenschaftlicher Erkenntnisprozesse werden dichotomische Setzungen wie analog/digital, qualitativ/quantitativ sowie kontinuierlich/diskret auf den Prüfstand gestellt. Gleichzeitig gewinnen auch andere „epistemische Tugenden“ (Daston und Galison 2007), wie zum Beispiel Transparenz, Evidenz und Reproduzierbarkeit, für geisteswissenschaftliche Forschungsfelder eine neue Relevanz. Im Zuge dessen lässt sich eine umfassende Neuvermessung disziplinspezifischer Kulturen des Verstehens beobachten, die jenseits eines tradierten „Two Cultures“-Paradigmas (Snow 1959) verfährt. Kennzeichnend sind vielmehr Querverbindungen und Verschränkungen, welche die disziplinären Profile der Geisteswissenschaften sowie der Informatik neu ins Verhältnis setzen. Als eine solche mögliche disziplinäre Querverbindung möchten wir im Workshop 
                    
Algorithmizität
 verhandeln.
                

                
Der von der AG „Digital Humanities Theorie“ organisierte Workshop widmet sich Voraussetzungen, Potenzialen und Implikationen von Algorithmizität in den Humanities. Dabei knüpft der Workshop an aktuelle Forschungsinteressen der AG an und setzt zugleich Diskussionen der im Juni 2022 organisierten Tagung „Algorithmizität als Kultur des Verstehens“ fort. Einen ersten Ausgangspunkt des Workshops bildet nun die Annahme, dass sich in traditionellen und aktuellen Forschungspraktiken der Geisteswissenschaften latente Formen der Algorithmizität wiederfinden lassen. Darunter sind nicht nur digitale Formen geisteswissenschaftlichen Arbeitens im Sinne einer prozessual aufgefassten Algorithmisierung zu verstehen. Vielmehr bezeichnet Algorithmizität einen Formalisierungsgrad von Handlungsanweisungen, der sich in geisteswissenschaftlichen Methoden 
                    
sui generis
 manifestiert. Insbesondere Versuche einer systematischen und symbolischen Externalisierung geistiger Operationen sind seit langem Bestandteil philosophischen Denkens (vgl. Gramelsberger 2020). Im Verlauf der Tagung  wurden konkrete Beispiele latenter (d.h. impliziter) bis expliziter Algorithmizität in den Geisteswissenschaften skizziert und diskutiert, die sich etwa in der Mathematisierung der Musik und algorithmischer Musikproduktion in den Musikwissenschaften (vgl. Braguinski 2018), dem systematisierten Vorgehen der 
                    
Objektiven Hermeneutik
 und 
                    
Grounded Theory
 in den Sozialwissenschaften (vgl. Muller et al. 2016; vorgestellt durch Dennis Möbus) oder in Binarität als Zugang zu literaturwissenschaftlichen Untersuchungen von Textmaterialität (vgl. Coch, Hahn und Pethes 2022) ausdrückt. Weitere historische Beispiele wären die Konzeption von Rede und Gegenrede in den platonischen Dialogen, das dialektische Format der mittelalterlichen Quaestio im Anschluss an Petrus Abaelardus, die methodologischen Überlegungen der frühneuzeitlichen Wissenschaftsphilosophie wie sie beispielsweise bei Bacon oder Descartes zu finden sind, die phänomenologische Reduktion nach Husserl oder schließlich die Hermeneutik als verbindendes Element der Geisteswissenschaften, der als methodisches und stufenweises Vorgehen der generellen Textrezeption eine besondere Rolle zukommt.
                

                
Vor diesem Hintergrund verstehen wir Algorithmizität als ein graduelles Phänomen (vgl. Abb. 1). So können latente und explizite Formen der Algorithmizität unterschieden werden, die einerseits von Prozessen der Quantifizierung und Diskretisierung bis zu maschinenlesbaren Handlungsabläufen reichen, deren prozesshafte Kontingenzreduktion aufgrund zeichenbasierter Befehle maximal ist. Andererseits werden auch disziplinspezifische Umgangsformen mit Algorithmizität und Praktiken einer algorithmischen Gegenstandskonstruktion sichtbar.

                

                    

                    
Abb. 1: Algorithmizität als graduelles Phänomen

                

                
Algorithmizität wollen wir im Workshop aus drei unterschiedlichen Perspektiven betrachten und diskutieren: (1) als Konzept  in der theoretischen Begriffsarbeit, (2) als spezifisch (geisteswissenschaftliche) Erkenntnis- und Denkstruktur und (3) als (trans-)disziplinärer Kompass, d.h. als eine Art Navigationshilfe zwischen den einzelnen Disziplinen der Digital Humanities. Ziel des Workshops ist es, gemeinsam mit der DH-Community zu elaborieren, inwiefern Algorithmizität eine geeignete Beschreibungskategorie ist, um bestimmte Formen geisteswissenschaftlicher Wissensproduktionen zu charakterisieren. Die gemeinsame Diskussion von Beschreibungskategorien trägt dabei, so argumentieren wir, auch Forderungen von Open Science Rechnung, da die Dokumentation solcher Kategorien die Transparenz, Nachvollziehbarkeit und Reproduzierbarkeit von Interpretationen erhöht. Was wird eigentlich verhandelbar, wenn Strukturen der Problembehandlung und Bedeutungszuweisung in den Humanities als algorithmisch beschrieben werden? Inwiefern stellt Algorithmizität eine Bereicherung für den interdisziplinären Austausch dar? Denn Algorithmizität kann nicht nur als impliziter Teil von etablierten Kulturen des Verstehens, sondern selbst als Kulturtechnik des Verstehens begriffen werden. In welchen latenten und expliziten Ausprägungen Algorithmizität in einzelnen geisteswissenschaftlichen Erkenntnisprozessen stattfindet, wollen wir im Workshop diskutieren. Darüber hinaus fragen wir auch, inwiefern Algorithmizität in den computationellen Disziplinen einerseits und in den Geistes- und Kulturwissenschaften andererseits strukturelle Kongruenzen aufweisen, die methodisch, technisch und wissenschaftskulturell als Brücke zwischen diesen beiden Wissenschaftsbereichen dienen können.

            

            

                
Vermessung von Algorithmizität als Beschreibungskategorie

                
Aktuell wird Algorithmizität in den verschiedenen Disziplinen der (Digital) Humanities in unterschiedlicher Form diskutiert: Die Sozial- und Wirtschaftsgeschichte beispielsweise nimmt auf verschiedene Art Bezug auf algorithmische Methoden der empirischen Sozialforschung oder historischen Demographie, um ihre Interpretationen zu stützen (vgl. Schremmer 1998). In medienwissenschaftlichen Kontexten wird Algorithmizität einerseits als eine Eigenschaft der Kultur der Digitalität (vgl. Stalder 2019, 13) bezeichnet, andererseits aber auch als eine 
                    
potentia
 beschrieben, die dem Algorithmus als spezifische Form vorausgeht (vgl. Rutz 2016, 41). In einer informatischen Perspektive taucht Algorithmizität neben Sequenzialität und Abstraktion als Teilaspekt eines 
                    
computational thinking
 auf (vgl. Denning und Tedre 2019). Für die DH eröffnet sich ein Reflexionsraum für die eigenen Verstehenspraktiken (vgl. Rehbein 2022). Für die beteiligten geisteswissenschaftlichen Disziplinen stellt sich darüber hinaus die Frage nach dem Potenzial der Auseinandersetzung mit dem Begriff der Algorithmizität für die eigene Theoriereflexion.
                

                
Um dieses interdisziplinäre Feld aus Perspektive der Algorithmizität zu sondieren, widmet sich unser Workshop folgenden zentralen Fragekomplexen:

                
Algorithmizität und Begriffsarbeit



                    
Was sind Kriterien zur Bestimmung des Algorithmischen? (Performanz, Effizienz, Binarität …) Welche Rolle(n) spielen sie?

                    
Gibt es ein Spannungsverhältnis zwischen Algorithmizität und Verstehen oder ist das eine im anderen enthalten?

                    
Wie können Begriffe wie ‘Algorithmus’, ‘Algorithmizität’ oder ‘Algorithmisierung’ differenziert werden?

                    
Was sind die Grenzen des Algorithmizitätsbegriffs? Wie unterscheidet sich Algorithmizität von den Begriffen Kalkül (vgl. Krämer 1991), Modell (vgl. Flanders und Jannidis 2019) oder Regel (vgl. Daston 2022)?

                

                
Algorithmizität als Erkenntnis-und Denkstruktur



                    
Ist 
                        
computational thinking
 das Gleiche wie ein algorithmischer Erkenntnisprozess und das Gleiche wie ein regelgeleitetes Vorgehen (wie etwa Dilthey oder Descartes es beschreiben)?
                    

                    
Was nützt uns die Klassifizierung von (geisteswissenschaftlichen) Erkenntnisprozessen als algorithmisch oder nicht-algorithmisch?

                    
Wird alles algorithmisch? Was sind die Grenzen des Phänomens?

                

                
Algorithmizität als (trans-)disziplinärer Kompass



                    
Wie unterscheiden sich die in den einzelnen Disziplinen der Humanities angewandten Algorithmen oder regelgeleiteten Methodensettings? Was sind Gemeinsamkeiten?

                    
Was sind disziplinäre Unterschiede zwischen geisteswissenschaftlicher und informatischer Wissensproduktion?

                    
Was nützt uns das Wissen um Gemeinsamkeiten und Unterschiede der weniger oder stärker algorithmisch geprägten Art der Erkenntnisproduktion in den Disziplinen?

                    

                        

                            
Ist das „

                            
Two

                            
 Cultures“-Paradigma weiterhin gültig?

                        

                    

                

            

            

                
Methodik und Ablauf des Workshops

                
Um partizipative Strukturen vor, während und nach dem Workshop zu ermöglichen, haben wir ein begleitendes Programm geplant. Zur Vorbereitung des Workshops sollen bis zur DHd-Konferenz 2023 sukzessive drei Beiträge für den Theorie-Blog
 publiziert werden: Nach einer Zusammenfassung der Tagung von 2022 mit ihren zentralen Thesen und Diskussionspunkten planen wir, zwei kontrastierende 
                    
opinion pieces
 zu veröffentlichen, die das Für und Wider einer Verwendung des Begriffs ‘Algorithmizität’ im geisteswissenschaftlichen Diskurs versammeln. Um schon im Vorfeld Spannweiten des Algorithmizitätsbegriffs zu erproben, wird die Forschungscommunity ausdrücklich zur Partizipation aufgerufen, indem wir Diskussionen über die blogeigene Kommentarfunktion oder über Twitter
 und Mastodon
 ermöglichen. Im Workshop selbst sollen einerseits die vorläufigen Ergebnisse und Kernthesen der vorangegangenen Tagung in kondensierter Form präsentiert werden, vor allem wollen wir andererseits aber auch eine systematische Diskussion zur Verfasstheit und zum Potenzial von Algorithmizität führen. Unser Ziel für den Workshop ist es, den Teilnehmenden das Konzept bzw. Konzepte von Algorithmizität nahezubringen, anhand konkreter Beispiele aus den (digitalen) Geisteswissenschaften kritisch zu diskutieren und schließlich seine Eignung und seinen Mehrwert für den Theoriediskurs in den DH herauszuarbeiten.
                

                
Der Workshop ist für vier Stunden angesetzt und der Ablauf ist folgendermaßen strukturiert:

                
0:00–0:15 Kurze Vorstellung der AG Theorie und der Workshopteilnehmenden

                
0:15–0:30 Erste Annäherungen an den Algorithmizitätsbegriff 

                
0:30–0:45 Zusammenfassung der wesentlichen Thesen und vorläufigen Ergebnisse aus der vorangegangenen Tagung

                
0:45–1:30 Kontrastierende Stellungnahmen (pointierte Provokationen) und anschließende Diskussionen



                    
Algorithmizität als leere Worthülse

                    
Algorithmizität als Projektionsfläche für Theoriediskurse in den DH

                

                
1:30–2:00 
                    Pause
                

                
2:00–3:00 World-Café zu einzelnen Diskussionspunkten (vgl. zentrale Fragekomplexe), moderiert durch AG-Mitglieder; Ziele:



                    
Begriffsarbeit: Erarbeitung von interdisziplinären und disziplinspezifischen Kriterien und Definitionen von ‘Algorithmizität’

                    
Erkenntnisstruktur: Bestimmung (möglicher) Funktionen des Begriffs im wissenschaftlichen Diskurs

                    
(trans-)disziplinärer Kompass: Sammlung von Beispielen des Algorithmischen in bestehenden geisteswissenschaftlichen Wissensstrukturen

                

                
3:00–3:15 Pause
                

                
3:15–3:45 Vorstellung der Gruppenarbeiten und moderierte Diskussion

                
3:45–4:00 Dokumentation der Ergebnisse; Ziel: Potenzialbewertung des Algorithmizitätsbegriffs für die Charakterisierung der DH allgemein und die Theoriebildung in den DH im Besonderen

            

            

                
Zielpublikum und Teilnehmer*innenzahl

                
Am Workshop können bis zu 25 Theorie-interessierte Personen aus allen Teildisziplinen der Digital Humanities teilnehmen. Erfahrungen im Einsatz oder in der Reflexion von Algorithmen und/oder epistemischen Strukturen ist hilfreich, aber keine Teilnahmevoraussetzung.

            

            

                
Beteiligte und Forschungsinteressen

                
Manuel Burghardt ist Professor für Computational Humanities im Institut für Informatik an der Universität Leipzig. Seine Forschungsinteressen umfassen computergestützte Verfahren der Annotation, Analyse und Visualisierung von geisteswissenschaftlichen Forschungsdaten.

                
Jonathan D. Geiger arbeitet an der Akademie der Wissenschaften und der Literatur | Mainz im Infrastrukturprojekt NFDI4Culture. Seine Interessensschwerpunkte liegen auf der philosophisch-theoretischen Reflexion digitaler Forschungsmethoden in den Geisteswissenschaften und der Digitalität insgesamt, sowie auf Fragen infrastruktureller Bedürfnisse der NFDI4Culture-Bedarfe und der Philosophie-Community.

                
Jan Horstmann leitet das Service Center for Digital Humanities (SCDH) an der ULB der Westfälischen Wilhelms-Universität Münster. Seine Forschungsinteressen und -schwerpunkte liegen im Bereich der digitalen Methodologie mit besonderem Fokus auf die Textannotation, -analyse und Visualisierung im Bereich der computationellen Literaturwissenschaft.

                
Rabea Kleymann ist Postdoktorandin am Leibniz-Zentrum für Literatur- und Kulturforschung Berlin. Dort leitet sie seit 2020 das Projekt „Diffraktive Epistemik. Wissenskulturen in den Digital Humanities“. Ihre Forschungsinteressen liegen im Bereich der Wissenschaftstheorie, Science &amp; Technology Studies sowie den computationellen Literaturwissenschaften.

                
Jascha Schmitz studiert im Master Geschichtswissenschaften an der Humboldt-Universität zu Berlin mit dem Schwerpunkt Digital History und arbeitet als wissenschaftliche Hilfskraft am Max-Planck-Institut für Wissenschaftsgeschichte. Im Rahmen seiner Masterarbeit liegt sein Forschungsfokus auf Simulationsmethoden für die Geschichtswissenschaften.

                
Silke Schwandt ist Professorin für Digital History an der Universität Bielefeld. Sie forscht aktuell zur Veränderung von geschichtswissenschaftlichen Forschungspraktiken unter Bedingungen der Digitalität sowie zum Potential von digitalen Methoden für die Selbstreflexion der Geschichtswissenschaft.

            

            

                
Benötigte technische Ausstattung

                
Wir benötigen einen Raum mit flexibler Bestuhlung und Tischen, die zu Gruppentischen verschoben werden können. Außerdem benötigen wir einen Moderationskoffer mit Materialien für Gruppenarbeiten und einen Beamer.

            

        

            

                
Einleitung

                
Das BMBF-geförderte Forschungsprojekts „ArIS - Durch das Artefakt zur ‘infra structura’“
 hatte das Ziel, durch Erschließung von Arzneimittelrezepten einen erkenntnisermöglichenden Zugang zur Entstehung der Gesundheitsinfrastruktur in Deutschland und Österreich zu generieren.
 Im Fokus der Forschungsaktivität des Projekts lag das Vorhaben, über historische Rezepte Einblick in die Entwicklungsgeschichte des deutschen Gesundheitssystems zu erhalten. Die auf ihnen zu findenden Spuren bieten Hinweise auf kleinere und größere Veränderungen ihrer historischen Umwelt, wie bspw. die Entstehung der Krankenkassen oder neue Abrechnungsformen. Da auf keine vorhandenen Daten dieser Art zurückgegriffen werden konnte, wurden erstmals historische Arzneimittelrezepte in größerem Umfang digitalisiert und der Versuch unternommen, diese artefakt-individuell zu untersuchen und zu beschreiben. Gleichsam als Nebenprodukt entstanden bei der digitalen Erschließung Scans der Rezeptzettel, welche die Möglichkeit für eine weitere inhaltliche Erfassung im Rahmen von Anschlussforschung bieten. Das Bestreben, die Datenbank inkl. Scans als eine den FAIR-Prinzipien
 entsprechende Datensammlung hierfür zu veröffentlichen, stößt jedoch auf einige Hindernisse.
                

                
Das Projekt war eine Kooperation zwischen Wirtschaftsinformatiker*innen und Pharmaziehistoriker*innen der Universitäten Aachen, Münster und Marburg sowie dem Deutschen Apotheken-Museum Heidelberg und wurde über vier Jahre (2018–2022) gefördert.
 Trotz des von den Projektpartner*innen vertretenen Anspruchs an Offenheit musste an mehreren Stellen des Projekts aus unterschiedlichen Gründen von diesem Prinzip abgewichen werden. Dies ist sowohl auf projektspezifische Forschungsinteressen als auch rechtliche Einschränkungen zurückzuführen, die im Folgenden erörtert werden.
                

                
Obwohl es sich bei dem Arzneimittelrezept um einen jahrhundertealten, millionenfach ausgestellten Gegenstand der Gesundheitsversorgung im alltäglichen Leben handelt, wurde dieser bislang kaum als erhaltenswert wahrgenommen. Im ArIS-Projekt ist nun eine einmalige Datenbank mit digitalisierten Rezeptblättern aus mehreren Sammlungen unterschiedlicher Herkunft mit über 12.200 Datensätzen entstanden. 

                

                    

                    
Beispiele historischer Arzneimittelrezepte (Bildinhaber: Deutsches Apotheken-Museum Heidelberg)

                

                
Mit Blick auf den typischen Lebenszyklus von Forschungsdaten
 und den Anspruch von Open Science (vgl. Heise 2018) lassen sich bei einem interdisziplinären Projektvorhaben dieser Größenordnung unterschiedliche Herausforderungen beobachten: (1) Der große historische Rahmen, aus dem die Objekte stammen, wirft in der Modellierung die Frage nach einer geeigneten Definition des Terminus „Rezept“ auf. (2) Methodisch zeigt sich bei großen, heterogenen Handschriftenkorpora die Notwendigkeit, traditionelle Erschließungsmethoden durch visuelle Analysen zu ergänzen. (3) Neben rechtlichen Herausforderungen bei Open Data, wie Urheber- und Verwendungsrechten, sind bei Gesundheitsdaten besondere Schutzbedürfnisse und Fristen zu wahren, die nach einem Konzept für eine dynamische Öffnung von Daten verlangen.
                

                
Die verschiedenen Möglichkeiten der inhaltlichen Erschließung bei herausfordernder Rechtslage sind die zentralen Aspekte sowohl des Forschungsprojekts ArIS als auch des vorliegenden Beitrags.

            

            

                
Offene Supportstrukturen für das ArIS-Projekt

                
Ziel des ArIS-Projekts war es, die Rolle und Bedeutung des Arzneimittelrezepts in der Entstehung des modernen Gesundheitswesens zu untersuchen. Zu diesem Zweck sollte zunächst eine empirische Grundlage geschaffen werden, indem vorhandene Sammlungen von Schriftstücken (meist Zettel) mit Rezepten unterschiedlicher Standorte identifiziert, digitalisiert und virtuell zusammengeführt wurden.

                
Zur Erschließung, Modellierung, Analyse und Langzeitarchivierung der im Projekt entstehenden Forschungsdaten griffen die Münsteraner Projektpartner aus der Wirtschaftsinformatik auf die Kompetenzstrukturen der eigenen Institution zurück und erarbeiteten zusammen mit der Universitäts- und Landesbibliothek und ihren Service Centern for Data Management
 und for Digital Humanities
 ein strukturiertes Vorgehen.
 Die für die Forschenden an der Universität Münster frei und dauerhaft zur Verfügung stehenden Supportstrukturen ermöglichten sowohl eine enge und nachhaltige Zusammenarbeit als auch eine Orientierung an Standardformaten. Impulse für die Modellierung und den Einsatz von Methoden des maschinellen Lernens (vgl. Bönisch 2022) konnten so frühzeitig und während der gesamten Projektdauer aufgenommen werden.
                

                
Zu Beginn lag ein Großteil der physischen Artefakte als ungesichtete Blattsammlungen in Kartons vor, zu denen nur in Teilen Metadaten erfasst waren. Daten auf Artefaktebene oder auch Aussagen über mögliche Ordnungsprinzipien in Bezug auf Kartons oder andere physische Objektklammern gab es i.d.R. nicht. Die Digitalisate bisher nicht archivierter Schriftstücke wurden daher mit Inventarnummern versehen, die Rückschlüsse auf Ort, Lagerform und Blattnummer zulassen. Auf diese Weise sollte die Wiederauffindbarkeit der physischen Blätter an den jeweiligen (musealen) Standorten gewährleistet sein, um einem durch die Digitalisierung möglicherweise entstehenden Informationsverlust (Bindung, Ordnungsprinzipien) entgegenzuwirken. Anschließend wurden die Digitalisate inklusive der bereits existierenden Metadaten in eine easydb-Datenbank importiert.
 Die an der ULB Münster betriebene Datenbank erlaubt eine flexible Gestaltung des Datenmodells, sodass auf die Anforderungen des interdisziplinären Forschungs- und Erschließungsprojekts eingegangen werden konnte. Geschaffen wurde so eine standardisierte Datenbank, in welcher die heterogenen Bestände unterschiedlicher Provenienz zusammengeführt, erfasst und verwaltet werden.
                

                
Durch die neu gewonnene digitale Verfügbarkeit der Artefakte sowie das flexible Rechtemanagement der ULB Münster konnten Teilsammlungen, wie beispielsweise die des Deutschen Apotheken-Museums in Heidelberg, von ausgewiesenen Fachwissenschaftler*innen erschlossen werden. Zugänglichkeit und Multibenutzer-Betrieb der Forschungsdatenbank ermöglichten es den Projektpartner*innen des ArIS-Projekts asynchron und ohne Konflikte in den Daten zu erzeugen, ihr fachspezifisches Wissen bei der Erschließung einzubringen.

            

            

                
Standardisierte Metadatenmodellierung und projektspezifische Erweiterungen

                
Bei der Erschließung digitalisierter Sammlungen der Universität Münster liegt der Fokus auf einer standardisierten Erfassung der Metadaten, der Nutzung etablierter Datenformate wie LIDO oder METS/MODS sowie der Anbindung von Normdaten und kontrollierten Vokabularien (vgl. DFG-Praxisregeln). Nur unter Beachtung dieser Voraussetzungen ist die Entstehung von interoperablen (
                    
interoperabel
) und wiederverwendbaren (
                    
reusable
) Forschungsdaten gewährleistet. Bei der Bildung der umfassenden Datensammlung (vgl. Schöch 2017) wurden Sammlungen von Arzneimittelrezepten mit heterogenem Inventarisierungs- und Kuratierungsstatus zusammengeführt. Daraus ergibt sich die konzeptionelle Frage, ob der standardisierte Erschließungsworkflow den Anforderungen der vorliegenden Sammlung überhaupt gerecht werden kann (vgl. Dörk und Glinka 2018).
                

                
In enger Kooperation zwischen dem Teilprojektteam in Münster und der ULB Münster (Service Center for Data Management; SCDM) wurde entschieden, anstelle einer standardisierten Erschließung der einzelnen Rezepte zunächst eine flexible Modellierung der vorhandenen Bild- und Objektdaten anzustreben. Hintergrund ist, dass anders als in unserer Alltagsvorstellung ein Digitalisat bzw. Datensatz nicht zwangsläufig mit einem einzelnen Arzneimittelrezept auf einer Seite eines Blattes gleichzusetzen ist. Das Rezept als abstraktes Konzept, das erst in einem bestimmten sozialen Kontext seine Form und Definition erhält, ist vielmehr unabhängig von einer bestimmten physischen Repräsentation. Heutzutage wird unter einem Arzneimittelrezept die formelle, schriftliche Aufforderung von Ärzt*innen an Apotheker*innen zur Abgabe von Arzneimitteln an eine/n bestimmte/n Patient*in verstanden. Inwiefern dieses Begriffsverständnis und dessen definitorische Merkmale aber auf historische Rezepte übertragbar sind, ist Teil eines pharmaziehistorischen Forschungsdiskurses (vgl. Seidel 1977, 22f.) und soll an dieser Stelle nicht weiter ausgeführt werden. Für den Projektkontext ist jedoch bedeutsam, dass die einzelnen Digitalisate manuell dahingehend überprüft werden mussten, ob sie Rezeptinformationen enthalten.

                
Für die Modellierung der Daten wurden die durch den externen Dienstleister dokumentierten Informationen zu den Artefakten in die Datenbank übernommen, wie z.B. die Klammerung oder Bindungen der Blätter. Zu diesem Zeitpunkt war jedoch unklar, ob den wenigen ermittelten Informationen ein historisch relevantes Ordnungsprinzip zu Grunde lag oder ob es sich um zufällige Gegebenheiten handelte. Gleichwohl erschien die Abbildung dieser Parameter als ein sinnvoller erster Schritt, eine Ordnung für die Daten und damit einen Ausgangspunkt für die Analyse zu finden.

                
Alle vorhandenen Rezeptblätter wurden formal erschlossen, kategorisiert, datiert und zu einer übergreifenden Datenbank zusammengeführt. In einem zweiten Schritt können einzelne Arzneimittelrezepte mit spezifischen Fragestellungen aus dem Bereich der Medizin- und Pharmaziegeschichte nach den etablierten Standards des Sammlungsmanagements
 erschlossen werden. Zugleich können auf Grundlage der entstandenen Datenbasis zentrale Fragen wie die Entwicklung der Arzneimittelrezepte anhand der identifizierten, übergreifenden Kriterien nachgezeichnet werden.
                

            

            

                
Inhaltliche Erschließung als Herausforderung und Voraussetzung

                
Die Erschließung und Erfassung von grundlegenden Metadaten der losen Zettelsammlung ist ein enormer Gewinn für die Forschung, da hier die Existenz möglicher Quellen nachgewiesen wird. Ähnlich wie bei vergleichbaren Archivalien entsteht so eine Art Zusammenführung, die einen Überblick über bestimmte Informationen eines größeren Zeitraums zulässt, in Bezug auf den Inhalt der Rezepte jedoch noch wenig Aussagekraft hat und damit zwar eine gute, aber keine hinreichende Quelle darstellt. Die Forschungsfragen etwa nach den verwendeten Arzneimitteln/Präparaten beginnen aber erst auf einer feingranulareren Ebene der Modellierung, wenn auch die Inhalte identifiziert werden.

                
Um die Findbarkeit von Inhalten der Rezepte zu erhöhen (und anschließende Analysen zu ermöglichen), wäre eine vollständige Maschinenlesbarkeit der Rezepte optimal. Die Möglichkeiten der inhaltlichen Erschließung stoßen in diesem Fall jedoch an methodische Grenzen: Die Arzneimittelrezepte sind in der Regel vollständig handschriftlich geschrieben oder zumindest handschriftlich ausgefüllt. Es finden sich zu viele unterschiedliche Handschriften und sprachliche Besonderheiten (Abkürzungen, Apothekerlatein), die nach aktuellem Stand der algorithmischen Möglichkeiten eines Handwritten-Text-Recognition-Ansatzes
 nicht trainiert werden können, sondern händisch transkribiert werden müssten. Hierfür fehlten im Projekt die zeitlichen und personellen Kapazitäten. Bislang wurde daher lediglich ein Subset der Rezepte im Rahmen eines geplanten Webauftritts des Deutschen Apotheken-Museums Heidelberg vollständig transkribiert.
                

                
Konfrontiert mit diesen Herausforderungen wurde im Projekt nach alternativen Wegen zur Überschreitung der Objektebene gesucht. Die Interpretation der losen Blattsammlung als Kollektion visueller Artefakte erschien dabei vielversprechend. Mit Methoden der Computer Vision (vgl. z.B. Arnold und Tilton 2019; He et al. 2016; Redmon et al. 2016) lässt sich das Korpus als Ganzes in seiner visuellen Ausprägung analysieren und z.B. mithilfe einer Mustererkennung clustern. Denkbar wäre hier etwa die Suche nach verwendeten Stempeln, Schriftfarben/Schreibutensilien oder standardisierten Vorstrukturierungen der Rezeptzettel, anhand derer sich eine über die Zeit zunehmende Formalisierung nicht nur der Behandlung, sondern auch der damit einhergehenden Kommunikation der Akteure feststellen ließe.

                
Eine Herausforderung ist hierbei, dass die Rezeptzettel nicht mit dem Ziel einer automatisierten, visuellen Analyse eingescannt wurden, sodass häufig etwa Kriterien wie geknickte Ecken oder Verfärbungen fälschlicherweise zu einem Clustering führen, das für die inhaltsbezogene Forschungsfrage keine Aussagekraft hat. In einem verhältnismäßig aufwändigen Preprocessing müssten die Scans daher vor einer eingehenden Computer-Vision-Analyse einzeln bereinigt und normiert werden, um anhand inhaltlicher Kriterien aussagekräftig clustern zu können. Erste Schritte wurden hierfür mit dem am DHLab Yale entwickelten Tool PixPlot (vgl. Duhaime 2019) gemacht. Im Gegensatz zu eigentlich intendierten Anwendungsszenarien diente die Software dazu, im Zusammenspiel mit easydb eine Gesamtschau auf die Daten zu kreieren, die mit der Betrachtung einzelner Bilddateien in der Datenbank nicht erreicht werden konnte. 

                

                    

                    
Clusterbildung ausgewählter Arzneimittelrezepte mit PixPlot

                

                
Das Tool erlaubt es, die Rezeptzettel nach Ähnlichkeit oder auch anhand einer Zeitachse zu clustern. Durch das Zusammenspiel menschlicher und maschineller Analyse können darüber hinaus die identifizierten Erscheinungsformen der Arzneimittelrezepte bestätigt, auf visueller Ebene evaluiert und Datenanomalien erkannt werden. Obwohl diese Verknüpfung von Datenbank und visueller, software-gestützter Analyse bei der Erschließung der Daten nur erprobt werden konnte, erscheint sie als vielversprechendes Instrument, um traditionelle objektbasierte Methoden der Erschließung zu ergänzen.

                
Die inhaltliche Erschließung der Arzneimittelrezepte würde nicht nur die Findbarkeit der Daten enorm erhöhen, sie wäre auch Grundvoraussetzung, um die im Folgenden beschriebenen rechtlichen Grenzen der Offenheit überwinden zu können und die Daten vollständig 
                    
open access
 zugänglich zu machen.
                

            

            

                
Rechtsebene: Herausforderungen und Grenzen der offenen Datenmodellierung

                
Das ArIS-Projekt wurde Ende August 2022 beendet. Die entstandenen Daten bilden eine vielversprechende Grundlage für weitere Forschung. Die Objekte wurden im Projekt auf Einzelartefaktebene erschlossen, sodass bestimmt werden konnte, für welche Jahre Arzneimittelrezepte vorliegen und wo die physischen Artefakte jeweils auffindbar sind. Ein Subset der Artefakte konnte bereits vollumfänglich transkribiert, d.h. inhaltlich erschlossen werden.

                
In Bezug auf die Nachnutzbarkeit von Daten stehen üblicherweise Fragestellungen zu Urheber- und Verwendungsrechten im Vordergrund. Im vorliegenden Fall sind allerdings weitergehende Schutzrechte zu beachten, wie das im Deutschen Reich definierte besondere Schutzrecht für Patientendaten (vgl. Deutsches Reichsgesetzblatt 1871, §300). Das genaue Ende von Schutzfristen für in Arzneimittelrezepten vorkommende Daten ist juristisch nicht eindeutig zu klären.
 Hinzugezogene Experten haben dem Projekt geraten, dass Arzneimittelrezepte bis Ende des Jahres 1871 unbedenklich zugänglich gemacht werden können. Bei Rezepten, die ab dem Jahr 1872 ausgestellt wurden, müssten identifizierende Merkmale von Ärzt*innen oder Patient*innen unkenntlich gemacht werden. Dieser Umstand hatte besonderen Einfluss auf den Projektverlauf und offenbart eine zentrale Herausforderung für offene Forschungsdaten und ihre Langzeitverfügbarmachung. So musste hier zweifelsfrei sichergestellt werden, dass alle vorliegenden Digitalisate jahresgenau datiert wurden. Bei mehreren oder teilweise unleserlichen Datumsangaben wurde grundsätzlich das jüngste Datum als Datenbankeintrag gewählt.
                

                
Eine Ausgangslage des Projekts ist somit die Zwickmühle, dass die Güte einer automatisierten Erschließung hoch sein muss, um eine inhaltliche Aussage zu treffen – und bis dahin können auch nicht beispielsweise lediglich die Scans der nicht-transkribierten Zettel öffentlich zur Verfügung gestellt werden, da so potentiell personenrelevante Daten offengelegt würden. Da die aktuellen Methoden hier nicht ausreichende Ergebnisse liefern, können auch personenrelevante Daten nicht leicht bereinigt werden.

                
Die Existenz von langen Schutzfristen über mehrere Jahrzehnte hat damit zur Folge, dass eine öffentlich zugängliche Archivierung nur mit Schwärzung zentraler Merkmale der Artefakte möglich wäre. Hierfür wäre wiederum die Transkription sämtlicher Arzneimittelrezepte Voraussetzung, um etwa automatisiert nach benannten Entitäten suchen zu können. Alternativ könnte man nur einen eng umrissenen Teil der Sammlung zugänglich machen. Weiterhin ist zu beachten, dass mit fortschreitender Zeit Schutzfristen für bestimmte Jahrgänge der Artefakte aufgelöst werden müssten. Diese Dynamik in der Möglichkeit, Daten zugänglich zu machen, ist eine besondere Herausforderung für Open Data in einer projektbasierten Forschung. Auch dauerhaft bereitstehende Supportstrukturen können hier häufig keine Abhilfe schaffen. Dazu kommen Aspekte des ethischen Umgangs mit Kulturgut. Neben einer möglichst umfänglichen Umsetzung der FAIR-Prinzipien sollen die CARE-Prinzipien nicht minder Beachtung finden (vgl. Research Data Alliance 2019).

                
Im Sinne des Projekts wurde ein mehrstufiges Vorgehen gewählt, das dem Museum eine zentrale Rolle einräumt: Die Datenbank wird als „work-in-progress“ zum Projektende an das Deutsche Apotheken-Museum übergeben. Durch die initiale Analyse aller Artefakte ist es möglich, Teile des Datenschatzes zugänglich zu machen und diesen Teil sukzessive auszuweiten. Damit wird nicht nur den Schutzrechten Rechnung getragen, sondern auch sichergestellt, dass die physischen Artefakte weiterhin für Forscher*innen zugänglich bleiben. Vollständig erfasste und transkribierte Artefakte können sukzessive, vorausgesetzt die Schutzfristen entfallen, ebenfalls öffentlich zugänglich gemacht werden.

                
Niemand kann antizipieren, welche Forschungscommunitys sich in Zukunft mit welchen Fragen an bestimmte Datenschätze wenden. Das ist ein Grundprinzip des Forschens, auf das die FAIR-Prinzipien produktiv antworten. Die öffentliche Zugänglichkeit nützt damit der gesamten Forschung und macht vor allem die erhobenen Daten zukunftsfähig. Unser Verständnis von FAIR und Open Access ist, dass hier der normative Anspruch erhoben wird, zunächst keine Zugangsschranken zu erstellen. Dieser Anspruch muss vor anderen Rechtsgütern verantwortungsvoll ausgehandelt werden. Es gilt daher ganz im Sinne der European Commission (2021, 61) der Grundsatz: „as open as possible, as closed as necessary“. 

                
Wie diese Balance aussehen sollte, ist, wie das Beispiel der Daten der Arzneimittelrezepte zeigt, nicht einfach zu beantworten. Es zeigt sich, dass eine gesellschaftliche Auseinandersetzung mit dieser Frage hilfreich wäre, um gegebenenfalls rechtliche Rahmenbedingungen neu zu bewerten. Für Museen ergibt sich mit diesem Aspekt der Digitalisierung die neue Herausforderung, wie sie einerseits dem Anspruch des Open Access genügen können und andererseits den Schutz der Daten vor Zugriff durch rechtlich Unbefugte gewährleisten. Dieser Beitrag veranschaulicht jene Herausforderung, ohne eine eindeutige Empfehlung aussprechen zu können.

            

        

            

                
Intertextualität als theoretisches Konzept in der (digitalen) Forschung

                
Wie können intertextuelle Beziehungen formalisiert und annotiert werden? Was wäre ein kohärentes Kategoriensystem der Intertextualität und welche Formalisierung ist geeignet, um es computergestützt berechenbar zu machen, ohne seine Aussagekraft zu verlieren. Intertextualität ist eine komplexe und zugleich sehr zentrale Kategorie in der Literaturanalyse. Per Definition betrifft sie nicht nur 
                    
ein 
(literarisches) Artefakt, sondern mindestens zwei und beschreibt die Beziehung zwischen ihnen (vgl. Pfister 1985, 11). Diese Beziehungen können auf zahlreichen Ebenen zu finden sein und je nach persönlichem Verständnis von „Intertextualität“ können entweder Grenzen gezogen werden oder Beziehungen überall zu finden sein. Etliche literarische Gattungen konstituieren sich durch ihren inhärenten Verweischarakter: Persiflage, Parodie, Pastiche, Cento (Flickengedicht), Travestie, Stilkopien usw. Unterhalb der Ebene der Konstitution von Gattungen lässt sich Intertextualität als feingranulares Geflecht von Bezugnahmen in den meisten, wenn nicht gar allen Texten feststellen. Auf dieser Ebene wird Intertextualität mit Begriffen wie Hypolepse (textuelle Reaktionen auf andere Texte durch z.B. Zustimmung, Ablehnung, Weiterführung, Korrektur usw.) oder Anspielung beschrieben. Zudem können hier Bezugnahmen in Form von z.B. nicht-gattungskonstituierenden Stilkopien oder im parodistischen Modus einer Textpassage vorkommen. Generell unterscheidet man dabei Einzeltextreferenzen von sog. Systemreferenzen, d.h. Referenzen auf ganze „Systeme“ wie etwa Gattungen.
                

                
In der Forschung finden sich daher zahlreiche Ansätze, das von Julia Kristeva (1967) benannte theoretische Konzept Intertextualität als einen Beschreibungsbegriff für die Beziehung zwischen Texten zu systematisieren. Zu nennen sind in diesem Zusammenhang insbesondere Barthes (1984), Genette (1982), Pfister (1985), oder in digitaler Hinsicht Scheirer et al. (2016), Schlupkothen und Nantke (2019) und Burghardt und Liebl (2020). Die einzelnen Ansätze und Systematisierungen haben in der Regel verschiedene theoretische oder praxeologische Hintergründe (z.B. Strukturalismus, Poststrukturalismus oder Digital Humanities) und damit verbunden verschiedene Fokusse. Für Kristeva (1967) etwa bildet das Konzept Intertextualität einen theoretischen Zugang zur Dialogizität literarischer Texte. Mit Bezug auf Michail Bachtin entwickelt sie ein Verständnis von Text als „Mosaik von Zitaten“ (Kristeva 1967, 348). Genette (1982) beschreibt – ebenfalls mit dem Ziel einer Gattungstypologie – textuelle Bezugnahmen als Transformation oder Nachahmung hypertextueller Textgattungen. Er differenziert fünf Formen: 1. Intertextualität durch Zitate, Plagiate oder Anspielungen, 2. Paratextualität, womit er Rahmungen wie Titel, Genreklassifikationen, Autorname etc. meint, 3. Metatextualität (d.h. kritische Kommentare), 4. Architextualität (d.h. externe, z.B. durch Kritiker zugewiesene Rahmungen) sowie 5. Hypertextualität, bei der ein späterer Text (Hypertext) ohne einen vorherigen Bezugstext (Hypotext) nicht denkbar ist. Hypertextualität unterteilt er schließlich in Transformation (James Joyce transformiert in 
                    
Ulysses 
die 
                    
Odyssee 
Homers – „dasselbe anders sagen“) und Nachahmung (Vergil ahmt in der 
                    
Aeneis 
die 
                    
Odyssee 
nach – „etwas anderes auf dieselbe Weise sagen“; Genette 1982, 17). Pfister (1985) schließlich bietet ein graduelles System an, um die beiden Pole der sehr weiten und sehr engen (Inter)textualitätskonzeption miteinander zu verbinden.
                

                
In Operationalisierungsansätzen der Digital Humanities wird Intertextualität als 
                    
text reuse
 (vgl. Burghardt und Liebl 2020) oder im Sinne von 
                    
event alignments
 (vgl. Reiter und Frank 2015) verstanden. Scheirer et al. (2016) versuchen mithilfe des 
                    
Latent Semantic Indexing
 (LSI) darüber hinausgehend semantisch-thematische intertextuelle Bezüge zwischen Ausgangs- und Bezugstexten zu modellieren. Diese Ansätze zielen darauf ab, den Weg für die Entwicklung eines automatischen Detektors für intertextuelle Beziehungen zu ebnen. Dies ist nicht unser Ziel. Die Wiederverwendung von Texten, das automatisierte Auffinden von Zitaten oder ähnlichen Textpassagen in einem Textkorpus – an sich schon anspruchsvoll aus Sicht der Informatik – erscheint aus der Perspektive der Literaturwissenschaft, die sich traditionell mit semantisch viel komplexeren Formen intertextueller Beziehungen beschäftigt, häufig unterkomplex.
                

                
Ziel des Beitrags ist – statt von bestimmten digitalen Verfahren auszugehen – die theoriegeleitete Modellierung eines maschinenlesbaren Schemas, eines Kategoriensystems, das strukturell und grundlegend Analysen von Intertextualität, wie sie in literaturwissenschaftlichen und -theoretischen Abhandlungen zu finden sind, repräsentieren kann. Schlupkothen und Nantke (2019) verfolgen ein ähnliches Ziel. Bei ihnen ist aber nicht weiter ausgearbeitet, wie sich das Vorhaben, analytisch-interpretatorische Lektürepraktiken zu repräsentieren, zur eingesetzten Technologie X-Link verhält und welche Beziehung diese zu der von den Autoren ins Spiel gebrachten Situationslogik hat.

            

            

                
Formale Methoden und maschinelle Interpretierbarkeit

                
Ob ein logisches System (Prädikatenlogik, Beschreibungslogik, Situationslogik etc.) für die Formalisierung eines Forschungsgegenstandes geeignet ist, hängt von den Zielen ab, die mit der formalen Repräsentation des Gegenstandes erreicht werden sollen. Ist der Gegenstand formal repräsentiert, lassen sich durch ein formalen Kalkül Entscheidungfragen hinsichtlich ihres Wahrheitswertes auswerten und (neue) Aussagen aus dem Formalisierten ableiten, worunter auch das Abfragen der 'Fakten'-Basis zählt. Ganz allgemein sind bei der Wahl eines logischen Systems folgende Aspekte ausschlaggebend: Es sollte so ausdrucksmächtig sein, dass es für die formale Repräsentation des Gegenstandes geeignet ist (Ausdrucksmächtigkeit). Und für die DH ist es wünschenswert, dass der formale Kalkül von einem Computer ausgeführt werden kann (Implementierung), und zwar zudem effizient (Komplexität).

                
Ziel unserer Formalisierung der Domäne Intertextualität ist die Repräsentation von Intertextualitätsanalysen. Eine Einschränkung auf eine bestimmte Intertextualitätstheorie oder auf eine bestimmtes Teilphänomen, etwa werkästhetisch manifeste Intertextualität, soll zunächst nicht erfolgen. Stattdessen versuchen wir, einen gemeinsamen Kern von Intertextualitätskonzepten freizulegen, und zwar so, dass er für spezielle Theorien erweiterbar ist. Ziel ist also, intertextuelle Relationen zu annotieren, abzufragen und ggf. Aussagen abzuleiten.

            

            

                
Formalisierung des Kerns

                
Die Formalisierung wird zunächst mit halbformalen Mitteln durchgeführt: mit einer Liste dessen (der Aspekte oder Merkmale), was repräsentiert werden soll. Einen ersten Zugang zum gemeinsamen Kern der verschiedenen kursierenden Intertextualitätstheorien bietet eine Analyse des Wortes Intertextualität. Es besteht aus den lexikalischen Morphemen 
                    
inter
 und 
                    
text
 sowie dem Derivationsmorphem 
                    
-alität
, fr. 
                    
-alité
, lat. 
                    
-alitas
. Während 
                    
text
 gegenständlich ist, ist 
                    
inter
 präpositional. Also einfach zwischen Text bzw. zwischen Texten? Ganz so „durchsichtig und selbsterklärend“ (Adamzik 2004, 96) ist das Wort jedoch nicht, denn die Präposition 
                    
inter 
setzt nicht nur zwei (oder mehrere) Entitäten miteinander in Beziehung, sondern gibt dem Zwischenraum auch ein eigenes Sein; siehe z.B. '
                    
inter
lineare Annotationen'. Möglicherweise liegt genau hier der Designfehler des Neologismus, denn Theorien des globalen Intertexts, von dem im Singular gesprochen wird, rücken anscheinend diesen Zwischenraum ins Zentrum ihres Sprachspiels, ohne weiter zu bestimmen, was dort ist. Hinzu kommt, dass die Präposition 
                    
inter
, anders als z.B. 
                    
trans
 ungerichtet ist. Das steht im Gegensatz dazu, dass das theoretische Konzept auch entwickelt worden ist, um Texten eine historische Dimension zu geben, nämlich auf ihre Avant-Texte hin, so schon bei Kristeva und insbesondere in Genettes Palimpsest-Metapher und seinem Transtextualitätskonzept (vgl. Dosse 1997, II, 446f.). In der historischen Dimension gibt es aber Gerichtetheit; und ein Bezug auf zukünfitge Texte kann unschwer als gegenwärtige Vorstellung zukünftiger Texte angesehen werden. Im Kern, so scheint uns, rückt die Untersuchung von Intertextualität immer einen späteren Text ins Zentrum und untersucht seine Bezüge zu früheren Texten; andersherum handelt es sich um Wirkungs- und Rezeptionsforschung. Im Kern, so halten wir fest, zielt Intertextualität auf die Beziehung (1a) 
                    
zwischen Texten
, und zwar derart, dass eine intertextuelle Beziehung (1b) 
                    
anti-chronologisch gerichtet 
ist. Auch wenn Beschreibungen von Intertextualität mitunter Komplexe von vielen Texten in den Blick nehmen, so können solche Komplexe doch als eine Menge (1c) 
                    
binärer
 intertextueller Beziehungen repräsentiert werden, sofern die Möglichkeit gegeben ist, Vermittlung durch ein Drittes sowie transitive Relationen zu modellieren. Der Formalismus muss also erlauben, dass (2) Vermittlungsinstanzen notiert werden können. Solche können wiederum binäre intertextuelle Relationen sein, jedoch muss die Kategorie der Vermittlungsinstanz auf eine Vielzahl von Phänomenen hin erweiterbar sein. Selbstredend muss eine intertextuelle Relation zudem auch mit den (3) 
                    
speziellen Kategorien der verschiedenen Theorien
 bestimmt- bzw. beschreibbar sein, wobei diese Bestimmung nach Kategorien auch Grade (vgl. Pfister 1985) umfassen können muss. Das Textkonzept als solches, bis hin zu der Frage, ob Text manifeste Schrift in einem Zeichensystem ist oder ein weiteres Verständnis vorliegt, gehört aber nicht zum Kern von Intertextualität, sondern zur theoretischen Ausprägung, also aus unserer Perspektive zur Peripherie. Der Aspekt der Markiertheit auf der Textoberfläche, den Pfister und insbesondere die Textlinguistik interessieren, könnte sich nicht zuletzt im Hinblick auf Weiternutzung durch Textmining als fruchtbar erweisen und ist allgemeiner Art. Wir halten folgende Form intertextueller Relationen fest und notieren dabei zugleich die Anzahl der genannten Aspekte, wobei n eine natürliche Zahl ist und n
                    
0
 eine natürliche Zahl oder Null: TextHier/1, TextDort/1, Vermittlungsinstanz/n
                    
0
, Bestimmung/n, Marker/n
                    
0
.
                

                
Der auf diese halb-formale Art entworfene Kern von Intertextualität lässt sich nicht mit allen formalen Methoden repräsentieren. 
                    
 Zur Formalisierung von Intertextualität ist ein Ausdrucksmittel von Relationalität erforderlich. Damit scheidet die Aussagenlogik aus und es muss auf eine Prädikatenlogik zurückgegriffen werden. Die allgemeine Prädikatenlogik erster Stufe ist wiederum ausdrucksstärker als für unser Vorhaben nötig. Als ausreichend ausdrucksstark erweist sich die Beschreibungslogik, die in einfacher Ausprägung im Wesentlichen der Prädikatenlogik entspricht mit der Einschränkung auf ein- und zweistellige Prädikate (vgl. Baader at al., Hg. 2010, Harmelen et al., Hg. 2008).
 Für sie steht in RDF eine Implementierung zur Verfügung. Wir geben hier nur ein Beispiel für eine in RDF notierte intertextuelle Relation (für ihre Form) und verweisen für die Ontologie in RDFS/OWL auf unser Github-Repository.

                

                

                

                    
@prefix :
                    
https://intertextuality.org/abstract#
> .
                

                

                    
@prefix ex:
                    
https://example.org/my-intertextual-findings/
> .
                    
ex:i a :IntertextualRelation;
                    
:here ex:t1 [a :Reference]; # TextHier/1
                    
:there ex:t2 [a :Reference]; # TextDort/1
                    
:mediatedBy ex:md [a :Mediator]; # Vermittlungsinstanz/n
                    
0

                    
:specifiedBy ex:s [a :IntertextualSpecification]; # Bestimmung/n
                    
:markedBy ex:mrk [a :Marker ]. # Marker/n
                    
0

                

                

                
In der Ontologie ist ausgedrückt, dass intertextuelle Relationenen Mediatoren sein können: 
                    
:IntertextualRelation rdfs:subClassOf :Mediator.
 Auf diese Weise wird die Struktur rekursiv bzw. können vermittelte, transitive Relationen repräsentiert werden. Die Klasse 
                    
Reference
, welche für die Texte verwendet wird, zwischen denen die intertextuelle Relationen beschrieben wird, wird mit der Onotologie von Web Annotations modelliert.
 Dies gilt potentiell auch für die Klasse 
                    
Marker
, jedoch sind auch andere Marker, z.B. Typen aus einem System von Markierungen, realisierbar. Die Metadaten der Referenzen, insbesondere das Datum der Ersterscheinung, ermöglichen, die derart formalisierten Aussagen hinsichtlich ihrer anti-chronologischen Gerichtetheit auf Konsistenz zu prüfen (Reasoning).
                

            

            

                
Erweiterungen: Genette

                
Der Kern ist erweiterbar, indem die Klassen 
                    
Mediator
, 
                    
IntertextualSpecification
 und 
                    
Marker
 durch Bildung von Subklassen differenziert und spezifiziert werden. Beispielhaft soll dies hier für Genettes Beschreibungskategorie Hypertextualität durchgeführt werden. Verglichen mit Kristevas Intertextualitätsbegriff bezeichnet sie ein sehr enges Feld, das durch eine 1-zu-1-Beziehung zweier Texte, des Hypertextes B zu einem vorhergehenden Hypotext A, gekennzeichnet ist. Die 1-zu-1-Beziehung wird dabei weniger durch Einzelstellen der Texte gestiftet. Vielmehr setzen paratextuelle Signale (z.B. der Titel des Hypertextes) die Texte als Ganzes in hypertextuelle Beziehung. Dennoch manifestiert sie sich auch im Verhältnis einzelner Textstellen. Eine Formalisierung dieser auf einer Relation beruhenden Relationen, die bislang ein Desiderat der DH geblieben ist, wird auf Grundlage des vorgeschlagenen Kerns möglich. Wir schlagen vor, das paratextuelle Signal des Titels als Vermittlungsinstanz der vielen intertextuellen Relationen zwischen Einzelstellen aufzufassen.
                

                

                    

                        
@prefix g:
https://intertextuality.org/extensions/genette/hypertextuality#
> .
                        
@prefix pt:
https://intertextuality.org/extensions/genette/paratext#
> .
                        
@prefix :
https://intertextuality.org/abstract#
> .
                        
@prefix rdfs:
http://www.w3.org/2000/01/rdf-schema#
> .
                        
@prefix owl:
http://www.w3.org/2002/07/owl#
> .
                    

                    

                    
&lt;
https://intertextuality.org/extensions/genette/hypertextuality
> a owl:Ontology .
                    
&lt;
https://intertextuality.org/extensions/genette/paratext
> a owl:Ontology .
                    

                    

                        
pt:ParatextualSignal rdfs:subClassOf :Mediator .
                        
pt:title a pt:ParatextualSignal .
                    

                

                
Vor allem aber gewinnt Genette aus der Analyse der Hypertextualität eine Gattungstypologie. Dazu unterteilt er diese Beziehung nach zwei Gesichtspunkten: Es kann sich entweder im Hinblick auf den Relationstyp um eine Imitation oder um eine Transformation handeln, und sie kann im Hinblick auf die Art und Weise entweder spielerisch, satirisch oder ernst sein. Daraus gewinnt er durch Kombination sechs Gattungen. In RDFS/OWL formalisiert heißt das:

                

                    

                        
g: 
HypertextualRelation
 rdfs:subClassOf :IntertextualSpecification ;
                        
owl:disjointUnionOf (g:RelationalType g:ModalType) .
                        
g:RelationalType owl:ObjectOneOf (g:transformation g:imitation) .
                        
g:ModalType owl:ObjectOneOf (g:playfully g:satirically g:
                        
seriously
) .
                    

                

                
Die Formalisierung wäre dadurch zu ergänzen, dass eine solche hypertextuelle Relation auch über eine Vermittlungsinstanz verfügen muss.

            

            

                
Fazit und mögliche Anschlussforschung

                
Die hier vorgeschlagene Formalisierung auf Grundlage der Beschreibungslogik kommt an bestimmten Stellen an ihre Grenzen. Eine davon steckt im Begriff Situation: Eine der fundamentalen Unterscheidungen intertextueller Relationen ist die zwischen solchen, die werkästhetisch manifest sind (etwa durch paratextuelle Signale oder andere Marker), und solchen, die rezeptionsästhetisch gefunden worden und damit unklarer sind (vgl. Pfister 1985, 23f.). Mit einem rezeptionstheoretischen Hintergrund schlagen Schlupkothen und Nantke (2019) die Situtationslogik nach Barry und Parwise als angemessene formale Methode vor. Die hier vorgeschlagene auf der Beschreibungslogik bzw. OWL basierende Formalisierung bietet die Möglichkeit, die Situation durch Metadaten (Name, Datum) zu kodieren und einem Datensatz anzuhängen. Allerdings ist das keine Implementierung der Situationslogik, bei dem es insbesondere um eine Fromalisierung des Zusammenhangs von Situation und Konsistenz geht. Denselben Einwand wird man auch gegenüber dem Beitrag von Schlupkothen und Nantke einwenden können, denn die von ihnen eingesetzt Technologie X-Link ist ebenfalls keine Implementierung der Situtationslogik.

                
Unser Beitrag hat das Ziel, in der Pluralität unterschiedlicher Konzeptionen von Intertextualität einen Kern von Intertextualität herauszuschälen und so zu formalisieren, dass er durch Theorien erweiterbar ist. Er bahnt damit einen Weg zur Repräsentation intertextueller Beziehungen, welche einerseits der Komplexität der literaturtheoretischen Konzepte gerecht wird und die andererseits Berechenbarkeit gewährleistet.

                
Der Beitrag richtet sich damit einerseits an Intertextualitätstheoretiker*innen und -praktiker*innen. Erstere können durch unsere Formalisierung ihren Intertextualitätsbegriff schärfen: durch eine weitere Verfeinerung des von uns vorgeschlagenen Modells oder durch eine klare Abgrenzung des eigenen Intertextualitätsbegriffs. Intertextualitätspraktiker*innen wird ein (erweiterbares) Modell an die Hand gegeben, um Intertextualität zu identifizieren/zu annotieren und zu analysieren (z.B. mittels Netzwerkvisualisierung, Netzwerkanalyse oder durch eine synoptische Gegenüberstellung von Textpassagen mit intertextuellem Bezug). Andererseits kann unsere theoretische Konzeption die Grundlage für die Architektur einer möglichen Forschungsumgebung bilden, die den Intertextualitätsforschenden sowohl eine Weiterentwicklung oder Anpassung des vorgeschlagenen Intertextualitätsmodells als auch die Erforschung der Intertextualität eines annotierten Textkorpus auf Basis dieses Modells erlaubt.

            

        

            

                

                    
Motivation
                

                
Die Posterpräsentation stellt das Projekt „Kaleidoskopische Muster des Protests“ vor, das visuelle und textuelle (Selbst-)Repräsentationen osteuropäischer Protestkulturen sowohl aus qualitativer als auch aus quantitativer Perspektive untersucht. Dieses Projekt wird von der Österreichischen Akademie der Wissenschaften gefördert und startet im Jänner 2023.

                
Politische Protestbewegungen haben in Osteuropa, insbesondere in Russland, der Ukraine und Belarus in den letzten zehn Jahren großen Aufschwung erlebt. In diesen drei Ländern besetzten Protestierende öffentliche Plätze und verwendeten visuelle Symbole und Slogans, um andere Menschen dazu bewegen, sich den Protesten anzuschließen. Gleichzeitig wurden Bilder der Protestierenden auch von den autoritären Regierungen dieser Länder genutzt, um die Protestbewegungen zu delegitimieren.

                
Eine wichtige Rolle bei der Organisation der Proteste sowie deren Wahrnehmung spielen soziale Netzwerke (Smyth &amp; Oates 2015; Onuch 2015) und die Medien, vor allem das staatlich kontrollierte Fernsehen, das nach wie vor als das wichtigste Informationsmedium in diesen drei Ländern gilt (vgl. Szostek 2018). Aus diesem Grund sind medial vermittelte (Selbst-)Repräsentationen, u.a. YouTube-Videos, Blogbeiträge, Kommunikation über soziale Netzwerke, Fernsehnachrichten sowie Dokumentarfilme, ein integraler Bestandteil der Proteste selbst: Symbole und Slogans werden dazu genutzt, um die Ideen und Forderungen der Protestierenden zu verbreiten, weshalb Proteste 
                    
per se
 als „kommunikativer Akt“ charakterisiert werden können (Kuße 2021). So signalisierten beispielsweise bei den Protesten in Russland (2011/12) weiße Bänder eine regierungskritische Haltung, während eine regierungsfreundliche Einstellung durch schwarz-orangene Georgsbänder ausgedrückt wurde. Auf dem Euromajdan in der Ukraine (2013/14) waren neben der Europaflagge auch rechtsnationale Symbole präsent. In Belarus (2021) diente die weiß-rot-weiße Flagge als Hauptsymbol für die Proteste gegen die Regierung (vgl. Gaufman 2021).
                

                
Slogans verstehen wir im Sinne von Friedman (2019) als die „Spitze diskursiver Eisberge“ sowie als „wichtige symbolische Schlüssel“ zu sozialen Bewegungen, wobei wir uns bewusst sind, dass deren ursprüngliche Bedeutung mit der Zeit transformiert und neuinterpretiert werden kann. Im Unterschied zu Symbolen definieren wir Slogans als rein textuelle Erscheinungen.

                
Symbole lesen wir hingegen in Anlehnung an Cassirer (1996) als „soziale Phänomene“, die an die Stelle der rein sprachlichen Kommunikation treten. Sie sind mit einer bestimmten Bedeutung oder Bedeutungen aufgeladen, die sowohl individuell als auch im Kontext (im sozialen Raum) interpretiert werden können.

                
Im Laufe unserer quantitativen und qualitativen Analyse werden wir laufend überprüfen, ob diese Definitionen noch Gültigkeit haben oder überarbeitet bzw. angepasst/nachgeschärft werden müssen. Die Veränderung der Symbolik möchten wir jedenfalls miteinbeziehen und aus unserer Sicht ist das aufgrund der unterschiedlichen Perioden und Länder, die wir im Projekt abdecken, möglich. 

            

            

                
Methoden

                
Um die (Selbst-)Repräsentationen von Protest in den oben genannten osteuropäischen Ländern zu erfassen, betrachten wir diese aus drei verschiedenen Perspektiven: (1) die Selbstrepräsentation der Protestkulturen auf YouTube sowie in den sozialen Netzwerken, (2) deren offizielle Darstellung in regierungstreuen sowie in unabhängigen TV-Nachrichtensendungen und (3) ihre cineastische Darstellung in drei Dokumentarfilmen. Aufgrund dieser verschiedenen Blickwinkel sprechen wir in unserem Projekttitel auch metaphorisch von „kaleidoskopischen Mustern des Protests“, die unsere Forschung sichtbar machen soll. Zu diesem Zweck kombinieren wir eine automatische Symbolerkennung mittels künstlicher neuronaler Netze (R-CNN) mit der Multimodalen Diskursanalyse (MDA) aus der Linguistik (Kress 2011). Mit diesem Ansatz folgen wir N. K. Hayles’ (2010) Kombination von „close reading“, „machine reading“, und „hyper reading“; übertragen auf die visuellen Medien wird dabei aus dem „machine reading“ Arnold/Tiltons (2019) „distant viewing“.

                
                    

                        

                    
Abb. 1: Beispielframe aus einem YouTube-Video (links), Annotationen von einem vortrainierten Netz für panoptische Segmentierung (Mitte), Annotationen von unserem selbst trainierten Netz (rechts)

                    

                
Der Rückgriff auf „distant viewing“ ist nicht zuletzt aufgrund der großen Datenmenge notwendig: Seit Februar 2022 haben wir über 74.000 YouTube-Videos zu Belarus, Russland und der Ukraine inklusive Metadaten gesichert – mit einer Gesamtlaufzeit von knapp eineinhalb Jahren. Dieses Videokorpus wird von einem selbst trainierten neuronalen Netz nach nationalistischen Symbolen aus Osteuropa durchsucht. Konkret verwenden wir das Detectron2 Framework, weil es erlaubt, schnell eigene Netze zu trainieren, deshalb wird es verwendet; ergänzend werden wir auch vortrainierte Netze verwenden, etwa eine zero-shot-detection mit CLIP, um Szenenbeschreibungen zu bekommen. 

                
Das eigene Training ist insofern notwendig, als frei verfügbare vortrainierte Netze das notwendige domänenspezifische Wissen nicht mitbringen. Das Beispiel in Abb. 1 zeigt, wie das vortrainierte Netz zwar die grundlegende Konfiguration des Videoframes beschreiben kann; die für uns wichtigen Flaggen werden jedoch nicht oder falsch erkannt. Die kontextspezifische Information liefert somit unser eigenes Netzwerk, dessen Betaversion wir im Sinne der Open Humanities bereits veröffentlicht haben (Howanitz/Radisch 2022). Dieses Netz erreicht zur Zeit eine AP50 von 75.794 bei einem Trainingskorpus von 4.045 Bildern mit 8.156 Annotationen und einem Testkorpus mit 1.032 Bildern und 2.109 Annotationen. Wir haben 44 Symbolklassen definiert, und es sind knapp 100 Bilder pro Klasse im Korpus. Beim Training haben wir 20.000 Iterationen mit Batches von je 512 Bildern verwendet.

                
Die Resultate der Symbolerkennung werden zunächst evaluiert, visualisiert und anschließend für die MDA in MAXQDA aufbereitet. Die automatisch erstellten Annotationen dienen dabei nicht unmittelbar einem "close viewing". Vielmehr soll der quantitative Teil des Projekts helfen, gezielt Videos für die qualitative Analyse auszuwählen. Darüber hinaus interessiert uns eine statistische Auswertung des Symbolrepertoires im Korpus (z.B. Symbolverteilung über einen gewissen Zeitraum). Schließlich analysieren wir mithilfe der MDA ausgewählte Videos, um den Kontext, die Akteur:innen und ihre Rolle sowie die Interaktion verbaler und visueller Informationen zu untersuchen. Dabei konzentrieren wir uns auf die Fragen, welche allgemeinen Muster visueller und textueller (Selbst-)Repräsentationen von Protest erkennbar sind, welche Gemeinsamkeiten und Unterschiede es zwischen den einzelnen Ländern sowie den unterschiedlichen Medien gibt sowie auf die Frage, wie Protestsymbole und -slogans in den jeweiligen Medien (re-)kontextualisiert werden.

            

            

                
Ziele

                
Ziel unseres Projekts ist einerseits, ein Best-Practice-Beispiel für die Analyse eines großen visuellen Korpus zu liefern. Darüber hinaus erforschen wir visuelle und textuelle (Selbst-)Repräsentationen von Protestkulturen in Osteuropa und untersuchen, wie Bilder und Texte in verschiedenen Medien und Kontexten (wieder)verwendet werden. Schließlich stellt unser Projekt auch eine Momentaufnahme verschiedener osteuropäischer Protestkulturen dar und beantwortet die Frage, was von den Protesten nach Ablauf einer gewissen Zeit übrig bleibt: Insbesondere in autoritären Staaten wie Belarus und Russland ist nicht davon auszugehen, dass staatliche Medien die Erinnerung an regierungskritische Proteste bewahren.

            

        

            

                
Motivation

                
Das von der AG Film &amp; Video organisierte Panel setzt sich inspiriert vom Tagungsmotto „Open Humanities, Open Culture“ mit Fragen der Offenheit in der Filmwissenschaft auseinander. In den letzten Jahren wurden verschiedene Aspekte der Open Humanities im Kontext der Filmwissenschaft umrissen, und zwar in Einzelbetrachtungen, die zunächst den Umweg der Medienwissenschaften nehmen (Sondervan 2018; Hirsbrunner 2019). Darüber hinaus gab es Arbeiten zum Potential offener Forschungsdaten für filmwissenschaftliche Fragestellungen (Heftberger et al. 2020), zum Forschungsdatenmanagement in der Filmwissenschaft (Dang 2020) sowie zur Verfügbarmachung digitaler Filme durch das Bundesarchiv (Heftberger 2020). Auch die Frage der Analysevokabulare wurde aufgeworfen (Bakels et al. 2020), ebenso jene des Open Access (Kolleg-Forschungsgruppe 
                    
Cinepoetics
, FU Berlin; GfM-AG Open Media Studies: 
                    
)
                    
 

                

                
Das Panel möchte diese Einzelbeobachtungen zusammenführen und gemeinsam mit weiteren Fragestellungen diskutieren. Dabei erweist sich die Frage nach Offenheit für die Filmwissenschaft als besonders virulent. Ein zentrales Thema ist die Wiederverwertbarkeit von Software: Digitale Tools für Bewegtbilder sind komplex und aufwändig, der erhöhte Entwicklungsaufwand ‚rechnet‘ sich erst bei intensiver Nutzung. Forschungsdaten können häufig nicht zur Verfügung gestellt werden, aus urheberrechtlichen Gründen – die milliardenschwere Filmindustrie wirkt hier entgegen – ebenso wie aus technischen – Filme brauchen im Vergleich zu anderen Medien ein Vielfaches an Speicherplatz – und organisatorischen – es fehlen etablierte Annotierungsstandards.

                
Das Panel nimmt verschiedene Herausforderungen in den Blick: technische Infrastruktur und Standardisierungsbestrebungen, Lehre, Forschungsdatenmanagement und Citizen Humanities. Wir sind überzeugt davon, dass die Filmwissenschaft durch das Angehen ihrer spezifischen Probleme wesentliche Impulse auch für andere Geisteswissenschaften setzen und die Open Humanities entsprechend weiterentwickeln kann.

            

            

                
Organisation des Panels

                
Wie schon bei der DHd 2022 verzichten wir auf Kurzreferate der Teilnehmer:innen. Um eine lebendige Diskussion zu erleichtern, setzen wir auf pointierte Eröffnungsstatements. Dabei bringt jede:r Teilnehmer:in eine spezifische Perspektive und eigene Forschungsfragen in die Diskussion ein, die im folgenden Abschnitt kurz umrissen werden. Die Fokussierung auf das Gespräch soll helfen, das Publikum verstärkt anzusprechen.

                
Der Zeitplan sieht eine zehnminütige Einführung in das Thema durch den Moderator vor, die von sechs dreiminütigen Kurzstatements der Teilnehmer:innen abgerundet werden. Es folgt ein offenes Panelgespräch entlang vorher verschickter Leitfragen (30 Minuten), bei dem das Publikum natürlich auch gerne eingreifen darf. Die letzten 30 Minuten sind für Fragen aus und Diskussion mit dem Publikum vorgesehen; dabei möchten wir auch mit digitalen Feedbacklösungen, beispielsweise Umfragetools, experimentieren, die das Eintreten in einen Dialog möglichst niederschwellig gestalten sollen.

            

            

                
Spezifische Perspektiven

                

                    
Ralph Ewerth
 (Hannover) wirkt bei der Entwicklung einer offenen Forschungsinfrastruktur zur Film- und Videoanalyse mit (im DFG-geförderten Projekt TIB-AV-A,
                    
 

                    
). Dabei ergeben sich zahlreiche Herausforderungen: die freie Verfügbarkeit der Infrastruktur über das Web, der große Datenumfang der Videos, die Usability der Benutzerschnittstelle, die Auswahl der anzubietenden Funktionen inkl. Datenimport/-export, die Realisierung als Open-Source-Projekt, die Erweiterbarkeit durch Plugins, die Nachhaltigkeit der Lösung, und nicht zuletzt die Bewerbung in den verschiedenen Communities. Daher benötigt die Entwicklung einer solchen Lösung Expertise in verschiedenen Gebieten, u.a. im Bereich Software-Entwicklung (Web-Interface, Backend-Technologien inkl. Nutzung von GPU-Prozessoren, Hosting der Daten), fundierte Kenntnisse des aktuellen Forschungsstands in der Informatik (Computer Vision, natürliche Sprachverarbeitung, Mustererkennung, maschinelles Lernen, Informationsvisualisierung) und ein interdisziplinäres Verständnis für die Anforderungen, die der Arbeitsweise der Filmwissenschaft entsprechen. Hieraus ergibt sich die Notwendigkeit, dass die Zielgruppen von vornherein in die Entwicklung der Infrastruktur mit einbezogen werden. Im Panel sollen die Herausforderungen aus der Open-Humanities-Perspektive der Filmwissenschaft diskutiert werden.
                

                

                    
Sarah-Mai Dang
 (Marburg) diskutiert Forschungsdatenmanagement im Bereich des Filmkulturerbes, das intellektuelle Konventionen und institutionelle Rahmenbedingungen widerspiegelt, denen spezifische Vorstellungen von Film, Kanon und Autorschaft eingeschrieben sind. Möchten Wissenschaftler:innen mit filmhistorischen Datenbanken arbeiten, ist zunächst zu verstehen, mit welchen Daten sie es eigentlich zu tun haben. Auf welchen Quellen basieren die Daten? Nach welchen Kriterien wurden sie generiert? Von wem und zu welchem Zweck?
                

                
In einem Vergleich zweier Beispiele aus der Arbeit der BMBF-Forschungsgruppe „Datenvisualisierungen in der digitalen Filmgeschichtsschreibung am Beispiel der Forschung zu Frauen im Frühen Kino“ (DAVIF) sollen diese Fragen näher erläutert werden. Konkret geht es um die filmwerksbezogenen Daten des DFF – Deutsches Institut &amp; Filmmuseum sowie die biographischen Daten des an der Columbia University angesiedelten Women Film Pioneers Project (WFPP), einer kollaborativen Onlineplattform, die mehr als dreihundert Profile aus der Stummfilmzeit versammelt. Während das Forschungsdatenmanagement des DFF von kuratorischen Überlegungen im Sinne einer interoperablen Nachnutzung geleitet wird und gemäß eines standardisierten Verfahrens erfolgt (EN 15907), ist die Arbeit des WFPP von einer Vielzahl an Forschungsinteressen im Sinne einer pluralen Filmgeschichtsschreibung bestimmt. Die Konsequenzen eines solchen ‚offenen‘ FDM für unsere Forschung und die Implikationen standardisierter Taxonomien sollen im Panel diskutiert werden.

                

                    
Josephine Diecke
 (Marburg) und 
                    
Thomas Scherer
 (Berlin) erachten offene Filmanalysevokabulare zur Videoannotation in Forschung und Lehre als einen neuralgischen Punkt für digitale Open-Humanities-Bestrebungen in der Filmwissenschaft. Die Suche nach geeigneten Tools, Annotationsvokabularen und Austauschformaten ist zentral an eine Kultur des lebendigen und aktiven Methodenaustauschs gebunden. Eine kurze Gegenüberstellung aktueller Ansätze, wie dem interaktiven Thesaurus der 
                    
VIAN-Web-App
 (https://www.vian.app/keywords), der linked-open-data 
                    
AdA-Filmontologie
 (Bakels et al 2020) und der Erprobung und Kritik der computergestützten Filmanalyse im Kontext des 
                    
Digital Cinema-Hubs
 (Diecke 2022) soll aufzeigen, welche Praktiken es im Forschungsfeld zu offenen, modularen und skalierbaren Filmanalysevokabularien gibt, die als Fundament für die Vermittlung von Filmanalysekompetenzen und die wechselseitige (Nach-)Nutzung von Forschungsdaten fungieren können. Diecke und Scherer leisten damit einen Beitrag zu den zuletzt im Workshop der AG ‚Film und Video‘ debattierten engeren und früheren Rückkopplungsschleifen zwischen Entwicklungen in den Digital Humanities und filmwissenschaftlicher Lehr- und Forschungspraxis, die dem Vorbehalt des Überstülpens fachfremder Systematisierungszwänge entgegenwirken. Ein Schritt in diese Richtung ist die Entwicklung von Lehrmodulen zur Einführung in die annotationsbasierte Filmanalyse sowie die Konzeption und Verbreitung übergreifender Analysestandards, wie sie Vukovic und Baresch im Folgenden vorschlagen. Im Austausch mit den Panel-Teilnehmenden und dem Publikum sollen die vorgestellten Perspektiven und Bedürfnisse gegengeprüft werden, um diese in weiterführende Bestrebungen zur Anwendung und Verbesserung der digitalen Filmanalyse einbinden zu können.
                

                

                    
Bregt Lameris
 (Open University of The Netherlands) untersucht, wie das im Rahmen von Barbara Flueckigers FilmColors-Projekt entwicklete Annotationstool VIAN als Plattformen für Citizen-Humanities-Projekte dienen kann. Die kontinuierliche Weiterentwicklung digitaler Werkzeuge ermöglicht es, Bürger:innen in geisteswissenschaftliche Forschungsprojekte einzubeziehen, oft in Zusammenarbeit mit Kulturerbeinstitutionen. Dadurch wird das Wissen über geisteswissenschaftliche Forschung in der Bevölkerung gefördert; gleichzeitig erheben Citizen Humanists wichtige Daten – etwa durch Taggen – und produzieren Wissen auf neue, ungewohnte Weise.
                

                
Die Implementierung von VIAN als Citizen-Humanities-Tool ermöglicht es Bürger:innen, audiovisuelle Videoanalysen ohne fachspezifische Ausbildung vorzunehmen. Den Teilnehmer:innen muss dabei der Umgang mit dem Tool und auch mit filmanalytischen Konzepten nähergebracht werden. Eine für VIAN bereits umgesetzte Möglichkeit ist die Entwicklung eines Bildglossars für filmwissenschaftliche Begriffe. Darüber hinaus können Citizen-Humanities-Projekte auch über eine Vereinfachung der Begrifflichkeiten bzw. das Ziel einer weniger detaillierten Analyse an verschiedene soziale Gruppen angepasst werden.

                
Nach einer Präsentation des aktuellen Glossars und einiger Ergebnisse meiner Arbeit, VIAN für Forscher:innen mit intellektuellen Beeinträchtigungen zugänglich zu machen, hoffe ich, die Podiumsteilnehmer:innen und das Publikum zu einer Diskussion jener komplexen Probleme anzuregen, mit denen uns die Citizen Humanities konfrontieren.

                

                    
Teodora Vukovic
 (Zürich) konzentriert sich auf die technischen Aspekte einer möglichen Standardisierungsbestrebung für Filmannotation. Für die multimodale Analyse in der Filmwissenschaft müssen verschiedene Datentypen kombiniert und synchron gehalten werden, z.B. Kameraperspektiven mit Transkripten von Sprache und visuellen Inhalten. Die Komplexität dieser multimodalen Daten erschwert es, allgemeingültige und umfassende Standards vorzuschlagen. Es fehlt an standardisierten Softwarelösungen, und schließlich war die multimodale Analyse lange ein weitgehend qualitatives Gebiet, das keine ausgefeilten Datenstrukturen benötigte, wie sie in Gebieten mit einer längeren quantitativen Tradition, wie etwa der Korpuslinguistik, zu finden sind. Mit dem Aufkommen neuer Verarbeitungstechnologien, KI-Tools und der Zunahme multimodaler Software wie VIAN-DH wird die Frage nach Standards und Konventionen immer drängender. Eine der wichtigsten Anforderungen ist ein Datenrepräsentationsformat, das flexibel genug ist, um den verschiedenen Annotationstypen gerecht zu werden, und konkret genug, um in eine Datenbank implementiert oder für die Korpuskompilierung verwendet werden zu können. Eine weitere Anforderung sind Annotationskonventionen für multimodale Transkripte von nonverbalen Daten, die die vermittelte Bedeutung eindeutig und klar identifizieren, aber auch skalieren und korpusübergreifend verwendet werden können. Diese Aspekte sollen im Panel diskutiert werden.
                

                

                    
Ariadne Baresch
 (Trier) nimmt die strukturellen Aspekte potentieller Standardisierungsbestrebungen in den Blick. Eine Festlegung auf ein „open vocabulary“ für die digitale Filmannotation spiegelt den Bedarf der Community wider, sich auf Standards, gemeinsame Annotationskonzepte und ein stabiles, umfassendes Tool zu einigen. Das beim Annotieren entstehende Spannungsfeld zwischen idiosynkratischer Betrachtungsweise und einer möglichst objektiven Auszeichnung wird derzeit innerhalb jedes, sich damit beschäftigenden Forschungsprojekts im Bereich Film neu verhandelt. Die Erarbeitung eines Annotationsstandards könnte die Überlegungen und Erfahrungen, die bei diesen Forschungsprojekten erarbeitet werden, bündeln und zugleich den Einstieg in die Filmannotation erleichtern. Der Standard muss dabei folgenden Ansprüchen genügen: er sollte in einem offenen Format angeboten werden, interoperabel und von der Community bei Bedarf flexibel erweiterbar sein, um die entstandenen Daten und Metadaten zur Weiterverarbeitung verfügbar zu machen. Das Annotationsvokabular sollte unterschiedliche Annotationsphänomene und Analyserichtungen innerhalb der Filmwissenschaft beinhalten, sodass Nutzende, egal ob Anfänger:innen oder Fortgeschrittene, den Standard für ihre individuellen Bedürfnisse verwenden können. Im Panel soll daher der potenzielle Bedarf für eine Film Encoding Initiative in die Diskussion gebracht werden. Innerhalb dieser könnten, ähnlich der Modelle der TEI und der MEI xml-basiert Elemente für in der Filmannotation relevante Phänomene angeboten werden, welche die Frage der Multimodalität von Film widerspiegeln und strukturieren.
                

            

        

            

                
Hintergrund

                
Als Fanfiction können all jene Texte bezeichnet werden, die zur Veröffentlichung in eigens dafür eingerichteten Internetforen bestimmt sind und in „appropriativ-derivativer bzw. […] transformativer“ (Stemberger 2021, 10) Weise Bezug auf Mainstreammedien (meist Romane, Serien oder Filme) nehmen. Für die Autor*innen von Fanfiction, die meist unter Pseudonym schreiben, ist es somit möglich, in einem geschützten Raum (der Fancommunity) neben ersten Schreibversuchen, zu ihren Lieblingsfiguren auch das Verfassen von die Bezugsmedien kontrastierenden Erzählungen zu wagen. Catherine Tosenberger beschreibt letztere Kategorie als “a freedom especially felt with regard to non-normative and taboo forms and representations of sexuality” (Tosenberger 2014, 17). Fanfiction, die Literatur als Bezugsquelle gewählt haben, bauen dabei meistens auf großen Mainstream-Jugendbuchreihen aus dem angelsächsischen Raum auf. So sammeln sich in der deutschsprachigen Community auf fanfiktion.de unter der Kategorie ‚Bücher‘ die meisten Autor*innen in den Fandoms zu Harry Potter (über 55.000 Texte gesamt), Bis(s) (knapp 14.000) und Herr der Ringe (als allg. Gruppe Mittelerde definiert mit über 8.000 Texten). Praktisch befassen sich die Autor*innen oft mit Rekombinationen des Figurenarsenals der Originaltexte. Eine wichtige Rolle spielt dabei das Stereotyp der Slashes (männliche Figuren, die in eine romantisch-sexuelle Beziehung gesetzt werden; vgl. Brottrager et al. 2022). Die Moralvorstellungen der Originale können dabei auch übergangen werden, wenn zum Beispiel der Schüler Harry mit seinem Lehrer Severus Snape eine (romantisch-sexuelle) Beziehung eingeht.

            

            

                
Ziele des Projekts

                
Das Masterprojekt „Fanfiction Semantics – A Quantitative Analysis of Sensitive Topics in German Fanfiction” hat zum Ziel, diese Kontrastierungen in verschiedenen Fandoms zu betrachten. Gefragt wird dabei einerseits, wie virulent sensible Themen (sensitive topics) in Fanfiction sind und andererseits, wie diese Themen Wortbedeutungen beeinflussen. Als sensibel werden dabei jene Themen definiert, die in Originaltexten mehrheitlich ausgespart und von Fanfiction-Autor*innen demonstrativ eingeführt werden (v. a. extreme Gewalt und sexuelle Inhalte).
                    
 
Das Korpus entstammt einem seit 2020 andauernden Webscrapings am LitLab der Technischen Universität Darmstadt (vgl. Weitin 2022). Hier wurden alle in diesem Zeitraum bearbeiteten Texte sowie die von der Plattform definierten Metadaten (neben Selbstangaben der Autor*innen auch Angaben zu Altersbeschränkung der Texte und der eigenen Definition von Genre) heruntergeladen. Das Projekt bearbeitet darauf aufbauend in zwei Schritten zunächst die Metadaten zu den größten Fandoms im Bereich ‚Bücher‘ und wertet anschließend die dazugehörigen Texte quantitativ aus. Der untersuchte Ausschnitt des Fanfiction-Korpus umfasst über 8.000 Einzeltexte mit knapp 130 Mio. Tokens.
                

            

            

                
Methodik

                
1) Im ersten Schritt, bei der Analyse der Metadaten, kann beispielsweise eine Verteilung davon, welche der Kategorien zur Altersbeschränkung für wie viele Texte verwendet wurde,
                    
 
einen Hinweis auf die Unterschiede zum Bezugsmedium liefern. Die auf fanfiktion.de gesetzte Kategorisierung reicht dabei von Texten, die ab 6 Jahren geeignet sein sollen, bis hin zu solchen, die als „entwicklungsbeeinträchtigend“ (fanfiktion.de) gelten. Entsprechende Verteilungen werden auch zwischen Fandoms verglichen.
                

                
 2) Die Analyse der Texte erfolgt im folgenden Schritt mithilfe von Word Embedding Modellen (hier word2vec; vgl. Mikolov et al. 2013). In einem Word Embedding Modell können Ähnlichkeiten zwischen verschiedenen Schlüsselwörtern effizient verglichen werden. Die algorithmische Grundlage bietet dabei die Verwendung der Wörter in einem festgesetzten Kontextfenster (vgl. Distributionshypothese nach Firth; vgl. Firth 1957). Durch den Abgleich von Schlüsselwörtern, die auf verschiedene Themen verweisen, können Schnittmengen und Unterschiede untersucht werden. Die Schlüsselwörter wurden hierzu aus den Wortfeldern Gewalt und sexuelle Inhalte gewählt, welche Wörter unter den (hier) 30 ähnlichsten zu einem Zielwort zu finden sind, beschreibt in gewisser Weise wie dieses Zielwort verwendet wird (vgl. Abb. 1). Für das Subkorpus der 
                    
Harry Potter
-Reihe werden zudem dieselben Operationen auch für die Originaltexte durchgeführt. Abschließend wird ein Verfahren der Word Embedding basierten Sentiment Analyse nach SentiArt (vgl. Jacobs 2019; Brottrager et al. 2022) durchgeführt, welche anhand der Variablen Valence (emotionale Wertigkeit) und Arousal (emotionales Erregungspotential) Schlüsselwörter beschreibt.
                

                

                    

                    
Abb. 1: t-SNE-Visualisierung der ähnlichsten Wörter zu vier Schlüsselwörtern in einem Word2Vec-Modell zu 
                        
Harry Potter
-Fanfiction.
                    

                

            

            

                
Ausblick

                
Das Poster soll die Ergebnisse der oben beschriebenen Analysen grafisch darstellen und einen Eindruck über die thematische Zusammensetzung der Fanfiction-Korpora bieten. Das zugrundeliegende Korpus kann aus urheberrechtlichen Gründen nicht in der verwendeten Form veröffentlicht werden. Dennoch ist es beabsichtigt, nach Abschluss des Projekts alle verwendeten Codes auf GitHub bereitzustellen, um Transparenz zu schaffen und die Methodik für andere Projekte nachnutzbar zu machen (orientiert an der in den FAIR-Prinzipien definierten Wiederverwendbarkeit (vgl. GO FAIR 2022)). Anonymisierte Metadatentabellen sowie die Word Embedding-Modelle sollen zudem auch zugänglich gemacht werden.

            

        

            

                
Zum Verhältnis zwischen computationeller und traditioneller Literaturwissenschaft

                

                    
Operationalisierungsprobleme in der computationellen Literaturwissenschaft am Beispiel des unzuverlässigen Erzählens

                

                
Im Feld der Digital Humanities hat sich die computationelle Literaturwissenschaft als Teildisziplin etabliert. Von literaturwissenschaftlicher Seite wird allerdings immer wieder angemahnt, die computationelle Auseinandersetzung mit Literatur sei zu reduktionistisch – unter anderem weil Textanalysen nur statistisch-deskriptiv und weitgehend kontextfrei möglich seien (vgl. Gius/Jacke 2022). Derartigen Bedenken lässt sich auf unterschiedliche Weise begegnen. Eine Möglichkeit besteht in der Akzeptanz, dass die computationelle und die traditionelle Literaturwissenschaft schlichtweg unterschiedliche Fragen an Texte stellen. Soll die Idee eines Brückenschlags zwischen traditioneller und computationeller Literaturwissenschaft dagegen nicht verworfen werden, gibt es zwei weitere Möglichkeiten: Es kann der ambitionierte Versuch unternommen werden, auch für komplexere literaturwissenschaftliche Fragestellungen (vollständig) computationelle Lösungen zu finden. Solche Versuche sind zwar wünschenswert, sollten aber (durch den großen Aufwand und die oft eingeschränkten Erfolgsaussichten) nicht die einzige Möglichkeit darstellen, computationelle Modellierung für traditionelle literaturwissenschaftliche Fragen fruchtbar zu machen. 

                
Der vorliegende Beitrag stellt eine andere Möglichkeit vor, wie ein Brückenschlag aussehen kann: Es kann es fruchtbar sein, dezidiert nur eine computationelle Teil-Operationalisierung komplexer literarischer Phänomens anzustreben und möglichst genau zu ergründen und zu explizieren, welcher Status den entwickelten computationellen Analysemethoden im Zusammenhang mit komplexeren literarischen Phänomenen und Fragestellungen zukommt. Computationelle Modelle können deskriptiv-quantitativ Textmerkmale feststellen, die als Indikatoren für komplexere literaturwissenschaftlich interessante Phänomene verstanden werden können. Sinnvoll verwertbar sind solche Analysen, wenn das Indikationsverhältnis (also die genaue Beziehung zwischen Indikator und komplexem Phänomen) spezifiziert wird. Dabei geht es zum einen um die (aus literaturwissenschaftlicher Perspektive nachvollziehbare) 
                    
Erklärung der Indikationsbeziehung
 („Warum indiziert das sprachliche Merkmal das komplexe literarische Phänomen?“), zum anderen um die 
                    
Bestimmung der

                    
Indikationskraft
 („Wie eng ist der Zusammenhang zwischen Indikator und komplexem Phänomen?“). Für beide Fragen kann die Identifikation und Analyse von 'Zwischenphänomenen' sinnvoll sein – also von Phänomenen mittlerer Komplexität, die (inhaltlich-logisch) als Verbindungsglieder zwischen einfachem sprachlichen Indikator und komplexem literarischen Phänomen zu verstehen sind.
                

                
Die vorgeschlagenen Ideen zur
                    
Spezifikation des Indikationsverhältnisses basieren auf ersten Erkenntnissen aus dem Versuch der Operationalisierung und computationellen Teil-Modellierung des literaturwissenschaftlichen Konzepts „

                    
unzuverlässiges Erzählen“

                    
 

                    
im Rahmen des Projekts CAUTION.

                    

                    

                    
 U

                    
nzuverlässiges Erzählen liegt dann vor, wenn der Erzählfigur einer fiktionalen Erzählung insofern ‚nicht zu trauen‘ ist, als ihre Aussagen über die fiktive Welt teilweise falsch sind oder relevante Aussagen fehlen bzw. die moralischen Ansichten der Erzählfigur in Diskrepanz zu durch das Werk vermittelten Werten stehen (vgl. Kindt 2008, 53). In diesem Beitrag wird unzuverlässiges Erzählen als illustratives Beispiel angeführt. Da die präsentierten Ideen zur Spezifikation des Indikationsverhältnisses auch im Rahmen anderer Vorhaben genutzt werden können, in denen komplexe literaturwissenschaftliche Konzepte (teilweise) computationell modelliert werden, versteht sich dieser Ansatz als theoretischer Beitrag, der zur Stärkung der Brücke zwischen traditioneller und computationeller Literaturwissenschaft beitragen möchte.

                

                
Im Folgenden sollen zunächst kurz die für den vorgestellten Ansatz zentralen Begriffe „Operationalisierung“ und „Komplexität“ diskutiert werden (Abschnitt 2). Im Anschluss wird die vorgeschlagene Analyse des Indikationsverhältnisses genauer expliziert (Abschnitt 3), die auf einer theoretischen Analyse des Unzuverlässigkeitskonzepts sowie ersten Annotations- und Analyseerfahrungen im Rahmen von CAUTION basiert.

            

            

                
Begriffsklärung: Operationalisierung und Komplexität

                
Die Operationalisierung geisteswissenschaftlicher Konzepte für die computationelle Textanalyse ist bereits zum Thema extensiver Auseinandersetzung geworden (vgl. Moretti 2013, Döring/Bortz 2016, Pichler/Reiter 2021, Krautter et al. im Erscheinen). Unter der Operationalisierung von Begriffen ist die Angabe von Handlungsschritten zu verstehen, die ausgeführt werden müssen, um das Phänomen identifizieren (bzw. messen und quantifizieren) zu können. Im Feld der Digital Humanities wurde der Begriff von Moretti eingeführt (vgl. Moretti 2013). Seine literaturwissenschaftlichen Beispiele zeigen aber, dass die operationalisierten Modelle oft nicht oder nur lose an die zu operationalisierenden literaturwissenschaftlichen Konzepte angeknüpft sind (vgl. Krautter/Pichler/Reiter im Erscheinen). Es liegt der Schluss nahe, dass viele literaturwissenschaftliche Konzepte zu komplex sind, um sich unter Erhaltung ihrer ursprünglichen Bedeutung und Funktion vollständig und eindeutig operationalisieren bzw. computationell modellieren zu lassen. Dennoch scheinen die bisherigen Beiträge nicht grundsätzlich von dem Ziel Abstand zu nehmen, mit computationellen Operationalisierungen vollständige Übersetzungen bzw. direkte Entsprechungen komplexer Konzepte zu entwickeln. Reduzierte Ansprüche lassen sich lediglich insofern feststellen, als im Falle umstrittener Konzepte eine begründete Auswahl aus dem Definitionsangebot (vgl. Döring/Bortz 2016, 226) oder möglicher Kontexte (vgl. Pichler/Reiter 2021, 6) getroffen wird. Grundsätzlich wird aber davon ausgegangen, dass durch die Aufgliederung in Teilschritte (vgl. Pichler/Reiter 2021, 19–23) bzw. die Kombination unterschiedlicher Indikatoren (vgl. Döring/Bortz 2016, 229) die entwickelten Modelle direkt auf die komplexen geisteswissenschaftlichen Phänomene zielen. Es wird nicht in Betracht gezogen, dass es – um Reduktionismus zu vermeiden – notwendig oder sinnvoll sein könnte, mit dem computationellen Modell dezidiert nur einen Baustein zu ihrer Analyse beizutragen und dessen genaue Funktion zu explizieren. Das Kriterium der Validität (vgl. Drost 2011) computationeller Modelle wäre aus dieser Perspektive neu zu denken: Es ist nicht notwendig, dass die Modelle dasjenige messen, nach dem Literaturwissenschaftler:innen fragen – sofern die Relation zwischen Modell und Phänomen so expliziert wird, dass ein (nicht-computationelles) literaturwissenschaftliches Weiterarbeiten mit den erzielten Ergebnissen ermöglicht wird (vgl. hier auch Flick 2012 zu Triangulation).

                

                    
Aber warum bzw. inwiefern kann die Komplexität literaturwissenschaftlicher Phänomene es notwendig oder sinnvoll machen, von einer vollständigen computationellen Modellierung abzusehen? In diesem Zusammenhang sind unterschiedliche Dimensionen der Komplexität literarischer Phänomene (bzw. der literaturwissenschaftlichen Konzepte, die diese Phänomene fassen sollen) zu beachten. 
Literarische Phänomene können zum einen dadurch komplex sein, dass sie zusammengesetzt sind (vgl. Gius 2019). In solchen Fällen müssen mehrere Teilphänomene vorliegen bzw. untersucht werden, um Aussagen über das komplexe Gesamtphänomen treffen zu können. Eine Operationalisierung, die so ein zusammengesetztes Phänomen unter Erhaltung der literaturwissenschaftlichen Bedeutung in Teilschritte zur Erkennung übersetzen will, ist deshalb schwierig, weil die einzelnen Teilphänomene und ihre Beziehung zueinander identifiziert und diese dann jeweils operationalisiert und computationell modelliert werden müssen. Da literaturwissenschaftliche Konzepte häufig nicht derart analytisch aufbereitet sind, wäre die Rekonstruktion und behutsame Schärfung einer solchen Definition (vgl. Carnap 1959 zu Explikation), ebenso wie die Überprüfung, ob diese Definition mit der tatsächlichen Verwendung des Konzepts in der Literaturwissenschaft kompatibel ist, eine Aufgabe, die im Rahmen der Operationalisierung stattfinden müsste.
                

                
Unzuverlässiges Erzählen ist insofern zusammengesetzt, als es in distinkte Typen mit unterschiedlichen Eigenschaften zerfällt (vgl. Jacke 2020, 17–57). Zudem muss (zumindest laut einigen Definitionen) eine Kombination aus mehreren (Text-)Eigenschaften gegeben sein, damit unzuverlässiges Erzählen vorliegt. Diese Probleme ließen sich allerdings noch durch begründete Auswahl und ein Zergliedern in Analyseschritte adressieren, wie von Döring/Bortz bzw. Pichler/Reiter vorgeschlagen.

                
Zum anderen können literarische Phänomene dadurch komplex sein, dass ihre Feststellung in einem Text interpretationsabhängig ist. Interpretationsabhängigkeit ist dabei als gradierbare Eigenschaft zu verstehen – der Grad bemisst sich danach, in welchem Maße nicht-wahrheitserhaltende Schlüsse und strittige (Kontext-)Annahmen notwendig sind, um das Vorliegen des Phänomens festzustellen. Auch bei der Feststellung des Vorliegens interpretationsabhängiger Phänomene spielen aber in der Regel der literarische Text selbst bzw. konkrete identifizierbare Texteigenschaften eine zentrale Rolle – die alleinige Bezugnahme auf sie ist aber eben nicht ausreichend, um für ihr Vorliegen zu argumentieren. Bei starker Interpretationsabhängigkeit ist es in der Regel extrem kompliziert (bzw. möglicherweise unmöglich), ein literaturwissenschaftliches Konzept vollständig (computationell) zu operationalisieren. Dies ergibt sich zum einen durch die Schwierigkeit, die im Rahmen von Interpretation stattfindenden zahlreichen und schwer zu fassenden Prozesse überhaupt zu rekonstruieren, zum anderen durch die Herausforderung, extratextuelles Wissen in computationellen Analysen abzubilden.

                
Einige zentrale Aspekte, die für das Vorliegen unzuverlässigen Erzählens notwendig sind, sind interpretationsabhängig. Beispielsweise ist es für faktenbezogene Unzuverlässigkeit notwendig, dass eine Erzählfigur falsche Aussagen über die erzählte Welt tätigt (oder relevante Informationen auslässt, vgl. Kindt 2008, 53). Eine derartige Diagnose erfordert eine Entscheidung darüber, was in der erzählten Welt wahr und relevant ist. Für Entscheidungen dieser Art sind zwar Textargumente (vgl. Descher/Petraschka 2019, 88–93) sehr wichtig. Aber zum einen müssen diese Textargumente unter Umständen in komplizierter Weise gegeneinander abgewägt werden und zum anderen sind – gerade bei potenziell unzuverlässig erzählten Texten – Kontextannahmen in der Regel unerlässlich für die Rekonstruktion der fiktiven Welt.

                
Während eine vollständige (computationelle) Operationalisierung unzuverlässigen Erzählens also wenig aussichtsreich erscheint, bietet die Relevanz von Textargumenten dennoch einen aussichtsreichen Ausgangspunkt für die Entwicklung computationeller Modelle, die für die Analyse von Unzuverlässigkeit in Texten unterstützend herangezogen werden können. So werden in der (nicht-computationellen) Unzuverlässigkeitsforschung auch tatsächlich Indikatorenlisten zusammengestellt, die unter anderem auch wenig komplexe sprachliche Phänomene enthalten, deren computationelle Modellierung aussichtsreich bzw. bereits umgesetzt ist. Die Indikatoren reichen von konkreten sprachlichen Einzelphänomenen („Ausrufe, Ellipsen, Wiederholungen“) und nicht spezifizierten linguistischen Sammelphänomenen („linguistische Signale für Expressivität und Subjektivität“) über Eigenschaften bzw. Zustände von Erzähler:innen („Hinweise auf kognitive Einschränkungen“) sowie Sprachhandlungen und Absichten (versuchte „Rezeptionslenkung durch den Erzähler“, Nünning 1998, 27–28) bis hin zu inhaltlich-strukturellen (verschiedene Arten von Widersprüchen) und inhaltlich-kontextuellen Phänomenen (stark unwahrscheinliche oder unmögliche Aussagen). Eine Analyse der Indikationsbeziehungen wird in der Unzuverlässigkeitsforschung allerdings nicht vorgenommen.

            

            

                
Zum Status von Indikatoren für komplexe literaturwissenschaftliche Phänomene

                
Um Klarheit über die Relevanz konkreter computationell modellierter sprachlicher Indikatoren im Zusammenhang mit komplexen literarischen Phänomenen zu erlangen, sollte zum einen reflektiert und kommuniziert werden, welche Funktion dem Modell zukommen soll, also ob es beispielsweise als Heuristik zum Auffinden für eine Forschungsfrage potenziell relevanter Texte (bzw. zur Exploration von Korpora), in argumentativen Zusammenhängen genutzt (vgl. Gerstorfer 2020) oder in anderen Funktionen eingesetzt werden soll. Hiervon ist abhängig, wie stark die Indikationsbeziehung überhaupt sein muss, um valide (oder: plausible) Ergebnisse erzielen zu können (vgl. Gius/Jacke 2022).

                

                
Zwei weitere Aspekte der Indikationsbeziehung, die für eine literaturwissenschaftliche Verwertbarkeit computationeller Modelle im Zusammengang mit komplexen Phänomenen analysiert und kommuniziert werden sollten, werden im Folgenden etwas genauer vorgestellt.

                

                    
Erklärung der Indikationsbeziehung

                    
Grundsätzlich gilt, dass computationelle Modelle, die für die Textanalyse eingesetzt werden, für viele Literaturwissenschaftler:innen insbesondere dann interessant sind, wenn (zumindest ansatzweise) nachvollziehbar wird, 
                        
warum
 sie funktionieren. Es ist deswegen lohnenswert zu fragen, warum bestimmte einfache sprachliche Textmerkmale auf das Vorliegen eines bestimmten komplexen Phänomens hinweisen können – und ob sich zwischen diesen beiden Polen möglicherweise Phänomene mittlerer Komplexität identifizieren lassen, die dieses Indikationsverhältnis (logisch-inhaltlich) besser nachvollziehbar machen.
                    

                    
Warum, beispielsweise, soll ein sprachliches Phänomen wie Ausrufe unzuverlässiges Erzählen indizieren? Ausrufe sind ein Merkmal expressiver Sprache. Expressive Sprache weist auf eine emotional aufgewühlte Erzählinstanz hin. Eine emotional aufgewühlte Erzählinstanz neigt dazu, etwas durcheinanderzubringen. Eine Erzählinstanz, die etwas durcheinanderbringt, ist disponiert, inkorrekte Aussagen zu treffen. Eine Erzählinstanz, die inkorrekte Aussagen tätigt, lässt sich als unzuverlässige Erzählinstanz einordnen.

                    
Während diese Reihe das Fortschreiten von nicht zu stark interpretationsabhängigen Texteigenschaften illustriert, sollten insbesondere bei dem hier gewählten Beispiel auch kausale Zusammenhänge bzw. Richtungen beachtet werden. Eine entsprechende Analyse zeigt, dass das Vorliegen sprachlicher Merkmale wie Ausrufe im Text und das Vorliegen unzuverlässigen Erzählens nicht im eigentlichen Sinn kausal verbunden sind, sondern dass beide Phänomene (intrafiktionaler Logik folgend) die gleiche Ursache haben können – nämlich eine emotional aufgewühlte Erzählinstanz (siehe auch Reichenbachs 
                        
common cause principle
, vgl. Hitchcock/Rédei 2020).
                    

                

                

                    
Art und Stärke der Indikationsbeziehung

                    
Konkrete sprachliche Texteigenschaften, die als Indikatoren für komplexere literarische Phänomene betrachtet werden, können diese Phänomene in unterschiedlicher Art und mit unterschiedlicher Stärke indizieren:

                    
(1) Das Vorkommen bestimmter sprachlicher Indikatoren (ggf. mit einer bestimmten Frequenz, in bestimmten Kombinationen oder an bestimmten Stellen in einem Text) kann notwendig oder (meist in Kombination mit anderen Indikatoren) hinreichend für das Vorliegen eines bestimmten komplexen Phänomens sein. Ein Indikator ist dann notwendig für ein Phänomen, wenn er vorliegen muss, sofern das Phänomen vorliegt; hinreichend ist er dann, wenn sein Vorliegen das Vorliegen des Phänomens garantiert (vgl. Brennan 2022). Während ein einzelnes sprachliches Phänomen in der Regel weder notwendig noch hinreichend für das Vorliegen eines komplexen literarischen Phänomens ist, lassen sich unter Einbeziehung der oben genannten Zwischenphänomene als Verbindungsglieder zwischen sprachlichem Indikator und komplexem Phänomen aussagekräftigere Ergebnisse erzielen.
 Für jede dieser Indikationsbeziehungen lässt sich analysieren, ob der Indikator (möglicherweise auch in Konjunktionen oder Disjunktionen mit weiteren Indikatoren) notwendig oder hinreichend ist. Beispielsweise lässt sich feststellen, dass die Disjunktion aus Ausrufen und bestimmten weiteren sprachlichen Einzelphänomenen (wie beispielsweise expressiven Adjektiven etc., vgl. Gutzmann 2019) notwendig für expressive Sprache ist. Derartige Analysen lassen sich etwa auf der Basis von begriffsanalytischen Überlegungen und literaturwissenschaftlichen Argumentationsanalysen durchführen.

                    

                    
(2) Basierend auf dem Vorkommen bestimmter sprachlicher Indikatoren (ggf. mit einer bestimmten Frequenz, in bestimmten Kombinationen oder an bestimmten Stellen in einem Text) kann dem Vorliegen eines bestimmten komplexen Phänomens eine hohe Wahrscheinlichkeit zugeschrieben werden. Solche bedingte Wahrscheinlichkeit lässt sich zum einen ebenfalls auf einer Mikroebene analysieren: So ist beispielsweise anzunehmen, dass expressive Sprache zwar weder notwendig noch hinreichend für eine emotional aufgewühlte Erzählinstanz ist, diese aber mit hoher Wahrscheinlichkeit indiziert.
 Zum anderen kann aber auch die Wahrscheinlichkeit auf der Makroebene interessant sein, mit der ein einfaches sprachliches Phänomen ein komplexes literarisches indiziert. Derartige Untersuchungen lassen sich nur mithilfe von Studien durchführen, bei denen Literaturwissenschaftler:innen das Vorliegen relevanter mittelkomplexer und komplexer Phänomene in einem Testkorpus beurteilen und manuell auszeichnen, in dem zugleich sprachliche Indikatoren für das Phänomen computationell identifiziert und ausgewertet werden. Derartige Studien werden im Rahmen des Projekts CAUTION durchgeführt.

                    

                    
Auch bei der Analyse von Wahrscheinlichkeitszusammenhängen ist es wichtig, Kausalitätsrichtungen zu beachten, um die Relevanz eines computationellen Modells richtig einzuschätzen: Selbst wenn beispielsweise Emotionalität der Erzählinstanz sich mit sehr hoher Wahrscheinlichkeit in Form (automatisch messbarer) emotionaler Sprache niederschlägt, und wenn dieselbe Emotionalität ebenfalls mit hoher Wahrscheinlichkeit unzuverlässiges Erzählen hervorbringt, muss untersucht werden, ob die sprachlichen Indikatoren und unzuverlässiges Erzählen nicht mit ebenso hoher Wahrscheinlichkeit jeweils andere (verschiedene) Ursachen haben können.

                    
Insgesamt kann die Identifikation von mittelkomplexen Phänomenen als Verbindungsgliedern zwischen sprachlichen Indikatoren und komplexen literarischen Phänomenen also nicht nur die Relevanz computationell modellier- und auswertbarer Texteigenschaften für literaturwissenschaftlich interessante Phänomene logisch-inhaltlich besser begreifbar machen. Sie ermöglicht auch eine aussagekräftigere (theoretische und methodologische) Analyse des komplexen literaturwissenschaftlichen Konzepts sowie eine Analyse der Indikationsverhältnisse und schafft die Grundlage für Teilerfolge (bspw. falls zwar letztlich kein signifikantes Indikationsverhältnis zwischen sprachlichem Indikator und komplexem Phänomen festgestellt werden kann, wohl aber zwischen sprachlichem Indikator und literaturwissenschaftlich ebenfalls relevanten Zwischenphänomenen). Der hier vorgeschlagene Weg, komplexe literarische Phänomene im Rahmen computationeller Zugänge bewusst nur partiell zu operationalisieren und zu modellieren, stellt auf diese Weise eine Möglichkeit dar, wie
                        
die Untersuchung literarischer Phänomene von den Vorteilen computationeller Analysen (u.a. Exaktheit und Reichweite) profitieren kann, ohne dass der Vorwurf des Reduktionismus angebracht ist. Denn nicht nur wird der genaue (eingeschränkte) Stellenwert der Analysen explizit gemacht – dies geschieht zudem auf eine Weise, die den Zusammenhang zwischen sprachlichen Merkmalen und komplexen interpretationsabhängigen Phänomenen auch inhaltlich expliziert und dadurch geisteswissenschaftlich anknüpfbar macht.

                    

                

            

        

            
Als Google im Jahre 2009 die erste Version des Ngram-Viewer publizierte, hat die Digital Humanities-Community recht schnell die positiven und negativen Aspekte dieses Werkzeugs analysiert: Die Möglichkeiten einer wort- und begriffsgeschichtlichen Forschung sind sprunghaft erweitert worden, wie auch der begleitende Aufsatz von Michel deutlich belegte (Michel et al. 2011). Dessen teilweise zu naive Umgang mit dem Quellenmaterial machte aber auch deutlich, dass die Autorinnen und Autoren kein Bewusstsein für die möglichen Fallen von korpusbasierten Forschungen hatten. Die wechselnde Zusammensetzung der zugrundeliegenden Textsammlung, die Unmöglichkeit auf die dahinterliegenden Texte zuzugreifen, das Fehlen von Metadaten für die Texte, falsche Jahreszahlen, OCR-Fehler u.a.m. sind dem Ngramm-Korpus wiederholt vorgeworfen worden (z.B. Underwood 2012). Die Arbeit von (Pechenick et al. 2015) hat gezeigt, wie die zunehmende Menge von wissenschaftlicher Literatur die Korpusanteile in der zweiten Hälfte des 20. Jahrhundert merklich verschiebt; unklar bleibt allerdings, ob dies nicht auch eine gesellschaftliche Entwicklung reflektiert, also keineswegs nur als Manko zu betrachten ist. Besonders einschlägig ist die Arbeit (Koplenig 2017), in der Veränderungen der Korpuszusammensetzung während des zweiten Weltkriegs untersucht werden: „the German GB corpus was strongly biased toward volumes published in Switzerland during WWII“ (Koplenig 2017)

            
Google hat zwei größere Updates vorgelegt, die die zugrundeliegende Textmenge deutlich erweitert haben. Hier die Entwicklung des Umfangs der deutschsprachigen Korpora, auf die ich mich im Folgenden beschränke:

            

                

                    

                    
Tokens

                    
Bücher

                

                

                    
2009

                    
37.439.210.527

                    
406.666

                

                

                    
2012

                    
64.784.628.286

                    
657.991

                

                

                    
2019

                    
286.463.423.242

                    
3.843.962

                

            

            
Dadurch dass die Anzahl der digitalisierten Bücher noch einmal deutlich gesteigert werden konnte – und das über den gesamten Zeitraum, den der
                
Viewer 
abdeckt, – scheinen die Fragen nach einem Bias der Auswahl weniger relevant zu werden. Entsprechend wurde und wird der Ngram-Viewer auch weiterhin in vielen Kontexten als schnell zugängliches Werkzeug verwendet, das die Möglichkeiten einer auf sehr großen Datenbeständen basierenden Forschung anschaulich macht (z.B. Chen and Yan 2016, Gonçalves et al. 2018, Richey and Taylor 2020). Insgesamt gehören der 
                
Ngram-Viewer
 und die zugrundeliegenden freiverfügbaren Daten trotz aller berechtigten Kritik für viele Forschende zu den wichtigsten Daten-Publikationen der letzten Jahrzehnte. 
            

            
In diesem Sinne wollte auch eine Kollegin, die zu Fragen der literarischen Kanonisierung arbeitet, den Viewer verwenden, aber bei der Analyse der Ergebnisse fiel uns schnell auf, dass die Namen einer Reihe der hochkanonischen deutschsprachigen Autoren – Thomas Mann, Goethe, Schiller, Kafka, Brecht – nach 2005 einen auffälligen Abwärtstrend aufweisen (siehe Fig. 1).

            

                

                    

                    
Fig. 1: Kanonisierte Autoren (Goethe, Schiller, Thomas Mann, Kafka, Brecht) 1990-2019.

                
Das Phänomen zeigt sich noch deutlicher, wenn man sich nur für kanonisierte Autor
                
innen
 interessiert (siehe Fig. 2). 
            

            

                

                    

                    
Fig. 2: Häufigkeit kanonisierter Autorinnen im Ngram-Viewer (Lasker-Schüler, Seghers, Bachmann, Jelinek, Christa Wolf) 1990-2019.

                

            

            
Bevor wir nun allgemeinere kulturanalytische Thesen über das Ende der Bildungskultur formulierten, wollte ich die Solidität der Daten prüfen. Doch wie kann man ein Korpus auf möglichen Bias untersuchen, wenn weder die Liste der Texte geschweige die Texte selbst vorliegen, sondern nur eine Reihe von generischen Metadaten und 1-5 Gramme? 

            

                
Die Korpusanomalien

                
Da die NGramme, die dem 
                    
Ngram-Viewer 
zugrundeliegen, ebenfalls publiziert sind, ist es naheliegend, erst einmal die Rohwerte und deren Entwicklungstendenzen zu untersuchen. Das Ergebnis (Fig. 3) zeigt zumindest für drei der untersuchten Namen eine einheitliche Tendenz: aufsteigend – also genau das Gegenteil der absteigenden Entwicklung, die der 
                    
Ngram-Viewer 
präsentiert. (Auffällig ist außerdem ein Abfallen in allen Verteilungen im Jahre 2010.) 
                

                

                    

                        

                        
Fig. 3: Rohwerte für die Häufigkeiten im NGram-Viewer Korpus (1-Gramme, 2019)

                    

                

                
Wie lässt sich der Widerspruch zwischen den Ergebnissen erklären? Der 
                    
Ngram-Viewer 
zeigt die 
                    
relativen
 Häufigkeiten der Wörter an, d.h. den Anteil den das Wort, z.B. der Name ‚Goethe‘, an der Menge aller publizierten Wörter dieses Jahres hat. Dadurch kann man Frequenzen aus dem 18. Jahrhundert, die nur auf einigen wenigen Büchern beruhen, mit denen im 21. Jahrhundert vergleichen. Die Differenz der Ergebnisse könnte also so erklärt werden, dass zwar die Anzahl an Nennungen von Goethe weiter ansteigt, aber ab ca. 2005-2010 zugleich sehr viel mehr Bücher im Korpus sind, in denen die Namen der Autoren und Autorinnen nicht genannt werden. Das würde bedeuten, dass insgesamt im Korpus die Anzahl der Bücher pro Jahr deutlich angestiegen sein müsste. Die entsprechenden Daten sind im Ngram-Korpus vorhanden und sie bestätigen die These:
                

                

                    

                        

                        
Fig. 4: Anzahl der Bücher pro Jahr im DE-Ngramm-Korpus 2019

                    

                

                
Wenn die Rohdaten aus Fig. 3 nun mit den Daten über die Anzahl der Token pro Jahr normalisiert werden, dann ergibt sich der Trend, der im Ngram-Viewer sichtbar wurde, alle Werte stürzen nach 2005 mehr oder weniger steil ab. 

                

                    

                        

                        
Fig. 5: Häufigkeitswerte der kanonisierten Autoren und Autorinnen geteilt durch die Anzahl der Token pro Jahr

                    

                

                
Allerdings ist der sehr steile Anstieg der Buchzahlen in Fig. 4 ziemlich überraschend. Wir wissen, dass Google Books auf den Ergebnissen der Digitalisierungskampagne Googles in Kooperation mit einer ganzen Reihe von internationalen Forschungsbibliotheken beruht. Die deutschsprachigen Ergebnisse verdanken sich nicht zuletzt den Kooperationen mit der Österreichischen Nationalbibliothek und der Bayerischen Staatsbibliothek. Deren Bestände speisen sich in den letzten Jahrzehnten aus Pflichtabgabeexemplaren und einer umfassenden Erwerbspolitik. Woher kommt also der plötzliche Anstieg? Sind – den Klagen der Verlage zum Trotz – seit 2005 sehr viel mehr Bücher als früher gedruckt worden?. 

                
Für die Buchproduktion konnte ich zwei Quellen verwenden, die die Daten gleich in digitaler Form anbieten: Statista, ein kommerzieller Datenanbieter, der Zahlen des Börsenvereins des deutschen Buchhandels zur Anzahl der Neuerscheinungen aufbereitet hat (Börsenverein 2022),
 und die Angaben über die Anzahl der Buchpublikationen insgesamt in Thomas Rahlfs Zeitreihen zur historischen Statistik (Rahlf 2015). Sie decken allerdings nicht die gleichen Zeiträume ab. Statista hat die aktuelleren Daten, reicht aber nicht soweit zurück, während Rahlfs Datenreihe schon 2015 endet. Schon ein erster Blick auf die Fig. 6, die die Anzahl Bücher, die laut Börsenverein/Statista zwischen 2002 und 2019 gedruckt worden sind, mit der Anzahl der Bücher vergleicht, die dem Ngramm-Korpus zugrundeliegen, zeigt etwas Verblüffendes: In dem Korpus sind nach 2012 mehr Bücher enthalten, als Neuerscheinugen in dem Jahr gedruckt wurden. 
                

                

                    

                        

                        
Fig. 6: Bücher im deutschsprachigen Ngramm-Korpus und die Anzahl der jährlich publizierten Bücher (Statista)

                    
Vielleicht stimmen ja die Zahlen von Statista nicht. Vielleicht sind die Angaben in dem Ngram-Korpus aus dem Jahre 2019 falsch. In der Fig. 7 sind nun alle Informationen gemeinsam eingeflossen. Sie zeigt zum einen die Werte der Buchproduktion aufgrund der Daten des Börsenvereins – das ist der kurze violette Graph – und nach den Werten von Rahlf – die rote Kurve ab 1950. Es ist offensichtlich, dass die Werte zur Buchproduktion sich voneinander unterscheiden: Rahlf liegt immer etwas höher, da er ja nicht nur die Neuerscheinungen erfasst, aber beide beschreiben ziemlich parallel die gleiche Dynamik. Bis ca. 2009 steigt die Produktion immer weiter an und fällt danach ab. Aber welche der beiden Kurven man auch zugrundelegt, stets übersteigen die Zahlen des Ngramm-Korpus die Werte im Jahr 2013 und wohl auch danach.
                

                
Die Grafik enthält neben den Werten für das aktuelle Korpus von 2019 auch die Werte für die früheren Ngramm-Korpora von Google aus den Jahren 2009 und 2012 , die deutlich kleiner waren. Beginnen wir bei den Werten vor 1995: Es ist auffällig, dass Google mit dem letzten Update den Anteil an der Buchproduktion eines Jahres, der digitalisiert vorliegt, deutlich steigern konnte, so dass bis in die Mitte der 1960er Jahre teils 50% und mehr im Korpus enthalten sind. Für den Zeitraum von den späten 1960ern bis in die späten 1990er stagnieren die Werte der Korpora, während die Buchproduktion in diesen Jahren steil angestiegen ist. Während im Jahr 1967 erstaunliche 67% der nach Rahlfs publizierten Bücher im Korpus liegen, sind es 1997 ‚nur‘ noch 23%. Erst danach, 1998 bis 2006, wächst der Anteil wieder. In den Korpora von 2009 und 2012 geschieht dies noch relativ langsam, während die Erweiterung von 2019 hier deutlich stärker zulegt. Von 2006 auf 2007 springen die Werte allerdings steil nach oben. Das gilt für alle drei Stufen des Korpus, aber auch hier ist der Anstieg im 2019-Korpus noch einmal deutlich ausgeprägter. Danach, zwischen 2008 und 2010 verhalten sich die Werte im Korpus auf einem hohen Niveau parallel zu den Werten der Buchproduktion und fallen mit dieser sogar leicht ab. Das ändert sich wiederum 2011-2013, wo wir einen weiteren Anstieg beobachten können, der diesmal sogar das Niveau der Buchproduktion übertrifft. 

                

                    

                        

                        
Fig. 7: Entwicklung der Buchproduktion und der Google Korpora 1950-2019

                    

                

                
Wenn wir noch einmal auf Grafik 3 und 4 blicken, dann sehen wir dort, dass die Rohwerte für Goethe und zwei andere kanonisierte Autoren in den späten 1990er Jahren deutlich ansteigen, sie zugleich in der normalisierten Darstellung schon abfallen. Das bedeutet, dass bereits in der Phase von 1998-2006 ‚andere‘ Texte hinzugefügt wurden, deren Zusammensetzung anders war als das bisherige Korpus. Um welche Texte könnte es sich hierbei handeln?

            

            

                
Faktoren der Korpusverzerrung nach 1995

                
Die oben erwähnten verschiedenen Phasen der Veränderung legen es nahe zu vermuten, dass nicht ein einzelner Eingriff in das Korpus, sondern eine Reihe verschiedener Texthinzufügungen die beobachteten Phänomene bedingen. In einem anderen Projekt, in dem es um die Analyse von Heftromanen geht, war bereits aufgefallen, dass sich die einschlägigen Verlage der Komplexität der Feststellung der richtigen Metadaten, insbesondere des Publikationsdatums, dadurch entledigt haben, dass sie einfach alle retrodigitalisierten Texte unter dem Datum veröffentlichen, an dem die digitale Kopie publiziert wird. Um zu testen, ob diese Texte auch im Ngram-Korpus sind, wurden die entsprechenden Serienautoren und -helden gesucht: 

                

                    

                        

                        
Fig. 8: Häufigkeit von Heftroman-Helden im Ngramm-Korpus (2019)

                    

                

                
Die deutliche Steigerung ab 2010 spricht dafür, dass auch hier die Retrodigitalisate unter dem jeweiligen Jahr aufgeführt sind. Allerdings können selbst einige Tausend Heftromane nicht alleine verantwortlich sein.

                
Wie könnte man nun weitere Faktoren erkennen? Ein offensichtlicher Weg geht über das sprachliche Material. Die neuen Texte würden für bestimmte Worte zu einer relativen Erhöhung führen, selbst wenn viele andere Worte einen relativen Rückgang aufweisen. Da Google die Daten im 2019-Korpus mit Wortklasseninformation ausliefert, war es einfach, alle Substantive aus den 1-Grammen zu extrahieren, rd. 13 mio. Anschließend wurden die Substantive herausgefiltert, die von 1995 bis 2019 jedes Jahr auftauchen und zwar insgesamt mindestens 2 500 mal. Für diese restlichen rd. 350.000 Wörtern wurde für die Daten jedes Wortes eine lineare Regression berechnet und die Steigung der Geraden als Filterungsfaktor verwendet, um die rd. 250 Wörter zu finden, die in dieser Zeit den größten Anstieg verzeichnen. 

                
Eine manuelle Sichtung dieser Wortliste zeigte den Einfluss mehrerer Textsorten. Vor allem aber fiel der einzige Name in der Liste auf: GRIN. Es handelt sich um eine deutsche Verlagsgruppe, zu der neben dem GRIN-Verlag selbst u.a. auch die Webseite hausarbeiten.de gehört. Der Verlag, der von Beobachtern als ‚vanity publisher‘ oder ‚predatory publisher‘ (Shrestha 2021) eingeschätzt wird, publiziert alle Texte digital oder als Book on Demand. Eine „Lektorierung findet nicht statt“ (Wikipedia). Laut Verlagswebseite wurden bis ins Jahr 2018 200.000 Texte publiziert. Seitdem sind schätzungsweise mindestens weitere 40.000 Titel publiziert worden.
 Fig. 9 zeigt die Anzahl der Bücher im NGramm-Korpus, in denen das Token ‚GRIN‘ vorkommt, was wohl weitgehend identisch ist mit Titeln des Verlags. 
                

                

                    

                        

                        
Fig. 9: Anzahl der Bücher im Ngramm-Korpus (2019), in denen das Wort 'GRIN' vorkommt.

                    

                

                
Die Texte des GRIN Verlags haben zwar alle eine ISBN-Nummer, aber die meisten sind deutlich kürzer als Bücher im herkömmlichen Sinn, viele sind Aufsätze, die nur 20-30 Seiten lang sind. Beide Faktoren, die Unmenge an quasi-wissenschaftlichen kurzen Texten des GRIN-Verlags und die falsch datierten Heftromane führen dazu, dass die durchschnittliche Länge von Büchern im Ngramm-Korpus sich seit 2000 deutlich verringert hat (siehe Fig. 10), allerdings sind die Werte seit 2017 fast wieder auf dem alten Niveau:

                

                    

                        

                        
Fig. 10: Durchschnittliche Anzahl der Seiten pro Buch im Ngramm-Korpus

                    

                

                
Fassen wir zusammen: Nach 1998 ändert sich die Zusammensetzung des deutschsprachigen Ngramm-Korpus einschneidend, so dass es für die meisten Analysen zur Entwicklung von Sprache und Kultur weitgehend unbrauchbar wird. Dazu tragen eine Reihe von Faktoren bei, von denen zwei identifiziert werden konnten: Schwerwiegender ist, schon aus Umfanggründen, der Anteil der Publikationen des GRIN-Verlags. Sie geben zwar einen Einblick in eine bestimmte Form universitärer Wissenschaftskommunikation, haben aber nichts mit der sonstigen Buchproduktion zu tun. Hinzukommen die falsch datierten Retrodigitalisierungen einiger Verlage. Zugleich zeigt die Analyse der Daten, dass dies nicht die einzigen Faktoren sind, die hier ins Gewicht fallen. Wenn man sich auf die Wörter mit den steilsten Karrieren in den letzten 25 Jahren konzentriert (Fig. 11), dann fällt auf, dass dies allgemeine Token sind, die sich eher in Romanen als in Fachtexten finden (besonders die Anführungszeichen, mit denen in den meisten deutschen Drucktexten direkte Rede markiert wird): „Augen Blick Du Frau Gesicht Hand Kopf Leben Mal Mann Moment Mutter Stimme Tag Tür Vater « »“ Da zugleich die Länge ansteigt und die Anzahl der Texte sehr hoch bleibt, außerdem diese Texte aber wohl nicht in den offiziellen Verlagsstatistiken auftauchen, handelt es sich vermutlich um Texte aus literarischen 
                    
Selfpublishing
 Verlagen, die in der Umfrage des Börsenvereins nicht miteinbezogen waren. Darüber, ob diese nicht doch Teil eines Kulturgraphen sein sollten, lässt sich allerdings trefflich streiten. 
                

                
An diese explorative Studie könnte nun eine Untersuchung anschließen, die die Veränderungen der Korpuszusammensetzung als überdurchschnittlich starke Veränderung der Token-Verteilungen formalisiert (Koplenig 2017) und so den hier etwas vernachlässigten Aspekt, wann sich genau die Veränderungen ergeben, herausarbeitet. Die ‚typische‘ Verwendung des Ngramm-Korpus und -viewers, nämlich die Untersuchung der Verwendungshäufigkeit von Termen in der schriftlichen Öffentlichkeit, ist durch die starken Schwankungen in der Zusammensetzung des Korpus sehr fragwürdig geworden. Da nach 2000 sonst eher randständige Bereiche, quasi-wissenschaftliche Texte und die Produktion der selbstverlegten Autorinnen und Autoren, das Korpus dominieren, ist es auch für rein sprachanalytische Untersuchungen, etwa zur Kollokationsanalyse, kaum verwendbar. Aber insgesamt verdient die Frage, ob und unter welchen Vorzeichen die Daten nicht doch für bestimmte Analysen herangezogen werden können, eine genauere Untersuchung. 

                

                    

                        

                        
Fig. 11: Relative Häufigkeiten einiger Wörter, die 1995 bis 2019 zunehmend häufiger verwendet werden.

                        
 

                    

                

            

        

            
Das 
                
Women Film Pioneers Project 
(WFPP) ist eine etablierte Online-Ressource für die Forschung zu Frauen im Frühen Kino (Gaines, Vatsal und Dall’Asta, o. J.).
                
 
Der Fokus des WFPP liegt auf dem Erzählen individueller Geschichten von Filmpionierinnen und der Sichtbarmachung blinder Flecken, um die Aufmerksamkeit auf das zu lenken, was selbst in der feministischen Filmtheorie lange unvorstellbar schien: Die große Rolle die Frauen zu Beginn der Filmgeschichte gespielt haben (Dang 2020). Anfang der 1990er Jahre zunächst als klassisches Buchprojekt konzipiert, wurde das WFPP 2013 als digitale Plattform veröffentlicht, die vorrangig drei Ziele verfolgt: die Verfügbarmachung von Forschungsergebnissen, die Anregung weiterer Forschung und das Hinterfragen klassischer filmhistorischer Narrative, in denen die Beiträge von Frauen unerwähnt bleiben. Obwohl strukturierten Metadaten eine zentrale Bedeutung bei der Sichtbarmachung, Zugänglichkeit und Nachnutzung von Forschungsdaten und digitalen Publikationen zukommt (Baca 2016; Flanders und Jannidis 2018), arbeitet das WFPP bisher nur sehr eingeschränkt mit Metadaten.
                

                
 Das Ziel meiner Dissertation ist deshalb die Entwicklung eines Thesaurus und dazugehöriger Annotationsrichtlinien für die semantische Verschlagwortung der Profile über Filmpionierinnen. Die zentralen Forschungsfragen des Dissertationsprojekts lauten dabei: Wie kann ein Thesaurus für die feministische Filmgeschichtsschreibung am Beispiel des WFPP entwickelt und angewendet werden? Wie kann die Forschung zu Frauen im Frühen Kino mit Hilfe strukturierter Metadaten sichtbarer und zugänglicher gestaltet werden? Welche Geschichte kann durch das WFPP (nicht) erzählt werden?
            

            
Für die Entwicklung eines Thesaurus für das WFPP sind neben der feministischen Filmgeschichtsschreibung verschiedene Forschungsfelder relevant, die sich mit der Zugänglichkeit von Wissen, digitalen Publikationen, Kategorisierungspraktiken und Metadaten beschäftigen. Zentraler Bezugspunkt sind dabei die digitalen Geisteswissenschaften, hier insbesondere Forschung zur digitalen Modellierung und Verfügbarmachung von text-basierten Ressourcen (Flanders und Jannidis 2018; Blaney et al. 2021). In diesem Zusammenhang beschreibt Murtha Baca wie elementar Metadaten für die Gestaltung digitaler Ressourcen sind (Baca 2016), was ebenfalls an die anwendungsbezogene Forschung im Bereich des Forschungsdatenmanagements anschließt
                
 
(Dierkes 2021; Iglezakis et al. 2021). Um Metadaten im Sinne der FAIR-Prinzipien interoperabel zu gestalten und mit anderen Daten verlinkbar zu machen, bietet sich ihre Bereitstellung in Linked Open Data an (Schmidt, Thiery und Trognitz 2022). Diesbezüglich hat sich das 
                
Simple Knowledge Organization System
 (SKOS) als ein Standard etabliert, um Thesauri maschinenlesbar zu formalisieren (Zaytseva 2020). 
            

            
Die methodische Umsetzung des Dissertationsprojekts erfolgt in drei Schritten. Für die Modellierung der Themen- und Begriffsfelder des Thesaurus wurden zunächst neun Kategorien identifiziert, die für filmhistoriografische Untersuchungen besonders relevant sind: Personen, Werke, Tätigkeiten, Orte, Techniken, Institutionen, Filmgenre, Themen und Publikationen. Im Zuge einer manuellen Textanalyse und -annotation mit CATMA werden die knapp 300 Profile mittels der neun Kategorien verschlagwortet. Dabei werden die gewählten Kategorien evaluiert, jeweils um Begriffsfelder ergänzt und in Form eines Thesaurus organisiert. Im nächsten Schritt erfolgt die Formalisierung des Thesaurus anhand von SKOS. Abschließend wird der Thesaurus exemplarisch an einer Auswahl von Profilen erprobt und zusätzlich um Annotationsrichtlinien ergänzt. Im Rahmen von Workshops mit Autor*innen, Nutzer*innen und zentralen Akteur*innen des WFPP werden der Thesaurus und die Annotationsrichtlinien evaluiert und Empfehlungen für die Weiterentwicklung formuliert. 

            
Die Arbeit mit SKOS wird von einer kritischen Reflexion darüber begleitet, welche Form der Datenmodellierung durch die Anwendung dieses Standards ermöglicht und was dadurch verhindert wird (Flanders und Jannidis 2018, 11). Dabei geht es sowohl um eine Auseinandersetzung mit den epistemischen Bedingungen von SKOS (Drucker 2011), als auch um ein Nachdenken über die spezifischen Anforderungen feministischer Filmgeschichtsschreibung an die Gestaltung von Kategorien.
                

                
 Wie kann es gelingen, spezifisch feministische Werte in die Gestaltung von Kategorien einfließen zu lassen, wenn wir davon ausgehen, dass es sich dabei nicht um neutrale Praktiken handelt, sondern um Prozesse, in die politische Haltungen und Werte eingeschrieben sind (Bowker und Star 1999; Drabinski 2013; D’Ignazio und Klein 2020)? Im Zusammenhang mit feministischer Filmgeschichtsschreibung geht es konkret auch um die Frage, wie die zahlreichen Leerstellen und Wissenslücken, die es aufgrund verloren gegangener Quellen und der fehlenden Dokumentation „feminisierter Arbeit“ (Hill 2016, 5) in Bezug auf die frühe Filmindustrie gibt, in daten-basierten Ansätzen dargestellt werden können (Dang 2020). 
            

            
Der Vortrag präsentiert eine erste Version des Thesaurus und diskutiert zentrale Erkenntnisse aus der Annotation der Sammlung im Hinblick auf die Frage, inwiefern durch die formale Beschreibung der Profile die Struktur der Wissensproduktion im 
                
Women Film Pioneers Project
 sichtbar gemacht und somit auch daraufhin untersucht werden kann, 
                
wie
 das WFPP feministische Filmgeschichte schreibt.
            

        

            
Mit Python als vielgenutzter Programmiersprache in den Digital Humanities
 steigt auch der Bedarf an Möglichkeiten zur nachhaltigen Weitergabe und Wiederverwendbarkeit von Python-Quellcode. Softwareentwicklungsnahen Lösungen wie der Verfügbarmachung über Versionsverwaltungsrepositorien oder dem Einpflegen in eine Paketverwaltung wurde der ‚Notebook‘-Ansatz
 zur Seite gestellt, der Dokumentation, Ausführung und Visualisierung verzahnt und eine Aufbereitung für verschiedene Zielgruppen ermöglicht.
            

            
Im Forschungskontext werden solche Notebooks daher verwendet, um auf einer (Web-)Seite Datensätze einzulesen, zu analysieren, visualisieren und die verwendete Methodik zu erläutern, ohne dies auf verschiedene Orte oder Zugänge verteilen zu müssen. In der (Nach-)Nutzung können z. B. Parameter in der Analyse oder Visualisierung direkt im Browser verändert werden und eine Anpassung ohne Programmierkenntnisse oder -erfahrung ermöglichen. Die Notebook-Dateien können wiederum über entsprechende Softwareentwicklungs-Repositorien zur Verfügung gestellt werden, was Anpassungen für weitere Datensätze oder Forschungsfragen erlaubt. Jupyter-Notebooks sind dabei als JSON-Dokumente strukturiert verarbeitbar.

            
Im Rahmen unseres Projekts geht es uns um die Möglichkeit, Jupyter-Notebooks so zur Verfügung zu stellen, dass sie für eine sehr heterogene Nutzendengruppe (u. a. Autor*innen, Forschende, Schüler*innen) einen Mehrwert bedeuten.
 Wir möchten verschiedene Ebenen der Vorkenntnisse bedienen und gleichzeitig ermöglichen, eigene Forschungsfragen einzubringen. Im Beitrag soll aber auch der infrastrukturelle Aufwand verdeutlicht werden, der hinter der Möglichkeit nachhaltig ausführbarer Notebooks für die Forschung steht und auf einen Ausgleich zwischen Flexibilität der Nutzung und Sicherheit des Angebots hinausläuft. Letztendlich möchte der Beitrag die Diskussion befördern, inwieweit die Community von einer forschungsgetriebenen, unabhängigen und nachhaltigen Infrastruktur zum Umgang mit Jupyter-Notebooks profitieren würde, da ein entsprechendes Vorgehen für individuelle Projekte weniger umsetzbar ist.
            

            
Zu den Vorteilen der Bereitstellung von Zugängen zu Daten und Analysen durch Notebooks gehören (i) die Möglichkeit, ein Angebot an eine breite Nutzendengruppe zu machen: Je nach Aufbereitung der Notebooks (interaktive Elemente wie Dropdown-Menüs oder Range-Sliders sind möglich) können sie fast ohne Vorkenntnisse mit Python betrieben werden und an das individuelle Forschungsinteresse angepasste Ergebnisse produziert werden, (ii) dass die technischen Voraussetzungen, z. B. benötigte Pakete, im Notebook selbst spezifiziert sind. Diese Vorteile kommen allerdings nur in einer konfigurierten Ausführungsumgebung zum Tragen. Werden nur die Jupyter-Notebook-Dateien bereitgestellt, setzt das bei den Nutzenden Kenntnisse in Python, Bash o. Ä. sowie im Umgang mit Jupyter voraus. Oft sind Pakete in aufeinander abgestimmten Versionen erforderlich oder in Abhängigkeit vom Betriebssystem verfügbar, so dass nur eine vorkonfigurierte Umgebung den Nutzenden tatsächlich die technischen Hürden abnimmt.

            
So stellt sich die Frage, in welchem Rahmen ausführbare Jupyter-Notebooks zur Verfügung gestellt werden können. Der Betrieb einer zugänglichen Ausführungsumgebung (‚Hub‘) setzt Hardware, Administrations- und Wartungskapazitäten voraus. Eine Nutzungsverwaltung (Vergabe und Pflege von Accounts, Monitoring von Speicher- und Rechenkapazitäten) ist dabei ebenso unerlässlich wie Aktualisierungen mittels Updates auf Ebene von Maschine, Hub und Paketen und damit verbundene Wartungsarbeiten durch Abhängigkeiten in den Notebooks. Der Betrieb einer nachhaltigen Ausführungsumgebung setzt dies für einen längeren Zeitraum voraus, so dass die Idee der eigenen Ausführungsumgebung den Rahmen eines Forschungsprojekts oft übersteigt. Des Weiteren muss der Sicherheitsaspekt berücksichtigt werden, da es sich bei ausführbaren Jupyter-Notebooks um ausführbaren Quellcode handelt, der gewollt oder ungewollt Schaden am eigenen oder an externen Systemen verursachen kann.

            
Mit dem Service Colaboratory
 bietet Google an , Notebooks einzurichten, Pakete dafür dauerhaft zu installieren und diese Notebooks ggf. mit Zugangsbeschränkung zu veröffentlichen. Dies zeigt die technische Möglichkeit, einen Notebook-Service mit großen Gestaltungsmöglichkeiten für die Nutzenden zu hosten. Allerdings kann diese Lösung für die Forschung nicht als Standard vorgeschlagen werden – allein aufgrund der problematischen Speicherung aller Daten, aber auch weil hier aufgrund des unvorhersehbaren Umgangs mit den eigenen Diensten die Nachhaltigkeit nicht gesichert werden kann.
            

            
Eine Alternative hierzu kann der Betrieb einer stark restringierten Ausführungsumgebung sein, die zwar die vorhandenen Notebooks abspielen kann und Nutzende ggf. aus vorgegebenen Parametern wählen lässt, Forschenden aber kaum Flexibilität bezüglich einer eigenen Exploration oder Einbindung weiterer Pakete ermöglicht.

            
Sofern spezifische technische Expertise angenommen werden kann, ist eine weitere Möglichkeit, Docker-Container zum Download zur Verfügung zu stellen oder eine detaillierte Dokumentation zur Nutzung eines Notebooks innerhalb einer integrierten Entwicklungsumgebung zu liefern. Die Zielgruppe wird damit allerdings auf Nutzende der entsprechenden Infrastruktur einschränkt.

            
Notebooks, die über bestimmte Repositorien öffentlich zur Verfügung gestellt werden, können über Binder
 in eine Ausführungsumgebung gebracht werden. Dabei sind vor allem forschungsbezogene Repositorien wie Zenodo von Interesse, die gute Voraussetzungen für die langfristige Verfügbarkeit auf geschützten Servern bieten. Forschungsgetriebene Ansätze wie von GESIS
 mit Binder (Bleier und Erdogan 2020) sowie Forschungsumgebungen mit Nutzendenverwaltung wie das DHVLab
 und DH2go
 (Heckelen et al. 2022), die die Ausführung von Jupyter-Notebooks erlauben, ermöglichen ggf. auch den Umgang mit spezifischeren Daten, sind aber ggf. auf Nutzende aus bestimmten Fachbereichen oder Institutionen beschränkt.
            

            
Ein entsprechender Ansatz für die breite Forschungscommunity wäre ein großer Gewinn bezüglich der Verfügbarmachung, Nachnutzung und Dokumentation von Forschungsmethoden und -ergebnissen.

        

            

                
Einleitung

                
Die Computational Literary Studies (CLS) befinden sich als Disziplin zwischen der Literaturwissenschaft, der Informatik und der Computerlinguistik. Sowohl aus einer theoretischen als auch einer methodischen Perspektive spielen unterschiedliche Aspekte aus allen angrenzenden Fachbereichen in den CLS eine Rolle. So kommen in den CLS bspw. computergestützte Verfahren wie maschinelles Lernen und Annotationen mit literaturwissenschaftlichen Fragestellungen zum Einsatz und sorgen für eine starke heterogene Prägung des Forschungsfeldes. Entsprechend ergibt sich auch eine diverse Landschaft an genutzten Datentypen und -formaten sowie lebender Systeme, bspw. Software, Tools, Visualisierungen und Plattformen, die es im Sinne der FAIR Prinzipien (Wilkinson et al. 2016) zu managen gilt. Ebenso kombinieren sich Konventionen der unterschiedlichen, fachlichen Teildisziplinen der CLS in der Nutzung von Technologien, Infrastrukturen und der Publikation von Ergebnissen und Daten. Hieraus ergibt sich eine Heterogenität des Forschungsfeldes sowie des spezifischen Forschungsdatenmanagements, wie sie sich auch grundsätzlich in den Geisteswissenschaften allgemein darstellt (Pempe 2012).

            

            

                
Ausgangslage

                
Das DFG Schwerpunktprogramm 2207 „Computational Literary Studies“ (SPP CLS)
 bildet mit 10 geförderten Einzelprojekten sowie einem assoziierten Projekt seit 2020 einen Teil der deutschsprachigen CLS-Community. Im Rahmen des Zentralprojekts des Programms werden die einzelnen Teilprojekte individuell und projektübergreifend beim fachspezifischen Forschungsdatenmanagement (FDM) unterstützt.
                

                
Zu diesem Zweck wurde unter anderem eine Landschaftsvermessung vorgenommen, bei der in drei durch einen Leitfaden (Helling et al. 2020) gestützten Interviewrunden und einer Reviewrunde die Teilprojekte zum Umgang mit Forschungsdaten und lebenden Systemen, aber auch zu disziplinspezifischen Methoden und alltäglicher Projektarbeit befragt wurden. Zentrale Ziele der Landschaftsvermessung im SPP CLS sind die Entwicklung und Umsetzung einer möglichst fachspezifischen FDM-Strategie für das SPP CLS, sowie die Entwicklung einer Handreichung zu Best Practices und eines Anforderungsprofils für relevante/benötigte Infrastrukturen in den CLS.

            

            

                
Identifizierte Herausforderungen im FDM für die CLS

                
Neben einer grundsätzlichen Heterogenität in Bezug auf Datenformate und lebende Systeme in den CLS, für deren langfristige Sicherung und Verfügbarmachung kaum fachspezifische Lösungen existieren,
 konnte im Rahmen der Landschaftsvermessung insbesondere auch die zentrale Herausforderung kollaborativer Arbeit identifiziert werden. Viele Fragestellungen der CLS werden, wie in vielen anderen Fachdisziplinen auch, mit Hilfe diverser Methoden von interdisziplinären Teams aus Forschenden, gegebenenfalls an verschiedenen Standorten, bearbeitet. Dabei ist es wichtig gemeinsam an Daten und Dokumenten zu arbeiten, teilweise sogar gleichzeitig auf denselben Dateien.
                

                
Um die interdisziplinäre Zusammenarbeit zu fördern ist es daher unabdingbar, dass gemeinsam nutzbare Infrastrukturelemente verfügbar und leicht zugänglich sind. Institutionell aufgesetzte Versionskontrollsysteme, die den Zugang von Institutions-externen Forschenden nur über restringierte Gastzugänge zulassen, können dabei ebenso Hürden schaffen, wie verschiedene Vorgaben bezüglich der Nutzung von kommerziellen Angeboten oder proprietären Formaten.

                
Das Vorgehen und diese bisherigen Zwischenergebnisse der Landschaftsvermessung sowie pragmatische Lösungsstrategien zum Umgang mit Forschungsdaten in den CLS, wie bspw. der Betrieb einer gemeinsamen Gitlab-Instanz, wurden bereits mit den Communities der Digital Humanities (Helling et al. 2022a; Helling et al. 2022b) und des geisteswissenschaftlichen Forschungsdatenmanagements (Helling et al. 2021) diskutiert.

            

            

                
Ziele des vorgeschlagenen Workshops

                
Der Workshop soll eine oft implizit angenommene Ebene beleuchten, die im alltäglichen Umgang mit Forschungsdaten regelmäßig für kleine oder größere Ärgernisse sorgt oder sogar bestimmte Vorgehensweisen verhindert: Gemeinsames Arbeiten auf interaktiven Plattformen, Datenaustausch, unterschiedliche Datenformate, fehlende fachspezifische Infrastrukturangebote für die Publikation und Archivierung von Forschungsergebnissen sowie nicht mehr verfügbare oder lauffähige lebende Systeme wie Werkzeuge und Plattformen – dieser Ist-Zustand führt unter Umständen an entscheidenden Stellen zu pragmatisch- technischen Entscheidungen. Wir möchten die Community einladen, Erfahrungen aus ihrem Forschungsalltag zu teilen und Hürden aufzuzeigen, um dann gemeinsam eine Vision zu entwickeln, was wir benötigen um Wunsch und Wirklichkeit in Bezug auf Forschungsinfrastrukturen für die CLS in Einklang zu bringen, damit die technisch unterstützende Ebene ihre Rolle erfüllt und nicht zum Verhinderer wird.

                
Entsprechend möchten wir mit unserem Workshop die Ergebnisse der FDM-Landschaftsvermessung im SPP CLS als Ausgangspunkt nehmen und die damit verknüpften Fragestellungen mit der breiteren CLS-Community diskutieren, um das bisher entwickelte FDM-Anforderungsprofil der CLS um bisher ungesehene Aspekte ebenso zu erweitern wie die Konturen der identifizierten Best Practices zu schärfen. Der Workshop soll als offenes Forum verstanden werden, in dem die CLS-Community einerseits konkrete Bedarfe und Herausforderungen im FDM adressiert und an einem spezifischen, praxis- und community-getriebenen FDM-Bedarfsprofil arbeitet. Andererseits soll der Workshop auch auf operativer Ebene einen konstruktiven Austausch zwischen Fachwissenschaftler*innen der CLS und Datenmanager*innen ermöglichen.

                
Vor dem Hintergrund einer Ausgangslage mit Datentypen, Formaten und Methoden die - bedingt durch die Diversität der spezifischen Forschungsfragen - hochgradig heterogen ist, sollen unter anderem folgende Fragen in den Fokus genommen werden:

                

                    
Was benötigen Forschende der Computational Literary Studies für die tägliche Arbeit mit Forschungsdaten?

                    
Wie gelingt die Zusammenarbeit über Fach- und Institutionsgrenzen hinweg?

                    
Welcher Angebote bedarf es für die Sicherung, den Zugang, die Reproduzierbarkeit und Nachnutzbarkeit von CLS Forschungsergebnissen?

                

                
Dabei soll der Blick nicht nur auf disziplinspezifischen Werkzeugen und Infrastrukturen, wie sie z.B. über Initiativen wie DARIAH-DE
 und CLARIAH-DE
 zur Verfügung gestellt werden, liegen, sondern auch auf der disziplinspezifischen Nutzung von generischer Infrastruktur wie bspw. dem Forschungsdatenrepositorium Zenodo
 und der Softwareentwicklungs- und Versionskontrollplattform GitHub
.
                

                
Ein besonderes Augenmerk soll in diesem Zusammenhang auf der Unabhängigkeit von kommerziellen / proprietären Infrastrukturen liegen:

                

                    
Gibt es Zusammenhänge zwischen erzeugten Datenformaten und -strukturen und genutzten, proprietären Systemen?

                    
Welche Bedingungen verhindern möglicherweise die Nutzung spezifischer FDM-Lösungen, bspw. aufgrund von rechtlichen und finanziellen Hürden oder mangelnder Nachhaltigkeit?

                

                
Dabei ist es ein Anliegen des Workshops die Erfahrungen der Forschenden der CLS zu nutzen um strukturell wie anekdotisch den Ist-Zustand im Bezug zu den Ergebnissen aus dem Schwerpunktprogramm zu kartografieren und dabei Wunsch und Wirklichkeit einer interdisziplinären, modularen und vernetzten Infrastruktur in Beziehung zu setzen. Nicht zuletzt soll es um die Aussicht gehen, was von den digitalen Erzeugnissen der CLS die Chance hat auch in mehr als zehn Jahren noch nachvollziehbar zu sein.

                
Entsprechend möchten wir alle CLS-Community-Mitglieder und Interessierte einladen mit uns eine Bedarfsskizze für die Vision einer fachspezifischen und für alle zugänglichen Forschungsinfrastrukturlandschaft anzufertigen, die

                

                    
Zusammenarbeit über Institutions- und (Bundes-)Ländergrenzen ermöglicht,

                    
Nachnutzbarkeit, Zugänglichkeit und Reproduzierbarkeit unterstützt sowie

                    
(Langzeit)Archivierung in den Blick nimmt.

                

            

            

                
Ablauf des Workshops

                
Der halbtägige Workshop wird in drei Teile gegliedert sein (siehe Tab. 1), die durch zwei 15-minütige Pausen strukturiert werden. Im ersten Teil führen wir in Thema und Begriffe ein und berichten über Erfahrungen und Ergebnisse aus dem Forschungsdatenmanagement im DFG Schwerpunktprogramm „Computational Literary Studies". Dieser Teil endet mit einer kurzen Onlineumfrage, in der der bisherige Umgang mit Methoden des Forschungsdatenmanagements sowie typische Problemfälle der Teilnehmenden abgefragt werden. Ähnlich dem Format der CRETA-Werkstatt (Reiter et al. 2020) werden wir im zweiten Teil Thementische zur Archivierungsinfrastruktur, Arbeitsinfrastruktur und lebenden Systemen anbieten, an denen in vor Ort gebildeten Gruppen Erfahrungen, Herausforderungen, Lösungen sowie Visionen und Wünsche formuliert und diskutiert werden können. Dabei wird jeder Tisch von einer*m der Workshop-Organisator*innen begleitet, um im dritten Teil des Workshops Umfrage und Ergebnisse der Thementische gemeinsam auszuwerten und Wunsch und Wirklichkeit in einem gemeinsamen Anforderungsprofil zu beschreiben, das wir im Anschluss an den Workshop über Zenodo veröffentlichen werden.

                
                

                    

                        

                        
Dauer

                        
Inhalt

                    

                    

                        
Teil 1

                        

                    

                    

                        

                        
0-30 Min.

                        
Begrüßung und Einführung in das Thema / den Workshop

                    

                    

                        

                        
30-45 Min.

                        
Durchführung Onlineumfrage

                    

                    

                        

                        
45-60 Min.

                        
Kaffeepause

                    

                    

                        
Teil 2

                        

                    

                    

                        

                        
60-150 Min.

                        
Durchführung Thementische (jeweils 30 Min.)

                    

                    

                        

                        
150-165 Min.

                        
Kaffeepause

                    

                    

                        
Teil 3

                        

                    

                    

                        

                        
165-240 Min.

                        
Zusammenführung der Ergebnisse: Formulierung eines gemeinsamen Anforderungsprofils

                    

                    
Tabelle 1: Zeitplan des Workshops.

                

                
Neben dem unmittelbaren Bezug zum Forschungsdatenmanagement in den Computational Literary Studies und der Erweiterung der Ergebnisse aus der Landschaftsvermessung im SPP CLS soll der Workshop grundsätzlich zur Sichtbarkeit der FDM-Bedarfe der CLS-Community in Infrastrukturinitiativen wie dem NFDI-Konsortium Text+
 und dem EU-geförderten CLS INFRA
 Projekt beitragen.
                

            

            

                
Adressat*innen des Workshops

                
Der Workshop richtet sich an etablierte und potentielle Mitglieder der CLS-Community und Interessierte, die Erfahrungen auf ähnlichen Gebieten, mit interdisziplinären Methoden zur Untersuchung von Textgrundlagen haben und sich für die Methoden und Fragestellungen der CLS interessieren. Darüber hinaus möchten wir explizit auch Expert*innen im Bereich des geisteswissenschaftlichen Forschungsdatenmanagements einladen am Workshop teilzunehmen. Die maximale Teilnehmendenzahl beträgt 20. Bei größerem Interesse können die interaktiven Teile des Workshops ggf. in zwei bis drei Iterationen durchgeführt und für die Teilgruppen mit der Onlineumfrage verschachtelt werden.

                
Als technische Ausstattung wird vor Ort der Zugang zu Strom, stabilem Internet und einem Projektor mit Leinwand/Projektionsfläche benötigt. Um sich an der Online-Umfrage beteiligen zu können, sollten die Teilnehmenden über ein digitales Endgerät verfügen. 

            

            

                
Organisator*innen des Workshops

                
Kerstin Jung (Conceptualization, Writing – original draft) promovierte in der Computerlinguistik zum Thema der aufgabenbezogenen Kombination von automatisch erstellten Syntaxanalysen. Sie arbeitet im Zentralprojekt des SPP CLS zur disziplinspezifischen Unterstützung des FDM und bringt Erfahrung aus verschiedenen Infrastrukturprojekten und der Koordination kollaborativer Annotationsvorhaben ein. Ihre Forschungsinteressen liegen im Bereich der Nachhaltigkeit von Sprachressourcen und Abläufen sowie Metadaten- und Annotationsformaten. 

                
Steffen Pielström (Conceptualization, Writing – review &amp; editing) ist promovierter Biologe und arbeitet seit fast 10 Jahren im Bereich der Evaluation, Entwicklung und Vermittlung von quantitativen Methoden für die computergestützte Textanalyse in den Geisteswissenschaften. Er hat an verschiedenen Infrastrukturprojekten für die Digital Humanities mitgewirkt und ist zur Zeit im Zentralprojekt des SPP CLS tätig.

                
Patrick Helling (Conceptualization, Writing – review &amp; editing) ist Medienwissenschaftler und Medieninformatiker. Er arbeitet im Zentralprojekt des SPP CLS und ist für die Entwicklung einer umfassenden FDM-Strategie für das Schwerpunktprogramm zuständig. Darüber hinaus ist er bereits seit 2017 am Data Center for the Humanities (DCH) an der Universität zu Köln tätig und dort Teil des FDM-Beratungsteams. Patrick Helling verfügt über Expertise im geisteswissenschaftlichen Forschungsdatenmanagement. Im Rahmen seiner Promotion arbeitet er an der Entwicklung eines formalen Beschreibungsmodells für das Management von Forschungsdaten.

                
Daniel Kababgi (Writing – review &amp; editing) ist Masterstudent an der Universität Würzburg für Digital Humanities und Germanistik. Sein Studienschwerpunkt liegt auf NLP und dessen Anwendung innerhalb der Literaturwissenschaften. Das Hauptaugenmerk liegt auf der distanzierten Betrachtung der Literatur des 18. und 19. Jahrhunderts im Bezug auf die literarischen Epochen der Aufklärung und der Romantik.

            

        

            

                

                    
Abstract
                

                

                    
Datenmanagement ist eine zentrale Fragestellung für sämtliche Projekte im Bereich der Digital Humanities (Altenhöner et al. 2020). Warum also ein gesonderter Blick auf DH-Projekte mit einem musikeditorischen Hintergrund? Die Publikationspraxis musikwissenschaftlicher Editionen unterscheidet sich im Kern deutlich von derjenigen in anderen Disziplinen. Hieraus ergeben sich fachspezifische Herausforderungen, die durchaus Einfluss auf die Konzeption bestehender digitaler Musikeditionsprojekte nehmen. Im Rahmen des Vortrags wollen wir versuchen, diese Besonderheiten herauszuarbeiten und anhand des von der Mainzer Akademie der Wissenschaften und der Literatur geförderten Projekts “Beethovens Werkstatt” Ansätze zur Umsetzung von Open Data auch im Bereich der Musikphilologie aufzuzeigen. Potenziale ergeben sich dabei sowohl auf inhaltlicher Seite, also in Bezug auf die erarbeiteten Daten, wie auf methodischer Seite, d.h. in Bezug auf die entwickelten Tools.
 
                

            

            

                
Zur Ausgangslage: Publikationspraxis und Wertschöpfungsketten musikwissenschaftlicher Editionen

                

                    
W

                    
ährend es etwa in der Literaturwissenschaft eine Fülle verschiedener Publikationsformen gibt – Studienausgaben, Leseausgaben, kritische Ausgaben usw. –, konnte sich in der Musikwissenschaft nie eine entsprechende Vielfalt wissenschaftlicher Ausgabentypen entwickeln. Schon die ersten musikalischen Gesamtausgaben formulierten in ihren Vorworten den Doppelanspruch als Ausgaben ‘für Wissenschaft und Praxis gleichermaßen’ (vgl. Fellerer 1980:185; Kepper 2011:180). Die Ursache waren schon damals finanzielle Erwägungen: Die manuelle Erstellung der Druckvorlagen für Notenstich bzw. -druck war schon immer ein sehr zeitintensiver Prozess. Breitkopf und Härtel etwa hatten 1869 insgesamt 14 Notenstecher in Dienst (Hase 1968:386), die in 1867 eine Jahresleistung von “über 5000 Platten” hatten (Hase 1968:394). Setzt man diese Zahlen ins Verhältnis, landet man bei etwa 357 Platten pro Notenstecher. Dieser Zeitbedarf von etwa einem Arbeitstag pro 

                    
Seite Notentext blieb im Notenstich bis zur Verbreitung des computerbasierten Notensatzes gegen Ende des 20. Jahrhunderts recht konstant (Popp 2017). Dem standen bereits im 19. Jahrhundert recht geringe Auflagenhöhen gegenüber, und auch heutige Gesamtausgaben haben oft Subskribentenzahlen im unteren dreistelligen Bereich. Die Bände dieser Gesamtausgaben sind i.d.R. sehr aufwendig ausgestattet und werden entsprechend teuer verkauft – oft genug ebenfalls im unteren dreistelligen Bereich. Die Inhalte für diese Bände werden zumeist durch öffentlich geförderte Projekte erstellt und den Verlagen kostenfrei, teils sogar mit einem zusätzlichen Druckkostenzuschuss zur Verfügung gestellt. Auf dieser Basis liegt die eigentliche Wertschöpfung für die Verlage dennoch an nochmals anderer Stelle: Erst mit sogenannten praktischen Ausgaben für den professionellen wie nichtprofessionellen musikalischen Gebrauch wird ein größerer Markt jenseits von Bibliotheken und Musik

                    
wissenschaft

                    
 adressiert. Außerdem eröffnen der Vertrieb von Leihmaterial und v.a. die Lizenzierung von Aufführungen bzw. Rundfunksendungen wichtige Einnahmequellen. Aufgrund der bereits erwähnten hohen Produktionskosten ist es üblich, diese praktischen Ausgaben mit möglichst geringem Aufwand aus den wissenschaftlich-kritischen Ausgaben zu destillieren und, beispielsweise mit dem schillernden Label “Urtext”, zu vertreiben. Dafür wird oft der bereits vorhandene Notentext mit einem neuen Vorwort versehen, der Kritische Bericht – wenn überhaupt berücksichtigt – auf wenige Seiten kondensiert, und so mit minimalem Aufwand eine Ausgabe erstellt, die immer noch den aus Marketingsicht günstigen Anspruch erhebt, auf neuesten wissenschaftlichen Erkenntnissen zu basieren. Dass diese Wissenschaftlichkeit gerade aus den entfallenden Teilen resultiert, ist aus dieser Perspektive unerheblich. Viel wichtiger ist es aus Verlagssicht, dass sich das Notenbild schon der wissenschaftlichen Ausgaben nicht durch ein (vermeintliches) Übermaß diakritischer Auszeichnung für eine praktische Nachnutzung disqualifiziert, sondern mit einem vertretbaren Aufwand an diese Zweitverwertung angepasst werden kann. Solche wissenschaftsfremden Prämissen sind prägend für Ausgaben, die dem Paradigma “für Wissenschaft und Praxis gleichermaßen” unterworfen sind.

                

                

                    
Nicht

                    
 erst angesichts der sich stark verändernden gesetzlichen Rahmenbedingungen (v. a. das im Juli 2021 in Kraft getretene 

                    
Zweite Open Data Gesetz

                    
 der Bundesregierung verpflichtet ab 2024 zur Veröffentlichung aller mit Bundesfinanzierung erhobenen Forschungsdaten als Open Data) und der bereits entsprechend veränderten Vorgaben der Drittmittelgeber (vgl. z. B. die DFG-Information Nr. 25 “Konkretisierung der Anforderungen zum Umgang mit Forschungsdaten in Förderanträgen” vom 14.3.2021:

                    
 

                

                

                    

                    
) für den Umgang mit Forschungsergebnissen erscheint diese Wertschöpfung fragwürdig. 

                

                

                    
Es 

                    
dürfte offensichtlich sein, dass Musikverlage bislang kaum als treibende Kraft für Open Data / Open Culture in Erscheinung getreten sind. Anders sieht es bei vielen DH-Projekten aus. Die freie Zugänglichkeit der Forschungsdaten spätestens zum Ende des Förderzeitraums ist inzwischen weitgehend obligatorisch. Auch Förderinstitutionen im Bereich Musikwissenschaft wie die Union der Akademien in Deutschland als Förderer der meisten musikphilologischen Langzeitprojekte in Deutschland legen insbesondere bei Neuvorhaben inzwischen verstärkt Wert auf Open Access – bei bestehenden Projekten stehen dem i.d.R. die bestehenden, langfristigen Verlagsverträge im Wege. Damit stehen die Geldgeber an der Seite der meisten Projekte im Umfeld der Digital Humanities. Im Sinne des 

                    
ratchet effect

                    
 (Tomasello 2009) arbeiten diese Projekte oft in der Annahme, im Zuge ihrer Forschung Daten zu erstellen, die auch für nach– oder anders gelagerte Forschungsfragen hilfreich sein können. Hierfür ist es aber zwingend notwendig, nicht nur die abgeschlossenen Ergebnisse zu publizieren, sondern auch die ihnen zugrundeliegenden Rohdaten. Zum Wandel der förderpolitischen Rahmenbedingungen gesellt sich dabei ein arbeitskultureller, vor allem aber auch ein Wandel editionstheoretischer Herangehensweisen vor dem Hintergrund der Digitalität, die a) die Statik und Linearität von (musikalischem) Text zugunsten netzförmiger, aber auch dynamischer, prozessualer Darstellungsweisen erweitert (Droese/Münzmay 2015; Stadler 2019), b) editorische und historische Objekte in hybriden Textgebilden zusammenführt (die häufig komplette digitalisierte Re-Publikationen von Kulturobjekten einbinden, die digital ‚beschriftet‘ und vernetzt werden; Münzmay 2018) und c) Multimodalität editorisch handhabbar macht. Neben die herkömmliche Befassung mit dem Werktext treten neue musikphilologische Gegenstände, wie etwa phonographische Quellen (Orcalli 2017; Münzmay/Siegert 2019; Pasdzierny 2019) oder textgenetische bzw. Schreibprozesse (Kepper/Sänger 2017; Kepper/Cox 2021).

                    
 

                

            

            

                
Beethovens Werkstatt

                

                    
Das 

                    
etwa in der Mitte seiner auf 16 Jahre angelegten Laufzeit stehende Projekt Beethovens Werkstatt reiht sich ein in eine Riege musikeditorischer Projekte, die bewusst mit den Konventionen der Musikphilologie, d. h. mit Konzepten wie ‚Historisch-kritische Werkedition‘ und ‚Gesamtausgabe‘, brechen und so neue Perspektiven aufzuzeigen versuchen. Dies wird in der Projektkonzeption unmittelbar deutlich: Wie kein anderes musikwissenschaftliches Akademieprojekt versteht sich Beethovens Werkstatt als 

                    
methodisches

                    
 Grundlagenforschungsprojekt, das ganz bewusst schon im Antrag auf eine Auflistung der zu behandelnden Werke verzichtet, sondern lediglich für bestimmte Fragestellungen mit Beispielen versieht. Ziel ist demnach die Untersuchung dieser philologischen Fragestellungen (Darstellung von Varianz, Schreibprozessen und Revisionsprozessen; Fassungsvergleiche; Skizzeneditionen), nicht das starre Abarbeiten von Werklisten. In vielen Fällen ist dafür die Betrachtung von teils stark begrenzten Auszügen vollkommen 

                    
ausreichend – die thematisierten Werke werden also nicht in Gänze thematisiert. Angesichts der eingangs geschilderten üblichen Verwertungsketten im Musikbereich ergibt sich daraus quasi zwangsläufig eine weitere Besonderheit des Projekts: Der bewusste Verzicht auf die Zusammenarbeit mit einem Musikverlag. Auch wenn in bestimmten Modulen des Projekts durchaus vollständige Werke im Sinne genetischer Textkritik erarbeitet werden, so dass eine musikpraktische Nutzung zumindest einiger Projektdaten nach entsprechender Aufbereitung durchaus vorstellbar wäre, ist dies gerade nicht die Zielsetzung – auch, weil das Projekt aller Voraussicht nach nicht in entsprechendem Maß an der Wertschöpfung beteiligt würde, um den damit verbundenen Aufwand zu kompensieren. Stattdessen werden sämtliche Projektdaten – XML-Daten nach dem Standard der Music Encoding Initiative MEI, SVG-Daten sowie die im Projekt entwickelte modulare Forschungs- und Visualisierungssoftware „VideApp“ – frei zur Verfügung gestellt. Damit steht es Verlagen offen, nun allerdings bei eigenem Aufwand entsprechende Ausgaben auf dieser Datengrundlage zu erstellen. Für das Projekt ergibt sich daraus kein echter Nachteil. Ein gründliches Verlagslektorat – was wohl der aus Projektsicht attraktivste Vorteil einer Zusammenarbeit wäre – ist ohnehin längst nicht mehr bei allen Musikverlagen bzw. Verlagsverträgen selbstverständlich.

                    
 

                

                
An 
                    
die Stelle der Ausrichtung auf Verlagspublikationen tritt eine enge und wechselseitige Verzahnung von inhaltlicher Erschließung des Gegenstands, Datenerstellung, -aufbereitung und -pflege sowie Entwicklung der Software, die eine Darstellung und Interaktion mit diesen Daten erlaubt. Auch hier zeigt sich eine Eigenheit des Projekts, die es stärker in eine Reihe mit DH-Grundlagenforschungsprojekten als mit herkömmlichen Musikeditionen stellt: Die erstellten Daten sind für sich genommen nicht ausreichend, um die Editionsinhalte zu transportieren. Zumindest auf absehbare Zeit ist nicht davon auszugehen, dass Standardsoftware wie der „DFG-Viewer für musikalische Quellen“ oder die diesem zugrunde liegende Verovio-Rendering-Bibliothek für MEI die Projektdaten in Gänze sinnvoll auswerten können. Vielmehr bedarf es auch der im Projekt erstellten Software, da erst diese die spezifischen Datenmodelle und Konzepte zur genetischen Edition von Musik greifbar macht.
 
                

                
Beethovens 
                    
Werkstatt verfolgt damit einen integralen Ansatz für eine verschränkte, Hand-in-Hand-gehende Daten-Erarbeitung und Toolentwicklung. Dieser Ansatz ist dabei inhaltlich notwendig – auch mit viel Fantasie und ohne äußere Vorgaben fällt es schwer, sich eine auch nur ansatzweise vergleichbare Umsetzung der Projektergebnisse außerhalb digitaler Medien vorzustellen. Es handelt sich also um eine genuin digitale Edition im von Patrick Sahle beschriebenen Sinne (Sahle 2017:239)
. 
                

                

                    
Aus 

                    
diesem grundlegend digitalen Ansatz folgen aber andere Anforderungen, als sie in musikwissenschaftlichen Editionsprojekten sonst üblich sind. Neben Überlegungen zur Publikation der Daten bedarf es auch entsprechender Überlegungen zum Umgang mit der Forschungssoftware. Beethovens Werkstatt verfolgt von Beginn an die Strategie, sämtliche 

                    
Software als 

                    
Open Source

                    
 in öffentlichen Repositorien auf GitHub zu entwickeln. Abgesehen von terminologischen Diskussionen, deren Zwischenschritte auf dem Weg zu einer ersten veröffentlichten Version im Glossar des Projekts zunächst in einem nur intern zugänglichen Wiki-System festgehalten werden, findet auch die gesamte inhaltliche Arbeit des Projekts als 

                    
Open Data

                    
 öffentlich statt (

                    
Open Access

                    
). Über parallele 

                    
Continuous Integration-Pipelines

                    
 werden stabile Zwischenstände dieser Arbeit öffentlich, tagesaktuelle Stände hingegen zunächst nur für den internen Gebrauch in leicht zugänglicher Form aufbereitet und zur Verfügung gestellt. Unabhängig davon kann die Arbeit des Projekts über die entsprechenden Repositorien jedoch jederzeit vollständig und frei zugänglich nachvollzogen werden. Ein völlig freier “Blick in die Werkstatt” ist damit jederzeit möglich. Auch in dieser Meta-Perspektive wird also das “deiktische Prinzip” des Projekts umgesetzt: Statt teils umständlicher Erläuterungen versucht die sog. VideApp, genetische Prozesse möglichst anschaulich zu visualisieren. Entsprechend direkt ist der Zugang zur Arbeit des Projekts (der freilich der gängigen Logik von Forschungsförderung folgend dennoch in regelmäßigen Berichten dokumentiert wird).

                

                

                    
Damit 

                    
bietet Beethovens Werkstatt ein ideales Anschauungsbeispiel zur Komplexität digitaler musikwissenschaftlicher Editionen, die sich u. a. aus nicht-linearen bzw. dynamischen Inhalten bzw. Visualisierungen historischer Objekte und darin manifester genetischer Prozesse sowie der Multimodalität der zur Umsetzung notwendigen Daten speist. Für einzelne der genannten Bereiche gibt es 

                    
best practices

                    
, an denen sich Projekte orientieren können: Für den Bereich der Editionsdaten ist eine revisionssichere Publikation beispielsweise bei Zenodo gut möglich. Für den Bereich der Softwareentwicklung lassen sich 

                    
Continuous Integration

                    
 / 

                    
Continuous Delivery

                    
-Workflows mit Docker und GitHub Actions umsetzen. Was aber, wenn die genutzte Software sowohl integraler Bestandteil der Edition ist als auch zeitgleich unabhängig von diesen Daten zur Nachnutzung in anderen Kontexten nutzbar sein soll? Bislang gibt es keine etablierten Standards, die sich einfach für neue Projekte adaptieren ließen, und auch Beethovens Werkstatt hat in diesem Feld wiederholt neue Ansätze erprobt. Der Vortrag wird diese Erfahrungen dokumentieren und problematisieren, was für eine weitergehend standardisierte Lösung notwendig wäre.

                    
 

                

            

        

            

                
Einleitung

                
Die Überlieferung von Texten ist vorwiegend an die schriftliche Form gebunden, die bis zur Erfindung von Tonaufnahmetechniken in der zweiten Hälfte des 19. Jahrhunderts die einzige Möglichkeit war, von Sprachbeiträgen nicht nur den Inhalt, sondern weitgehend auch die Darbietung festzuhalten. So haben sich literarische Traditionen und die wissenschaftliche Auseinandersetzung mit Literatur überwiegend entlang der schriftlichen Überlieferung entwickelt. Selbst bei Gattungen wie der Lyrik, in der Klang eine wichtige inhaltliche und ästhetische Rolle spielt (vgl. Richter et al. 2022), steht die Textform im Zentrum der kanonischen Überlieferung. Erst am Ende des 20. Jahrhunderts (u.a. angeregt durch die Sound Studies) haben sich in der Literaturwissenschaft Forschungsfelder zu Stimme, Klang, Akustik, Auditivität und Audioliteralität etabliert (Göttert 1998, Meyer-Kalkus 2001, Schulz 2018, Meyer-Kalkus 2020, Meyer-Sickendiek 2020). Bis auf wenige Ausnahmen (z.B. Rhythmicalizer, vgl. Meyer-Sieckendiek et al. 2017) folgen die Digital Humanities bislang recht stark dieser eingespielten Zugangsweise – obgleich seit etwa 1900 unzählige Tonaufnahmen von Rezitationen vorliegen. Auch in der linguistischen Prosodieforschung und in der Sprachtechnologie wurde über die letzten Jahrzehnte ein Methodeninventar entwickelt, das eine sehr differenzierte Formulierung von Hypothesen zur Beziehung zwischen Text und lautlicher Realisierung erlaubt. Unser Workshop führt empirische Methoden aus der Phonetik mit aktuellen Technologien der Sprachsynthese und literaturwissenschaftlicher Forschung zur Lyrik der Romantik in einem Mixed-Methods-Workflow zusammen und bietet den Teilnehmenden auf diese Weise die Möglichkeit, das Wechselspiel von Textlichkeit und lautlicher Realisierung im Gedichtekorpus explorativ zu erkunden.

                

                
Der Workshop knüpft an Arbeiten aus dem BMBF-geförderten Projekt »textklang«
 an. In »textklang« kooperieren das Deutsche Literaturarchiv (DLA) Marbach sowie das Institut für Maschinelle Sprachverarbeitung und das Institut für Literaturwissenschaft der Universität Stuttgart, die Expertise in unterschiedlichen relevanten Fachgebieten vereinen. Der Fokus des Projekts liegt auf der Erschließung und Analyse lyrischer Texte der Romantik, wobei der Zusammenhang zwischen dem geschriebenen Text und seiner lautlichen Realisierung in Rezitationen und Vertonungen in den Blick genommen wird. 
                

                
Das beim Workshop verwendete Forschungskorpus zur Lyrik der Romantik speist sich aus der Mediendokumentation des DLA Marbach, die etwa 2700 Audioaufzeichnungen verschiedener Sprecher*innen seit den 1920ern beherbergt. Diese werden im Zuge des Projekts digitalisiert und um die dazugehörigen Metadaten und Transkripte ergänzt; darüber hinaus werden Texte und Rezitationen mit automatisch erzeugten Annotationen angereichert (siehe Schauffler et al. 2022b für eine Übersicht). Aktuell umfasst das »textklang«-Korpus 1261 Audioaufnahmen zu 786 Gedichten. Metadaten, Textdateien und lizenzfreie Audiodaten werden kontinuierlich über eine interaktive Webseite veröffentlicht.

                

                
In unserem Workshop kommen alle Bereiche des Mixed-Methods-Workflows zum Einsatz, indem Ansätze aus traditionell sehr unterschiedlich arbeitenden Disziplinen zusammengeführt werden. Das Analysetool ICARUS (Gärtner et al. 2015) unterstützt den korpus- und textorientierten Zugang, bildet dabei aber neben morphosyntaktischen Annotationen der Texte auch die phonetischen Annotationen der Rezitationen ab. Hierfür kommen Verfahren aus der Phonetik zum Einsatz, die die Eigenschaften des Sprachsignals systematisch erfassen. Sprachtechnologische Verfahren der Signalanalyse und -manipulation ermöglichen es sodann, bestimmte Annahmen über ein Re-Synthese-Tool kontrolliert zu testen. Der Bedarf für ein so weit gefasstes Methodenspektrum folgt aus den Grundeigenschaften des Untersuchungsgegenstands selbst. Der Workshop leistet einen Beitrag, die fachspezifischen Ansätze methodologisch zusammenzuführen und auf diese Weise den insbesondere für Lyrik zentralen Zusammenhang von Text und Klang in den Blick zu rücken.

            

            

                
Use-Cases

                
Idee des Workshops ist, dass die Teilnehmenden ihre eigenen Fragestellungen an Rezitationen von Lyrik der Romantik mitbringen können und darauf aufbauend während der Datenexploration Hypothesen entwickeln. Alternativ können die von uns vorgeschlagenen Fragestellungen aufgegriffen werden. Im Workshop thematisieren wir mehrere Use-Cases aus dem Projektkontext, darunter die Realisierung paralleler Strukturen (z.B. Reim, Satzbau), die unter strukturellen, semantischen und melodischen Aspekten von Interesse sind. Eine andere Fallstudie untersucht unterschiedliche Realisierungen von Enjambements (Schauffler et al. 2022a), die im Spannungsfeld von Vers- und Satzstruktur stehen. In Rezitationen können Sprecher*innen die syntaktische Einheit betonen, die Versgrenze markieren oder einen Mittelweg wählen (vgl. Tsur und Gafni 2019).

                
Ein weiterer Anwendungsfall, der exemplarisch etwas näher erläutert werden soll, beschäftigt sich mit Interjektionen. Interjektionen bezeichnen Ausrufe- oder Empfindungsworte (z. B. “ach”, “oh”, “juchhe”) und stehen im Grenzbereich von Schriftlichkeit und Mündlichkeit (Wharton 2003, Liedtke 2019). Sie nehmen eine syntaktische Sonderrolle ein und werden in der Linguistik als eigenständige Klasse behandelt, den Partikeln zugeordnet oder als Satzäquivalente angesehen (Liedtke 2019). Sie tragen einerseits denotativ keine Bedeutung, bringen andererseits Emotionen verschiedenster Art und in unterschiedlichen Intensitätsgraden zum Ausdruck (Schwarz-Friesel 2013, 155-157). Mit dem hier vorgestellten Mixed-Methods-Ansatz soll der Spielraum und der besondere textlich-klangliche (Zwischen-)Status von Interjektionen untersucht werden. Dabei interessiert zum einen die syntaktische Stellung von Interjektionen, zum anderen ihr Bedeutungsspektrum sowie, als dritter Aspekt, ihre lautliche Ausprägung. Die “Offenheit” dieser Wortart legt die Hypothese nahe, dass die verschiedenen Ebenen sich gegenseitig beeinflussen können, beispielsweise das syntaktische Umfeld die lautlichen Realisierungen in der Rezitation prägt oder bestimmte klangliche Merkmale die Bedeutung von Interjektionen ausmachen. 

                    

                        

                        
Abb.1

                    

                    

                        

                        
Abb.2

                    

                
Die abgedruckten Beispiele deuten die syntaktisch-lautlichen Spielräume der Interjektion “Ach” im Gedichtekorpus an: Während sie im ersten Beispiel syntaktisch isoliert steht (markiert durch den Tonhöhenverlauf und die Sprechpause), wird sie im zweiten Beispiel syntaktisch und lautlich in den Satz integriert. Auch die mit dem “Ach” ausgedrückten Emotionen (im ersten Beispiel Schwermut, im zweiten Freude) changieren und werden – neben dem semantischen Kontext des Wortes – von der jeweiligen sprachlichen Realisierung beeinflusst. Mögliche Leitfragen für weitere Untersuchungen könnten sein: Welche syntaktischen Merkmale von Interjektionen gehen mit welchen lautlichen Merkmalen einher? Werden Interjektionen in gleicher (syntaktischer) Position lautlich parallel realisiert? Welche Varianz ist zwischen unterschiedlichen Sprecher*innen zu beobachten? Inwiefern beeinflusst die lautliche Realisierung die Bedeutung oder Wahrnehmung von Interjektionen? 

            

            

                
Tools

                

                    
Icarus

                    
Für die Exploration und Visualisierung des Korpus mit allen Annotationsebenen verwenden wir ICARUS (Gärtner 2015) als Anfrageschnittstelle. ICARUS erlaubt eine gemeinsame Visualisierung von prosodischen Informationen und klassischen morphosyntaktischen Annotationen. Darüber hinaus können gezielt Anfragen unter Einbeziehung aller im Korpus verfügbaren Annotationsebenen gestellt werden, um Instanzen bestimmter Phänomene zu finden. An Annotationen stehen sämtliche für das GRAIN Korpus (Schweitzer et al. 2018) beschriebenen morphosyntaktischen und prosodischen Ebenen zur Verfügung. Darüber hinaus sind die Gedichte auch mit Markierungen zu Vers- und Strophenenden versehen, welche ebenfalls in Abfragen benutzt werden können. Je nach Entwicklungsfortschritt wird ICARUS als Desktop-Applikation
 eingesetzt oder in der Variante einer auf das »textklang«-Korpus zugeschnittenen Web-Oberfläche bereitgestellt. 
                    

                

                

                    
IMS Speech Synthesis Toolkit Toucan

                    
Die durch die Datenexploration entwickelten Hypothesen über Zusammenhänge zwischen Text und lautsprachlicher Realisierung sollen in Perzeptionsexperimenten untersucht werden. Mittels Sprachsynthese erstellen wir zu diesem Zweck eine prosodische Replikation der Originalaufnahmen, wobei phonetische Details (z.B. Lautdauer, Tonhöhe) gezielt manipuliert werden können (Koch et al. 2022). Unser Synthesemodell basiert auf der Modellarchitektur von FastSpeech 2 (vgl. Ren 2021), für die Implementierung nutzen wir das open-source Toolkit IMS Toucan
 (Lux et al. 2021, Lux und Vu 2022). Die Workshopteilnehmer*innen können über eine Bedienoberfläche mit dem Modell interagieren, indem sie spezifische, mit einem Phänomen verbundene Merkmale verändern und anschließend die Effekte der veränderten Parameter in der Perzeption testen. Beispielsweise kann die Längung, mit der ein Sprecher etwa das Versende markiert, verkürzt werden, die Tonhöhe an einer bestimmten Stelle angepasst oder die Dauer von Pausen verändert werden.
                    

                

            

            

                
Ablauf und Ziele

                
Wir beginnen den Workshop mit einer Einführung in den multimodalen Ansatz und adressieren die methodologisch wie wissenschaftstheoretisch relevante Frage, wie die Spezialisierungen der Fachgebiete innerhalb der DH sinnvoll zusammengeführt werden können. Anschließend präsentieren wir mögliche Forschungsbeispiele und führen in die verwendeten Tools ein.

                
In zwei Praxisrunden haben die Teilnehmenden die Möglichkeit, das Lyrikkorpus zu erforschen, eigene Forschungsfragen zu entwickeln sowie diese exemplarisch zu untersuchen. Dies kann individuell oder in Kleingruppen geschehen. Die erste Praxisrunde dient der Exploration des Korpus und der Entwicklung möglicher Hypothesen. Hierfür kommt das Tool ICARUS zum Einsatz, über das die Teilnehmer*innen die verschiedenen Annotationsebenen (u.a. morphosyntaktisch, phonetisch) sichten und komplexe Suchanfragen an die Texte modellieren können. Auf Grundlage der Annotationen zur Text- und Lautgestalt können Forschungsfragen entwickelt oder eine der vorgestellten Fragestellungen aus der theoretischen Einführung exploriert werden. Nach einer Zusammenschau der Hypothesen dient die zweite Praxisrunde dazu, ausgewählte Fragestellungen probeweise zu validieren, indem die Annahmen in das Sprachsynthesemodell überführt werden. Wenn beispielsweise die Annahme besteht, dass die Längung und die Tonhöhe einen Einfluss darauf haben, ob die “bedeutungsfreie” Interjektion “Ach” negativ oder positiv konnotiert ist, können ebendiese Merkmale in der Sprachsynthese gezielt modifiziert und die Effekte dieser Veränderungen getestet werden.

                
Die Ziele des Workshops bestehen folglich darin, die Möglichkeiten des Mixed-Methods-Ansatzes auszuschöpfen und Lyrik in ihrer Multimodalität erforschbar zu machen. Dabei liegt ein besonderer Schwerpunkt darauf, zu zeigen, wie fruchtbar das Zusammenspiel von textlicher und klanglicher Ebene sein kann. Zwar können die zu behandelnden Fragestellungen im Rahmen des Workshops nur ansatzweise durchgespielt werden, sie können dabei aber die Potenziale des interdisziplinären Ansatzes offenlegen.

            

            

                
Anhang

                

                    
Zeitplan



                        
Einführung und Ablauf (15 Min) 

                        
Theoretischer Teil (30 Min)


                                
Vorstellung der Projektidee

                                
Einführung in die Use-Cases

                                
Einführung in die verwendeten Tools 
                                    (anschließende Pause, 15 Min)
                                

                            

                        

                        
Praktischer Teil


                                
Erste Praxisrunde: Exploration der Daten, Entwicklung von Hypothesen (45 Min)

                                
Sammeln der Ergebnisse, Vorstellung möglicher Fragestellungen (15 Min) 
                                    (anschließende Pause, 30 Min)
                                

                                
Zweite Praxisrunde: Bearbeitung der Fragestellungen, Syntheseexperimente (45 Min)

                                
Sammeln der Ergebnisse (15 Min)

                            

                        

                        
Abschlussdiskussion (30 Min)

                    

                

                

                    
Teilnehmer*innen

                    
Unser Workshop ist für ca. 20 Teilnehmer*innen geeignet und richtet sich an Interessierte aus den digitalen Geisteswissenschaften. Bestimmte technische Vorkenntnisse sind nicht erforderlich. 

                

                

                    
Technische Ausstattung

                    
Die Teilnehmenden arbeiten an ihren eigenen Laptops. Ausreichend Steckdosen, stabiles Wifi und ein Beamer sollten vorhanden sein. Installationshinweise werden im Vorfeld an die Teilnehmer*innen verschickt.

                

                

                    
Beitragende

                    
Nora Ketschik (Institut für Maschinelle Sprachverarbeitung (IMS), Universität Stuttgart, 
                        
nora.ketschik@ims.uni-stuttgart.de
) ist wissenschaftliche Mitarbeiterin an der Universität Stuttgart. Sie promoviert zu Netzwerkanalysen von mittelhochdeutschen Romanen und setzt sich kritisch mit der Verwendung computergestützter Methoden für literaturwissenschaftliche Analysezwecke auseinander. 
                    

                    
Toni Bernhart (Institut für Literaturwissenschaft, Universität Stuttgart, 
                        

                            
toni.bernhart@ilw.uni-stuttgart.de

                        
) ist Privatdozent für Neuere deutsche Literatur und wissenschaftlicher Mitarbeiter der Abteilung Digital Humanities an der Universität Stuttgart. Seine Forschungsschwerpunkte sind die Imaginationsgeschichte von ‘Volkspoesie’, Auditivität und Literatur, Quantitative Literaturwissenschaft und Wissenschaftsgeschichte der Digital Humanities.
                    

                    
Markus Gärtner (IMS, Universität Stuttgart, 
                        

                            
markus.gaertner@ims.uni-stuggart.de

                        
) ist wissenschaftlicher Mitarbeiter und Doktorand an der Universität Stuttgart und regelmäßig in der technischen Konzeption und Umsetzung von infrastrukturell fokussierten Projekten tätig. 
                    

                    
Julia Koch (IMS, Universität Stuttgart, 
                        

                            
julia.koch@ims.uni-stuttgart.de

                        
) ist wissenschaftliche Mitarbeiterin und Doktorandin an der Universität Stuttgart. In ihrer Promotion arbeitet sie an Deep Learning Modellen für Sprachsynthese mit besonderem Fokus auf Kontrollierbarkeit.
                    

                    
Nadja Schauffler (IMS, Universität Stuttgart, 
                        

                            
nadja.schauffler@ims.uni-stuttgart.de

                        
) ist wissenschaftliche Mitarbeiterin an den Instituten für Maschinelle Sprachverarbeitung und Linguistik an der Universität Stuttgart und Postdoc im Projekt »textklang«, wo sie sich vor allem mit prosodischer Varianz beschäftigt. 
                    

                    
Jonas Kuhn (IMS, Universität Stuttgart, 
                        

                            
jonas.kuhn@ims.uni-stuttgart.de

                        
) ist Professor für Computerlinguistik am Institut für Maschinelle Sprachverarbeitung und seit vielen Jahren an interdisziplinären Projekten zur Methodenentwicklung für die Digital Humanities beteiligt. Er ist federführender Projektleiter des BMBF-Projekts »textklang«.
                    

                

            

        

            

                
Einleitung

                
Woran erkennt man die Handschrift eines Malers? Was macht den Stil eines Künstlers aus? Diese Fragen beschäftigen die Klassisch Archäologie bezüglich antiker griechischer Vasenbilder seit über einem Jahrhundert. Im Projekt EGRAPHSEN
 wird die Methode der Malerzuweisung als Klassifikationsproblem mit Supervised Machine Learning Verfahren untersucht. Das Ziel ist es, einerseits neue Erkenntnisse über Maler und Ihre Stile zu gewinnen, andererseits über die Methode zu reflektieren und das Vorgehen eines neuronalen Netzes dem traditionellen menschlichen Zugang gegenüberzustellen.
                

                
In der Klassischen Archäologie hat sich mit dem Vasenforscher John D. Beazley (* 13. September 1885; † 6. Mai 1970) eine Expertise und Kennerschaft ausgebildet, die ihm eine kaum anfechtbare Autorität verliehen hat. Er hat hunderttausenden Vasenbildern Maler zugewiesen, indem er sie insbesondere in ihren zeichnerischen Details verglichen hat (zu Beazley und seiner Methode s. Neer 1997, 7-16; Driscoll 2019, 106-110). Wenn auch die Methodenkritik und -reflexion in den letzten Jahren zugenommen hat (Graepler 2016, 18-21), so werden auch noch in aktuellen Publikationen Maler identifiziert und ihre Œuvre erweitert (z.B. Padgett 2017, 392-399). In den letzten Jahren fand mit dem Aufkommen des maschinellen Lernens in den Geisteswissenschaften jedoch auch ein Perspektivwechsel statt, und sowohl die Archäologie und Kunstgeschichte als auch die Informatik sind sehr am Erkenntnisgewinn und an der Methodenreflexion durch die Verwendung künstlicher neuronaler Netze in diesem Gebiet interessiert (Ma et al 2017, 1174-1176; Elgammal und Kang und DenLeeuw 2018, 42-49; Bell und Offert 2021, 4-9; Langmead 2021, 2-19). Das führt auch zu Veränderungen im Anspruch an die der Publikation von Forschungsergebnissen und -daten.

                
Beim Training eines Convolutional Neural Network (CNN) entstehen große Datenmengen: Annotationen, vorverarbeitete Bilder, Merkmalsvektoren, Meta- und Paradaten. In unserem Projekt streben wir an, diese Daten auf eine Art und Weise zu veröffentlichen, die einen forschungsorientierten und methodenkritischen Zugang erlauben. Das Konzept dieser Veröffentlichung in Form einer Datenbank soll im Zentrum dieses Beitrags stehen. Um das Problem zu verdeutlichen, soll zu Beginn der methodische Zugang kurz umrissen werden. Dann soll zunächst die Publikation der Bild- und Metadaten vor dem Hintergrund bestehender Tools der digitalen Kunstgeschichte erläutert werden. Schließlich wird auf die trainierten Modelle eingegangen und besprochen werden, inwiefern diese Verwendung in der Datenbank finden können.

            

            

                
Versuchsaufbau und Vorgehen

                
Zunächst soll die Datengrundlage benannt werden: Welche Informationen werden dem CNN zugeführt, um es zu trainieren? Bei der traditionellen Methode der Malerzuweisung stehen sehr spezifische Details im Vordergrund. Wir möchten mit computergestützen Methoden untersuchen, welche Bildelemente und -eigenschaften tatsächlich die Handschrift eines Malers erkennen lassen. Allerdings sind nur wenige Vasenbilder tatsächlich mit einer Malersignatur versehen, und auf diese Weise ergibt sich keine kritische Menge für das Training eines CNN. Deswegen haben wir uns in EGRAPHSEN entschieden, zusätzlich zu signierten Vasen auch die Malerzuweisungen von John D. Beazley als Ground Truth für die Klassifikation zu verwenden.

                
Da es um die Details im Bild gehen soll, nutzen wir außerdem nicht die gesamte Darstellung für das Training. Stattdessen trainieren wir mit unterschiedlichen Kombinationen von vordefinierten Bildausschnitten. Für diesen Zweck wurde eine kleinteilige Ontologie entwickelt, die die Körperteile der Figuren und die Bildbestandteile in ihren räumlichen Ausmaßen, Bezeichnungen und Bezügen zueinander klar festgelegt. So sind die Bildbestandteile nicht nur benannt, sondern meistens auch mit weiteren Informationen zu ihrer Darstellung angereichert. Sie sind außerdem in einem hierarchischen System strukturiert, sodass eine Figur auf unterschiedlichen Detailebenen betrachtet werden kann: Es könnte eine gesamte Figur für das Training verwendet werden, auf der nächsten hierarchischen Detailstufe lediglich der Arm, oder auf der nächsten hierarchischen Detailstufe auch nur die einzelnen Bestandteile des Armes (für eine ausführliche Beschreibung der Ontologie s. Kipke und Brinkmeyer, 2022, 3-5). Auf diese Weise kann man die Bilder ihrer Komplexität angemessen analysieren und mit unterschiedlichen Merkmalen auf verschiedenen Detailstufen experimentieren, ohne den Bildkontext vollständig zu verlieren.

                
Nach diesem System wurden 4.188 einzelne Figuren und damit insgesamt über 200.000 kleinteiligen Einzelannotationen von 38 Malern vorgenommen.
 Damit liegt eine hohe Dichte an Informationen pro Vasenbild vor. Diese Einzelannotationen werden extrahiert, vorverarbeitet und dem CNN zugeführt. Der Nutzen der ausgeschnittenen Annotationen soll an dieser Stelle jedoch nicht enden. Im Gegenteil sollen diese der Klassischen Archäologie als weitere Hilfestellung bei der Malerzuweisungdienen und der unkontrollierbaren Abstraktion des CNN sowie der autoritätsgesteuerten Zuweisung einzelner Forscher:innen gegenüberstehen.Somit soll der Malerzuweisung eine visuelle Evidenz verschafft werden, die in einer Abwägung unterschiedlicher Methoden zu neuen Erkenntnissen führen soll.
                

                
Denn während das Modell erfreuliche Ergebnisse bei Malern mit zahlreichen bekannten Werken und häufig verwendeten Labeln wie Augen und Händen liefert, werden auch die Grenzen und Gefahren schnell deutlich: Erstens bleibt der Mensch dem neuronalen Netz dort überlegen, wo nur wenige Werke bekannt oder wo diese sehr heterogen sind. Schließlich reichten John D. Beazley zuweilen nur zwei Vasenbilder, um einen Maler zu identifizieren (z.B. Beazley 1963, 21). Das ist eine Datenlage, auf der Supervised Machine Learning Verfahren aktuell noch keine befriedigenden Lösungen liefern können. Zweitens besteht noch Unklarheit darüber, ob nicht stärkere Eigenschaften des Bildes, wie etwa Zeit-, Gattungs-, oder Gefäßstil trotz Bildausschnitten zu einem unerwünschten Bias im Training führen und so den Erkenntnissen über den persönlichen Stil der Maler im Weg stehen könnten. Dies soll in unseren Daten mithilfe strukturierter Analysemöglichkeiten problematisiert werden können.

            

            

                
Konzeption: Der digitale Bildvergleich als Grundlage visueller Evidenz

                
Wie kann eine Publikation der Annotationen nun bestmöglich diesen Zweck erfüllen? Der detaillierte Bildvergleich steht nicht nur im Zentrum der Meisterforschung und Malerzuweisung, sondern bildet den Kern aller Bildwissenschaften. So steht die digitale Kunstgeschichte bereits in einer Tradition von Bilddatenbanken, die ein assoziatives Vorgehen und Sortieren ermöglichen und sich dabei auf Aby Warburg und den Entstehungsprozess seines Bilderatlas (Hristova 2016, 117-120; Du Preez 2020) berufen. Ein Beispiel hierfür ist die Anwendung Meta-Image, die im Rahmen eines gleichnamigen, DFG geförderten Projekts in Köln und Lüneburg entwickelt wurde. Die Anwendung erlaubt das stetige Neuanordnen von Bildern in Netzwerke, was eine Nachvollziehbarkeit des Erkenntnisgewinns ermöglicht, und damit eine visuelle Evidenz für die Beantwortung ikonographischer oder gestaltungstechnischer Fragestellungen schafft (Dieckmann und Warnke 2018, 79-90). Die Anwendung simuliert den Leuchttisch von Kunsthistoriker:innen, jedoch ohne von seinen physischen Grenzen eingeschränkt zu sein. Dieser Ansatz scheint auch für den Detailvergleich einzelner Bildausschnitte sehr lohnend.

                
Mehr Funktionalitäten im Analyseprozess und Unabhängigkeit zu Bilddatenbanken bietet das webbasierte Tool 
                    
ARIES. 
Es wurde Team von Forschern aus Amerika (New York) und Brasilien (Rio de Janeiro) entwickelt (Projektwebsite: 
                    
). Dort können eigene Meta- und Bilddaten importiert und mithilfe unterschiedlicher Tools analysiert werden. So kann man beispielsweise unterschiedliche Formen des Überlagerns der Bilder (Crissaff 2017 1-8; Deutch 2021, 7-12) simulieren. Jedoch ist die Möglichkeit zur Verwendung der Metadaten in einem Umfang und einer Komplexität, wie sie in diesem Projekt vorliegen, nicht möglich. Um den bestmöglichen Nutzen in einer kontrollierten Umgebung zu gewährleisten, wird stattdessen die Entwicklung einer eigenen Benutzeroberfläche und Exportmöglichkeiten für die Weiterverwendung in anderen Anwendungen angestrebt.
                

                
Eine solche Datenbank soll dabei nicht nur die annotierten Bildausschnitte zur Verfügung stellen, sondern auch Metadaten zu den Vasenbildern im Umfang des Beazley Archives (Smith 2005, 23-24; Kurtz 2009, 39-46) enthalten, die im Projekt um weitere Informationen wie beispielsweise eine kleinteilige Datierung der untersuchten Vasenbilder und Maße der Vasen ergänzt wurden. Zusätzlich zu den Metadaten soll das hierarchische Annotationssystem mit all seinen weiteren Informationsebenen als Grundlage für die Suchmaske dienen.Dabei soll die Suche nach drei Kategorien aufgefächert sein:

                
1. Annotationslabel: Es ist möglich, ein oder mehrere Labels (z. B. Hände, s. Fig. 1) auszuwählen, die für den Vergleich verwendet werden sollen. Dabei können auch bestimmte Zustände des Labels gewählt werden, die ebenfalls in der Annotation berücksichtigt wurden (z. B. nur Hände, die etwas halten oder Münder, die Flöte spielen, etc.).

                
2. Malerauswahl: Man kann einen oder mehrere Maler auswählen, die mit den gewählten Labels untersucht werden sollen. Dabei werden die Zuweisungen in vier Stufen von Zuordnungssicherheit geteilt: 1. Signierte Werke, 2. von Beazley zugeordnet, 3. von Œuvre-Forschern zugeordnet (z.B. J. H. Oakley beim Achilleus Maler (Oakley 1997)) und 4. von weiteren Vasenforscher:innen zugeordnet. Dies soll Transparenz und Nachvollziehbarkeit über die Sicherheit der Zuordnung gewährleisten.

                
3. Externe Kriterien: Die Zuweisung selbst ist bereits ein subjektives Kriterium. Deswegen soll es auch möglich sein, die Labels nach übergeordneten Kriterien in unterschiedlichen Kombinationen zu suchen und sie so im Kontext ihrer Datierung, Gefäßform, Figurengröße und ihres Motivspektrums zu betrachten, um damit das Verhältnis von Zeit- und Gattungsstil zum persönlichen Stil des Malers beurteilen zu können.

                

                    

                        

                        
Abb. 1: Ein Vergleich der Hände lässt die 'Handschrift' des Berliner Malers erkennen.

                    

                

                
Schließlich können sowohl die Zuweisung als auch die externen Kriterien vernachlässigt und die Bilder unabhängig davon angezeigt werden. Diese facettierte Suche soll dazu beitragen, sich von der Malerzuweisung durch bestimmte Forscher:innen zu lösen und einen Erkenntnisgewinn aus den Bildern heraus zu ermöglichen. Das Ergebnis dieser Suche soll dann ebenfalls eine Leinwand sein, auf der die Bilder nach den bereits genannten Kriterien sortiert werden können. Weiterhin hat jede Einzelannotation einen Datenbankeintrag, in dem die Metadaten eingesehen werden können. Ein Export des Suchergebnisses soll es schließlich ermöglichen, die Bilder auch in anderen Anwendungen zu importieren um weitere Untersuchungen durchzuführen.

            

            

                
Konzeption: Einsatz künstlicher neuronaler Netze für die Bildsuche

                
Diese Suchmaske basiert auf den Metadaten zur Vase und der Ontologie, die im Projekt entwickelt wurde. Die Merkmalsvektoren, Zuweisungen und Funktionalität des CNN sind darin noch nicht inbegriffen. Im Folgenden soll erörtert werden, inwiefern diese Daten nutzbar gemacht werden sollen.

                
Die keyword basierte Suche in EGRAPHSEN steht einem Trend entgegen, der eine gewisse Loslösung von Schlagworten in Bilddatenbanken anstrebt. Im Bereich des Content Based Image Retrieval suchen Forscher:innen nach bildimmanenten Eigenschaften, mit denen Deskriptoren für jedes einzelne Bild definiert werden können. Zwischen diesen Deskriptoren können Ähnlichkeitsbeziehungen berechnet und so Suchergebnisse generiert werden, die sowohl den Suchprozess erleichtern, als auch das Bild mit seinen Eigenschaften in den Mittelpunkt stellen (Tyagi 2017, 1-22). Dabei können neben einfach auslesbaren low-level-features wie Farben und Formen auch künstliche neuronale Netze verwendet werden, um Features zu extrahieren und für die Deskription der Bilder zu verwenden (Aasia und Sharma 2017, 1049; Hameed 2021, 21-32) Insbesondere Methoden der Computer Vision können bei komplexen und heterogenen Bildern, wie sie von den digitalen Geisteswissenschaften erforscht werden, den Umgang mit großen Bildkorpora erleichtern (Bell und Ommer 2016, 71-72; Bell und Ommer 2018, 67-72; Resig 2014).

                
Da es in EGRAPHSEN explizit um Stilanalyse geht, ist die Verwendung von low-level-features zu banal. Die Experimente im Projekt haben unterschiedliche trainierte Modelle ergeben, die genutzt werden können, um Features zu extrahieren und zu visualisieren. Diese Features könnten zwar auch für ein Content Based Image Retrieval verwendet werden, jedoch ist für EGRAPHSEN eine derartige Funktionalität aus verschiedenen Gründen nicht vorgesehen. Im Projekt ist das Ausmaß der experimentellen Abstraktion durch das CNN sehr deutlich geworden: Noch mehr als bei Zuweisungen durch Archäolog:innen ist die Nachvollziehbarkeit der Ergebnisse stark mit einer Interpretation dieser verbunden. Das führt zu spannenden Erkenntnissen über die Methode der Malerzuweisung und über die Funktion neuronaler Netze als solche, würde eine Datenbank in ihrer Funktionalität jedoch zu stark mit einer Subjektivität färben, die nicht mehr nachvollziehbar sein kann. Statt also die Ergebnisse der Experimente in der Datenbank funktional zu nutzen, soll sie ihnen gegenüberstehen und zur weiteren Forschung, Verifizierung und Vertiefung der methodischen Reflexionen dienen – insbesondere dort, wo die Verfahren des maschinellen Lernens derzeit an ihre Grenzen kommen. Auf der einen Seite steht also die Analyse der Maler und ihrer Beziehungen zueinander mithilfe eines CNNs, und auf der anderen Seite eine Anwendung zur Nachvollziehbarkeit dieser Ergebnisse und Vertiefung der Forschung durch menschliche Expert:innen.

                
Für den Kern der Datenbank – ihre Strukturierung und Funktionalität – ist also kein Einsatz von neuronalen Netzen vorgesehen. An zwei weiteren Stellen sollen aber die Ergebnisse der Experimente und Nebenprodukte des Vorgehens genutzt werden.

                
So werden die vom CNN extrahierten Features, die Merkmalsvektoren und die Zuweisungen als reine Werte in der Datenbank enthalten sein, um die Nachvollziehbarkeit der Experimente nutzerfreundlich zu halten und in einer Domäne zu dokumentieren.

                
Zudem sollen für das Wachstum und die Pflege der Datenbank Teile unseres semi-automatischen Annotations-Workflows nutzbar gemacht werden. Um auf eine kritische Datenmenge für das Training der Modelle zu kommen, wurde ein Annotationstool entwickelt, das auf einer Open Source Software Version des Tools LabelMe (Wada 2022) basiert und in EGRAPHSEN um eine Object Detection Komponente erweitert wurde. Der Workflow sieht vor, dass die Object Detection Vorschläge zur Annotation macht, die dann individuell angepasst werden können (Kipke und Brinkmeyer, 2022, 5-6). Da im Hintergrund von EGRAPHSEN unsere Projektdatenbank steht, liegen bereits Metadaten zu den Vasenbildern vor. Eine Datenpipeline mit Nutzung dieses Tools und der Pre-Processing Algorithmen soll es ermöglichen, weitere annotierte Ausschnitte aus Vasenbildern der Datenbank hinzuzufügen (vgl. Fig. 2).
                    

                        

                        
Pipeline zur weiteren Anreicherung und Nutzung der Datenbank. 
                            
Abb. 2: Blau betrifft die Datenbank, ihren Aufbau und Nutzung, orange den Anreicherungsprozess.

                        

                    

                

            

            

                

                    Fazit
                

                
In EGRAPHSEN wurde die Malerzuweisung als traditionelle Methode untersucht und diese der Funktionsweise von CNNs gegenübergestellt. Obwohl beide Methoden spezifische Bilddetails in den Mittelpunkt stellen, lassen sich auf beiden Seiten Vorteile benennen: Das menschliche Auge hat die Fähigkeit, auch bei geringen Bildmengen komplexe Transferleistungen zu erbringen und die Bilder in ihrer Heterogenität sowie im Aufbau zu verstehen, während das CNN einen Blick auf die Bilder ermöglicht, der nicht durch spezifisch menschliche Expertenkenntnis, motivische Zusammenhänge und unterbewusste Annahmen gefärbt sein muss. Jedoch besteht auch stets die Gefahr, dass menschliche Expert:innen bereits in der Bildauswahl und im Training die Ergebnisse beeinflussen und das CNN durch fehlendes Bildverständnis andere Fehlannahmen, z. B. über die Bildqualität, aufweisen kann.

                
Deswegen wurde in EGRAPHSEN großer Wert darauf gelegt, dass die Kategorisierung der Bildausschnitte auf einer Ontologie basiert, die bewusst auf interpretative Aspekte verzichtet und die Bilder primär in ihrer Form beschreibt. Dadurch, dass die Bilder aus ihrem Kontext extrahiert und nach frei wählbaren, formalen Kriterien sortiert werden können, bekommt die Malerzuweisung ihrerseits eine Evidenz, wie sie häufig sonst nicht vorhanden ist. Denn sei es Mensch oder Maschine – viele, stärkere Bildmerkmale beeinflussen die Zuweisung häufig erheblich.

                
Es soll eine Anwendung geschaffen werden, in der der Einfluss solche Merkmale, wie etwa des Motivs oder der Vasenform, möglichst reduziert werden. Mithilfe unterschiedlicher dynamischer Kriterien können sich Expert:innen zwischen den reinen Bilddaten auf der einen Seite und experimenteller Abstraktion durch das CNN auf der anderen Seite positionieren. So entsteht eine Forschungsumgebung, in der das menschliche Auge und die hochkomplexen Transferleistungen ausgebildeter Bildwisseschaftler:innen im Wechselspiel mit der KI ihr Potential weiter entfalten können.

            

        

            

            
Ausgangslage: 

            
GEI-Digital startete als Digitalisierungsprojekt im Jahr 2009 und bietet Zugang zu digitalisierten Schulbüchern zahlreicher Fächer, Epochen und Regionen. Das Schulbuchkorpus umfasst die Themengebiete Geschichte, Geographie und Politik sowie Fibeln, Atlanten, Realien- und Lesebücher („GEI-Digital“ unter 
                
https://gei-digital.gei.de/viewer/index/
). 
            

            
Der Digitalisierungsprozess umfasst dabei die Herstellung hochauflösender digitaler Images, die Erzeugung von Meta- und Strukturdaten, die Volltexterfassung, die Präsentation der Digitalisate auf der GEI-Digital-Website und Maßnahmen zur Langzeitsicherung. Gegenwärtig liegt der Fokus von GEI-Digital auf deutschsprachigen Werken, die in der Zeit zwischen dem ersten Aufkommen des Schulbuches im 16. Jahrhundert bis zum Ende des Kaiserreiches im Jahre 1918 erschienen sind. 

            
Die aktuell verfügbaren Sammlungen bzw. Teilkorpora auf GEI-Digital stellen eine Auswahl an historischen Schulbüchern dar, die als repräsentativ für das jeweilige Fach, die einzelnen Schulstufen und -formen und regional unterschiedlichen Ausgaben gelten kann. Im Hinblick auf die hohe Schulbuchproduktion im Deutschen Reich und die Bedeutung der Epoche für die Konstruktion einer nationalen Identität wird das digitale Korpus fachspezifisch und zeitlich in separaten Kollektionen auf GEI-Digital präsentiert. Internationale historische Schulbücher sind punktuell vorhanden und werden perspektivisch weiter ausgebaut (Hertling und Klaes 2018a, 21-44 und Hertling und Klaes 2018b 45-68).

            
Digitalisiert und unter einer Public Domain-Lizenz zur freien Nachnutzung zur Verfügung gestellt, werden nach dem geltenden Urheberrechtsgesetz (UrhG) bislang nur Werke, deren Autoren seit mehr als 70 Jahren verstorben sind. Derzeit beinhaltet GEI-Digital circa 7.400 Schulbücher mit über 1,8 Millionen Seiten. Davon konnten bereits 1,4 Millionen einer Volltexterkennung zugeführt werden. Zu den nicht im Volltext vorliegenden Beständen zählen aufgrund der überwiegend grafischen Darstellungen, Atlanten, sog. Schreiblesefibeln, und Bestände in Frakturschrift mit einem Erscheinungsjahr vor 1800. Die Implementierung einer Volltexterkennung insbesondere für ältere Frakturschriften bei mittelgroßen Sammlungen ist Gegenstand eines von der DFG geförderten Verbundprojekts „OCR-D OCR4all-libraries“ (Engl et.al. 2020 und „DFG – GEPRIS“ o.J.). Dabei wird in Zusammenarbeit mit dem Zentrum für Philologie und Digitalität (ZPD) der Universität Würzburg das Open-Source-Werkzeug OCR4all (Reul et. al. 2019) so erweitert und angepasst, dass es von Bibliotheken und Archiven bei der Digitalisierung größerer Mengen eigensetzt werden kann. 

            
In den letzten 10 Jahren haben sich die Anforderungen an das Design und Präsentation digitaler Inhalte massiv geändert. Vor dem Hintergrund hat sich das GEI entschieden den Web-Auftritt von GEI-Digital grundlegend zu erneuern und zu optimieren. Die Präsentation der Inhalte erfolgte bisher auf Basis des von der Firma Intranda und unter der Open Source-Lizenz stehenden Softwarelösung „Goobi Workflow“ und „Goobi Viewer“, die am GEI seit 2012 unverändert eingesetzt wird (Hankiewicz 2019, 77-88). 

            

            
Vorgehen: 

            
Bereits 2014 wurde eine umfangreiche Nutzer:innenbefragung durchgeführt, die wichtige Bedarfe sichtbar machte. In der Folge wurden gezielte Feedbackrunden mit Forschenden in den Digital Humanities etabliert, um die sichtbaren Bedarfe zu konkretisieren. Unter Berücksichtigung dieser Ausgangslage begann am GEI eine interdisziplinäre Arbeitsgruppe aus den forschenden Abteilungen und der Forschungsbibliothek einen Katalog mit Optimierungsanforderungen zusammen zu stellen. Dabei kristallisierten sich Volltextverfügbarkeit, Sammlungsverwaltung und Schnittstellen für die Datenbereitstellung als zukünftige Kernbedarfe dieser Zielgruppe heraus. 

            
Allerdings erforderte die 2020 veraltete Softwareinfrastruktur des Goobi Viewers als Fundament für die Realisierung der Kernbedarfe ein umfassendes Upgrade. Um aber den weiterentwickelten Goobi Viewer auch für andere Anwenderinstitutionen möglichst einfach nachnutzen zu können, hat das GEI zusammen mit dem Entwicklerteam von Intranda entschieden, diese Software in Form einer unter Open Source laufenden Containervirtualisierung, auch Docker genannt, zur Verfügung zu stellen. Docker erlaubt eine vereinfachte Bereitstellung von Software-Anwendung, da alle relevanten Softwarepakete enthalten und somit schnell für eine Installation bereitstehen. Die gesamte Goobi Viewer-Software steht jetzt als Docker-Anwendung unter GitHub zur Nachnutzung bereit (Goobi-Viewer-Docker o.J.). Eng verbunden mit dem Implementierungsprozess war auch eine starke Fokussierung auf die Lokalisierung, Eingrenzung und Behebung von Problemen und Fehlern. Die interdisziplinäre Arbeitsgruppe konnte gerade im Sinne eines agilen Vorgehens schnell und effektiv Herausforderungen angehen und Lösungsansätze erarbeiten. 

            

            
Zusammenfassung und Ausblick:

            
Das abgestufte Vorgehen mit einer Nutzer:innenbefragung, gezielten Feedbackrunden mit Forschenden und einer interdisziplinären Arbeitsgruppe erwies sich in dem hier vorliegenden Szenario als gewinnbringend. Mit Abschluss der Phase eines ersten technischen Upgrades und einer umfassenden Erneuerung der Software-Infrastruktur steht die digitale Schulbuchbibliothek GEI-Digital in einem neuen Outfit für die Nutzer:innen in den Digital Humanities zur Verfügung. Gerade die interdisziplinäre Zusammenarbeit von Akteur:innen aus den Forschungsinfrastrukturen und den wissenschaftlichen Anwender:innen konnte einen erfolgreichen Neustart sicherstellen und die nachhaltige Grundlage für weitere Optimierungen legen. 

            
Der kontinuierliche Betrieb einer digitalen Bibliothek und deren Optimierung sollte als eine Daueraufgabe verstanden werden. Vor dem Hintergrund ist vorgesehen, die Nutzungsmöglichkeiten von GEI-Digital für die vielfältigen Bedarfe weiter zu verbessern. Zu den nächsten Schritten zählen die Bereitstellung nachnutzbarer Datenformate nach aktuellen Standards und die Persistenz der Daten im Hinblick auf z.B. die verschiedenen Volltextversionen bei Anwendung verschiedener OCR-Engines im Kontext von Forschungsdaten.

        

            
Der Workshop adressiert eine signifikante Lücke in den Praktiken und Formaten der digitalen Geisteswissenschaften als 
                
Open Humanities
: die bisher noch untergeordnete Rolle von Softwarerezensionen von 
                
Forschungssoftware
.
            

            
Wissenschaftliche Rezensionen von Forschungssoftware schätzen deren Beitrag zur Lösung einer Aufgabe im Forschungsprozess, ihre Anwendbarkeit und Zielgruppe, sowie ihre handwerkliche Qualität und Nachhaltigkeit ein. Softwarerezensionen wären daher ein wesentlicher Bestandteil einer Wissenschaftspraxis, die ihre Methoden offenlegt und kritisch reflektiert. Bislang aber erscheinen noch wenige Rezensionen und das, obwohl es mittlerweile in den Geisteswissenschaften im deutschsprachigen Raum erste Zeitschriften zu ihrer Veröffentlichung gibt. Es finden sich jedoch noch kaum Autor*innen.
 Der Workshop soll Interessierten einen Einstieg bieten und somit zur Verbreitung und Anerkennung dieses wichtigen wissenschaftlichen Formats beitragen.
            

            
Gemeinschaftlich mit den Teilnehmer*innen möchten wir am Beispiel kleiner, überschaubarer Tools alle Arbeitsschritte einer Rezension von Forschungssoftware am praktischen Beispiel durchführen. Im Mittelpunkt steht das Kennenlernen und Anwenden von Aspekten, mit denen eine Software besprochen werden kann. Die Beurteilung von Forschungssoftware muss ihre Aufgabe in der Forschung, ihre Nutzbarkeit aus Anwender*innsicht und ihre Nachhaltigkeit aus technischer Sicht umfassen. Damit fragen Rezensionen von Software ganz unterschiedliche Kompetenzen ab und sind daher besonders gut und effizient als Team zu bearbeiten. Im Idealfall entsteht während des Workshops genug Material, um unterstützt von den Workshopanbieter*innen ohne umfangreiche Nacharbeiten eine Rezension beim Journal CKIT oder den Archäologischen Informationen einzureichen.

            
Forschungssoftware ist ein wesentlicher Bestandteil (digitaler) geisteswissenschaftlicher Forschung. Sie ermöglicht und steuert oft den gesamten Forschungsprozess (Katerbow u. Feulner 2018; Schmidt u. Marwick 2020). Dieser Einfluss reicht von der Auswahl und Strukturierung der Daten, über die angewandten rechnerischen Verfahren bis hin zu den Ausgabeformaten. Entsprechend können fehlerhaft implementierte Algorithmen, irreführende Nutzeroberflächen und unvollständige Dokumentationen die Forschung erschweren oder sogar zum Scheitern von Forschungsprozessen führen. Auch bei einem positiven Verlauf schreiben sich die Tools so tief in die Ergebnisse ein, dass eine digitale Quellenkritik immer wieder auch die Betrachtung der vorhergehenden Prozessierung und somit der Tools einbezieht. Dokumentierte, quelloffene Software ermöglicht schließlich, sie gezielt für spezifische Bedarfe weiterzuentwickeln oder ihre grundlegende Idee und Konzeption in Softwareprojekten mit aktuellen Bibliotheken erneut umzusetzen.

            
Aus unterschiedlichen Gesichtspunkten ist somit die kritische Betrachtung und Würdigung von Software, die in spezifischer Weise in der Forschung eingesetzt wird, sehr wünschenswert. Bislang wird der Bedarf, verstreut und über verschiedene Nutzergruppen verteilt, oft durch Erfahrungsberichte, Tutorials und zitierfähige Publikationen der Software etwa über Github oder Zenodo bedient. Insbesondere Erfahrungsberichte enthalten vielfach bereits Informationen, die Auswahlkriterien für die Nutzer*innen sind. Dies betrifft nicht alleine Anmerkungen zur Form und Verständlichkeit der Interaktion mit dem Programm oder die Import- und Exportfunktionen, sondern die Berichte zeigen zudem, wie die Software für die Bearbeitung einer geisteswissenschaftlichen Forschungsfrage eingesetzt wird. Allerdings finden Merkmale zur Beurteilung der Stabilität der Programme, zu ihrer Erweiterbarkeit oder auch ihrem Einsatz in anderen technischen Setups kaum Berücksichtigung. Tutorials, oft von den Entwickler*innen selbst verfasst, geben meist ebenfalls umfassend Einblick in die Funktionen, bewerten diese aber nicht. 
                
Benchmark papers
 und Softwarepublikationen richten sich schließlich nur an Entwickler*innen.
            

            
Nach der Bestimmung von Zielen und Blickwinkeln für Softwarerezensionen in der geisteswissenschaftlichen Forschung stellt sich die Frage nach den Kriterien, der Vorgehensweise und spezifischen Anforderungen. Die Zeitschrift Archäologische Informationen hat 2021 einen Vorschlag veröffentlicht (Homburg et al. 2021), der gemeinsam von Fachwissenschaftlerinnen und Informatiker*innen unterschiedlicher Schwerpunktsetzungen verfasst, und inzwischen kommentiert wurde (Carloni 2021; High-Steskal 2021). Der Text war eine wichtige Grundlage für die Ausformulierung der von den Herausgeber*innen der Zeitschrift CKIT formulierten Leitfragen, an denen sich die erwünschten Beiträge ausrichten sollen (CKIT 2021). Sie sind im Expert*innenforum der Task Area 3 “Research tools and data services” von NFDI4Culture 2022 diskutiert und angenommen worden, so dass sie heute in einer inhaltlichen und funktionalen Verbindung mit der entstehenden Software Registry stehen. Sie sind damit ein Anfang, um Gewohnheiten und 
                
best practices
 in der Forschungsgemeinschaft zu entwickeln. Entsprechend werden sie auch von den Workshopanbieter*innen als Orientierung verwendet.
            

            

                
Gemeinsame Ziele

                

                    
Erwerb der Kenntnis von wesentlichen Parametern zur Beurteilung einer Software im Forschungskontext

                    
Sammeln von Erfahrungen in der Zusammenstellung von spezifischen Nutzeranforderungen an eine Software

                    
Erwerb der Kenntnis von Vorgehensweisen, um Informationen zur handwerklichen Qualität von Software zu sammeln

                    
Sammeln von Erfahrungen im Einschätzen eigener Kompetenzen zur Beurteilung von Software

                    
Sammeln von Erfahrungen im (gemeinsamen) Verfassen einer Softwarerezension im Forschungskontext

                

            

            

                
Ergebnisformat

                
Ziel des Workshops ist es, auf Grundlage der genannten Handreichung (CKIT 2021) gemeinsam die Funktionalität und handwerklichen Qualität einer Software zu beschreiben sowie ihre Nutzbarkeit im Zuge der Bearbeitung einer geisteswissenschaftlichen Frage zu beurteilen.

                
Im Idealfall soll daraus eine gemeinsam verfasste Rezension entstehen, die bei der Zeitschrift CKIT oder den Archäologischen Informationen zur Veröffentlichung eingereicht wird.

            

            

                
Beispiele zur Rezension vorgeschlagener Forschungssoftware

                
1. SPARQLing Unicorn QGIS Plugin

                
Beschreibung: QGIS-Plugin (noch kein 
                    
stable release
, experimental), das eine einfache Integration von Geodaten aus Wikidata und anderen Linked Open Data SPARQL Endpoints ermöglicht. (Plugin: 
                    
; Github: 
                    
)
                

                
2. 
                    
PixPlot

                

                
Beschreibung: WebApp / eigenständige Software zum Clustern und anschließender Visualisierung von Bildern auf Grundlage von 
                    
neural network features
. (Projekt: https://dhlab.yale.edu/projects/pixplot/ ; Github: 
                    
)
                

                
3. 
                    
Wax

                

                
Beschreibung: Eine Software-Lösung, um einfach digitale Ausstellungen mit IIIF-Technologien zu realisieren. Die Software verfolgt einen 
                    
minimal computing
-Ansatz. (Projekt: https://minicomp.github.io/wax/ ; Github: 
                    
)
                

            

            

                
Workshoporganisation

                
Der Workshop wird in fünf Schritten durchgeführt. Dabei übernehmen die Workshopanbieter*innen die Funktion als Lotsen und geben zu jedem Schritt einen inhaltlichen Input. Anschließend wird gemeinsam von den Teilnehmer*innen die Untersuchung der Software auf die zuvor festgelegten Kriterien vorgenommen und die Ergebnisse von ihnen dokumentiert. Als Arbeitsumgebung wird ein GitHub-Repositorium genutzt. Der Fokus liegt auf der Evaluierung der Software in der Gruppe und dem Austausch über die Anwendbarkeit und Bedeutung der verschiedenen Kriterien zur Beschreibung der ausgewählten Software. Der Workshop schließt mit einer gemeinsamen Reflexion zum Verlauf des Workshops ab.

                

                    
Ablauf

                    

                        
Vorfeld

                        
Schritt 1 (im Vorfeld der DHd) 

                        

                            
Im Vorfeld des Workshops installieren die Teilnehmer*innen die Software und benutzen sie in einem vorgegebenen 
                                
use 

                                
c

                                
ase
 mit einem Testdatensatz. Sie notieren dabei, unterstützt durch einige Leitfragen, für sich ihre Erfahrungen.
                            

                            
In einer gemeinsamen Tabelle geben sie anonym Kennwerte zu ihrem technischen Setup an. Arbeitsumfang: etwa 120 min.

                        

                    

                    

                        
Workshop (0,5 Tage)

                        

                            
Kennenlernen

                            
Erste Eindrücke zur Software sammeln

                        

                        
Schritt 2 - Forschungskontext der Software und grundlegende Funktionen

                        

                            
Impuls 1 (5-10 min.): 
                                
Vorstellung des Anwendungsbereichs der Software in der geisteswissenschaftlichen Forschung

                            

                            
Gemeinsame Auswahl und Anwendung von Kriterien, um die generelle Funktion der Software zu beschreiben und zu beurteilen.

                            
Beurteilung der Software nach diesen Kriterien (Gruppenarbeit)

                        

                        
Schritt 3 - Perspektive Anwender*innen

                        

                            
Impuls 2 (5-10 min.): 
                                
Den Wald vor 

                                
lauter 

                                
Bäumen nicht 

                                
sehen

                                
 - Schwerpunkte in der Beschreibung aus Anwender*innensicht setzen

                            

                            
Gemeinsame Auswahl von Kriterien 

                            
Einbeziehung subjektiver Aspekte (Kompetenzen, Forschungsinteresse, Gewohnheiten)

                            
Beurteilung der Software nach diesen Kriterien (Gruppenarbeit)

                        

                    

                    

                        
Workshop (0,5 Tage)

                        
Schritt 4 - Perspektive Entwickler*innen

                        

                            
Impuls 3 (5-10 min.): 
                                
Finden, was für Entwickler*innen interessant ist

                            

                            
Gemeinsame Auswahl und Anwendung von Kriterien, um die Software mit Blick auf ihre handwerkliche Qualität, Nachhaltigkeit und Entwicklungsfähigkeit zu beschreiben

                            
Zwischenbilanz : Einschätzung der Software

                        

                        
Schritt 5 - Bilanz und Ausblick

                        

                            
Zusammenführung der ausgewerteten Kriterien und der Zwischenbilanzen

                            
Gemeinsame Verständigung, ob eine Fortführung als Publikationsprojekt möglich, sinnvoll und machbar ist

                            
Feedback zum Workshop

                        

                    

                

                

                    
Vorkenntnisse und Kompetenzen

                    

                        
Es sind keine spezifischen Vorkenntnisse erforderlich. Wesentlich ist ein allgemeines Interesse an Software und die Motivation zum offenen Arbeiten im Team.

                    

                

                

                    
Technisches Setup

                    

                        
Die Teilnehmer*innen bringen ihren eigenen Rechner mit. Vor Ort wird ein Bildschirm benötigt.

                    

                

                

                    
Teilnehmer*innenzahl

                    
Max. 15

                

            

            

                
Anbieter*innen Workshop

                

                    
Timo Homburg ist Wissenschaftlicher Mitarbeiter am i3mainz Institut für Raumbezogene Informations- und Messtechnik der Hochschule Mainz. Er erforscht Anwendungen im Bereich (Geospatial) Semantic Web, Computerlinguistik und den Digital Humanities. Seit 2019 ist er auch aktiver Wegbereiter für neue Geodatenstandards in der Standardisierungsgruppe zu GeoSPARQL, der OGC.

                    
Anne Klammt ist zum Zeitpunkt der Einreichung als Forschungsleiterin am Deutschen Forum für Kunstgeschichte Paris verantwortlich für die Digital Humanities. Seit 2020 publiziert sie zur Frage, wie Forschungssoftware und Datendienste wissenschaftlich rezensiert werden können.

                    
Fabian Offert ist Assistant Professor for the History and Theory of the Digital Humanities an der University of California in Santa Barbara. Er ist Mitherausgeber der Zeitschrift CKIT und forscht zur Nutzung von neuesten Praktiken des maschinellen Lernens in den digitalen Geisteswissenschaften, mit einem Schwerpunkt im Bereich digitale Kunstgeschichte.

                    
Florian Thiery ist Research Software Engineer im “Arbeitsbereich Wissenschaftliche IT, digitale Plattformen und Tools” des Römisch-Germanischen Zentralmuseums und forscht dort und entwickelt Forschungssoftware im Sonderforschungsgebiet “Explorative Forschung, Theorien- und Methodenentwicklung” im Handlungsfeld “Semantic Modelling and Knowledge Graphs”. Seit 2020 ist er Mitglied des Vorstands der Gesellschaft für Forschungssoftware (de-RSE e.V.), Mitentwickler des SPARQLing Unicorn QGIS Plugin und forscht im Bereich semantischer Modellierung und Linked Open Data und Wikidata zu Irischen Ogham Steinen, was im Rahmen des Wikimedia Deutschland Fellow-Programms Freies Wissen gefördert wurde.

                

            

        

            

                
1. Normdaten für Werke: Ausgangslage und Nutzen

            

                
Werknormdaten bieten die Möglichkeit, zentrale werkbezogene Angaben, wie Autorschaft, Erscheinungsjahr, Erscheinungsort und Genre, normiert – sprich standardisiert – in einem digitalen Datensatz abzubilden und mittels eines unikalen Identifiers persistent zur Verfügung zu stellen. Normdaten bilden wichtige Bausteine beim explorativen Suchen und Finden wissenschaftlich valider Informationen, schaffen durch ihre Vernetzung mit anderen Daten und Entitäten ein Semantic Web, aus dem sich neue fachübergreifende Forschungsfragen generieren lassen. Obwohl der wissenschaftliche Nutzen hoch ist, werden Werknormdaten im bibliothekarischen Arbeitsalltag in der Regel nur vereinzelt und rudimentär erstellt. Systematisch und in hoher Qualität werden Werknormdaten meist nur in drittmittelfinanzierten Projekten für eine konkrete Medienform oder Epoche erfasst, z.B. für Druckgrafiken, Bühnenstücke oder Werke der Musik (zu Letzterem: Bicher &amp; Wiermann 2018). Das Projekt »Werktitel als Wissensraum«

                
 hat das Ziel, den Grundstock für ein zentrales elektronisches, dynamisches Werklexikon zur deutschen Literatur innerhalb der GND zu legen. Der Werkbegriff folgt dem FRBR-Modell, das die Grundlage für RDA bildet, dem internationalen Regelwerk für Bibliotheken und Archive.

                
Das Konzept der werkorientierten Erschließung ist nicht neu und nimmt seinen Anfang in den von Antonio Panizzi entwickelten, 1841 erschienenen »Rules for the Compilation of Catalogue«, denen „das Prinzip zugrunde [liegt], alle Katalogeintragungen für unterschiedliche Ausgaben (Auflagen, Übersetzungen) eines Werkes zusammenzuführen“ (Barnert, Dietrich, Kolbe und Schmidgall 2021, 140). Durch die Einführung des Regelwerks RDA (Resource Description and Access) in Deutschland wird das zugrundeliegende FRBR-Modell mit den Ebenen Werk, Expression, Manifestation und Exemplar bei der Katalogisierung berücksichtigt. In Datenbanken können die miteinander in Beziehung stehenden Entitäten wie Werke, Übersetzungen (Expressionen), Ausgaben (Manifestationen) und Exemplare virtuell zusammengeführt werden. Die Idee mit Hilfe von Werknormdaten Informationen zu bündeln, wird auch in anderen Ländern umgesetzt, etwa in Finnland, Frankreich und den Vereinigten Staaten. In den jeweiligen Verzeichnissen der Nationalbibliotheken – in denen die Werknormdaten unterschiedlich umfangreich aufbereitet sind – finden sich neben den Übersetzungstiteln und Editionen teils auch Adaptionen des Grundwerks.

                
 Neben Bibliotheken können andere Einrichtungen ihre Bestände durch die Nutzung der Werknormdaten anreichern.  Somit schafft unser von der DFG gefördertes Projekt entscheidende Voraussetzungen für eine materialübergreifende Vernetzung musealer, bibliothekarischer und archivarischer Sammlungen und verbessert die Recherchemöglichkeiten für Nutzende (vgl. Althage 2019).

            

            

                
2. Zur Arbeitsweise des Projekts

            
            

                
Vor Projektbeginn wurden die deutschsprachigen Werke aus „Kindlers Literatur-Lexikon“, Wilperts „Lexikon der Weltliteratur“ und Frenzels „Daten deutscher Dichtung“ als wichtigste Registrationsmedien der Nachkriegsgermanistik sowie vier neuere, nach der Jahrtausendwende erschienene Werklexika, die auch nicht-kanonisierte Einzelwerke enthalten, sowie die Werknormdatenpools in Marbach und Weimar ausgewertet. Die in den 10 Datenquellen am häufigsten vertretenen deutschsprachigen Werke bilden die Grundmenge von 4.625 Werken, wovon 2.050 Werktitel mit Erscheinungsjahr von 1700 bis 1914 in der Herzogin Anna Amalia Bibliothek Weimar und 2.575 Titel erschienen von 1915 bis 2015 in der Bibliothek des Deutschen Literaturarchivs Marbach bearbeitet werden. Der Befund, dass die Menge der in den Lexika aufgeführten Werke für das 20./21. Jahrhundert größer ist als für das 18./19. Jahrhundert, ist an sich schon aussagekräftig. Insgesamt sind ca. 800 Autor:innen vertreten, darunter nur 65 Frauen. Die Auswahl der bekanntesten Werke erfolgte mit der Absicht, einen möglichst großen Nutzen für die Katalogisierung zu bieten, da die verbreitetsten Werke in verschiedenen Übersetzungen und Auflagen vorliegen.

                
 Wir empfehlen, in Folgeprojekten Werklexika mit anderen Schwerpunkten auszuwählen, z.B. „Die deutschsprachigen Schriftstellerinnen des 18. und 19. Jahrhunderts“ von Elisabeth Friedrichs oder „Frauen Literatur Geschichte“ von Hiltrud Gnüg und Renate Möhrmann.

            

            

                
Jedes der 4.625 Werke wird in der Gemeinsamen Normdatei (GND) und in Wikidata ergänzt beziehungsweise neu erstellt. Die GND ist eine kooperativ gepflegte digitale Normdatenbank für Personen, Körperschaften, Konferenzen, Geografika, Sachbegriffe und Werke. Wikidata hat sich in den letzten Jahren zu einem Knotenpunkt für die Vernetzung von Wissen entwickelt. In einem zweiten Arbeitspaket werden für jedes Werk der Grundmenge in Beziehung stehende Werke ermittelt. Mit Hilfe zahlreicher fachspezifischer Nachschlagewerke werden Werkbearbeitungen wie Vertonungen, Bühnenbearbeitungen, Verfilmungen sowie Vorlagen, Fassungen und Nachfolger identifiziert und wiederum als Werknormdaten erfasst. Im Laufe des Projekts wurden bereits 12.000 Werknormsätze in der GND bearbeitet oder neu erstellt. In einem dritten Arbeitspaket werden die Normdaten mitsamt der GND-Identifikationsnummern in bestehende Wikidata-Einträge eingepflegt oder in neue Einträge übernommen. Die so entstehenden Netzwerke erlauben Einblicke in die intellektuelle Produktion und Kollaboration und offenbaren literaturhistorische Entwicklungen, Trends, Debatten und Themenschwerpunkte (Märchen, Wertheriaden, Exilliteratur). Mit diesem Portfolio wird das Projekt seinem Anspruch einer offen zugänglichen und vernetzten literarischen Gedächtniskultur mithilfe von standardisierten Daten gerecht und trägt damit dem Tagungsmotto 

                
Open Humanities, Open Culture

                
vollends Rechnung.

            

            

                
3. Auswertungsmöglichkeiten und Anwendungsszenarien der Daten

            

                
Bei der Recherche und Anreicherung der Werknormdaten in der GND wurden bereits einige Besonderheiten registriert. Eine erste Übersicht über die Verteilung der Werkformen je Zeitraum zeigt zum einen die Vielfalt der Werkformen bei den Beziehungswerken (Tabelle 1). Zum anderen lässt sich im Vergleich feststellen, dass Film- und Hörspielbearbeitungen für die Werke der neueren Literatur häufiger vertreten sind. Auch geschlechtsspezifische Prozesse lassen sich anhand des Datensets nachvollziehen: die schrittweise Eroberung weiterer Handlungsräume und Schreibpraktiken im 20. Jahrhundert lassen allmählich Frauen als Autorinnen hervortreten (vgl. Seifert 2021, 93). Überwiegen bei den Frauen die Genre Prosa, Lyrik, Drama und Autobiografie, zeigt sich bei den fachspezifischen Werken zu Staatskunde, Philosophie, Geschichte, Religion oder Medizin, dass die Wissenschaftsgeschichte und -literatur bis weit ins 20. Jahrhundert überwiegend männlich dominiert war.

                
 Systematische Auswertungen der Art und Häufigkeit der Werkbeziehungen sowie von Verfasserschaft je Zeitraum können interessante Aufschlüsse zur Literaturproduktion und -rezeption geben

                

                
.

            

            

                

                    

                        
Neue Werktitel:

                        
Werkformen

                    

                    

                        
 Werke 

                        

                            
1700–1914 

                        

                    

                    

                        
 Werke 

                        

                            
1915–2015

                        

                    

                

                

                    
Werke der Literatur (Libretti, Manuskripte, Fachliteratur, Bühnenstücke, Lyrik)

                    
 37,2 %

                    
26,2 %

                

                

                    
Werke der Musik (Opern, Lieder, Musicals)

                    
 19,8 %

                    
 18 %

                

                

                    
Filmwerke (Spiel- und Fernsehfilme, Serien)

                    
 20 %

                    
21,7 %

                

                

                    
Hörspiele

                    
 15 %

                    
32,4 %

                

                

                    
Werke der Bildenden Kunst (Gemälde, Grafik, Objekte, Installationen)

                    
 6,8 %

                    
 1 %

                

                

                    
Ballette/ Tanztheater

                    
 0,6 %

                    
0,17 %

                

                

                    
Computerspiele (virtuell)

                    
 0,1 %

                    
 0,1 %

                

                

                    
Spiele (konventionell, haptisch)

                    
 0,2 %

                    
 0,1 %

                

                
Tab. 1: Übersicht der Werkformen der neuen Werktitel mit prozentualer Verteilung pro Zeitraum, eigene Darstellung

            


            

                
Der Schwerpunkt des Projekts liegt auf der Erfassung der Werknormdaten. Die Expertise bei der Auswertung der Daten mit DH-Methoden liegen bei Ihnen, den Wissenschaftler:innen der Digital Humanities. Wir sind gespannt auf Ihre Anregungen, welche Anforderungen an die Daten in der Community bestehen, damit diese für zukünftige Projekte berücksichtigt werden können. Der Frage nach der Verknüpfung der Werknormdaten mit verfügbaren Volltexten muss gemeinsam weiter nachgegangen werden. Die Auswertung von Volltexten in Bezug auf Werke als Entitäten sowie um Bezüge zwischen Werken zu erkennen, scheint vielversprechend. Die Daten könnten auch als Trainingsdaten für maschinelle Lernverfahren genutzt werden. Die digital zugängliche Datengrundlage aus dem Projekt lässt sich auf vielfältige Weise nachnutzen, etwa um Werkbeziehungen anhand kartografischer Anwendungen zu visualisieren und Intellektuellen-Netzwerke in Europa abzubilden. Auch spielbasierte Anwendungen ließen sich anhand des Datensets erstellen, z.B. in Form eines Werke-Quartetts, in das bestimmte werkbasierte Kategorien eingefügt werden, etwa die Anzahl oder Vielfalt an Beziehungen oder der Publikumserfolg. Diese möglichen Anwendungsbeispiele zeigen, wie normierte Daten auch außerhalb bibliothekarischer Zusammenhänge von Nutzen sind und auf spielerische Weise Wissen vermitteln und neue fächerübergreifenden Forschungsprojekte im Bereich Open Humanities initiieren können.

            

            

                
4. Fallbeispiele: Möglichkeiten &amp; Desiderata in Katalogen

            

                
Im Folgenden wird anhand von je einem Fallbeispiel aus den beiden Zeiträumen präsentiert, wie Werknormdatensätze die Funktion zentraler Sucheinstiege übernehmen können, die unterschiedliche bibliografische Informationen bündeln, vernetzen und jeweils in den Komplex der in Beziehung stehenden Werke hineinführen. Die unterschiedlichen Visualisierungen dieser Datensets zeigen bestehende Möglichkeiten, zentrale Werkinformationen übersichtlich darzustellen und komfortable Sucheinstiege anzubieten. 

            

            

                
Anhand des Werknormsatzes für Johann Wolfgang von Goethes Briefroman

                
Die Leiden des jungen Werthers

                
 wird die Varianz von Bearbeitungen im Kontext der Werk- und Rezeptionsgeschichte veranschaulicht. Die gegenwärtigen Möglichkeiten einer Visualisierung des Datensets werden hier mithilfe des GND-Explorers umgesetzt (Abb. 1). Anhand der Relationen-Ansicht werden die Vor- und Nachteile im Vergleich mit der Darstellung im „Faktenblatt“ sowie im Vergleich mit der konventionellen Ansicht im Katalog der deutschen Nationalbibliothek (DNB) erläutert. Gut zu unterscheiden sind im GND-Explorer die Bezüge zu (literarischen) Vorbildern, Adaptionen und Bearbeitungen sowie zu anderen Entitäten mittels einer farblichen Markierung (Autor, Geografika). Die jeweiligen Vorlagen für den Werther einerseits und seiner Vorbildfunktion andererseits sind über das Faktenblatt überschaubarer. Wie die Thematik des Werthers bildhaft verarbeitet wurde, also zu Kunstwerken anregte, ist in der Ansicht ungünstig übersetzt und der Zusammenhang zwischen Grund- und Beziehungswerk nur über das Faktenblatt nachvollziehbar.

            

            

                

                
Abb. 1: „Die Leiden des jungen Werthers“,Quelle: GND-Explorer, Screenshot der Ansicht „Relationen“
                

                    Die @Leiden des jungen Werthers - Relationen - GND-Explorer
(zugegriffen: 19.07.2022)

            

            
            

                

                
Abb. 2: „Die Leiden des jungen Werthers“, Quelle: GND-Explorer, Screenshot der Ansicht „Faktenblatt“ (Ausschnitt), 
                

                    Die @Leiden des jungen Werthers - Relationen - GND-Explorer
                
 (zugegriffen: 19.07.2022)
            

            

            

                
Das zweite Beispiel ist Ilse Aichingers Roman 

                
Die größere Hoffnung

                
 von 1948. Hier wird der Werknormsatz im neuen Online-Katalog Kallías des DLA gezeigt (Abb. 2), dessen Beta-Version seit 2021 verfügbar ist. Unter der Detailansicht werden die mit dem Werk in Verbindung stehenden Bestände des DLA in vier Gruppen angeboten: unter „primäre Quellen“ findet man die Ausgaben des Romans und die Handschriften, unter „sekundäre Quellen“ die Literatur, die sich mit dem Roman beschäftigt, daneben werden Übersetzungen und Rezensionen zusammengestellt. Auf der rechten Seite befinden sich unterhalb der Abbildung eines Buchcovers und der Bezeichnung „Werkbeziehungen“ die im Werktitelprojekt angelegten Werke. Folgt man einem Link, wird wiederum ein Werknormsatz, z.B. eines Hörspiels, angezeigt. Nicht zu allen diesen ermittelten Werken hat das DLA auch Bestände. Die Verknüpfung der Manifestationen mit Werknormdaten erfolgt bisher händisch bei der Katalogisierung oder in der Normdatenredaktion. Im Projekt ist die Entwicklung eines Tools geplant, das Manifestationen und Werke semi-automatisch verknüpft. Bis Ende 2022 wird zum Katalog ein Bereitstellungsdienst hinzugefügt, der es ermöglicht, auch sehr umfangreiche Datensets z.B.im csv-Format zu exportieren, um mit den Daten weiterzuarbeiten.

            

                

                    

                

                

                    

                
                
Abb. 3, „Die größere Hoffnung“, Quelle: Kallías – der Online-Katalog des Deutschen Literaturarchivs Marbach, Screenshot,
                
https://www.dla-marbach.de/find/opac/id/AK00140601
                
(zugegriffen: 26.07.2022)
            

            

            

                
Wir freuen uns auf Ihre Fragen und Hinweise zur generellen Aufbereitung von Normdaten und zu unserem projektbezogenen Datenset. Wir möchten die in den Digital Humanities aktiven Wissenschaftler:innen über unseren Projektstand und die Potentiale werkbasierter Daten informieren und freuen uns über einen Austausch zu Auswertungen und Nachnutzungsoptionen der bereitgestellten Forschungsdaten.

            

        

        

            

                
Einleitung

                
Der literarische Wandel vom Realismus zur Moderne ist nach wie vor Gegenstand vielfältiger literaturwissenschaftlicher Debatten. Eine dieser Debatten betrifft die Frage, wie sich die Gestaltung von Emotionen in lyrischen Texten veränderte. Während einige typologisch argumentierende Forscher:innen (z. B. 
                    
Andreotti 2014)
 davon ausgehen, dass die moderne Lyrik gegenüber der traditionelleren Lyrik des Realismus zu einer nüchternen, nicht-emotionalen Ausdrucksweise tendiert, argumentieren andere (z. B. 
                    
Winko 2003)
 ausgehend von zeitgenössischen Selbstbeschreibungen, dass die Lyrik der frühen Moderne – historisch verstanden – sehr wohl Emotionen gestalte, wenn auch in modifizierter Weise (vgl. zu dieser Debatte auch 
                    
Konle u. a. 2022)
.
                

                
Zu bedenken ist, dass die damalige Lyrik keine homogene Einheit bildet. Aussagen darüber, inwiefern sich die Emotionsgestaltung ‘der’ Lyrik veränderte, lassen sich differenzieren. Ein naheliegendes, wichtiges Differenzmerkmal lyrischer Texte – und damit ein potentiell relevanter Einflussfaktor auf die Emotionsgestaltung – ist die Gattung. Ziel dieses Beitrags ist deshalb, zu prüfen, inwiefern sich lyrische Gattungen in unserem Untersuchungszeitraum durch spezifische Emotionsprofile auszeichnen und ob die Entwicklung der Lyrik vom Realismus zur frühen Moderne unter der Perspektive der Emotionsgestaltung, die wir in  (
                    
Konle u. a. 2022
) herausgearbeitet haben, mit der Zugehörigkeit zu bestimmten lyrischen Gattungen zusammenhängt. An diesem Beispiel soll gezeigt werden, wie wichtig es auch für quantitative Untersuchungen ist, literarische Phänomene nicht isoliert zu betrachten, sondern als Teil eines komplexen Systems zu modellieren, in dem unterschiedliche Faktoren einander beeinflussen. Der Beitrag versteht sich als Schritt hin zu einer solchen multifaktoriellen Modellierung von Literatur und literarischem Wandel.
                

            

            

                
Ressourcen

                
Das zu analysierende Korpus besteht aus Texten in Lyrikanthologien aus dem Untersuchungszeitraum, die sich auf Gedichte von Zeitgenoss:innen konzentrieren. Das Teilkorpus ‘Realismus’ umfasst Gedichte aus Anthologien, die zwischen 1850 und den frühen 1880er Jahren publiziert wurden; das Teilkorpus ‘Moderne’ enthält Texte aus Anthologien, die um 1900 erschienen sind und deren Herausgeber:innen die Gedichte aufgrund ihrer Modernität ausgewählt haben. Welche Texte als ‘realistisch’ und welche als ‘modern’ gelten, wird in diesem Beitrag also aus zeitgenössischer Sicht (und nicht aus Sicht heutiger Forscher:innen) modelliert.
 Da keine der einbezogenen Anthologien nach 1911 erschienen ist, beschränken sich die Analysen auf die 
                    
frühe
 Moderne.
                

                
                

                    

                        

                        
Anthologien

                        
Gedichte

                        
Wörter

                    

                    

                        
Realismus

                        
8

                        
3367

                        
400k

                    

                    

                        
Moderne

                        
12

                        
2882

                        
320k

                    

                    

                        
Insgesamt

                        
20

                        
6249

                        
720k

                    

                    
Tabelle 1: Korpus Statistik

                

                
Für 1412 Korpustexte wurden die Gattungszugehörigkeit und die Emotionsgestaltung annotiert. Die Gattungsannotation hält fest, ob ein Text bestimmten im Untersuchungszeitraum relevanten thematischen Gattungen (Naturlyrik, Liebeslyrik usw.) sowie nicht-thematischen Gattungen (Ballade, Lied usw.) angehört und ob er situativ bestimmt oder situativ unbestimmt ist. Wenn im Folgenden von ‘Gattungen’ die Rede ist, sind in aller Regel die thematischen Gattungen gemeint, auf die sich dieser Beitrag erst einmal konzentriert. Die Gattungszuordnung ist weder exklusiv noch zwingend: Während der Annotation konnten einem Text genau eine, aber auch keine oder mehrere Gattungen zugewiesen werden.
 Die Emotionsannotation zielt darauf ab, die im Text gestalteten Emotionen (und nicht die Leseremotionen) zu erfassen. Genutzt wurde ein Set von 40 diskreten Emotionen, darunter zum Beispiel Liebe, Trauer, Hoffnung, Sehnsucht oder Hass. Einerseits handelt es sich um Emotionen, die in gängigen Emotionstheorien 
                    
(Ekman 1992; 1999; Plutchik 1980b; 1980a; 2001)
 als grundlegend angesehen werden, andererseits wurden zusätzliche Emotionen, die in den Korpustexten häufig vorkommen, aufgenommen, um das Emotionsset an das historische Material anzupassen. Die Annotationseinheiten sind Wörter bzw. Wortfolgen.
 Da für viele einzelne Emotionen nicht genügend Annotationen vorliegen, um ihre maschinelle Detektion trainieren zu können, werden die Emotionen nachträglich zu sechs Gruppen zusammengefasst: Liebe, Freude, Trauer, Erregung/Überraschung, Angst und Wut. Die Gruppierung orientiert sich an der Emotionshierarchie in 
                    
Shaver u. a. (1987)
.
                

                

                    

                        
Gattung

                        
Liebe

                        
Natur

                        
Philosophie

                        
Religion

                    

                    

                        
Anzahl

                        
350

                        
286

                        
163

                        
100

                    

                    

                        
Gattung

                        
Poetologie

                        
Politik

                        
Kultur

                        
Geschichte

                    

                    

                        
Anzahl

                        
61

                        
21

                        
104

                        
118

                    

                    
Tabelle 2: Annotierte Gattungen

                

                
Jedes Gedicht wurde zunächst von zwei Annotator:innen annotiert; anschließend haben die beiden Annotator:innen ihre Annotationen miteinander verglichen, die Disagreements diskutiert und eine Konsensannotation erstellt. Auf den Konsensannotationen beruhen alle weiteren Auswertungen. Das Agreement der Einzelannotationen beträgt 0.69 (Krippendorffs Alpha).

            

            

                
Methoden

                
Um Gattungslabel für das gesamte Korpus herstellen zu können, sollen Classifier trainiert und angewandt werden.
 Da Gedichte sehr kurz sind, kommt ein Verfahren auf bag-of-words-Basis nicht in Betracht. Stattdessen wird auf das Fine-Tuning neuronaler Sprachmodelle zurückgegriffen. In einer Vorstudie zur Gattung Liebeslyrik sollen das geeignetste Sprachmodell und Hyperparameter
 ermittelt werden. Getestet werden die Modelle gbert-large 
                    
(Chan, Schweter, und Möller 2020)
 und gottbert-base 
                    
(Scheible u. a. 2020)
. Das beste Ergebnis wird mit gbert-large erzielt (Batchsize 20, Learning Rate 1e-5 und 50 Epochen). Durch eine Anpassung auf das Lyrikkorpus durch fortgesetztes Pretraining

                    
(Gururangan u. a. 2020)
 kann die Performance weiter gesteigert werden (siehe Tab. 3 ,gbert-large-poetry).
                

                

                    
                    

                        
Modell

                        
Accuracy (std)

                    

                    

                        
gbert-large-poetry

                        
.880 (.022)

                    

                    

                        
gbert-base

                        
.854 (.040)

                    

                    

                        
Gottbert-base

                        
.826 (.032)

                    

                    
Tabelle 3: Ergebnis der Vorstudie zur Gattung ‘Liebeslyrik’. Evaluation durch 5-fold-cross validation.

                

                
Das Modell wird mit den ermittelten Hyperparametern verwendet, um binäre Classifier für alle übrigen Gattungen zu trainieren. Da die Klasse der nicht zur fokussierten Gattung gehörenden Gedichte in jedem Fall größer ist, wird epochenweise Random Undersampling angewandt. Für politische Lyrik wird kein Modell trainiert, da nicht genügend Beispiele vorhanden sind (siehe Tabelle. 2).

                

                    
                    

                        
Gattung

                        
Liebe

                        
Natur

                        
Poetologie

                        
Geschichte

                        
Politik

                        
Philosophie

                        
Religion

                        
Kutlur

                    

                    

                        
Acc

                        
.880

                        
.833

                        
.623

                        
.872

                        
-

                        
.723

                        
.815

                        
.470

                    

                    

                        
Std

                        
.022

                        
.024

                        
.224

                        
.026

                        
-

                        
.028

                        
.059

                        
.169

                    

                    
Tabelle 4: Ergebnisse der Evaluation der Classifier für thematische Gattungen (5-fold cross-validation).

                

                
Tabelle 4 zeigt die Qualität der trainierten Classifier. Da die Performance für die Gattungen Kultur und Poetologie nicht ausreicht, um solide Analysen zu ermöglichen, werden diese im Weiteren nicht behandelt und fallen, wie Politik, der Kategorie ‘Sonstige’ zu, die daneben diejenigen Gedichte umfasst, die keiner thematischen Gattung zugeordnet wurden.

                
Für Informationen zur Annotation von Emotionen und Modellen zur Emotionsdetektion siehe 
                    
(Konle u. a. 2022)
.
                

            

            

                
Ergebnisse 

                

                    

                    
 Verteilung von thematischen Gattungen in Realismus und Moderne. Absolute Anzahl über Balken. Die Kategorie ‘Sonstige’ repräsentiert diejenigen Gedichte, die keiner der übrigen in dieser Abbildung gezeigten thematischen Gattungen zugeordnet wurden.

                

                
Verteilung von thematischen Gattungen in Realismus und Moderne. Absolute Anzahl über Balken. Die Kategorie ‘Sonstige’ repräsentiert diejenigen Gedichte, die keiner der übrigen in dieser Abbildung gezeigten thematischen Gattungen zugeordnet wurden.

                
60% der untersuchten Gedichte konnten (mindestens) einer der fünf Gattungen Liebeslyrik, Naturlyrik, religiöse Lyrik, philosophische Lyrik oder Geschichtslyrik zugeordnet werden. Liebeslyrik und Naturlyrik sind im Korpus deutlich verbreiteter als religiöse Lyrik, philosophische Lyrik und Geschichtslyrik. Im Epochenvergleich werden nur begrenzte Verschiebungen sichtbar; die Rangfolge der Häufigkeiten bleibt konstant (siehe Abb. 1).

                
Wie die Gattungen kommen auch die Emotionen im Korpus unterschiedlich oft vor: Emotionen der Gruppen Liebe, Freude und Trauer werden deutlich häufiger gestaltet als Emotionen der Gruppen Erregung/Überraschung, Angst und Wut. Die Häufigkeit positiver Emotionen – Liebe und Freude – nimmt zur Moderne hin ab 
                    
(Konle u. a. 2022)
.
                

                

                    

                    
Emotionsprofile nach Gattungen. Die Emotionen wurden mit dem Median 
                        
aller
 Gedichte normalisiert, um auch Unterschiede in wenig häufigen Emotionen (Anger, Fear, Agitation) sichtbar zu machen. Die 0-Linie markiert die Übereinstimmung mit dem Median aller Gedichte, darüber bedeutet mehr und darunter weniger Emotion.

                    

                

                
Die Ergebnisse (Abb. 2) zeigen, dass Gattungen eigenständige Emotionsprofile ausbilden; dies gilt auch für die nicht durch Emotionen definierten Gattungen. Einige Gattungen verhalten sich in Hinsicht auf bestimmte Emotionen ähnlich, z.B. philosophische Gedichte und Geschichtslyrik in Hinsicht auf die Gruppen Freude und Trauer, weichen aber in anderen Emotionen stark voneinander ab. Während z.B. in Liebeslyrik – wenig überraschend – Emotionen der Gruppe Liebe dominant sind, werden in Geschichtslyrik Emotionen der Gruppen Liebe und Freude unter- und Emotionen der Gruppen Wut und Angst überproportional dargestellt.

                
Ein kontrastiver Blick auf Gattungen unter der Perspektive von Epochen (Abb. 3) zeigt, dass sich (1) die lyrische Emotionsgestaltung in der frühen Moderne gegenüber dem Realismus verändert, und zwar (2) je nach Gattung in unterschiedlicher Weise. 

                

                    

                    
Emotionsprofilenach Epochen und Gattungen. Die Abbildung zeigt anders als Abb.2 nicht die Abweichung vom Median, sondern den Mittelwert der Emotionen.

                

                
Der summarische Trend über alle Gattungen zu weniger Emotionen, verursacht vor allem durch den Rückgang positiver Emotionen in der Moderne (siehe Konle u.a. 2022), betrifft nicht alle Gattungen in gleicher Weise. Während sich die Ergebnisse zu Liebes-, Natur- und philosophischer Lyrik noch diesem Trend zuordnen lassen, entwickeln sich Geschichts- und religiöse Lyrik durch Zunahme negativer Emotionen eigenständiger. Religiöse Lyrik läuft dem Trend sogar durch ein vermehrtes Auftreten von Emotionen der Gruppe Liebe entgegen. Diese Befunde erlauben einen differenzierten Blick auf Emotionen, Gattungen und deren Veränderung in der frühen Moderne.

                

                    
Case Study: Religiöse Lyrik

                    
Anschließend an unsere Ergebnisse stellt sich die Frage nach den Faktoren, welche die gattungsspezifischen Emotionsprofile zu unterschiedlichen Zeitpunkten beeinflussen. Für Geschichtslyrik bietet sich die These an, dass mit der Thematisierung von Konflikten und Feindseligkeiten 
                        
(Detering und Trilcke 2013)
 negative Emotionen einhergehen. Ob in der frühen Moderne diese Themen zunehmen oder stärker mit Emotionen verbunden werden, muss geprüft werden. 
                    

                    
Die Entwicklung der Emotionen in religiöser Lyrik ist noch komplexer, es nehmen nicht nur negative Emotionen zu, sondern auch die der Gruppe Liebe, bei gleichzeitigem Rückgang der Gruppe Freude.
 Diese Beobachtung kann an Einsichten der Forschung anschließen und diese ergänzen. Bekannt ist, dass moderne Lyrik trotz z.T. dezidierter Kritik noch „vielfältig auf religiöse Traditionen bezogen“ bleibt 
                        
(Detering 2016, 126)
, dass sie sich u.a. stärker von christlichen Institutionen entfernt und religiöse Motive umdefiniert. Offenbar verbindet sie aber auch mehr und andere Emotionen mit dem Thema als die Lyrik des Realismus. Einen ersten Hinweis auf den Zusammenhang zwischen Motiven und Emotionen gibt Tabelle 5, die u.a. eine Verschiebung der distinktiven Substantive von der Institution zu Personen sowie eine Tendenz zu symbolfähigen Ausdrücken zeigt. Die Verschiebung zum persönlichen Glaubensaspekt könnte die erhöhte Anzahl der Emotionen erklären. Betrachtet man die in der frühen Moderne häufiger werdenden Emotionen (Tab. 6) zeigt sich, dass ein Teil der Zunahme über die Sexualisierung religiöser Inhalte erklärt werden kann. Zur Erklärung der vermehrten negativen Emotionen wären weitere Untersuchungen nötig. Aufschlussreich ist auch, dass sich der Darstellungsmodus religiöser Lyrik insofern ändert, als in Gedichten der frühen Moderne verstärkt Emotionswörter und Wörter vorkommen, die Emotionen körperlich, z.B. gestisch oder mimisch ausdrücken.
                    

                    

                        
                        

                            
Realismus

                            
Moderne

                        

                        

                            

                                
Wald

                                
Kirche

                                
Kaiser

                                
Bischof

                                
Segen

                                
Grab

                                
Priester

                                
Glocke

                                
Mönch

                                
Heil

                                
Werk

                                
Sieg

                                
Haus

                                
Friede

                                
Dom

                                
Andacht

                                
Lenz

                                
Sonntag

                                
Mahl

                                
Ehr

                            

                            

                                
Gott

                                
Seele

                                
Nacht

                                
Auge

                                
Mensch

                                
Tod

                                
Weib

                                
Licht

                                
Welt

                                
Stimme

                                
Traum

                                
Kreuz

                                
Brust

                                
Blut

                                
Volk

                                
Erde

                                
Herz

                                
König

                                
Kind

                                
Leib

                            

                        

                        
Tabelle 5: Distinktive Substantive der Gattung Religion. Ranking nach Keyness. genauer: Simple Maths Parameter 
                            
(Brezina 2018, S.85)
.
                        

                    

                    

                        
                        

                            
Abnehmend

                            
Zunehmend

                        

                        

                            

                                
Freude

                                
Ausgeglichenheit

                                
Lust (nicht-sexuell)

                                
Aufregung

                                
Dankbarkeit

                            

                            

                                
Liebe

                                
Abneigung

                                
Sehnsucht

                                
Leid

                                
Lust (sexuell)

                            

                        

                        
Tabelle 6: Verschiebung der Emotionen zwischen Realismus und Moderne in religiöser Lyrik (Datengrundlage: Manuell annotierte Gedichte).

                    

                

            

            

                
Fazit

                
Die gleichzeitige Beobachtung von Gattung und Emotion zeigt nicht nur, dass Gattungen wesentlichen Einfluss auf die Verteilung von Emotionen haben, sondern auch, dass der Übergang von Realismus zu früher Moderne innerhalb von Gattungen eigenen Dynamiken folgt. Im Fall religiöser Lyrik und Geschichtslyrik verlaufen diese Dynamiken sogar in Widerspruch zum Gesamttrend. Im Anschluss ergeben sich drei weitere Perspektiven: Wie sehen die Emotionsprofile der nicht-thematischen Gattungen aus? Welche anderen differenzierenden Faktoren neben der Gattung können wir identifizieren, z.B. Autorschaft, Publikationsort, Intertextualität usw.? Ein wichtiger Aspekt könnte die Zeit sein, bezogen auf kleinere Einheiten als Epochen: Wie entwickeln sich die Lyrik und die einzelnen Gattungen unter der Perspektive der Emotionsgestaltung in der Zeit, etwa von Jahrzehnt zu Jahrzehnt?

            

        

            

                
Overview and Research Questions

                
My dissertation examines media narratives built around the multicultural German national football teams (both men and women) and argues that football not only helps pursue and formulate national identity, but also has become a battleground for both the promotion and contestation of German national identity. I aim to answer the following research questions: How was this multicultural team portrayed, utilized, and interpreted differently in the media between 2006 to 2018? What social forces created the demographically diverse national team and discourse surrounding them between this period? How has the rhetoric “multicultural national team” influenced ongoing debates over German national identity?

            

            

                
State of Research

                
This research builds on works drawn from the fields of sports studies, German studies, and digital humanities. Sports historian Kay Schiller (2015) warned readers not to confuse the rising acceptance of a multi-ethnic society with a positive endorsement of multiculturalism, which became a trend when studying football and national identity. Building on his argument, my research hopes to find out in which ways was the team perceived in the media as a representative and an endorsement of a supposed multicultural society. While Schiller recognized emerging discussions of multicultural society in debates over football nationalism, German studies scholars Stehle and Weber (2013) have focused on the phenomenon where media portrayed players differently based on their race rather than performance. However, most humanity scholars selected several articles and conducted close reading despite the number of materials they had at hand. With the help of computational methods, my work takes the approach of scalable reading (Müller) -- combining close and distant reading (Moretti, 2013), and treats computational methods as an addition to rather than a replacement of close reading when analyzing cultural phenomena. I believe that only through detecting long-term trends and telling individual stories can one better answer the open-ended research questions formulated above.

            

            

                
Research Program

                
Since no databases exists containing news articles around the national teams, one of the central tasks of this dissertation is building a customized corpus consisting of news reports written and published between 2006 - 2018. The corpus consists of news reports collected from the LexisNexis database. This database contains enormous amount of data and is tricky to navigate. Therefore, I developed a strategy where I first read a sample collection of articles, documented and categorized key terms used in these articles, and eventually wrote three search strings to test which one gives me the best search results. Using the Precision at k metric, I determined the best search string that can be applied to LexisNexis’ database. This innovative approach on extracting data from a rather complicated database could also be applied to other research in the future. As for now, I have completed data collection. To complete my dissertation, I need to finish data cleaning, and then conduct topic modeling and co-occurrence network analysis. Latent Dirichlet Allocation (LDA) is a widely used technique for topic modeling, which is the process of uncovering hidden topics in a collection of documents. Co-occurrence network analysis can help uncover hidden relationships and provide insights into the structure of my corpus. In this project, I use it to show the relationship between certain players and language used to describe them. These results could help me locate individual narratives that require closer examination. My analysis will then combine this qualitative approach with closely reading representative news articles. With this goal in mind, my work hopes to bring new perspectives to understand the complex role of the multicultural 
                    
Mannschaft
 in the process of shaping contemporary German identity.
                

            

            

                
Schedule



                    
December 2022: Cleaning and testing corpus; conducting initial topic modeling analysis

                    
January – early March 2023: Conducting co- occurrence network analysis; compiling articles that need to be read closely; presenting at the 2023 DHd conference and receiving feedback

                    
April &amp; May 2023: Writing the distant reading section of my dissertation, including descriptions of methods, process, and results; compiling articles that need to be read closely

                    
June – September 2023: Completing the qualitative section where I conduct analysis on news articles that touch on football players involved in political debates

                    
October – February 2023: Writing the three main chapters of my dissertation

                    
March and April 2024 – Finishing the Epilogue chapter and revising all chapters

                

            

        

            

                
Anliegen

                
Sowohl die in der Forschungslandschaft inzwischen gut etablierten Digital Humanities, als auch das gewünschte Zusammenwachsen der Prozesse in den verschiedenen GLAM-Einrichtungen wären ohne Community Building jenseits der tradierten Communities nicht denkbar. Der Aufbau dieser neuen Fachgemeinschaften zwingt zu einer Reflexion der etablierten Arbeitsweisen und bezieht die Verwendung von Richtlinien und Werkzeugen für den Umgang mit Daten ein. Unser Anliegen ist es zu verdeutlichen, auf welche Weise sich eine gemeinsame Infrastruktur für Forschungsdaten und das Community Building gegenseitig beeinflussen können. Eine solche Infrastruktur kann etwa in Analogie zu einer "offenen Werkstatt” verstanden werden, die Raum für einfache Zugänge und Hilfsmittel zur gemeinschaftlichen Nutzung in strukturierter Form anbietet.

                
Die Frage nach den Alleinstellungsmerkmalen einzelner Disziplinen steht vor allem im Kontext globaler Digitalisierung und Standardisierung von Forschungsprozessen, der Einführung digitaler Curricula sowie aufgrund der zunehmenden Forderung nach Interoperabilität im Raum (Balsinger 2005). Ebenso besteht die Anforderung, durch die Integration „analoger“ und „digitaler“ Aspekte in die Forschung oder die berufliche Praxis neue, „hybride“ Praktiken zu etablieren (Zaagsma 2013). Innerhalb vornehmlich digital arbeitender Fachcommunities stehen außerdem zunehmend generisch einsetzbare Werkzeuge und Methoden im Spannungsfeld mit den speziell auf bestimmte Datendomänen zugeschnittenen Tools.

                
Angesichts der Entwicklung gemeinsamer Infrastrukturen haben gemeinschaftlich entwickelte und genutzte Tools mittlerweile eine rasante Dynamik erfahren. Viele Projekte ließen sich ohne kollaborative und transparente Arbeitsgestaltung kaum realisieren und werden daher von Virtuellen Forschungsumgebungen, Wikis, Messaging-Diensten, Cloud- und Ticketsystemen, Sync&amp;Share Systemen, Versionierungstools und spezieller Software unterstützt. Das Community Building kann innerhalb dieser Infrastrukturen als Prozess verstanden werden, der zwischen generischer Offenheit und Zielgruppenspezifik anzusiedeln ist und Fächergrenzen überschreitet. Das Panel möchte in der Diskussion rezenten Problemlagen nachgehen und eruieren, wie sich Prozesse des Community Building im Digitalen an Orten des Lernens, der Forschung oder Vernetzung gestalten und welche Veränderungen, Herausforderungen sowie Chancen und Risiken dies für Universitäten wie GLAM-Einrichtungen birgt.

            

            

                
Impulsvorträge

                

                    
Von geisteswissenschaftlichen Forschungsdaten zu 
                        
NFDI4Culture
 als Open Community
                    

                    
Blickt man zurück auf die Entwicklung der letzten Jahre, so hat die Diskussion um den Begriff „Forschungsdaten“ das Community Building in den Geisteswissenschaften maßgeblich befeuert (Andorfer 2015). Die ausgeprägte Unschärfe dieses Begriffs stand einer von allen Communities akzeptierten Definition lange entgegen. Umfragen halfen einer Konkretisierung näherzukommen und legten Sichtweisen und Umgang mit Daten offen. Auf diese Weise hat man versucht, Kategorien zu bilden, um eine übergreifende Ordnung zu finden. Etwa zeitgleich wurden allgemeine und fachspezifische Policies und Empfehlungen zum Umgang mit Forschungsdaten für die Communities herausgegeben (z. B. DFG 2015ff.). Diese mündeten schließlich – getragen vom Aufbau spezieller Repositorien und Datenzentren (
                        

                            
https://dhd-ag-datenzentren.github.io/

                        
) – in den konkreten Bedarf einer Infrastruktur, welche eine langfristige Sicherung und nachhaltige Bereitstellung digitaler Forschungsdaten garantieren kann, wobei der Akzent auf einer fachübergreifenden Zusammenarbeit liegt.
                    

                    
Resultat dieser Bestrebung ist seit Herbst 2020 die Nationale Forschungsdateninfrastruktur (NFDI), darunter das Konsortium 
                        
NFDI4Culture
 für das (im)materielle Kulturerbe (
                        

                            
https://nfdi4culture.de/

                        
). Die Zielgruppen dieser Infrastruktur sind einerseits die Datenerzeuger und Vertreter verschiedener Wissensgebiete, andererseits die Datenanbieter (i.d.R. Infrastruktureinrichtungen, GLAM), deren Funktionsbereiche sich teilweise überlappen und für einen reibungslosen Datenfluss ineinander greifen. Entscheidend ist die Konsolidierung der Communities: Es gilt, die Interessen unterschiedlicher Teilgruppen zusammenzuführen und zielgruppenspezifische Angebote für qualitativ hochwertige Forschungsdaten zu entwickeln. Eine Verständigung zwischen den Disziplinen wird gefördert, indem Fachvertreter:innen in gemeinsamen Task Areas zusammenarbeiten (Altenhöner et al. 2020). Für eine solche Zusammenarbeit sind auch fachübergreifende Angebote gefragt, weshalb die NFDI gemeinsame Querschnittsthemen identifiziert hat, die über die Konsortien hinweg in (bislang) vier Sektionen („Gemeinsame Infrastruktur“, „Metadaten, Terminologien, Provenienz“, „Recht und Ethik“ sowie „Training und Ausbildung“) bearbeitet werden.
                    

                

                

                    
Kleine Tools und schwach strukturiertes Publizieren (Code and Data Literacy am Beispiel medienkulturwissenschaftlicher Lehre)

                    
Data Literacy kann als Indikator dienen, wenn es um die Zugehörigkeit zu digital geprägten Communities geht, ebenso bietet sie Möglichkeiten, diese zu verbinden. Der Vortrag möchte den kritischen Umgang mit Code und Daten als „ability to collect, manage, evaluate, and apply data, in a critical manner“ (Ridsdale et al. 2015) in einem erweiterten Sinne anhand ausgewählter Beispiele verdeutlichen und für einen offenen Umgang mit Methoden, Tools sowie Lerneinheiten plädieren. Zunächst sollen die Vorteile von Open Educational Resources (OER) aufgezeigt werden. Etwas globaler formuliert soll damit auch für das zügige und eher schwach strukturierte Publizieren unfertiger, kürzerer und kleinerer Recherchen und Übungen durch Studierende argumentiert werden. Dies führt nicht nur über das gegenseitige Lesen, Kommentieren und Korrigieren im Seminar hinaus, sondern öffnet die Publikation von Unterrichtsmaterial auch für einen Austausch mit den Fachcommunities (Bsp. 
                        

                            
https://zfmedienwissenschaft.de/online/open-media-studies-blog

                        
). Ressourcen, Tutorials und Anleitungen auch außerhalb des angestammten Lehrbetriebs gibt es viele (u.a.
                        

                            
https://programminghistorian.org/

                        
 oder
                        

                            
https://digital-history-berlin.github.io/Python-fuer-Historiker-innen/

                        
). Ihre Auffindung und Nutzung gehört auch zur Data Literacy. Mit der Öffnung der Seminare, insbesondere durch die gezielte Ansprache von Infrastruktureinrichtungen/GLAM (Bsp. DNBLab sowie lokale Angebote der Universitätsbibliotheken), können nicht nur Lehr- und Lerneinheiten, sondern auch die eingesetzten Tools kontextabhängig und datenspezifisch weiterentwickelt werden. So werden neben allgemein nützlichen Redaktionsabläufen auch eine Vielzahl von Herangehensweisen an unterschiedlichste Medieninhalte und Schnittstellen auf allen Ebenen wissenschaftlichen Arbeitens eingeübt, also Datenkompetenz wie Datenkritik praktisch erprobt. In der Diskussion verweisen wir auf Trainings- und Ausbildungsszenarien, die nicht nur von einer Einrichtung allein geleistet werden können, sondern sich entlang von Forschungsdateninfrastrukturen und dem Austausch von OER bewegen. Schwerpunkte sind medien- und kulturwissenschaftliche Kontexte und die Ausprägung bestimmter Datenbegriffe.
                    

                

                

                    
Humanities im Wandel – Neue Möglichkeiten durch Community Building

                    
Das letzte Statement liefert unter Einbezug des genannten Praxisfalls eine zusammenfassende Definition des Begriffs „Community Building”, skizziert nochmals seinen praktischen Nutzen und legt offen, welche Kriterien und Aspekte einer Forschungsdateninfrastruktur dafür entscheidend sind. 

                    
Da im Digitalen viele Prozesse in den Geistes- und Kulturwissenschaften gemeinsam, also fachübergreifend, erfolgen, soll auch allgemein darüber reflektiert werden, auf welche Weise die Vernetzung von Personen und Einrichtungen in diesem Feld spezifisch verläuft. Einerseits wird diese unmittelbar durch die projektbedingte Entwicklung bestimmter Tools und Services für die User angeregt („If we build it, they will come“: Ramsay 2016). Andererseits tragen länger andauernde, übergreifende Prozesse wie die Entwicklung von Auszeichnungsformaten, Vokabularen, Ontologien und Metadatenprofilen, die Ausarbeitung von Daten- und Qualitätsmanagement-Strategien (etwa mit Datenmanagementplänen), die digitale Kuratierung oder Archivierung oder die Herstellung von Linked Open Data zum Community Building bei. Daneben ist eine Übertragbarkeit von Methoden in der Datenverarbeitung zu beobachten, und zwar nicht nur zwischen einzelnen Disziplinen innerhalb der Digital Humanities, sondern darüber hinaus zwischen diesen und der Informatik sowie anderen MINT-Fächern (Musikwissenschaft: Plaksin 2021).

                    
Besonders relevant wird die Frage sein, ob es sich um eine interdisziplinäre oder vielmehr eine transdisziplinäre, sehr offene Form der Zusammenarbeit handelt (Balsiger 2005, 140ff. 148f. 166f. 174. 179, Jungert 2010). In einem Fall werden Wissen und Methoden aus dem anderen Wissensgebiet genutzt, ohne den Methoden- und Erkenntnisraum des eigenen Gebietes zu verlassen. Im anderen Fall können Wissen und Methoden der eigenen Disziplin einen Gegenstand der Nachbardisziplin erschließen, wobei der Methoden- und Erkenntnisraum überschritten wird. Quasi disziplin-unabhängig – wie in einer offenen Werkstatt – kann dann an Aspekten gearbeitet werden, die jede Disziplin für sich allein womöglich nicht behandeln würde (z. B. Mittelstraß 1992).

                

            

            

                
Leitfragen für die Diskussion

                
1. Wo finden sich Beispiele für generische und spezifische Bedarfe innerhalb der/einer Forschungsdateninfrastruktur?

                
2. Inwieweit werden konkrete Interaktions- und Partizipationsmöglichkeiten an 
                    
NFDI4Culture
 als offene Werkstatt von den Zielgruppen bereits wahrgenommen?
                

                
3. Abseits unseres Beispiels: Welche konkreten Lösungen bieten Forschende, um ihre (datenorientierten und auf digitalen Tools beruhenden) Arbeitsweisen in den NFDI-Aufbauprozess zu integrieren und inwieweit ist es überhaupt möglich und sinnvoll?

                
4. Wie gelingt es, Forschungscommunities über ein Service-Portfolio dauerhaft in den NFDI-Entwicklungsprozess einzubinden?

                
5. Wie wirken sich – datenspezifisch betrachtet – die community-übergreifend genutzten Werkzeuge und Methoden auf das Community Building aus?

                
6. Wie sollen Forschungs- und Infrastrukturprojekte auf die zunehmende praxisorientierte Diversität reagieren?

                
7. Wie beeinflusst das Community Building den Transfer in die Öffentlichkeit und die Rezeption in anderen Bereichen der (Wissens-)Gesellschaft, die ebenfalls dem digitalen Wandel unterliegen (GLAM als „Knowledge Broker“, Simon 2018, 320)?

            

        

            

                
Einleitung

                
In den vergangenen Jahren haben Publikationen aus dem Bereich der digitalen Literaturwissenschaft vermehrt auf das durch den Altphilologen und Anglisten Martin Mueller geprägte Konzept s
                    
calable reading
 hingewiesen, es diskutiert oder sogar zum Vorbild für das eigene methodische Vorgehen auserkoren (vgl. exemplarisch Arnold, Fiechter 2022, 162–165; Horstmann, Kleymann 2019, insb. Kap. 1 und 5; Schruhl 2018, Kap. 5; Weitin 2017, 1–6; Willand, Reiter 2017, 178). Ein Vorzug des „integrative[n] Konzept[s]“ – so wird immer wieder betont – sei dessen Anlage, die eine Verbindung von „qualitativ-hermeneutische[n] und quantitativ-statische[n] Methoden“ erlaube (Weitin 2015, 2).
 Dementsprechend ist der von Mueller als „happy synthesis of ‚close‘ and ‚distant‘ reading“ (Mueller 2012, o.S.) angelegte Begriff des 
                    
scalable reading
 verbreitet als 
                    
mixed-methods-
Ansatz wahrgenommen worden (vgl. etwa Herrmann 2018, § 5–7; Kleymann 2022, § 23–25) und wird in der Hauptsache als methodologisches Schlagwort verwendet, das die Verknüpfung qualitativer und quantitativer Methoden anzeigen soll (vgl. etwa Viehhauser 2017, Kap. 4 oder Krause und Pethes 2017, 108). Die unterschiedlichen Skalierungspraktiken, die dem Konzept anhaften, und das häufig betonte brückenbildende Potential von 
                    
scalable reading
 scheinen mir bislang aber noch nicht ausreichend reflektiert worden zu sein.

                

                
In einem ersten Schritt meines Beitrags werde ich die verschiedenen Dimensionen, auf die sich Muellers Konzeption von 
                    
scalable reading
 erstreckt, ausdifferenzieren und erläutern. Daran anschließend werde ich in einem zweiten Schritt am Beispiel von literarischen Netzwerkanalysen dramatischer Texte exemplarisch darlegen, wie sich etablierte computergestützte Methoden (vgl. Jannidis 2017, 147–161; Trilcke 2013, 201–247) zu diesen Dimensionen verhalten. Um das Konzept des 
                    
scalable reading
 für die analytische Praxis fruchtbar zu machen, scheint es mir grundlegend, die dafür angedachten computergestützten Methoden auf ihre Skalierbarkeit hin zu prüfen. Denn während Franco Moretti und Matthew Jockers mit ihren Begriffen 
                    
distant reading
 und 
                    
macroanalysis
 die gewohnte literaturwissenschaftliche „Beobachtungshaltung, die ihre Gegenstände auf einer ‚mittleren Skala‘ situier[t]“ (Spoerhase 2020, 7), als ungeeignet für eine umfassende Literaturgeschichtsschreibung kritisieren (vgl. etwa Moretti 2000a, 207–209),
 versucht Mueller mit 
                    
scalable reading
 Mikro-, Meso- und Makro-Skalen zusammenzudenken.
                

            

            

                
Die Dimensionen von 
                    
scalable reading

                

                
In einem programmatisch ausgerichteten Blogbeitrag hob Mueller 2012 hervor, wie ihn „[t]he charms of Google Earth“ (Mueller 2012, o.S.) zu 
                    
scalable reading
 als methodischer Metapher für die Betrachtung literarischer Texte geführt habe. Seine Überlegungen konzentrieren sich hauptsächlich auf die Operation des 
                    
Zoomens 
(vgl. dazu Krautter, Willand 2020, 77–79). Durch Herein- und Herauszoomen würden in Google Earth unterschiedliche Repräsentationsformen entstehen, die je verschiedene Informationen tragen: „different properties of phenomena are revealed by looking at them from different distances“ (Mueller 2012, o.S.).
 Ein ähnliches Verfahren imaginiert Mueller nun auch für die Analyse literarischer Texte. Er zielt darauf ab, Verbindungslinien zwischen Einzeltextbetrachtungen und der Untersuchung größerer Zusammenhänge in Textsammlungen ziehen zu können und dabei die aufwändige Aufarbeitung von Kontexten, das Lesen sehr vieler Texte und letztlich die Identifikation von aufschlussreichen Mustern zu beschleunigen und zu vereinfachen.
 Grundlage dafür sind die verschiedenen Skalen, die sich hinter Muellers 
                    
reading
- bzw. Analysebegriff verbergen. 
                    
Scalable reading
 erstreckt sich nach meiner Ansicht auf mindestens vier skalierbar gedachte analytische Dimensionen. 
                

                
Die Skalenpluralität beginnt erstens bei der Textgrundlage: Literarische Texte liegen in „einer weiten ‚Scale‘ von Surrogaten“ (Weitin 2015, 10) vor, die nebeneinander koexistieren: „Our typical encounter with a text is through a surrogate“ (Mueller 2013, o.S.). Mueller spricht an dieser Stelle von Surrogaten, da immer schon mit unterschiedlich gearteten Repräsentationen des Originals gearbeitet wurde und wird: Das können beispielsweise Faksimiles, Text- und Werkausgaben, Digitalisate oder auch speziell kodierte Textsammlungen sein (vgl. dazu Mueller 2014, § 4–20). Surrogate können darüber hinaus in stark transformierter oder abstrahierter Form auftreten, beispielsweise in Gestalt von Häufigkeitswortlisten. Auch die Netzwerkanalyse fußt demnach auf Surrogaten. Peer Trilcke und Frank Fischer sprechen von einem „Zwischenformat“, das in ihrem Fall nur noch diejenigen Strukturinformationen der Dramen vorhalte, die zur Netzwerkerstellung herangezogen werden (Trilcke, Fischer 2018, Kap. 3). Der Dramentext selbst ist nicht mehr Teil des Zwischenformats.

                
An diese unterschiedlichen Repräsentationsformen von Literatur ist zweitens die Frage des Umfangs geknüpft: Wie groß ist der Untersuchungsgegenstand? Handelt es sich nur um einen einzelnen Text, vielleicht sogar nur um einen Ausschnitt des Textes, oder aber um eine größere Sammlung von Texten? Wie umfangreich ist diese Sammlung? Nicht nur die Zahl der zu betrachtenden Texte, auch die Textsorte kann hier Teil der Skalierungsfrage sein: Sollen kurze Novellen oder 1000-seitige Langromane untersucht werden, ein kurzer Einakter oder Karl Kraus’ monumentales Lesedrama 
                    
Die letzten Tage der Menschheit
, in dem in 220 Szenen fast 1000 sprechende Figuren auftreten (vgl. Fischer u.a. 2020, 279).
                

                
Drittens stellt sich die Frage nach der Größe der Analyseeinheiten. Morettis 
                    
distant reading
 grenzt sich, wie Carlos Spoerhase herausgearbeitet hat, von der üblichen Meso-Skala literaturwissenschaftlicher Untersuchungen ab, bei der das Verständnis eines oder einiger weniger literarischer Texte im Fokus stehe (vgl. Spoerhase 2020, 7). Für Moretti ist dagegen alles interessant, was abseits dieser mittleren Skala liegt, das sind „units that are much smaller or much larger than the text“ (Moretti 2000b, 57). Moretti geht es also nicht mehr um die ganzheitliche Interpretation von Texten, sondern um Mikro- und Makro-Eigenschaften von Textsammlungen, wie die Verteilung einzelner Wortformen oder die diachrone Entwicklung von Gattungen. Anders als 
                    
close reading
, das an die Meso-Skala gebunden sei, würden 
                    
distant reading
 oder 
                    
macroanalysis
 hinsichtlich der Analyseeinheiten sowohl ein „zooming in“ als auch ein „zooming out“ ermöglichen (Jockers 2013, 23). Auch Mueller betont, dass quantitative Methoden gleichermaßen ein Heraus- wie ein Hereinzoomen erlauben würden. Die Metapher des Zoomens ist bei ihm aber nicht an ein bestimmtes methodisches Instrumentarium gebunden (vgl. Mueller 2014, § 31). 
                

                
Mueller denkt die methodische Bezugsgröße viertens vielmehr selbst auf einer Art Skala. Wie Weitin gemeinsam mit Thomas Gilli und Nico Kunkel (2016, 115) herausstellt, umfasse 
                    
scalable reading
 bei Mueller nämlich „prinzipiell alle Akte des Lesens und Analysierens von Texten“. Unterschiedliche qualitative und quantitative Methoden würden dann gleichberechtigt nebeneinanderstehen und könnten den analytischen Anforderungen der Fragestellung und der gewählten Textsammlung gemäß kombinatorisch zusammengedacht werden. Anders als es das Begriffspaar 
                    
close
 und 
                    
distant reading
 nahelegt, ist der Einsatzzweck verschiedener Formen der Analyse bei Mueller nicht im Vorhinein determiniert. Relevant ist für ihn stattdesse, wie sich qualitative und quantitative Methoden für eine bestimmte Fragestellung so kombinieren lassen, dass ein analytischer Mehrwert entsteht.
                

            

            

                
Praktische Überlegungen zum scalable reading

                
Im folgenden Abschnitt möchte ich die mit einer Praxis des 
                    
scalable reading
 verbundenen Herausforderungen genauer beleuchten. Zur Veranschaulichung greife ich dabei auf literarische Netzwerkanalysen zurück. Abbildung 1 zeigt ein Kopräsenznetzwerk von Friedrich Schillers 
                    
Die Räuber 
(1781). Jeder Knoten im Netzwerk repräsentiert eine Figur des Dramas, die Kanten zwischen zwei Knoten zeigen an, dass die beiden verbundenen Figuren innerhalb eines bestimmten Textsegments interagieren. Im vorliegenden Fall bedeutet Interaktion, dass die beiden Figuren in der gleichen Szene sprechen (vgl. Trilcke u.a. 2015, 1). Solche Netzwerke können automatisiert erstellt werden, wenn die digitalisierten Dramen entsprechend kodiert vorliegen, wie es etwa beim 
                    
Drama Corpora Project 
der Fall ist (siehe Fischer u.a. 2019).
 Dadurch lässt sich eine große Zahl an Netzwerken nicht nur visuell, sondern vor allem mit Blick auf mathematische Netzwerkmetriken vergleichen. 
                

                
Die Automatisierung führt jedoch zu einigen Einschränkungen. So ist die oben dargelegte Formalisierung von Figureninteraktionen zwar ähnlich, aber nicht deckungsgleich mit dem von Solomon Marcus (1973, 358) vorgeschlagenen und zum kodifizierten Handbuchwissen (vgl. etwa Pfister 2011, 235–240) gewordenen Begriff der Konfiguration. Die Figurenkonfiguration eines Dramas ändert sich immer dann, wenn eine Figur die Bühne betritt oder verlässt, also das am Bühnengeschehen beteiligte Personal zumindest in Teilen wechselt. Dramen, die Prinzipien des französischen Klassizismus folgen, sind durch die im Nebentext markierten Auf- und Abtritte strukturiert. Konfiguration und Szenengrenze fallen dann – zumindest in der Theorie – zusammen. Anders ist das bei Stücken, die sich an Shakespeares Poetik orientieren. Hier sind die Szenengrenzen zumeist an einen Ortswechsel gebunden. Daher können Figuren auf- oder abtreten, ohne dass zwangsläufig eine neue Szene konstituiert wird. Da Auf- und Abtritte von Figuren im 
                    
Drama Corpora Project
 (noch) nicht kodiert sind, ist die automatisierte Erstellung von Kopräsenznetzwerken auf die Szenengrenzen als Segmentierung angewiesen.
 Das kann Begleiterscheinungen zur Folge haben, die es in der Untersuchung zu reflektieren gilt. Schillers Stück 
                    
Die Räuber
 ist dafür ein gutes Beispiel. So hat schon Marcus (1973, 326–333) darauf hingewiesen, dass sich zwischen der Anzahl an Szenen und der Anzahl an Konfigurationen eine große Diskrepanz auftue. Im Verlauf der 15 Szenen von 
                    
Die Räuber
 zählt Marcus ganze 78 Konfigurationen.
                

                
Nun stellt sich die Frage, wie sich solche Kopräsenznetzwerke sinnvollerweise in die etablierte Dramenanalyse integrieren lassen. Einen prominenten Versuch, Netzwerkanalysen in den Verstehensprozess literarischer Texte zu integrieren, unternimmt Moretti in seinem Essay 
                    
Network Theory, Plot Analysis 
(2011). Entgegen seiner Rhetorik der 
                    
large scale
 erprobt er die Methode nur an einzelnen literarischen Texten, insbesondere an Shakespeares 
                    
Hamlet 
(1609). Sein Vorgehen lässt sich als 
                    
close reading
 von Netzwerkvisualisierungen beschreiben. Moretti zerlegt die Visualisierungen in verschiedene Teile und versucht seine netzwerkanalytischen Beobachtungen anschließend mit generellen textanalytischen Erkenntnissen zu verbinden (vgl. Moretti 2011, 3–7).
                

                

                    

                    
Abbildung 1: Kopräsenznetzwerk von Friedrich Schillers Die Räuber (GEM force directed layout algorithm). Die Knotengröße repräsentiert den Grad. 

                

                
Wie verhält sich Morettis Studie aber zu Muellers 
                    
scalable reading
 und wie lässt sie sich hinsichtlich der vier von mir herausgearbeiteten Dimensionen (Textgrundlage, Größe des Untersuchungsgegenstandes, Analyseeinheiten und Methoden) einordnen? Verglichen mit einer herkömmlichen Interpretation von 
                    
Hamlet 
ist die auffälligste Veränderung mit Sicherheit die fundamentale Modifizierung der Textgrundlage. Moretti interpretiert nicht den Dramentext, sondern ein auf Strukturdaten basierendes abstraktes Netzwerk in Form von Knoten und Kanten. Die Größe des Untersuchungsgegenstandes bleibt zwar auf einen literarischen Text beschränkt, das spezifische Surrogat beziehungsweise Textmodell gibt aber die im Zentrum der Untersuchung stehenden Analyseeinheiten vor: Untersucht wird die zu einem Figurennetzwerk subsumierte Kopräsenz von Dramenfiguren. Aufschlussreich ist Morettis methodisches Vorgehen. Grundlage des Netzwerks sind quantitative Strukturdaten. Erkenntnisse gewinnt er aber vor allem im Modus der Interpretation und nur äußerst begrenzt aus statistischen Auswertungen der mathematischen Netzwerkmetriken. Moretti beschreibt das als „using networks to gain intuitive knowledge of plot structures“ (Moretti 2011, 12). Entspricht das nun Muellers Vorstellung von 
                    
scalable reading? 

                    
Insbesondere Morettis Schlussfolgerung ist dahingehend bezeichnend. Seine Analysen, so urteilt er, würden nämlich nach einer „radical reconceptualization of characters and their hierarchy“ in der Literaturwissenschaft verlangen (Moretti 2011, 5). Etablierte Konzepte – etwa das Protagonistenkonzept – ließen sich aus seiner Perspektive kaum produktiv mit seinen netzwerkanalytischen Ergebnissen verbinden. Denn die explanative Funktion abstrakter Modelle sei mit „concepts of ‚consciousness‘ and ‚interiority‘“ nicht kompatibel (Moretti 2011, 4). Für Moretti ist es demnach nicht einmal dann erstrebenswert, die Netzwerkanalyse in die typische Meso-Skala literaturwissenschaftlicher Interpretationen zu integrieren, wenn nur ein einzelner Text untersucht wird. Denn schon hier zielt er auf textübergreifende Konzepte ab. Beim Zoomen zwischen Mikro- und Makro-Ebene, so ließe sich schließen, wird die Meso-Skala übersprungen.

                

                
Abbildung 1 verdeutlicht zudem, dass nicht alle Kopräsenznetzwerke vom „‚intermediate‘ status of visualization“ profitieren, den Moretti in seinem Essay als so wichtig erachtet (Moretti 2011, 11). Das Netzwerk von Schillers 
                    
Die Räuber 
kann nämlich auch in die Irre führen. Die getrennten Sphären von Familie und Räubern, die die Struktur des Stücks in den ersten drei Akten stark prägen (vgl. Krautter, Willand 2021, 115–118), lassen sich in der Abbildung nicht identifizieren. Ganz im Gegenteil: Die Netzwerkvisualisierung stellt die Räuberbande, insbesondere Schwarz, Grimm und Karl Moor ins Zentrum des Dramas, während die Familie um Franz, den alten Moor und Amalia an den Rand rückt. Grund dafür ist der Grad der entsprechenden Knoten. Dabei handelt es sich um eine simple Metrik, die die Anzahl an Kanten misst, die ein Knoten auf sich vereint (vgl. Newman 2010, 168–169). Während Schwarz und Grimm alle 25 möglichen Verbindungen zu anderen Figuren realisieren, sind es bei Franz nur deren zwölf. Selbst Moretti würde von Schwarz’ und Grimms Zentralität aber kaum darauf schließen, dass sie die Hauptfiguren des Stücks sind. Zu marginal ist ihr Einfluss auf die Handlung, zu gering sind ihre Redeanteile. 
                

                
Der Mehrwert solcher Figurennetzwerke wird jedoch meist auf die Analyse größerer Textsammlungen verschoben, die schon aufgrund ihrer Menge nur schwer durch 
                    
close reading
 erschließbar erscheinen (vgl. Trilcke, Fischer 2018, Kap. 3). Hinsichtlich der vier Dimensionen würde das einer Höherskalierung des Untersuchungsgegenstands entsprechen. Ziel ist es hierbei, die als Netzwerke modellierten literarischen Texte durch mathematische Metriken – wie den Grad der Knoten – in einen Vergleichszusammenhang zu stellen. Davon sind auch die Analyseeinheiten betroffen. Die Kopräsenzen der Figuren sind zwar weiterhin die Grundlage der Analyse, als infratextuelle Merkmale sollen sich durch sie aber supratextuelle Muster identifizieren lassen. Hierdurch könnten sich, so die Hoffnung, entweder neue Einsichten in die Literaturgeschichte ergeben oder existierende Hypothesen anhand umfassender Textsammlungen nachvollzogen werden.
                

                

                

                    

                    
Abbildung 2: Durchschnittlicher Grad von 583 deutschsprachigen Dramen. Die Abbildung zeigt die Mittelwerte pro Dekade.

                

                

                
Abbildung 2 zeigt ein Beispiel für eine diachrone Analyse anhand 583 deutschsprachiger Dramen, die zwischen 1730 und 1930 veröffentlicht oder uraufgeführt wurden (German Drama Corpus). Die Abbildung reproduziert eine Untersuchung von Trilcke und Fischer (2018, Abbildung 6). Wie Trilcke und Fischer habe ich aus dem durchschnittlichen Grad der einzelnen Dramen die Mittelwerte für jedes Jahrzehnt von 1730 bis 1930 ermittelt. Rein deskriptiv ist festzuhalten, dass der durchschnittliche Grad ab dem späten 18. Jahrhundert langsam ansteigt. Zwischen 1830 und 1880 sind dann nur relativ geringe Schwankungen zu erkennen, ehe auf einen Anstieg bis etwa 1890 ein abrupter Fall und ein erneuter starker Anstieg folgen. Trilcke und Fischer haben diese Werte als Indikator dafür gedeutet, dass Dramatiker:innen mit ihren Stücken „auf die gesellschaftliche Modernisierung und Ausdifferenzierung seit der zweiten Hälfte des 18. Jahrhunderts“ reagieren. Sie weisen im Anschluss gleichwohl darauf hin, dass diese Erkenntnis nichts Neues sei (Trilcke, Fischer 2018, Kap. 4.1). Mit Fotis Jannidis (2019, 65) gesprochen lässt sich diese Art der Wissenskonsolidierung als Form der „Kreuzpeilung“ begreifen.

                
Wie überzeugend ist diese Deutung der Werte aber? Vergleicht man den Werteverlauf des durchschnittlichen Grads in Abbildung 2 mit der Zahl der auftretenden Figuren aus Abbildung 3, scheint ein Zusammenhang zu bestehen. Die Berechnung der Korrelation zwischen Grad und Figurenzahl bestätigt diese Relation: 
                    
Spearman’s 
                        
ρ 
beträgt 0,75. Aus konzeptioneller Perspektive erscheint der Zusammenhang schlüssig. Je größer die Zahl der auftretenden Figuren, umso höher ist der maximal mögliche Grad einer Figur. Gleichzeitig sinkt mit steigender Figurenzahl die Wahrscheinlichkeit, dass alle möglichen Kanten tatsächlich realisiert werden. Folglich sinkt die Dichte des Netzwerks. Die diachrone Analyse des durchschnittlichen Grads beruht also zu großen Teilen auf der sich verändernden Anzahl auftretender Figuren in den Dramen. Ob und wie die Interaktion der Figuren davon beeinflusst wird, müsste jedoch genauer untersucht werden. Dabei könnten Netzwerkmetriken, die die Zentralität eines Knotens möglichst unabhängig von der Netzwerkgröße ermitteln, künftig eine wichtige Rolle einnehmen (vgl. Szemes, Vida 2023 [in Vorb.]).
                    

                

                

                    

                    
Abbildung 3: Zahl der Figuren (schwarz) und durchschnittlicher Grad (grau) von 583 deutschsprachigen Dramen. Die Abbildung zeigt die Mittelwerte pro Dekade.

                

            

            

                
Fazit

                
Die vier von mir beschriebenen Dimensionen des 
                    
scalable reading
 können helfen, Möglichkeiten und Grenzen der Skalierung mit Bezug auf spezifische Forschungsfragen oder das methodische Vorgehen auszuloten. Wie meine Ausführungen gezeigt haben, sind die Textgrundlage, die Größe des Untersuchungsgegenstandes, die Analyseeinheiten und die eingesetzten Methoden nicht als voneinander unabhängige Variablen zu denken. So ist es einerseits nicht selbstverständlich, dass Kopräsenznetzwerke für die typische literaturwissenschaftliche Meso-Skala produktiv gemacht werden können. Bei der Höherskalierung des Untersuchungsgegenstandes ist es andererseits wichtig, die Werte der Netzwerkmetriken zu kontextualisieren und zu reflektieren, da die Analyseeinheiten als stark abstrahierte Datenwerte repräsentiert werden. Die immer wieder als zentral erachtete Verbindung von Einzeltextlektüren mit der Analyse größerer Textsammlungen (vgl. Weitin 2017, 1) steht somit nicht nur vor der Herausforderung, qualitative und quantitative Methoden zu kombinieren. In Abhängigkeit davon müssten zeitgleich unterschiedliche Textgrundlagen, unterschiedlich skalierte Untersuchungsgegenstände und unterschiedliche Analyseeinheiten gewinnbringend zusammengeführt werden.
                

            

        

            

                
Digital scholarly editions are key resources for arts and humanities research, and predate in various forms the concepts of digital humanities or humanities computing (Sula and Hill 2019). While individual projects are remembered for their contribution to the field, few comprehensive data sources exist to show the development of the field. This poster is both an analysis of the sources from which to write a history of digital scholarly editing, and an overview of the state and development of the field using quantitative methods.
            

            
Digital editions are positioned between drawing from archived material, and being an archive themselves (Dillen 2019, 266). In addition to that, digital editions also are web resources in need of archiving, lest they fall subject to link rot and very soon disappear from the web either for the lack of a persistent identifier or lack of maintenance. For digital editions past and present, two main data sources are available. Patrick Sahle lists around 700 editions in a curated catalog (Sahle, n.d.), while the Catalogue of Digital Editions features about 320 digital editions in a database (Franzini 2012). Both sources have different criteria for inclusion, overlap in content and differ in granularity, yet these are the sources from which a history of digital scholarly editions will mostly draw. Analysis of these sources will present them in their scope, aim and usability for research, while highlighting underrepresented areas of data collection on digital scholarly editions.

            
Adding to the collection of material to describe the history and development of digital editions, the second phase of C21 Editions project engaged in semi-structured interviews with an extensive group of 50 experts and stakeholders from a range of relevant disciplines, including digital scholarly editing, digital publishing, archiving and preservation, interface design, and creative practice. As the field of digital scholarly editions is by now old enough to span entire academic careers, these interviews represent a wealth of insight into the developments of the field. The interviews have been coded and allow for automated analysis of this novel primary source. This poster will present the preliminary results of analyzing the coded interviews with a special emphasis on digital publishing, open data and open access and research infrastructures. 

            
A quantitative analysis of the data sources combined with the coded interviews will then provide data-driven insight into the development of digital scholarly editions since the 1970s. The analysis will in a first step focus on the amount of projects and their average duration over time to produce an overview of the field. In a second step, long-term cycles such as the adaptation of TEI-XML and open access standards will be analyzed. Preservation and availability of all editions listed in both data sources will show the loss rate affecting digital scholarly editions and lead back to a discussion of the current state and history of the field based on the work currently being undertaken within the C21 Editions project.

            
Acknowledgements

            

                

                
This research is part of 

                
C21 Editions: Scholarly Editing and Publishing in the Digital Age

                
, a three-year international collaboration jointly funded by the Arts &amp; Humanities Research Council (AH/W001489/1) and Irish Research Council (IRC/W001489/1).

            

        

            
Die leichte Zugänglichkeit von Publikationsmedien wie Blogs ermöglicht es Forschenden seit der Jahrtausendwende, selbst zu entscheiden, wann, wo und was sie veröffentlichen. Diese selbstbestimmte Aneignung eines wissenschaftlichen Publikationsraums war ein durchaus spektakulärer Schritt (König 2015, 58). Durch die zunehmende Verbreitung von Wissenschaftsblogs und Angebote von qualitätsgestützten Plattformen speziell für die Geisteswissenschaften wie hypotheses
, die persistente URLs genauso bieten wie eine Archivierung der Inhalte und einen Beratungsservice für Bloggende, hat diese Form der Kommunikations- und Publikationspraktik in der Wissenschaft zwischenzeitlich viel von ihrem revolutionären Charakter verloren. Vielmehr ist sie selbst in den Geisteswissenschaften in der Mitte der Disziplinen angekommen, wie beispielhaft der stetig wachsende Katalog der Blogplattform hypotheses zeigt, der gegenwärtig 4326 Wissenschaftsblogs umfasst
. Wissenschaftsblogs – so aktuelle Resümees - sind aus der wissenschaftlichen Publikations- und Kommunikationslandschaft nicht mehr wegzudenken 
                
(Baillot 2022, 105; Wuttke und Gebert 2021, 428-431; Gebert und van Beek 2019, 274–276).
            

            
Damit erübrigt sich aber das Thema Wissenschaftsblogs oder ein Workshop dazu keineswegs, im Gegenteil: Vielmehr hat Wissenschaftskommunikation seit einigen Jahren Konjunktur angesichts einer spürbaren Polarisierung der Gesellschaft, einem Glaubwürdigkeitsverlust der Wissenschaft eingeheizt in der Pandemiekrise und einem Bedürfnis von Forschenden, die Transformation der verlagszentrierten wissenschaftlichen Publikationslandschaft mitzugestalten. Aktiv Wissenschaftskommunikation zu betreiben, über Projekte früh im Forschungsprozess zu kommunizieren, sich in hochschulpolitische sowie gesellschaftliche Debatten einzumischen und Open Access zu publizieren, sind die Gebote der Stunde, wie sich auch in Leitlinien und Debatten der Wissenschaftsinstitutionen zeigt
. Die Nachfrage bei der deutschsprachigen Blogplattform für die Geisteswissenschaften de.hypotheses nach einführenden Workshops hat zudem seit Beginn der Pandemie stark zugenommen. Dies gibt zusammen mit der wahrnehmbaren Professionalisierung Anlass genug, im geplanten halbtägigen Workshop inhaltliche, technische und gestalterische Unterstützung rund um das Thema Wissenschaftsblogs speziell in den Geisteswissenschaften zu geben und aufzuzeigen, wie sich das Wissenschaftsbloggen als Teil des eigenen Publikations- und Lehrportfolios und als Vernetzungsbaustein kompetent und strategisch verwenden lässt. 
            

            

            
Wissenschaftsblogs: Einblicke in die laufende Forschung

            
Wissenschaftliche Blogs sind ein Ort für die Veröffentlichung und Diskussion laufender Forschungsarbeiten, ein Kanal für die Selbstdarstellung und Vernetzung von Forschenden. Sie zeigen Wissenschaft im Entstehen und sorgen für Reichweite und Sichtbarkeit der eigenen Forschung. Blogs wirken im Vergleich zu statischen Websites sehr viel dynamischer, zumal es über die Kommentarfunktion einfach ist, Diskussionen anzustoßen (Wuttke und Gebert 2021, 429). Der wissenschaftliche Austausch über Blogeinträge, Kommentare und Links ist interaktiv, schnell und direkt, auch wenn sich Bloggende gerade in den Geisteswissenschaften häufig eine höhere Anzahl an Kommentaren wünschen (vgl. die Umfrage bei de.hypotheses, König 2019, 12-14). Studien über die Nutzung von soziale Medien in den Wissenschaften weisen ein breites Spektrum unterschiedlicher Verwendungszwecke, Ziele und Motive wissenschaftlicher Bloggenden auf (Mahrt und Puschmann 2014), die von der Plattform, dem akademischen Rang und dem Status der Forschenden, Alter und Geschlecht ebenso abhängig sind wie von der jeweiligen Disziplin und vom Herkunftsort der oder des Bloggenden (Sugimoto et al. 2017, 2039, 2046). Die meisten Plattformen erlauben eine sehr breite Palette an Nutzungsszenarien, was sich insbesondere beim Wissenschaftsbloggen zeigt: Es gibt keine Vorgaben, Richtlinien oder Beschränkungen im Hinblick auf Umfang und Länge der Beiträge, auf die Häufigkeit der Publikation, auf den verwendeten Stil. Neben Text können ohne Kosten genauso Abbildungen, Podcasts, Videos, animierte Karten etc. eingebunden werden. Wissenschaftsbloggen erleben viele Forschende daher als Befreiung, auch wenn die völlige Offenheit im Blog ein Phantasma sein mag (König 2015, 66) und nicht zuletzt die Wahl der Sprache Auswirkungen auf die Ausdrucksmöglichkeiten hat (Baillot 2022). Zugleich mag diese Freiheit gerade beginnende Bloggerinnen und Blogger einschüchtern und in der kreativen Nutzung ihres Blogs begrenzen. Durch das Aufzeigen von Beispielen sollen im Workshop genau dieses Spannungsverhältnis adressiert und Lösungsmöglichkeiten aufgezeigt werden.

            
Gleichzeitig haben sich Blogs als ein Ort erwiesen, an dem in traditionellen Zeitschriften veröffentlichte Artikel kritisiert und korrigiert werden (Sugimoto et al. 2017, 2045-2046). Neuere Projekte zeigen außerdem Wege auf, um Wissenschaftsblogs an Fachzeitschriften heranzuführen, indem etwa wie beim Mittelalterblog
 eine Redaktion Auswahl und Lektorat von Beiträgen übernimmt, diese in Repositorys hinterlegt und in Kooperationen mit Bibliotheken für die Katalogisierung sorgt (Döring &amp; Gebert 2022, 93–97). Andere Nutzungen zielen in Richtung „kleine Editionen“ für das Edieren von Texten im Blog und mit Wikisource (Bemme 2022).
            

            
Wissenschaftliches Bloggen gehört in praktischer wie in theoretischer Hinsicht zum Gegenstandsbereich der DH: Es ist ein zentrales Mittel der Community Building, hat Berührungspunkte zum weiten Feld des digitalen Publizierens und ist ein Baustein im Medien-Portfolio von Open Science (Wuttke und Gebert 2021). In der Lehre finden Blogs Einsatz als Schreib- und Übungsblog für Studierende wie als Gegenstand, um vielfältige Themenbereiche wie Publikation, Open Science, ethische und rechtliche Fragen etc. zu diskutieren (Tantner 2015). 

            

            
Didaktischer Zugang und Aufbau des Workshops

            
Im Workshop sollen grundlegende inhaltliche und technische Kenntnisse des Bloggens mit Wordpress vermittelt und Ansätze für das Entwickeln einer Blogstrategie diskutiert werden. Workshopteilnehmende sollen hinterher in der Lage sein, ein Blog einzurichten und zu führen (ob bei hypotheses oder anderswo), Inhalte dafür zu produzieren, sichtbar zu machen und zu lizenzieren sowie verschiedene Blogpraktiken zu kennen. 

            
Der halbtägige Workshop (4h) ist didaktisch als Hands-On-Workshop aufgebaut und gliedert sich in drei Teile: In einem ersten Teil (1,5h) werden grundlegende Fragen zum Wissenschaftsbloggen thematisiert. Die Vermittlung von Inhalten erfolgt nach einem kurzen Input in Form eines Gesprächs dreier erfahrener Wissenschaftsbloggerinnen (Anne Baillot, Mareike König, Ulrike Stockhausen) über Best Practice-Beispiele entlang der Fragen: Wie sieht ein guter wissenschaftlicher Blogbeitrag aus? Wie funktioniert die Interaktion mit den Leser:innen? Welche Sprache, welcher Stil ist bei wissenschaftlichen Blogs angemessen? Wann ist ein Blog erfolgreich? Wie geht man mit unsachlichen Kommentaren oder gar mit Shitstorms um? Wie lassen sich Blogs in der Lehre fruchtbar einsetzen?

            
Der zweite und dritte Teil (je 1h) sind als praktische Übungseinheiten mit inhaltlichen Vertiefungen konzipiert, bei denen die Teilnehmenden die Schritte in Schulungsblogs von de.hypotheses sofort umsetzen können. Geübt werden grundlegende technische und grafische Einstellungen eines Blogs, das Anlegen eines Artikels, das Verschlagworten und Zuordnen von Kategorien. Die Übungen werden ergänzt durch eine Vertiefung der Diskussionen über Themenfindung, Aufbau und Gliederung von Beiträgen sowie sachliche Erschließung von Webinhalten.

            
Im dritten Teil wird das Einbinden von multimedialen Elementen geübt, kombiniert mit Exkursen zu Bildrechten, zu Anbietern von OA-Inhalten (Audio, Video), zu CC-Lizenzen, Impressum und zur DSGVO. Ebenso werden Hinweise zur Suchmaschinenoptimierung und zur Erhöhung der Sichtbarkeit von Bloginhalten diskutiert. Abschließend werden Tipps für die Anfangsphase eines Wissenschaftsblogs sowie zu Diskussionen über Kommentare gegeben. Die Schulungsblogs stehen den Teilnehmenden auch nach dem Workshop zur Verfügung, so dass sie Inhalte daraus in ihr zukünftiges Blog übernehmen können. 

            
Max. 20 Teilnehmende

            

            
Beitragende

            

                
Dr. Mareike König
 ist stellvertretende Direktorin am Deutschen Historischen Institut und leitet das Blogportal de.hypotheses. Zu ihren Forschungsinteressen gehören: Digitale Geschichtswissenschaft, Wissenschaftskommunikation mit sozialen Medien, Wissenschaftliches Bloggen, Open Access. 
            

            
Deutsches Historisches Institut Paris, 8, rue du Parc Royal, 75003 Paris, Frankreich, 
                
mkoenig@dhi-paris.fr

            

            

                
Prof. Dr. Anne Baillot
 ist Professorin für Germanistik an der Universität Le Mans. Zu ihren Forschungsinteressen gehören Digitale Editionen, Open Access, Archive, Wissenschaftsbloggen, Greening DH. 
            

            
Faculté des Lettres, Langues &amp; Sciences Humaines. Avenue Olivier Messiaen, 72085 LE MANS Cedex 09, France, 
                
anne.baillot @ univ-lemans.fr
.
            

            

                
Dr. Ulrike Stockhausen
 ist Community Managerin bei de.hypotheses. 
            

            
Max Weber Stiftung, Rheinallee 6, 53173 Bonn, 
                
stockhausen@maxweberstiftung.de
.
            

        

            

                

                    
Zusammenfassung
                

                
Die digitale Vermittlung von kulturellen Inhalten gewinnt für Museen und Gedenkstätten immer mehr an Bedeutung (u.a. Kohle 2021); sie hat durch die Pandemie, nicht zuletzt durch gesonderte Ausschreibungen etwa der Bundeskulturstiftung,

                nochmals einen Aufschwung bekommen (Richard et al. 2022). Aber auch schon davor existierten Förderkonzepte, die ihren Fokus explizit auf experimentelle Formate digitaler Kulturvermittlung legten, so z.B. die Reihe „experimente#digital“ der Aventis Foundation, die gezielt experimentelle Digitalkonzepte in Museen fördert.
                    

                

                
Dabei geht es nicht nur darum, innovative Inszenierungen im Museum zu kreieren, sondern auch um die Entwicklung von Vermittlungsformen, die allein für den digitalen Raum bestimmt sind und sich auch abseits des analogen Museumsortes virtuell erfahren lassen – und die damit neue Möglichkeiten und Chancen sowohl der Aufbereitung als auch der Verbreitung von kulturellen und wissenschaftlichen Inhalten eröffnen (Franken-Wendelsdorf 2019). Die Frage nach ‚Open Culture‘ bildet dabei einen Dreh- und Angelpunkt: ist mit ihr im Museumskontext doch erstens ein freier und transparenter Zugriff auf die Daten sowie zweitens eine Usability gemeint, die verschiedene Zielgruppen adressiert. Drittens sollte es – zumindest im Idealfall – auch darum gehen, Open Culture als Sujet der virtuellen Inszenierung mitzureflektieren und neu zu verhandeln. 

                
Wie dies umgesetzt werden kann, was hierbei zu beachten ist und wo die Herausforderungen liegen, möchte der geplante Vortrag am Beispiel des digitalen Museumsprojekts „Skandal-KULTUR reloaded – Literarische Affären INTERAKTIV erkunden“ vorstellen und zur Diskussion stellen. Das Projekt wurde am Freien Deutschen Hochstift – Frankfurter Goethe-Museum in Kooperation mit dem Trier Center for Digital Humanities durchgeführt und von der Aventis Foundation gefördert.

                     Die inhaltliche und gestalterische Konzeption ist weitgehend abgeschlossen (s. Abbildung), die noch offenen Fragen der Umsetzung, die insbesondere das Thema „Interaktivität“ betreffen, möchte der Vortrag diskutieren. Die Plattform wird zur DHd 2023 freigeschaltet.
                

            

            

                

                    
Skandal und Open Culture
                

                
Das Thema Skandal eignet sich für die Reflexion von Open Culture besonders. In Skandalen verdichten und konzentrieren sich Konflikte und Veränderungsprozesse einer Gesellschaft (z.B. im Umgang mit Minoritäten, Wertewandel etc.). Sie sind Indizien von offenen Gesellschaften und für deren Aushandlungsprozesse und Selbstverständnis unverzichtbar (Blasberg 2009). Anders gesagt: Dort, wo es keine Skandale gibt bzw. diese unterdrückt oder deren Auslöser:innen bestraft werden, kann es auch keine offenen Debatten geben – Modernisierungsprozesse bleiben notwendig aus. Dabei war und ist die Sensibilität für das, was als ‚Skandal‘ wahrgenommen und definiert wird, in von der Zensur betroffenen Gesellschaften meistens sehr hoch, in offenen Gesellschaften dagegen deutlich geringer.

                
Skandale gelten als zentrale Kommunikationsphänomene der Moderne: Sie vermessen das Spannungsfeld zwischen öffentlich Sagbarem und Unsagbarem, zwischen öffentlich Zeigbarem und Nicht-Zeigbarem stets neu und bewegen sich dabei auf der brisanten Grenze zwischen Herabwürdigung, Beschämung und Bloßstellung einerseits sowie satirischer Polemik und sprachlich-medialer Virtuosität andererseits (Friedrich 2011, Roßbach 2020). Neben kurzfristigen, heute allgegenwärtigen Medien‚skandalen‘, wie sie insbesondere in den Social Media mit allen fragwürdigen Grenzüberschreitungen (Hate Speech, Fake News etc.) inszeniert werden (Pörksen 2012), gibt es Skandale mit langfristiger Wirkung, die an gesellschaftlichen Grundfesten rütteln und die Grenzen des Sag- und Zeigbaren erweitern (Blasberg 2009).

            

            

                

                    
Skandal-KULTUR reloaded
                

                
Der Rückblick auf historische Situationen kann somit Spiegel und Anstoß sein für eine vertiefende Auseinandersetzung mit auch heute noch brandaktuellen Themen und Problemen wie Ausgrenzung, Antisemitismus, Religions- und Moralfragen, neue Lebens- und Liebeskonzepte, Homophobie etc. Das digitale Museumsprojekt, das hier vorgestellt und in seiner Konzeption und Methodik zur Diskussion gestellt werden soll, widmet sich Literaturskandalen zwischen den beiden großen Revolutionen 1789 und 1848 (Gelz et al. 2014); dabei stehen die genannten Konflikte im Fokus, deren Themen bis heute von großer Aktualität sind: Um ‚verbotene‘ Liebe, Selbstmord und Jugendgefährdung wird in Goethes Briefroman „Die Leiden des jungen Werthers“ (1774) und den zeitgenössischen Reaktionen heftig gestritten. Einen Skandal um Fragen von Sexualität und Gender sowie von neuen, provokativen Ausdrucksmöglichkeiten in der Kunst entfachten Friedrich Schlegels romantischer Roman „Lucinde“ (1799) und die romantische Zeitschrift „Athenaeum“ (1798–1800). So konnte man z.B. in Friedrich Schlegels Roman „Lucinde“ die poetische Inszenierung des Rollentauschs von Mann und Frau beim Liebesspiel finden („
                    
Eine unter allen [Situationen] ist die Witzigste und die Schönste: wenn wir die Rollen vertauschen und mit kindischer Lust wetteifern, wer den andern täuschender nachäffen kann, ob dir die schonende Heftigkeit des Mannes besser gelingt, oder mir die anziehende Hingebung des Weibes [...]
“, Schlegel 2020, S. 19), und im 272. Athenaeumsfragment heißt es: „
                    
Warum sollte es nicht auch unmoralische Menschen geben dürfen, so gut wie unphilosophische und unpoetische? Nur antipolitische oder unrechtliche Menschen können nicht geduldet werden
“ – beides war für die Zeitgenossen eine unerhörte Provokation. Am Skandal um Karl Sessas Posse „Unser Verkehr“ (1812/1815) lassen sich antisemitische Angriffe bis hin zu den pogromartigen sog. „Hepp-Hepp-Krawallen“ (1819) verfolgen. Die Affäre um Heinrich Heines Judentum und Graf August von Platens Homosexualität (1826–1830) eignet sich für Analysen rhetorischer Strategien zur Ausgrenzung von Minoritäten. Schließlich ist ein Religionsskandal Thema: Karl Gutzkows Roman „Wally, die Zweiflerin“ (1835) wurde als heftiger Angriff auf Moral und Religion gewertet – der Autor musste sogar ins Gefängnis.
                

            

            

                

                    
Konzeption
                

                
Das Hauptziel der digitalen Wissensvermittlung im rein virtuellen Museum war es, die Skandale für ein breites Publikum möglichst verständlich und real zu machen, ohne dabei den wissenschaftlichen Fokus zu verlieren. Das umfasst zum einen die Themen der Skandale, die Akteure und den historischen Kontext, aber auch die Form der Kommunikation.

                
Die Akteure der Skandale, die im Projekt thematisiert werden – insbesondere die Frühromantiker:innen – galten bereits als Medienvirtuosen, die die Spielarten des Druckmediums voll ausreizten (z.B. im Billet und der Anzeige) und gegen ihre literarischen Gegner, insbesondere die ältere Generation, einsetzten (vgl. Oesterle 2017). Zugleich sind die von ihnen skandalisierten Themen damals wie heute aktuell, so dass es in der Konzeption des Projekts eine Zielsetzung war, ein gemischtes Publikum für ihre Aktualität zu sensibilisieren und diese erlebbar zu machen. Um das Erleben kreativ und innovativ zu gestalten, bieten sich zwei grundsätzlich Alternativen einer stilistisch tonalen Umsetzung an, sowohl vom Design her als auch von der User Experience insgesamt. Einerseits besteht die Möglichkeit, durch das Gestalten einer historisch anmutenden Umgebung eine Atmosphäre zu schaffen, ähnlich wie sie damals herrschte. Eine zweite Möglichkeit, das Szenariums in die heutige Zeit mit heutigen Formen der Medialität zu übertragen, erschien dem Projektteam reizvoller, weil dadurch die Parallelen zur Skandalkultur heute sehr deutlich werden können. Ein wesentlicher Unterschied zwischen damals und heute ist der Faktor Zeit und Beschleunigung medialer Kommunikation. Während früher zwischen den Botschaften der Parteien und der Gegenrede Tage, Wochen oder gar Monate vergingen, sind es heute oft nur noch wenige Minuten (s. zur Entwicklung der Massenmedien und zum zunehmenden Tempo der Kommunikation Jäckel 2011). Gleichwohl waren die Texte des Disputs nicht weniger emotional aufgeladen und impulsiv, wie es heute der Fall ist, was zusätzlich für eine Umsetzung der Plattform in heutigem Stil spricht.

            

            

                

                    
Technische Umsetzung
                

                
Die Datenerfassung wurde mit der virtuellen Forschungsumgebung „Forschungsnetzwerk und Datenbanksystem“ FuD (
                    

                        
https://fud.uni-trier.de

                    
) umgesetzt. Die Datenstruktur ist auf die konkreten Anforderungen des Projektes angelegt: Skandale als zentrale Objekte sind verknüpft mit zugehörigen Ereignissen, diese wiederum mit Akteuren, welche wiederum mit Personen bzw. Körperschaften verlinkt sind, die ihrerseits verschiedenen Skandalparteien angehören. Insbesondere werden eine Reihe an Informationen wie Quellen, Orte, Bilder etc. zu den jeweiligen Objekttypen erhoben. Das Modell wird in die FuD-Logik überführt und in einer relationalen Datenbank abgebildet. Durch die Flexibilität von FuD und das agile Datenmodell, das wir definiert haben (siehe Abbildung), sind wir in der Lage, jederzeit in dessen Komplexität einzugreifen und individuelle Anpassungen bzw. Erweiterungen vorzunehmen.
                

                

                    

                        

                        
Datenmodell

                    

                

                
Während das Design (Details s.u.) keineswegs typisch ist, sind die Ansichten der Weboberfläche eher konventionell aufgebaut. Bei der Online-Präsentation greifen wir auf bereits am TCDH etablierte Strukturen und Prozesse zurück und setzen auf neue, moderne und nachhaltige Technologien. Dabei bauen wir auf unsere Expertise aus der Arbeit mit Elasticsearch (
                    
https://www.elastic.co
), NodeJS (
                    
https://expressjs.com
), PHP (
                    
https://www.php.net
), Angular (
                    
https://angular.io
) und setzen auf JSON (
                    
https://www.json.org
) als Austauschformat.
                

                
Nachdem die Daten in der Forschungsumgebung erfasst und freigegeben sind, werden diese in deren logische Einheiten exportiert, in das vordefinierte JSON Objekt Modell überführt und in Elasticsearch importiert. Die Weboberfläche wird modular, generisch und projektspezifisch mit dem Front-End-Framework Angular umgesetzt. Der Einsatz von Angular gibt uns die Möglichkeit, auf bereits gut erprobte Methoden zu setzen. 

                
Als Verbindungsgrundbaustein zwischen Front-End und Elasticsearch setzen wir einen NodeJS Express Server als Open API ein, mithilfe dessen über vordefinierte Suchanfragen die RESTful API von Elasticsearch abgefragt wird (siehe Abbildung).

                

                    

                        

                        
Infrastruktur

                    

                

            

            

                

                    
UX, Usability und Design
                

                
Bezüglich der Usability gelten auf der Plattform alle Regeln, die es auch sonst beim Aufbau einer Webapplikation zu beachten gilt. Seitenbesucher:inner sollen sich schnell zurechtfinden und in Bezug auf ihre Navigationsgewohnheiten nicht gestört werden. Es wurde auf bekannte Gestaltungsregeln und -merkmale zurückgegriffen und es wurden keine exotischen oder gar neu erfundenen Gestaltungselemente genutzt (zu grundlegenden Usabilitygrundsätzen vgl. Jacobsen et al. 2019).

                
Hinsichtlich der User Experience hingegen wurde versucht, bestimmte für gewöhnlich erwünschte emotionale Reaktionen von v.a. wissenschaftlich traditionell geprägten Nutzer:inner bewusst nicht zu erfüllen. Im Gegenteil sollen diese eine gewisse Irritation in Bezug auf die ästhetische Ansprache der gesamten Applikation erfahren.

                    Das ist notwendig, damit im Anschluss ein Reflexionsprozess in Gang gesetzt werden kann. Zum einen darüber, wie eine Variante dieses Schlagabtauschs in der heutigen Zeit mit den heutigen Medien ausgetragen werden könnte, zum anderen darüber, was heute in den (sozialen) Medien tagtäglich passiert und sich ebenso zu Skandalen steigern kann. Der Zielgruppe junger Menschen wird die Ästhetik vertrauter sein, wobei sich auch hier spätestens beim genaueren Lesen eine gewisse Diskrepanz 
                    
zwischen Sprache, Sprachduktus und bestimmten Inhalten auf der einen und dem Darstellungsstil auf der anderen Seite zeigt
.
                

                

                    

                        

                        
Auszug aus einem Skandalverlauf

                    

                

                
Die ästhetische Ansprache besteht hauptsächlich im Design und der grafischen Aufmachung der Seite. In allen Belangen wurden diese in Anlehnung an sensationsgierige Newsseiten, wie man sie etwa in der Boulevardpresse oder Yellow Press findet, sowie an Social-Media-Seiten gestaltet: Hierzu gehören eine schrille Farbgebung, übertrieben große fette Lettern, YouTube-Videos, Personendarstellungen in Form von Avatarbildchen, Ereignisse als Posts, ausgeschmückt mit vielen Emoticons usw. All dies soll auf heutige skandalträchtige Onlinemedien hinweisen. Inhaltlich wurde gleichfalls versucht, das Wortgefecht auf die Spitze zu treiben. Dafür haben die Mitarbeiter:innen des Projektes verbal drastische Zitate für die Überschriften und Textausschnitte der Skandalereignisse ausgewählt und als Schlagzeilen formuliert. Über Emoticons
                    

                    sollen die Stimmung des Beitrages sowie die emotionale Verfassung der Skandalakteure verdeutlicht werden.
                

                

                    

                        

                        
Startseite von „Skandal-KULTUR reloaded“

                    

                

                
Vom Seitenaufbau her gibt es neben der Startseite eine Überblickseite der Skandale. Von dort aus gelangt man zur Einzelansicht des gewählten Skandals, die das Thema erläutert und die Skandalparteien mit den zugehörigen Akteuren sowie weiterführende Quellen auflistet. Diese Seite entspricht vom Design her einer Newsseite in einer Boulevard-Onlinezeitung. Von dort aus kann man dann über einen (unübersehbar großen) Button zum Skandalverlauf navigieren. Hier sind ähnlich einer vertikalen Infinite-Scroll

                    Zeitleiste links und rechts die jeweiligen Beiträge (und Reaktionen) mit Akteuren, O-Ton und Kommentierung in chronologischer Reihenfolge gelistet. Zudem gibt es zu jeder Akteurin / jedem Akteur einen Steckbrief, der sich öffnet, sobald man den Avatar anklickt. Ferner präsentiert eine Seite alle Akteure mit den zugehörigen Skandalen. Eine zusätzliche Seite präsentiert darüber hinaus über YouTube eingebettete Videos, die im Rahmen eines Praxisseminars an der Goethe-Universität Frankfurt entstanden sind und in denen Studierende die historischen Skandale aktualisiert haben. Die Videos bieten gegenüber dem systematischen Einstieg der Skandalübersichtsseite einen thematisch emotional motivierten Zugang zum Skandal, zu dem man über einen Link navigieren kann.
                

            

            

                

                    
Brisanz der Themen und Risiken der Interaktivität
                

                
Die Gegenstände der historischen Skandale haben nicht im Geringsten an Aktualität und Brisanz verloren. Die gefährlichen Konsequenzen von systematischer ‚Hate Speech‘ und von ‚Fake News‘ als absichtlich emotionalisierende Formen der (unwahren) Kommunikation sind insbesondere bei öffentlicher antisemitischer Hassrede, aber auch homophober Hate Speech unumstritten. Bei der Darstellung dieser Themen in einem Museumskontext, bei dem es um einen neuen, aktualisierenden Blick auf historische Ereignisse geht, besteht indessen immer die Gefahr, auf der Ebene der Präsentation gesellschaftliche und persönliche Toleranzschwellen zu übertreten und die Distanzierung von den Inhalten zu schwach zu markieren – zumal wenn man sich im Design der Social Media bewegt – und dann auch selbst in der Kritik zu stehen.

                
Darum bedarf es hierfür eines wohl durchdachten und kritisch geprüften Konzepts. Das gilt auch für das Anliegen von „Skandal-KULTUR reloaded“, bei dem nach der Fertigstellung des Grundkonzepts nun auch eine interaktive Erkundung der thematisierten Literaturskandale entwickelt wurde. Die Nutzer:innen können sich beispielsweise Personensteckbriefe anschauen oder Ereignisse im zeitlichen Verlauf in den Fokus nehmen. Das Kommunikationsmodell ist dabei in historischer Betrachtung wechselseitig (zwischen den Skandalparteien) und es ist für uns von außen beobachtbar. Jedoch wird es den Besucher:innen der virtuellen Ausstellung nicht ermöglicht, auf der Plattform sozial zu interagieren, sie können sich also nicht in das Skandalgeschehen einmischen. Die Inhalte und somit der Zustand der Applikation erfahren keine Veränderung im Nachhinein, wie dies bei offenen sozialen Medien der Fall wäre.

                
Im Kontext von Social Media könnte dies zwar sehr spannende (und spannungserzeugende) Wirkungen zeigen. Man stelle sich vor, die/der Besucher:in könnte im Ereignisverlauf die Beiträge liken oder disliken, teilen sowie diese und ganze Skandale kommentieren. Andere würden diese Kommentare up- und downvoten, und es könnten durch algorithmische Auswertungen interessen- und meinungsbezogene Netzwerke (Stichwort Filterblase, vgl. u.a. Pariser 2011) und Hierarchien der Museumsbesucher:innen entstehen. Indem man sich beispielsweise über Google, Facebook oder E-Mail registriert, könnte man mit anderen (ähnlich gesinnten) Nutzer:innen und den Akteuren in Kontakt kommen und so Follower finden – die Möglichkeiten sind vielseitig.

                    Bei einer solchen Erweiterung stünde jedoch neben rechtlichen Gründen und Regelungen im Internet der Aufwand der Moderation und nachhaltigen Pflege in keinem Verhältnis zum Umfang des Projektes. Auf inhaltlicher Ebene würde solch ein digitales historisches Social-Media-Museum mit ziemlicher Sicherheit einen Skandal in den realen heutigen Medien auslösen.
                

                
Dennoch bleibt die Idee des digitalen Museums mit mehr sozialer Interaktion der Besucher:innen. So möchte der Vortrag nach einer Präsentation des Projekts danach fragen, wie interaktiv solche Unternehmen sein sollten, wo die inhaltlichen und gestalterischen Grenzen der medialen Aktualisierung liegen und welche Strategien für eine ‚kontrollierte‘ Interaktivität entwickelt werden können. ‚Open Culture‘ im Kontext des Ausstellungsmachens bedeutet immer auch, solche Fragen offen auszutragen, um Szenographien zu entwickeln, die möglichst viele Besucher:innen erreichen und dabei zugleich das Wertesystem einer offenen Kultur achten.

            

        

            

                
Ein Plädoyer für mehr Offenheit 

                
Das Tagungsmotto 
                    
Offenheit
 hat für die Provenienzforschung
 eine große Relevanz und bezieht sich unter anderem auf den offenen Umgang mit Lücken in Provenienzangaben, die z.B. auf eine:n unbekannte:n Besitzer:in hinweisen. Damit Angaben den bekannten Informationsstand widerspiegeln und korrekt bewertet werden, müssen Lücken deutlich gekennzeichnet sein – auch im digitalen Raum. Aufgrund dieser gesteigerten Bedeutung widmet sich der Vortrag der Lücke
                    
:
 Wie werden Provenienzlücken
 im digitalen Raum abgebildet und welche weiteren provenienzbezogenen Lücken lassen sich identifizieren? Warum müssen Lücken vor allem im Digitalen thematisiert und gekennzeichnet werden? Dazu werden verschiedene museale Online-Kataloge untersucht, wobei die Provenienzangaben auf Basis des 
                    
Leitfadens zur Standardisierung von Provenienzangaben
 (fortan:
                    
 Leitfaden Standardisierung)
, herausgegeben vom 
                    
Arbeitskreis Provenienzforschung e.V.
, bewertet werden. 
                

                
Die Auswahl der Kataloge erfolgte anhand folgender Kriterien: eine umfassende und gut aufbereitete Online-Sammlung, (sichtbare) Provenienzforschung am Museum, Beispiele aus verschiedenen deutschen Bundesländern.
 Die ausgewählten Werke zeigen eine Bandbreite, wie Lücken in Provenienzketten abgebildet werden, und stammen von Künstler:innen, die häufig Gegenstand der Provenienzforschung sind. Die Bewertung der Objekteinträge erfolgte anhand folgender Fragestellungen: Sind Provenienzinformationen vorhanden? Folgen sie dem im 
                    
Leitfaden Standardisierung
 veröffentlichten Standard? Wurden Lücken explizit gekennzeichnet? Sind sie verständlich? Spiegelt der Objekteintrag den Informationsstand wider und verlinkt auf interne und externe Quellen?

                

                
Lücken sind ein Bestandteil vieler Wissenschaften: Im Kontext von Archiven und Sammlungen beschäftigen sich Forschende intensiv mit Lücken (Farrenkopf et al. 2021); in der Kunstgeschichte wird das fragmentarische Objekt thematisiert (Schädler-Saub/Weyer 2021) und gefragt, wie man über Objekte schreibt, die abwesend sind (Fricke/Kumler 2022). In der Provenienzforschung wird auch der Umgang mit und die Bewertung von (Informations-)Lücken im Rahmen von Provenienzrecherchen diskutiert (Geldmacher/Kulbe 2022). Die Aufgabe der Vermittlung von Forschungsergebnissen und Provenienzen – auch im digitalen Raum – wird zudem einschlägig besprochen (Türnich 2019). Hierbei findet auch eine Beschäftigung mit musealen Webseiten, im Besonderen mit Online-Sammlungen, und der Darstellung von Provenienzangaben statt (Haffner 2020; Haffner 2019).
 Grundsätzlich ist die digitale Erfassung und online Veröffentlichung von musealen oder universitären Beständen ein wichtiger Forschungsgegenstand, der zudem Fragen nach Zugang und Verhältnis von Original und Digitalisat miteinbezieht (Andraschke/Wagner 2020). Vorhandene Forschungsbeiträge liefern eine wichtige Grundlage für den vorliegenden Beitrag. Eine dezidierte Auseinandersetzung mit Lücken in Online-Katalogen und ihrer Problematik im Digitalen erfolgte bisher aber nicht in ausreichendem Maß. Derzeit widmen sich zudem mehrere Initiativen historischen Forschungsdatenstandards (z.B. 
                    
NFDI4Memory
). Damit Lücken in diese Überlegungen mit einbezogen werden, muss eine Auseinandersetzung zu diesem Zeitpunkt stattfinden. Indem der Beitrag Defizite aufzeigt, sollen Bestrebungen hinsichtlich der Entwicklung und Etablierung notwendiger Standards für Provenienzangaben unterstützt werden
                    
. Schließlich fordert der Beitrag zu einer Zusammenarbeit der mit Daten arbeitenden Akteur:innen auf. Der im Oktober 2020 gegründete Verein 

                    
Nationale Forschungsdateninfrastruktur (NFDI e.V.) 

                    
und die darin agierenden themenübergreifenden Konsortien könnten die dafür notwendigen Plattformen bieten (NFDI).

                

            

            

                
Die Visualisierung von Lücken

                
Der 
                    
Leitfaden Standardisierung
 bietet Richtlinien für die Erstellung von Provenienzangaben, einschließlich Lücken (vgl. Abb. 1).
 Demnach sollen unbekannte Besitzstationen entweder mit 
                    
[…]
 oder dem Hinweis 
                    
Verbleib unbekannt
 gekennzeichnet werden. Hingegen sind einzelne unbekannte Informationen innerhalb einer Besitzstation wie Zeitraum oder Besitzer:in folgendermaßen zu kennzeichnen: Die Abkürzungen 
                    
o.D.
 (= ohne Datum) oder 
                    
o.J. 
(= ohne Jahr) markieren eine unbekannte Zeitangabe, 
                    
unbekannter Besitzer/Käufer
 entsprechend einen unbekannte:n Besitzer:in (Arbeitskreis Provenienzforschung 2018, 11-12, 15-18).
 Wurden diese Empfehlungen von Museen umgesetzt?
                

                

                

                    

                    
Abb. 1: Screenshot der Provenienzangabe für Lyonel Feiningers 
                    
Kirche von Niedergrunstedt
 (1919) (Arbeitskreis Provenienzforschung 2018, 29).

                

                

                
Der Online-Objektkatalog des 
                    
Germanischen Nationalmuseums (GNM)
 in Nürnberg umfasst derzeit über 168,000 Einträge (GNM a). Eine erste Durchsicht zeigt, dass der Katalog keine Provenienzangaben für die Objekte bereithält – eine erste signifikante Informationslücke. Ein:e interessierte:r Nutzer:in mag bald auf die Seite des Provenienzforschungsprojekts stoßen, das rund 1,300 Objekte umfasst (GNM b). Die Rechercheergebnisse werden allerdings nicht im Objektkatalog, sondern in einer dazu separaten Datenbank präsentiert, wo für jedes Objekt ein Eintrag mit umfassenden Provenienzinformationen einschließlich Abbildungen angelegt wurde (GNM c). Abb. 2 zeigt beispielhaft die Provenienzangabe für 
                    
Die Muttergottes mit der Meerkatze
 (spätes 16. Jhr.) eines Augsburger Dürernachahmers (GNM d). Erkennbar ist, dass das Museum weitestgehend den Empfehlungen des 
                    
Leitfadens Standardisierung 
folgt und unbekannte Elemente mit 
                    
Unbekannte(r) Vorbesitzer
 (anstatt 
                    
Unbekannter Besitzer
) und fehlende Besitzstationen mit 
                    
Verbleib unbekannt
 kennzeichnet (Arbeitskreis Provenienzforschung 2018, 15,18). Das 
                    
Städel Museum
 in Frankfurt visualisiert letzteres mit 
                    
…
 und weicht dadurch etwas von den im 
                    
Leitfaden Standardisierung
 alternativ vorgeschlagenen Auslassungspunkten in eckigen Klammern ab (Arbeitskreis Provenienzforschung 2018, 15): Der Eintrag für Max Liebermanns (1847-1935) 
                    
Freistunde im Amsterdamer Waisenhaus
 (1881-1882) zeigt dies exemplarisch (vgl. Abb. 3) (Städel Museum). Der Eintrag für 
                    
Die Muttergottes 
enthält zudem einen Link zur entsprechenden Seite im Objektkatalog (GNM d). Dort findet man keine Provenienzangaben und auch ein Link zur Seite in der Datenbank des Provenienzforschungsprojekts fehlt. Obwohl umfassende Informationen zur Herkunft bekannt sind, zeigt der Objektkatalog eine erhebliche Informationslücke (GNM f).
                

                

                    

                    
Abb. 2: Screenshot der Provenienzangabe für 
                    
Die Muttergottes mit der Meerkatze
 (spätes 16. Jhr.), Augsburger Dürernachahmer (GNM b; GNM d).
                

                

                

                    

                    
Abb. 3: Screenshot der Provenienzangabe für Max Liebermanns 
                    
Freistunde im Amsterdamer Waisenhaus 
(1881-1882) (Städel Museum).
                

                

                
Die Sammlung Online des 
                    
Sprengel Museums
 in Hannover hält mehr als 19,000 Einträge bereit, darunter auch Max Beckmanns (1884-1950) 
                    
Stilleben [sic.] mit schiefer Schnapsflasche und Buddha
 (1939) (Sprengel Museum). Im Vergleich zu anderen Beispielen ist die Provenienzangabe für Beckmanns Stillleben sehr unübersichtlich und schwer verständlich (vgl. Abb. 4). Auffällig ist die Abwesenheit von Lücken; ob für das Werk tatsächlich eine vollständige Biografie vorliegt oder ob fehlende Informationen nur nicht abgebildet werden, ist schwer zu sagen. Sowohl Format als auch bereitgestellte Informationen entsprechen allerdings nicht dem Standard, der im 
                    
Leitfaden Standardisierung
 vorgeschlagen wird (vgl. Abb. 1). 
                

                
Weitaus transparenter ist das 
                    
Museum Folkwang
 in Essen, das neben genauen Angaben zur Provenienz einzelner Objekte auch Lücken deutlich kennzeichnet (Haffner 2019, 93). Die Sammlung Online hält aktuell über 93,000 Werke bereit, darunter auch Lyonel Feiningers (1871-1956) 
                    
Leuchtbarke I
 (um 1913) (Museum Folkwang a; Museum Folkwang b). Abb. 5 zeigt die Provenienzangabe für das Ölgemälde, deutlich erkennbar sind Lücken vor 1930 und zwischen 1930 und 1951, visualisiert durch einen Platzhalter in Form von eckigen Klammern und Auslassungspunkten. Die Darstellung weicht damit auch von den Empfehlungen des 
                    
Leitfadens Standardisierung
 ab. Hierin werden die beschriebenen Platzhalter für unbekannte Besitzstationen und die Abkürzungen 
                    
o.D.
 oder 
                    
o.J.
 (anstatt 
                    
[…]–[…]
 und 
                    
[…]–04.1951
) für unbekannte Zeitangaben vorgeschlagen (Arbeitskreis Provenienzforschung 2018, 15-17). Es sei zudem auf die zusätzlichen Vermerke und das Ampelsymbol hingewiesen.

                

                

                    

                    
Abb. 4: Screenshot der Provenienzangabe für Max Beckmanns 
                    
Stilleben [sic.] mit schiefer Schnapsflasche und Buddha
 (1939) (Sprengel Museum).
                

                

                

                    

                    
Abb. 5: Screenshot der Provenienzangabe für Lyonel Feiningers 
                    
Leuchtbarke I
 (um 1913) (Museum Folkwang).
                

                

                
Seit September 2022 enthält die Online-Sammlung der 
                    
Bayerischen Staatsgemäldesammlungen (BStGS)
 ausführliche Provenienzinformationen für über 1,200 Werke, weitere Einstellungen sollen folgen (BStGS a; BStGS b). Davor waren die Angaben zur Herkunft der Werke nur sehr rudimentär. Am Beispiel des Gemäldes 
                    
Die Falknerin 
(um 1880) von Hans Makart (1840-1884) wird dies deutlich. Abb. 6 zeigt den Eintrag in der Online-Sammlung vor der Aktualisierung: Er enthält eine Beschreibung des Werkes und weitere Objektinformationen. Ein Provenienzfeld fehlt, einzig ein Herkunftsvermerk informiert, dass das Werk „1962 als Überweisung aus Staatsbesitz erworben [wurde].“ Dass tatsächlich sehr viel mehr über die Provenienz des Werkes bekannt ist, war nicht ersichtlich. Darüber erfuhr man zum Beispiel in der 
                    
Lost Art
 Datenbank, in welcher das Gemälde seit 2007 als Fundmeldung gelistet ist (vgl. Abb. 7) (Lost Art b). Diese Information sowie ein Link zu 
                    
Lost Art
 fehlte in der Online-Sammlung der 
                    
BStGS
. Der neue Eintrag zeigt diese Lücken nicht mehr: Nutzer:innen finden nun umfangreiche Provenienzinformationen und einen Link zu 
                    
Lost Art
 (vgl. Abb. 8 und 9) (BStGS c). Entsprechend der im Leitfaden formulierten Standards werden unbekannte Zeitangaben mit 
                    
o.D.
 gekennzeichnet (Arbeitskreis Provenienzforschung 2018, 16-17). Problematisch ist, dass obwohl einige Übergänge als unsicher einzustufen sind, mögliche Lücken zwischen den Besitzstationen nicht explizit sichtbar sind.
                

                

                    

                    
Abb. 6: Screenshot des Eintrags für Hans Makarts 
                    
Die Falknerin
 (um 1880). Alte Version. Online nicht mehr verfügbar. Vgl. (BStGS c).
                

                

                

                    

                    
Abb. 7: Screenshot der Provenienz für Hans Makarts 
                    
Die Falknerin
 in 
                    
Lost Art 
(Lost Art b).
                

                

                
Die Beispiele zeigen, dass deutsche Museen teilweise Provenienzangaben zu den Objekten bereitstellen (Haffner 2019, 93), Umfang und Darstellungsform aber stark variieren, letzteres spiegelt sich auch in der Kennzeichnung von Lücken wider. Mögliche Gründe sind die Heterogenität der Sammlungen oder technische Limitationen der Museumsmanagement-Systeme. Diese sind für eine allgemeine Bestandsinventarisierung ausgelegt und nicht für die umfassende und strukturierte Erfassung von Provenienzangaben (Haffner 2019, 95-96). Auch rechtliche Restriktionen und eine allgemeine Besorgnis der Museen sind weitere mögliche Gründe (Haffner 2020, 38). Viele Museen verwenden noch Freitextfelder für Provenienzangaben, was zwar die Eingabe von unsicheren oder fehlenden Infos vereinfacht, aber eine Maschinenlesbarkeit erschwert. Letzteres wird im Moment z.B. durch das 
                    
Provenance Lab
 an der Leuphana Universität durch die computergestützte Produktion von 
                    
Provenance Linked Open Data
 adressiert, wobei u.a. die Dokumentation von Unvollständigkeit berücksichtigt wird (Libeskind 2022, 23-25). Auch die Entwicklung von Normdaten für die Provenienzforschung ist in diesem Zusammenhang zu nennen: Ein Projekt innerhalb des Konsortiums 
                    
NFDI4Objects
 soll sich der Erstellung von provenienzbezogenen Personen-Normdaten (NFDI4Objects) widmen. Dies lässt hoffen, dass die Etablierung eines Standards für Provenienzen in Online-Sammlungen in nicht allzu ferner Zukunft liegt. Standards würden nicht nur zur Sichtbarkeit von Lücken beitragen, sondern auch eine maschinelle Verarbeitung und Verknüpfung der Datensätze ermöglichen und damit auch neue Forschungsfragen generieren, z.B. ob es Muster hinsichtlich der Provenienzlücken gibt.
                

                
Die angeführten Beispiele haben weitere provenienzbezogene Lücken aufgezeigt, die über die Provenienzangaben selbst hinausgehen: Zum einen gibt es eine Diskrepanz zwischen den Angaben in den Objekteinträgen und den tatsächlich bekannten Provenienzinformationen, die oft auf separaten Seiten abgelegt sind. Auch einfache Verknüpfungen der Seiten fehlen in den digitalen Sammlungen der Museen. Bei der Analyse verschiedener musealer Online-Kataloge wurde zudem sichtbar, dass in der englischen Version die Provenienzangaben oder Teile der Objektinformationen weiterhin auf Deutsch erscheinen.
 Die Datenbank des Provenienzforschungsprojekts am 
                    
GNM
 fehlt in der englischen Version sogar völlig (GNM e). Dies ist für internationale Provenienzforschende und die ursprünglichen Eigentümer:innen der Werke und deren Nachkommen äußerst problematisch, da sie oftmals kein Deutsch sprechen. 
                

                

                    

                    
Abb. 8: Screenshot des Eintrags für Hans Makarts 
                    
Die Falknerin
 (um 1880). Aktualisierte Version (BStGS c).
                

                

                
                

                    

                    
Abb. 9: Screenshot der Provenienz für Hans Makarts 
                    
Die Falknerin
 (um 1880), Auszug. Aktualisierte Version (BStGS c). 
                

                

                
            

            

                
Schlussbemerkung: “Mind the Gap
                    
“

                

                
Warum müssen Lücken vor allem im Kontext des Digitalen besprochen und gekennzeichnet werden? Die Auseinandersetzung mit Lücken ist auch im Analogen unerlässlich, da es auch hier zu Fehlinterpretationen kommen kann. Im digitalen Raum scheint dieser Anspruch und die damit verbundene Sichtbarmachung von Lücken aber noch gesteigert: Aufgrund der Informationsmenge kann der Eindruck von Vollständigkeit geweckt werden. Etwaige Lücken können einfach durch Verknüpfungen mit externen Datensätzen geschlossen werden. Die konstante Datenproduktion erzeugt zudem eine hohe Dynamik und betont den Aspekt der Flüchtigkeit im digitalen Raum: Vorhandene Bild- oder Textdaten werden ständig durch neue oder ergänzende Daten überlagert; bestehende Lücken werden damit verwischt oder treten erst gar nicht in Erscheinung. Nutzende können zudem schnell zwischen einzelnen Seiten wechseln, was die Gefahr birgt, dass Inhalte nur selektiv wahrgenommen und Lücken deshalb schlichtweg übersehen werden. Aufgrund der Menge an Daten und der Flüchtigkeit des digitalen Raumes gilt “
                    
Mind the Gap“
. 
                

            

        

            

                
Einleitung

                
Das Ziel von 
                    
Data Feminism
 ist es, ausgehend von Positionen des intersektionalen Feminismus die vorwiegend männlich dominierten Narrative in den Digital Humanities kritisch zu reflektieren und bislang marginalisierten Stimmen insbesondere von Frauen* mehr Sichtbarkeit zukommen zu lassen. In den Geisteswissenschaften ist häufig zu beobachten, dass sich der weiß, cis-männlich und hegemonial dominierte Bias aus den Archiven und Quellen in die erhobenen Daten und digitalen Technologien überträgt. Die vom 
                    
Data Feminism
 angestrebte Balance im Sinne einer gendersensiblen Repräsentation ist ein wichtiger Baustein, um der Forderung nach einer grundlegenden Offenheit geisteswissenschaftlicher Forschung nachzukommen.
                

            

            

                
Data Feminism in den DH

                
Das 2020 von Catherine D'Ignazio und Lauren F. Klein veröffentlichte Buch 
                    
Data Feminism
 (MIT Press) hat für die Digital Humanities eine hohe Relevanz, wendet sich allerdings eher an ein Publikum aus der Data Science. Zwar bieten Kleins Forschungen durchaus Schnittstellen zu Fragestellungen in den DH, doch lässt sich aus dem Buch keine unmittelbar anwendbare Anleitung für die Umsetzung von Data Feminism in den Digital Humanities generieren. Vielmehr handelt es sich um ein Manifest, das vor allem die Wichtig- und Dringlichkeit von intersektionalen, datenfeministischen Ansätzen deutlich macht und grundlegende Leitsätze zu deren Umsetzung formuliert. Diese Leitsätze auf konkrete Forschungsfragen oder Datenrepositorien aus den Digital Humanities zu übertragen, ist nach wie vor ein Desiderat.
                

                
Hier setzt unser Workshop an. Gemeinsam mit den Teilnehmenden soll eine Handreichung erarbeitet werden, die grundlegende Informationen und Leitlinien bezüglich folgender Fragen zusammenstellt: 

                
1) Was lässt sich unter dem Begriff Data Feminism verstehen?

                
2) Wieso braucht man Data Feminism in den DH und inwiefern sollten dessen Ansätze in den bestehenden Diskursen der DH Berücksichtigung finden?

                
3) Wie können datenfeministische Projekte in den DH konkret aussehen? Inwiefern lassen sich Forschungsfelder definieren, die für Interessierte einen guten Ausgangspunkt bilden, um einige Ansätze des Data Feminism unmittelbar und praktisch umzusetzen?

                
Ein solche Handreichung sollte die Umsetzung datenfeministischer Ansätze in den DH insofern befördern, als sie die Einstiegshürden senkt und Interessierten diejenigen Informationen zur Verfügung stellt, die sie für einen effizienten, konkreten und praxisbezogenen Einstieg in Data Feminism benötigen. 

            

            

                
Forschungsstand: Offene Fragen und Desiderate im Kontext des Data Feminism in den DH

                

                    
Problemstellung 1: Data Gender Gap und das kulturelle Erbe

                    
Beim Arbeiten mit Quellen zum kulturellen Erbe wird eine Vielzahl von Problemen offenbar, die bei vielen anderen Datensätzen (die beispielsweise auf statistischen Erhebungen und Befragungen beruhen) nicht bestehen und die sich genuin aus dem geisteswissenschaftlichen Forschungsgegenstand ergeben: So lassen sich Positionen des Data Feminism zum Teil gar nicht auf historische Daten anwenden, wenn beispielsweise Informationen über Frauen* oder andere Gruppen, die der hegemonialen Norm nicht entsprechen, gar nicht erst dokumentiert wurden (Mandell 2019, Lampe 2021, Rezai 2022). Nicht nur in den materiellen Beständen von vielen GLAM-Institutionen, sondern auch in den daraus hervorgegangenen Datenbeständen sind Frauen* unterrepräsentiert (Flanders 2018, Wiens et al. 2020). Diese Problematik eines Gender Data Gaps (Criado-Perez 2020) spiegelt sich auch in den verfügbaren Normdaten wider, da – etwa in der GND – zunächst vor allem publizierende Personen systematisch erfasst wurden.
 Auch eine adäquate Verschlagwortung etwa in Bibliothekskatalogen stellt in diesem Zusammenhang eine große Herausforderung dar (vgl. Juen 2021).
                    

                    
Bevor also eine datenfeministische Analyse überhaupt beginnen kann, ist umfangreiche Erschließungsarbeit zu leisten und es ist je nach konkretem Anwendungsfall gegebenenfalls notwendig, bestehende Korpora um Quellen von oder über Frauen* zu erweitern. Ein Beispiel für solche Arbeiten, bei denen bestehende Datensätze mit bislang unterrepräsentierten Gruppen ergänzt werden, ist das Women Film Pioneers Project (WFPP) (https://wfpp.columbia.edu/, vgl. Wreyford/Cobb 2017, Dang 2020, Dickel et al. 2022, Gaines et al. 2022). In Themenfeldern, in denen Daten von Frauen* zwar vorhanden, aber noch tiefergehender erschlossen oder digitalisiert werden müssen, ist dies arbeitsaufwendig und wird erfordern, dass die DH auf Dauer ausreichend Energie und Fördergelder in solche Tätigkeiten investieren. Wenn es also um Datenfeminismus und intersektionale Arbeit im Kontext der Digital Humanities geht, handelt es sich oftmals noch gar nicht um Datenanalysen, sondern vielmehr um erschließungsbezogene Tätigkeiten. Denn es gilt zunächst, die statistischen Ungleichgewichte der uns zur Verfügung stehenden Datensätze zum Thema zu machen und in diesem Sinne auf Lösungsstrategien hinzuarbeiten.

                    
Eine weitere Herausforderung besteht hier ausgerechnet in der erstrebenswerten Offenheit von Daten, die inzwischen bei allen Förderinstitutionen und -richtlinien verlangt wird: Für marginalisierte und oftmals sensible Daten kann diese Offenheit aus Datenschutzgründen jedoch schnell zum Ausschlusskriterium werden. Hier müssen förderpolitisch Lösungen implementiert werden, die eine Erschließung und wissenschaftliche Nutzung ermöglichen, ohne den Datenschutz zu verletzen. Nicht zuletzt stellen Fälle, in denen Frauen* oder unterrepräsentierte Gruppen in Datensätzen unsichtbar, schwer sichtbar oder überhaupt nicht vorhanden sind, eine schwer zu lösende Aufgabe dar. Was ist mit Fällen, in denen es nicht möglich ist, diese Lücke durch neue Datenerhebungen zu füllen? Kann man unterrepräsentierte Daten aus bestehenden Datensätzen interpolieren? 

                

                

                    
Problemstellung 2: Modellierung, Kuratierung, Daten- und Korpuskritik 

                    
Positionen des Data Feminism halten bereits insofern Einzug in die DH, als in den vergangenen Jahren Hegemonie-kritische Forschungsthemen wie Gender-sensible Datenmodellierung, Korpuskritik, Decolonializing DH usw. stärkere Berücksichtigung finden (Risam 2015, Wernimont 2015, Koh/Stommel 2018, Losh/Wernimont 2018, Kim/Koh 2021, Guiliano/Heitmann 2019, Mandell 2019, Risam/Bordalejo 2019). Da ein feministischer und intersektional angelegter Umgang mit Daten in der Regel mit einer Analyse von Machtstrukturen beginnt, bieten die DH einen besonders geeigneten Nährboden für diesen Ansatz. Grundsätzlich gilt es nicht nur, dem Gender Data Gap durch die Erhebung von Daten zu Frauen* entgegenzuwirken, sondern es müssen auch die bestehenden Kategorien, Klassifizierungen und Datenmodelle auf Grundlage intersektionaler feministischer Perspektiven kritisch hinterfragt und ggf. komplexer gestaltet werden (Kyvernitou/Bikakis 2017) – dazu gehören Überlegungen zu Marginalisierungen an der Intersektion von Gender und rassistischen, klassistischen, ableistischen und weiteren Diskriminierungserfahrungen. . Denn personenbezogenen Daten lassen sich in der Regel nicht durch Selbstbezeichnungen und/oder zusätzliche Befragungen ergänzen – Gender-bezogene Daten sind bei historischen Datensätzen in der Regel Zuschreibungen, die auf äußerlichen Merkmalen beruhen. In aller Regel erlauben diese Datensätze weder eine Unterscheidung zwischen biologischem Geschlecht und Gender, noch gehen sie über das binäre Geschlechtermodell hinaus. Data Feminism muss in den DH nicht nur den Data Gender Gap im Blick behalten und die Digitalisierung von bislang nicht digitalisierten Quellen forcieren, sondern auch bestehende Klassifizierungen, Datenmodelle und -kuratierungen kritisch hinterfragen.

                

                

                    
Problemstellung 3: Anwendung außerhalb offensichtlicher Anwendungsfälle

                    
Online finden sich eine ganze Reihe von Arbeiten, die sich des Data-Feminism-Begriffs bereits bedienen, doch handelt es sich häufig noch um Work-in-Progress, das noch nicht in Form von wissenschaftlichen Publikationen veröffentlicht ist (Bui/Gleißner/Kühn/Nenninger 2021, Keck 2021a, 2021b, Klein 2018, 2022). Zudem ergibt sich hier die Auseinandersetzung mit Data Feminism häufig unmittelbar aus einer thematischen Schwerpunktsetzung heraus. Wie aber könnte man mit Datensätzen oder Forschungsfragen umgehen, die nicht für Ansätze des Data Feminism prädestiniert scheinen, aber dennoch von diesen Ansätzen in hohem Maße profitieren würden? Um ein solches Paradigma zu etablieren, genügt es nicht, wenn sich lediglich einzelne Projekte für datenfeministische Perspektiven öffnen, bei denen der Nutzen des Data Feminism unmittelbar auf der Hand liegt. Um die Erkenntnisse der Intersektionalitäts- und Datenfeminismus-Forschung konsequent in den DH zu etablieren – wie unter anderem von Roopika Risam 2015 gefordert –, werden Leitlinien benötigt, die aufzeigen, wie diese auf eine größere Breite an Datensätzen und Forschungsfragen Anwendung finden können. 

                

                

                    
Problemstellung 4: Maschinelles Lernen als Bias-Verstärker oder Chance?

                    
Zu fragen wäre schließlich, ob in diesem Kontext Algorithmen des maschinellen Lernens Abhilfe leisten könnten. Bisherige Publikationen zur Daten- und Korpuskritik in Bezug auf marginalisierte Gruppen halten sich diesbezüglich eher bedeckt und betonen, dass Algorithmen vor allem Bias der sie vornehmlich programmierenden (weißen cis-männlichen) Gruppen (Klinger/Svensson 2021) reproduzieren (Guiliano/Heitmann 2019). 

                    
Andere Publikationen verweisen allerdings darauf, dass die Aufgabe der DH in Bezug auf maschinelles Lernen gerade darin besteht, den Maschinen jenen “computer science gaze” abzutrainieren und stattdessen einen “curatorial gaze” zu etablieren (Bönisch 2021). Zu fragen wäre folglich, ob Algorithmen des maschinellen Lernens programmiert werden könnten, die trotz der in Problemstellung 1 und 2 skizzierten Herausforderungen einen “intersectional decolonialist gaze” auf Daten des kulturellen Erbes etablieren.

                

            

            

                
Aufbau des Workshops

                
Ziel dieses Workshops ist zum einen die Vernetzung aller Forschenden, die Positionen des Data Feminism bereits in ihren Forschungen berücksichtigen oder perspektivisch berücksichtigen wollen. Zum anderen geht es um die Erarbeitung konkreter Umsetzungsstrategien, wie sich die Forderungen des Data Feminism im Kontext der DH realisieren lassen. Der Workshop nennt sich Hackathon, um die kollektive Arbeitsform im Sinne einer gemeinsamen Daten-bezogenen Arbeitspraxis zu betonen. Zu Anfang werden zunächst die grundlegenden Konzepte des Data Feminism vorgestellt, um den Einstieg auch Personen ohne Vorerfahrungen zu ermöglichen. Im Anschluss können Teilnehmende eigene Arbeiten kurz vorstellen und ihren Zugriff auf Data Feminism im Kontext der DH erläutern. Aus diesen Inputs ergibt sich dann ein Austausch in Kleingruppen. Aufbauend auf den Erläuterungen und Diskussionen zu Beginn erarbeiten die Teilnehmenden in der Folge gemeinsam eine Handreichung, die auf wenigen Seiten grundlegende Informationen zum praktischen datenfeministischen Arbeiten in den Digital Humanities vermittelt und konkrete Leitlinien enthält. Auf diese Weise können sich Interessierte durch die Lektüre grundlegende Kenntnisse des Data Feminism selbstständig aneignen. Wir hoffen, dass damit die Einstiegshürde in datenfeminisitische DH-Forschung gesenkt und Data Feminism zukünftig in den DH einen breiteren Rückhalt finden wird. Die Handreichung soll idealerweise auf Deutsch und Englisch herausgegeben werden. Um all diese Aspekte (Einführung in grundlegende Konzepte, Kurzvorstellung eigener Arbeiten, Kleingruppendiskussion sowie Erstellung der Handreichung) zeitlich abdecken zu können, werden zwei Halbtage für den Workshop veranschlagt.

            

            

                
Organisationsteam

                
Sarah Lang, Universität Graz, 
                    
sarah.lang@uni-graz.at
, 
                    
. Forschungsinteressen/Hintergrund: Digital Humanities PostDoc, Wissenschaftsgeschichte (Alchemie).
                

                
Luise Borek, Technische Universität Darmstadt, 
                    
luise.borek@tu-darmstadt.de
, 0000-0001-5849-374X. Forschungsinteressen/Hintergrund: PostDoc (TU Darmstadt) Vertretungsprofessur Digital Humanites (Universität Graz); germanistische Mediävistik. DFG-Netzwerk Offenes Mittelalter.
                

                
Nora Probst, Universität zu Köln, Deutschland, 
                    
nora.probst@uni-koeln.de
, 0000-0001-6932-0879. Forschungsinteressen/Hintergrund: Digitale Kulturwissenschaften, Vertretungsprofessorin der Professur "Kulturen der Digitalität".
                

                
Alle Einreichenden verbindet durch ihre Aktivitäten in der AG Empowerment das Interesse an Korpuskritik und Datenfeminismus.

            

        

            

                
Einführung

                

                    
Format:
 Workshop, ganztags
                

                

                    
Gruppengröße
: max. 30 Teilnehmende
                

                

                    
Techn. Ausstattung
: Beamer, bevorzugt digitales Whiteboard oder Pinnwände &amp; Medienkoffer, evtl. einen weiteren Raum für Gruppenarbeit, ausreichend Steckdosen für Laptops
                

                
Bei den Einreichenden handelt es sich um Vertreter*innen von Datenzentren und universitären Infrastruktureinrichtungen, die Mitglied in der DHd AG Datenzentren sind. Ihre Aufgabe ist es u. a. Forschende bei der Entwicklung und Umsetzung des Forschungsdatenmanagements (FDM) in den Geistes-, Sozial- und Kulturwissenschaften zu unterstützen sowie Forschungsinfrastrukturen und Daten für diese Disziplinen bereitzustellen. Dabei fallen häufig Beratungs- und Kompetenzvermittlungsaufgaben an, die tief in die Forschungsprozesse der Wissenschaftler*innen hineinreichen und Fragen nach Art und Umfang der Dokumentation der Forschungsdaten aufwerfen. Während die Einreichenden im Rahmen des Workshops ihre disziplinäre und infrastrukturelle Expertise und Erfahrung aus der Projektbegleitung und -durchführung einbringen, werden Forschende der Geistes- und Kulturwissenschaften aus ihren Erfahrungen bei der Erstellung und / oder Nachnutzung von Forschungsdaten berichten und im Datathon Datensätze bereitstellen, die sie selbst erstellt oder in eigenen Projekten nachgenutzt haben. Hieraus sollen perspektivisch Anforderungen auch an die Infrastrukturangebote der Einreichenden abgeleitet werden.

            

            

                
Workshopkonzept

                
Die Veröffentlichung von Forschungsdaten, d. h. von Daten, die im Rahmen der Planung, Durchführung und Dokumentation wissenschaftlicher Projekte entstehen, erlebt eine Konjunktur. Diese liegt einerseits in wachsenden Anforderungen seitens der Fördermittelgebenden begründet, die in zunehmendem Maße von den durch sie finanzierten Forschungsvorhaben eine Bereitstellung der Datenbasis als Fundament wissenschaftlicher Arbeit erwarten. (Vgl. DFG 2015; ERC 2017) Andererseits kann die steigende Zahl durch eine sich erweiternde Methodologie in den Geisteswissenschaften, d. h. in einer Hinwendung zu daten- und rechnergestützten Forschungsmethoden erklärt werden: Digitale Editionen, Text Mining oder Bildähnlichkeitsanalysen stellen heute zwar eher noch Ausnahmefälle dar, rücken aber zunehmend in das Methodenrepertoire geisteswissenschaftlicher Forschung vor. Somit steigt die Zahl an Datensätzen, die im Rahmen derart gelagerter Projekte entstehen und für eine begleitende Publikation in Frage kommen. In der Folge ergibt sich immer häufiger die Möglichkeit, eben diese Daten als Grundlage für neue Forschungsprojekte zu verwenden.

                
Vor dem Hintergrund einer zunehmenden Zahl an potenziell verwendbaren Forschungsdaten mag es verwundern, dass bislang eher selten auch eine Nachnutzung dieser Daten in neu gelagerten Forschungskontexten erfolgt. Es entsteht der Eindruck, dass „Success Stories“ im Bereich der Nachnutzung insbesondere bei geisteswissenschaftlichen Forschungsdaten ein Desiderat darstellen. Ein Grund hierfür mag darin liegen, dass eine strukturierte und detaillierte Form der Datendokumentation bislang wenig im Fokus stand. (Vgl. Daudrich 2018, 13) Entsprechend fehlen fach- bzw. methodenspezifische Best Practice-Modelle, wie sie zunehmend im Kontext generischer Ansätze formuliert werden. (Vgl. dazu z. B. CESSDA 2020, Kap. 2. Organise &amp; Document; Dierkes 2021) Eine strukturierte, standardisierte Dokumentation ist jedoch zwingend erforderlich, um Daten in neuen Projekten nachnutzen zu können. Insbesondere die Grundlagen der Datenerhebung (Auswahl, Begrenzungen, Ursprung, Datenqualität, Prozessierungen usw.) müssen nachvollziehbar sein, um eine spätere Verwendung überhaupt erst zu ermöglichen. Dabei wird deutlich, dass selbst hinsichtlich der Ziele, der Definition und der Grundelemente einer Datendokumentation keine einheitliche Auffassung besteht.

                
Ein Kernelement im Bereich der Dokumentation stellt die Beantwortung und sukzessive Anpassung eines Datenmanagementplans (DMP) dar. Verstanden als „Living Document“, kann ein DMP dazu beitragen, die in einem Projekt verwendeten Daten, Software und Methoden detailliert darzustellen und die aus dem Projekt resultierenden Forschungsdaten damit zu kontextualisieren. Während die Erstellung eines DMP inzwischen vermehrt seitens der Fördermittelgebenden als obligatorisch betrachtet wird, besteht weitgehend noch keine Pflicht, diesen zusammen mit den Forschungsdaten zu veröffentlichen. Im Sinne der Nachvollziehbarkeit aller im Projekt unternommenen Schritte wäre die Veröffentlichung des DMP als Beitrag zur Dokumentation jedoch anzuraten.

                
Einen weiteren Baustein hinsichtlich der Dokumentation von Forschungsdaten stellt ihre umfassende Beschreibung mit Metadaten dar. Der Grad der Nachnutzbarkeit ist dabei in hohem Maße davon abhängig, welches Metadatenschema verwendet wird und in welcher Detailtiefe es befüllt wird – gerade abseits der (häufig) geringen Zahl an Pflichtfeldern. Aber auch ein vergleichsweise umfangreich angelegtes Metadatenschema wie das weltweit und disziplinübergreifend verbreitete DataCite (Vgl. Brase u. a 2015) bietet nur begrenzte Möglichkeiten, Angaben zu verwendeter Software, Modellen und Methoden im Feld “Description” in Freitextform und damit nicht strukturiert zu tätigen. (Vgl. DataCite 2021) Ein tiefergehendes Verständnis des Datensatzes und die Nachvollziehbarkeit seines Entstehungsprozesses wird damit zwar angedeutet, jedoch nicht in Gänze ermöglicht. Es offenbart sich in diesem Kontext eine Kluft zwischen den Anforderungen von Datenzentren auf der einen (Metadatenqualität) und Forschenden, die die Daten nachnutzen möchten, auf der anderen Seite (ausführliche Dokumentation des Entstehungsprozesses).

                
Neben Datenmanagementplänen und beschreibenden Metadaten bedarf es für die Nachnutzung von Forschungsdaten jedoch weiterer Hilfsmittel. Hier lohnt ein Blick in andere Fachbereiche, in denen die komplementäre Bereitstellung von Materialien wie Codebüchern, elektronischen Laborbüchern oder Data-Curation-Profiles bereits gängige Praxis ist. (Vgl. Heuer u. a. 2020; Hermann u. a. 2018; Jensen 2012)

                
Im Rahmen des Workshops soll dieses Desiderat rund um das Thema Datendokumentation aufgegriffen und mit den Teilnehmenden diskutiert werden. In einem ersten Schritt wird das Ziel verfolgt, eine Arbeitsdefinition herzustellen, um eine gemeinsame Vorstellung davon zu erhalten, was unter „Datendokumentation“ zu verstehen ist, welche Komponenten (z. B. DMP, Metadaten, Codebook) zwingend erforderlich sind und welche dagegen eher optionalen Charakter besitzen. Darauf aufbauend soll praxisnah ergründet werden, welche Formen der Dokumentation benötigt werden, um nicht nur die Auffindbarkeit von Forschungsdaten, sondern auch ihre Nachnutzung zu vereinfachen bzw. überhaupt zu ermöglichen. In diesem Kontext wird auch zu diskutieren sein, wer – d. h. Forschende oder Kuratierende – für die Dokumentation der Daten verantwortlich zeichnet. Schließlich wird als weiteres Ziel des Workshops vorgegeben, ein besseres Verständnis davon zu erlangen, welche Informationen zwingend Teil einer Datendokumentation sein sollten (z. B. Kontext der Erhebung, Erhebungsmethode, Struktur der Daten und deren Beziehung zueinander). Der Workshop bezieht die Perspektive der Infrastruktureinrichtungen ein (z. B. Repositoriumsbetreibende, Datenzentren) und kann dazu dienen, einen Überblick zu bereits bestehenden Formen der Datendokumentation zu erhalten.

                
Die DHd-AG Datenzentren hat sich in bisher zwei verschiedenen Veranstaltungen
 mit der Dokumentation von Forschungsdaten beschäftigt. Dabei wurden vor allem die Herausforderungen der datenhaltenden Institutionen diskutiert, die besonders in der Standardisierung und effizienten Ausgestaltung von Workflows zur Dokumentation von Forschungsdaten liegen. Ziel des hier eingereichten Workshops ist dagegen die Perspektive der Nutzer*innen selbst. Gezielt soll für unterschiedliche, aber typische geisteswissenschaftliche Daten die Dokumentation von Forschungsdaten hinsichtlich ihres Informationswertes, der Verständlichkeit, der Vollständigkeit und ihres tatsächlichen Gebrauchswertes zur Nachnutzbarkeit geprüft werden.
                

                
Längerfristiges Ziel ist die Entwicklung von Standards und Guidelines, die Nutzer*innen und Forschungsdatenzentren in die Lage versetzen, aussagekräftige Dokumentationen von Forschungsdaten zu erstellen. Im Mittelpunkt des Workshops stehen daher die Analyse von Use Cases zur Dokumentation von Forschungsdaten, die aus der Ersteller- wie aus der Nachnutzungsperspektive diskutiert werden sollen, und die Erarbeitung eines Dokumentationsschemas für die einzelnen Datentypen.

                

                    
Workshop-Programm

                    
Der eintägige Workshop der DHd-AG Datenzentren gliedert sich in zwei Teile. Am Vormittag werden nach einem einführenden Vortrag durch die Organisator*innen zu den Zielen und zentralen Fragen des Workshops vier Praxisbeispiele präsentiert, die erläutern, welche Daten sie mit welchen Zielen und Methoden nachgenutzt bzw. weiterverarbeitet haben und welche Probleme sich ihnen aufgrund mangelnder oder gar fehlender Dokumentation gestellt haben. Nach jeder Präsentation soll in einer Diskussion gemeinsam mit dem/der Referent*in auf einem digitalen Whiteboard zusammengetragen werden, welche Aspekte in der Datenver- und -aufbereitung in diesem konkreten Fall hätten dokumentiert werden sollen, um die geschilderten Probleme zu vermeiden. Bewusst wurde eine spezifische Vielfalt an Fallbeispielen ausgewählt, um eine hinreichende Breite für geisteswissenschaftliche Dokumentationstypen zu analysieren. Dabei sind für jeden Vortrag 20 Minuten Referat und 20 Minuten Diskussion vorgesehen.

                    
− Für den Bereich der quantitativen Daten wird Paul Beckus (Historiker an der Martin-Luther-Universität Halle/Wittenberg) aus der datenerstellenden Perspektive berichten, welche Fragen und Probleme sich ihm bei der Dokumentation von Datensätzen ergaben und welche Unterstützungsangebote sich ein historisch arbeitender Wissenschaftler in diesem Prozess erhofft. (Vgl. Beckus 2021) 

                    
− Aline Deicke (Professorin für Digital Humanities, Philipps-Universität Marburg / Digitale Akademie, Akademie der Wissenschaften und der Literatur Mainz) wird für den Bereich Netzwerkanalyse aus ihrer Arbeit zur Analyse der "Streitkultur" in innerprotestantischen Auseinandersetzungen anhand polemischer Flugschriften (Vgl. Deicke 2017) berichten. 

                    
− Für die Bildwissenschaften wird Stefanie Schneider (Wissenschaftliche Assistentin für Digitale Kunstgeschichte an der Ludwig-Maximilians-Universität München) am Beispiel von ARTigo – Das Kunstgeschichtsspiel (https://www.artigo.org) nicht nur aus einer Außen-, sondern ebenso aus einer Innenperspektive heraus berichten und skizzieren, wie sich Datenbereitstellung und -dokumentation im Laufe der Versionen verändert haben.

                    
− Abschließend wird Yvonne Rommelfanger (Datenkuratorin am Servicezentrum eSciences der Universität Trier) für den Bereich der qualitativen Daten am Beispiel der (Re-)Retrodigitalisierung der Edition der Kabinettsprotokolle des Landes Nordrhein-Westfalen (http://protokolle.archive.nrw.de/), von der Datenaufbereitung für die online-Publikation berichten.

                    
Nach den Berichten sollen in einer halbstündigen Gruppenarbeit alle gesammelten Aspekte zur Datendokumentation gesichtet und versucht werden, eine erste Kategorisierung auf einem gemeinsam zu bearbeitenden Whiteboard vorzunehmen. Die Ergebnisse werden im Plenum diskutiert und zusammengeführt. Hierzu wird ein Schema
 verwendet, das die Sicht der Datenzentren repräsentiert. Beides dient als Grundlage für die nachfolgenden Gruppenarbeiten am Nachmittag während des Datathons. Beim Datathon stellt jeweils eine Person einen Datensatz vor und macht einen Vorschlag für ein Nachnutzungsszenario, anhand dessen die Gruppen gemeinsam versuchen, den Datensatz zu verstehen und das Dokumentationsschema weiterzuentwickeln. In der Gruppenarbeit sollen sie einerseits feststellen, welche Informationen fehlen und hierfür Anforderungen formulieren, und andererseits überlegen, was sie dokumentieren müssten, damit ihre Ergebnisse wiederum verstehbar und nachnutzbar werden. Auf diese Weise sollen sie die Kategorien und Aspekte der Datendokumentation auf dem Whiteboard überarbeiten und weiterentwickeln. Für die Bereitstellung eines Datensatzes haben sich bislang folgende Personen bereit erklärt:
                    

                    
− Tinghui Duan vom DFG-Graduiertenkolleg “Modell Romantik” an der Universität Jena – deutsche literarische Prosatexte des langen 19. Jahrhunderts (https://github.com/t-duan/dissertation/tree/main/data)

                    
− Svenja Guhr vom Institut für Sprach- und Literaturwissenschaft der Technischen Universität Darmstadt (fortext lab) – deutsche literarische Prosatexte d-Prose 1870 1920 (Vgl. Gius u. a 2021)

                    
− Mareike König vom Deutschen Historischen Institut Paris - Adressbuch Deutscher in Paris von 1854 (https://perspectivia.net/publikationen/quellen​/adressbuch) &amp; Inventar der Korrespondenz der Constance de Salm (1767 1845) (https://constance-de-salm.de/)

                    
− Katrin Moeller vom Historischen Datenzentrum Sachsen-Anhalt an der Martin-Luther-Universität Halle/Wittenberg – Interviewdaten der BOLSA-Längsschnittstudie (https://bolsa.uni-halle.de/suche/)

                    
− Julia Röttgermann vom Trier Center for Digital Humanities an der Universität Trier - Französische Romane des 18. Jahrhunderts (https://github.com/​MiMoText/roman18/tree/master/XML-TEI)

                    
Neben den hier genannten können auch andere Teilnehmende des Workshops Datensets für den Datathon mitbringen.

                    
Nach Abschluss der Gruppenarbeiten werden die Ergebnisse im Plenum präsentiert. Zum Abschluss wird die Workshop-Gruppe diskutieren, welche weiteren Schritte notwendig sind, um erste Empfehlungen für die Dokumentation von geisteswissenschaftlichen Forschungsdaten zu erarbeiten.

                

                

                    
Programmablauf

                    

                        

                            
Uhrzeit

                            
Programmpunkt

                        

                        

                            
09:00

                            
Begrüßung und Einführungsvortrag

                        

                        

                            
09:20

                            
Use Case 1: Quantitative Daten (20+20 Min.)

                        

                        

                            
10:00

                            
Use Case 2: Netzwerkanalyse (20+20 Min.)

                        

                        

                            
10:40

                            
Kaffeepause

                        

                        

                            
11:10

                            
Use Case 3: Bildannotationsdaten (20+20 Min.)

                        

                        

                            
11:50

                            
Use Case 4: Qualitative Daten/Re-Retrodigitalisierung (20+20 Min.)

                        

                        

                            
12:30

                            
Mittagspause

                        

                        

                            
13:30

                            
Kategorisierung der Aspekte der Datendokumentation auf der Basis der Berichte

                        

                        

                            
14:00

                            
Vorstellung, Diskussion und Zusammenführung der Gruppenergebnisse

                        

                        

                            
14:30

                            
Datathon

                        

                        

                            
16:00

                            
Kaffeepause

                        

                        

                            
16:15

                            
Vorstellung, Diskussion und Zusammenführung der Gruppenergebnisse

                        

                        

                            
16:45

                            
Next Steps

                        

                        

                            
17:00

                            
Ende

                        

                    

                

            

        

            
TEI-XML ist eine exzellente Technologie für digitale Editionen. Aber für die computationelle Analyse ist ein Korpus von XML-Dokumenten keine günstige Grundlage, weil Algorithmen erheblich komplexer werden, wenn statt linearem plain text ein XML-Baum durchlaufen werden muss. Zwar ist die Extraktion von plain text aus XML nicht aufwendig, aber für die Rückführung von Analyse-Ergebnissen in den XML-Baum gibt es bislang keine allgemeine Lösung. Dennoch ist eine solche in vielen Fällen wünschenswert, weil dann Analyse-Ergebnisse und die Struktur des Dokuments in einer allgemeinen Abfragesprache wie XQuery ausgewertet werden können. Es existieren Insellösungen zum Enrichment von TEI-XML, die auf bestimmte Tools ausgerichtet sind, etwa Spacy (Andorfer und Schlögl 2021, Meyer 2022), oder auf bestimmte Anwendungsfälle, etwa Alliterationen (Consalvi und Fumagalli 2022). Wünschenswert wäre jedoch, ganz allgemein Analyse-Tools für plain text entwickeln zu können und sie auch zum Enrichment von XML einsetzen zu können. Genau dies ermöglichen die hier vorgestellten 
                
StandOff Tools
.
            

            

                

                    
Komponenten für Annotationspipelines
                    

                

                
Die 
                    
StandOff Tools
 bestehen aus zwei Komponenten:
 dem Extraktor 
                    
E
 und dem Internalizer 
                    
I
. In einer Annotationspipeline ist zwischen 
                    
E
 und 
                    
I
 ein anwendungsspezifischer Plain-Text-Tagger 
                    
T
 geschaltet. (Abb. 1) 
                    
E
 extrahiert aus dem XML-Quelldokument plain text, der an 
                    
T
 geleitet wird. 
                    
I
 nimmt von 
                    
T
 gelieferte Referenzierungen von Plain-Text-Fragmenten entgegen und bringt sie als Inline-Markup stets so in den XML-Baum ein, dass wohlgeformtes XML herauskommt.
                

                

                    
E
 und 
                    
I
 sind insofern generische Komponenten, als dass ein beliebiger Tagger in eine Pipeline zum Enrichment von XML eingesetzt werden kann, solange er folgende Anforderungen erfüllt: a) Er muss plain text verarbeiten, b) er muss darin Textpassagen (ranges) per 
                    
character offsets
 referenzieren, genauer: nach den in RFC 5147, Abschnitt 2.1.1 und 2.2.2 beschriebenen Regeln für 
                    
character ranges
.
 Die vom Tagger ausgegebenen Ranges dürfen einander umfassen oder überlappen, so dass auch komplexe Strukturen in der XML-Quelle ausgezeichnet werden können. Die Ranges werden von 
                    
I
 so zerschnitten, dass Überlappungen mit anderen Ranges und dem internen Markup der XML-Quelle aufgelöst werden (Splitting).
                

                

                    

                        

                        
Abb. 1: Schema einer Annotationspipeline für TEI XML

                    

                

                
            

            

                

                    
Ansätze
                    

                

                
Die Komponenten 
                    
E
 und 
                    
I
 müssen so aufeinander abgestimmt sein, dass 
                    
I
 hinreichend Informationen hat, um die Annotationen von 
                    
T
 in das XML-Dokument einzubauen. Neben Ansätzen, die ein Sprachmodell, und sei es auch nur eine Tokenisierung, einführen und insofern nicht generisch sind (Schopper 2021, Andorfer und Schlögel 2021), lassen sich in der DH-Software-Entwicklung zwei Ansätze beobachten.
                

                
Nach dem ersten extrahiert 
                    
E
 die Textknoten und sammelt dabei Informationen darüber, wo Elemente anfangen und aufhören. Dies ist auf DOM-Ebene möglich. 
                    
I
 kann dann Blätter des DOM-Baums so ersetzen, dass an ihrer Stelle neue Teilbäume die (zerschnittenen) Annotationen darstellen. Diesen Ansatz verfolgen die Lösung von Meyer (2022) und der Prototyp von Lassner (2021). Sein Vorteil ist, dass gängige XML-Parser eingesetzt werden können. Sein Nachteil ist, dass manche Merkmale der serialisierten XML-Quelle nicht bewahrt werden können, weil sie nicht zur DOM-Spezifikation gehören: 
                    
character
 und 
                    
entity references
 sowie 
                    
whitespace
 in Tags.
                

                
Die 
                    
StandOff Tools
 verfolgen einen anderen Ansatz: Hier speichert 
                    
E
 Informationen über die Position, die jedes Zeichen des extrahierten Textes im XML-Quelldokument gehabt hat (
                    
offset mapping
). DOM-Manipulationen erfolgen nicht. Ein Parser ermittelt Positionsdaten aus dem XML-Quelldokument. Die Extraktion erfolgt dann im Wesentlichen durch Kopieren von Zeichenketten aus dem serialisiert vorliegenden XML-Quelldokument. 
                    
I
 kann aufgrund des 
                    
offset mapping
 die Positionsdaten des Taggers auf die XML-Quelle beziehen. Wie im ersten Ansatz zerschneidet 
                    
I
 die Annotationen. Das Splitting, das Herzstück des Algorithmus, basiert auf einer Auswertung der Positionsdaten (offsets), die der XML-Parser für das interne Markup und der Tagger 
                    
T
 für die Annotationen liefert. Anschließend werden aufgrund der Positionsdaten Portionen der XML-Quelle in die Ausgabe kopiert und neue Tags dazwischen gesetzt. Dieser Ansatz hat den Nachteil, dass kein üblicher XML-Parser eingesetzt werden kann. Aber er reproduziert diejenigen Merkmale der XML-Quelle, welche im ersten Ansatz verloren gehen.
                

            

            

                

                    
Einsatz für manuelle Annotationen
                    

                

                

                    
I
 kann auch allein betrieben werden, um manuelle Annotationen, die Passagen eines XML-Dokuments per character offsets referenzieren, zu internalisieren und zu anschließend zu visualisieren oder auszuwerten. Solche Annotationen können z.B. als Web Annotations (OA-Ontology) realisiert werden. Wenn dabei allerdings wie in CATMA eine schlichte Extraktion von Textknoten aus HTML oder XML erfolgt, gehen dabei Informationen verloren, welche für eine Internalisierung der Annotationen erforderlich sind. Insofern bleibt die Internalisierung von CATMA-Annotationen in TEI noch Desiderat (Cayless 2019). Dennoch weisen die verschiedenen neuen Entwicklungsansätze einen Weg abseits vom nicht-funktionalen Standoff-Konzept der TEI (Banski 2010), auf welchem der Mehrwert sowohl von Standoff-Annotationen als auch von TEI realisierbar ist.
                

            

        

            

                
NFDI4Culture

                
ist das Konsortium in der Nationalen ForschungsdatenInfrastruktur (NFDI
), das eine bedarfsgerechte Infrastruktur für Forschungsdaten zu materiellen und immateriellen Kulturgütern schafft.

            

            

                
Die in den Blick genommene Forschungslandschaft von Architektur-, Kunst-, Musik- bis hin zur Theater-, Tanz-, Film- und Medienwissenschaften ist durch starke Diversität gekennzeichnet. Sie umfasst Universitätsinstitute, Kunst- und Musikhochschulen, Akademien, Galerien, Bibliotheken, Archive, Museen und einzelne Forscher:innen ebenso Anbieter von Services und Infrastrukturen für diese Bereiche. Das Konsortium will ein Netzwerk etablieren, das an den FAIR-Prinzipien orientierte Angebote für alle Phasen des Forschungsdatenlebenszyklus von der Datengenerierung und -anreicherung über Datenanalyse und -archivierung bis hin zur Datendistribution und -nachnutzung sicherstellt. Eine Herausforderung sind dabei die sehr unterschiedlichen Anwendungsgebiete, Nutzungszwecke und Softwarelösungen, die in digital gestützten Prozessen berücksichtigt werden müssen, aber auch die häufig sehr unterschiedlichen Vorkenntnisse über einschlägige Verfahren des Forschungsdatenmanagements, Standards und nachnutzbare Angebote in einer bislang wenig vernetzt agierenden Fachgemeinschaft.

            

            

            
Profil des Helpdesks

            

                
Der NFDI4Culture-Helpdesk ist ein Beratungsangebot

                
, das eine individuelle, direkte Konsultation zu konkreten Fragestellungen ermöglicht und die Projektkontexte, institutionellen Gegebenheiten und Vorkenntnisse der Nutzenden berücksichtigt. Es ergänzt das Portfolio der Veranstaltungs- und Publikationsformate, mit denen NFDI4Culture Kenntnisse vermitteln und die Integration der Community voranbringen möchte. Der Helpdesk steht allen Akteuren der oben genannten Domänen zur Verfügung, unabhängig von Qualifikationsstufe oder institutioneller Zugehörigkeit. Die Beratung erfolgt gemäß den Schwerpunktthemen von NFDI4Culture:

            



                
Organisatorische und technische Aspekte der Digitalisierung von Kulturgütern (2D, 3D, Audio, Video, AR/VR)

                

                    
Datenqualität, Standards, Datenkuratierung

                

                
Umsetzung der FAIR-Prinzipien

                
Entwicklung, Konsolidierung, Betrieb und Zertifizierung von nachhaltigen, interoperablen Forschungswerkzeugen und Datendiensten

                
Publikationsprozesse, insbesondere für multimodale Publikationen und deren Archivierung in Repositorien

                

                    
datenrechtliche und ethische Fragen, etwa zum Urheberrecht, zu Eigentums- und Persönlichkeitsrechten

                    
, 

                    
zum Umgang mit Open Science oder mit kulturell sensiblen Objekten

                

                

                    
Inhalte, Organisation und Gestaltung von Schulungen, Workshops, Lehrmodulen etc. zu Data und Code Literacy

                

                
Informationen zu in Frage kommenden Förderprogrammen und Voraussetzungen für die Projektantragstellung 

                

                    
Planung des Forschungsdatenmanagements für Projekte und institutionelle Workflows

                    
 
                

            

            
Für die Beratung steht ein Team bereit, dessen Mitglieder durchweg selbst über Expertise in einem oder mehreren der Schwerpunktthemen verfügen. Eine Rechtsberatung im juristischen Sinne kann allerdings nicht durchgeführt werden.


            

                
Ablauf der Beratung

            

                
Der Helpdesk ist über das Kontaktformular auf der NFDI4Culture-Website erreichbar. Die oder der Ratsuchende beschreibt dort die Fragestellung und kann sie bereits einem Schwerpunktthema zuordnen. Die eingegangene Anfrage wird einem Mitglied des Helpdesk-Teams mit einschlägigen Kenntnissen zugeordnet, die oder der als „Datenlotsin“ oder „Datenlotse“ bearbeitet und über den gesamten Beratungsprozess steuernd begleitet. Bei komplexeren Anfragen zieht sie oder er weitere Expertise aus anderen Bereichen des Konsortiums oder auch von anderen vernetzten Forschungsdatenmanagement-Initiativen, Serviceeinrichtungen oder Fachleuten hinzu. Gleichzeitig ist sie oder er Ansprechpartner:in für die oder den Ratsuchenden und ist immer informiert über die Aktivitäten rund um die Beratung. 

            

            

                
Meist beginnt die Beratung mit einem Gesprächstermin, in dem das Problemfeld näher geschildert wird. Die Informationen werden auf Wunsch vertraulich behandelt. Die Bandbreite der Beratungen reicht von einmaliger Beantwortung einer konkreten Frage bis zur längerfristigen Begleitung eines Projekts von der Antragsphase an. 

                
Anfragen dürfen sich in alle Richtungen weiter entwickeln: Was beispielsweise als konkrete Frage zur Lizenzvergabe beginnt, kann in die Vermittlung einer Kooperation mit anderen Projekten münden.

            

            

                
Oft steht hinter einer Anfrage der Wunsch nach Vernetzung. Daher sieht der Helpdesk seine Aufgabe auch darin, Akteure, Projekte und Institutionen miteinander in Verbindung zu bringen, um so die Entwicklung der gesamte Domäne von NFDI4Culture zu unterstützen. 
 
            

            


            

                
Bedarfsgerechte Ausrichtung

            

                
Die Beratungsergebnisse werden protokolliert, denn sie dienen auch dazu, die Community und die Herausforderungen besser kennenzulernen. So können weitere Angebote von NFDI4Culture bedarfsgerechter ausrichtet werden. In regelmäßigen Treffen arbeitet das Team Helpdesk an der Optimierung seiner Services gemäß den Erfordernissen einer guten klientenzentrierten Beratung.

            

            


            

                
Akzeptanz des Angebots

            

                
Der Helpdesk-Service stößt auf gute Resonanz. Seit dem Projektstart von NFDI4Culture zu Jahresbeginn 2021 gingen 230 Anfragen ein (Stand 1.8.2022). Häufig beziehen sich Anfragen auf Aspekte der Projekt- und Workflowplanung bei der Digitalisierung, Erschließung und Sicherung von Beständen, auf rechtliche Themen, den Einsatz von Normdaten oder auf Softwaretools für bestimmte Anwendungskontexte der digital gestützten Forschung. Die überwiegende Anzahl der Anfragen stammte bis jetzt von Forscher:innen, die im institutionellen Kontext Projekte planen oder durchführen. Beachtlich ist aber auch das Interesse aus bestandshaltenden Institutionen und von Service- und Infrastruktureinrichtungen. Auch Anfragen aus Kontexten, die an den Schnittstellen zu anderen geisteswissenschaftlichen Disziplinen liegen, werden gern entgegen genommen. 

            

            

                
Zunehmend entsteht auch eine Zusammenarbeit mit anderen NFDI-Konsortien, den 

                
Landesinitiativen zum Forschungsdatenmanagement und ähnlichen Angeboten zum Aufbau 

                
effektiver und aufeinander abgestimmter Beratungsservices, so dass der Helpdesk hier einen wertvollen Beitrag zur Vernetzung dieser Initiativen leistet. 

            

            

                
Benötigen Sie Unterstützung zu Fragen rund um Forschungsdaten und –software? Kontaktieren Sie den NFDI4Culture-Helpdesk unter 

                
https://nfdi4culture.de/de/kontakt.html

            

            

           
        

            
Seit 2020 verbindet das Projekt 
                
Digitale Edition von Quellen zur habsburgisch-osmanischen Diplomatie 1500–1918
 (QhoD) schriftliche und dingliche Quellen zum diplomatischen Austausch zwischen dem Habsburgerreich und dem Osmanischen Reich. Der lange Zeitraum orientiert sich an der Aufnahme der diplomatischen Beziehungen der beiden Reiche zu Beginn des 16. Jahrhunderts bis zu ihrem politischen Ende 1918. Ediert werden Quellen zu fassbaren diplomatischen Missionen, die in diesen Zeitraum fallen. Bislang wurden bzw. werden vier solcher Missionen in Rahmen von Teilprojekten bearbeitet. Das QhoD-Projekt zeichnet sich durch eine breite Quellenbasis (sowohl quantitativ als auch medial und gattungsmäßig) und einen interdisziplinären Ansatz (Editionstechnik, Frühneuzeitforschung, Kunstgeschichte, Osmanistik, Turkologie) aus. Seit Juli 2022 sind die bislang bearbeiteten Quellen über &lt;
                

                    
https://qhod.net

                
> frei nutzbar. Das den Nutzern zur Verfügung gestellte Material wird regelmäßig erweitert.
            

            
Das Poster beschreibt die Quellen und die editorischen wie technischen Überlegungen zu ihrer Edition. Die technische Implementierung erfolgt mithilfe der GAMS repository software zur Datenaufbereitung und -archivierung. Die XML-Codierung erfolgt nach TEI-Standards.

            

                
Details zum Projekt:



                    
Die im Rahmen des Projekts bearbeiteten Quellen bestehen aus handschriftlichen Quellen (u.a. Briefe, Instruktionen, Berichte, Protokollregister, Befehle), gedruckten Quellen (u.a. publizierte Reiseberichte, Flugschriften, Zeitungsartikel, Karten) und Objekten (u.a. Militaria, Kunstgegenstände).

                    
Bearbeitet werden sowohl habsburgische als auch osmanische Quellen (aus heute öffentlichen wie privaten österreichischen bzw. türkischen Archiven).

                    
Von allen Quellen werden Faksimiles und bibliographische Metadaten bereitgestellt.

                    
Objekte werden zusätzlich kunsthistorisch beschrieben.

                    
Schriftliche Quellen werden transkribiert und annotiert (Personen, Orte, Datierungen, textkritischer Apparat) – es wird aber auch eine Lesefassung ohne Annotationen angeboten. Osmanische Texte werden zusätzlich zur Transkription ins Englische übersetzt. Deutsche Texte bleiben unübersetzt, erhalten aber, wie auch die osmanischen Texte, ein englisches Abstract.

                

            

            

                
Technische/editorische Details:



                    
Gedruckte Quellen werden mittels Transkribus HTR erstbearbeitet. Handschriftliche Texte werden vorerst direkt in XML codiert. Derzeit laufen Versuche zur Erkennung osmanischer Texte in Transkribus und dem Training eines entsprechenden Modells.

                    
Für die Transkription der osmanischen Texte orientiert sich das Projekt an den Richtlinien des 
                        
İslam Ansiklopedisi Transkripsiyon Alfabesi
.
                    

                    
Sammeln und Einspeisen von named entity data in das Austrian Prosopographical Information System (APIS), Anreichern mit GND-Identifiern.

                

            

            

                
Aktuell beinhaltet QhoD:



                    
4 Teilprojekte zu einzelnen diplomatischen Missionen

                    
nach Mission: 15 Schreiben Selims II. an Maximilian II. (1566-1574); 31 schriftliche Quellen zur Internuntiatur Johann Rudolf Schmid zu Schwarzenhorns (1649); 157 schriftliche und dingliche Quellen zu den gleichzeitig stattfindenden Großbotschaften Damian Hugo von Virmonts und Ibrahim Paschas (1719/20); Teilprojekt Nr. 4 zur Gesandtschaft Johann Jakob Kurtz von Senftenaus (1623/24) wurde erst kürzlich begonnen und befindet sich aktuell in der Materialsichtung

                    
sprachlich: 60 deutsche, 42 osmanische Texte

                    
nach Genre: 60 Briefe, 20 Protokollregister, 5 in LIDO beschriebene Objekte, 4 Reiseberichte, 4 Berichte, 3 Instruktionen, 16 behördliche Schriftstücke

                

                
QhoD versteht sich als offene Plattform zu Austausch und Sammlung von nachnutzbaren Editionsdaten mit dem thematischen Fokus auf die diplomatischen Beziehungen zwischen der Hohen Pforte und dem Wiener Kaiserhof vom Erstkontakt bis zum Ende beider Imperien. Dieser Kontakt bestand auch in Zeiten, in denen sich beide Reiche im Krieg miteinander befanden. Die edierten Quellen liefern somit einen wertvollen Zusatz zur bekannten (Militär-)Geschichte. Laufend werden neue Subprojekte hinzugefügt. Eine Kooperation zur Nutzung der QhoD-Infrastruktur ist nicht gebunden an bestimmte Institutionen, Arten von Fördermittel oder akademische Grade der Beitragenden (einige Transkriptionen wurden im Rahmen von Qualifikationsarbeiten nach QhoD-Richtlinien erstellt).

            

        

            

                
Hintergrund

                
Forschung zum Thema Open Science fokussiert typischerweise technische oder auch regulatorische und wissenschaftspolitische Aspekte (Vicente-Saez und Martinez-Fuentes 2018). Der Begriff der Openness bzw. der Offenheit selbst, der im Diskurs um digitale Forschung als zentrales Schlagwort fungiert, wird dabei eher selten in seiner Semantik reflektiert. Dabei führt der Begriff der Offenheit reichhaltige, auch alltagssprachlich verankerte Assoziationspotenziale mit sich, aus denen sich die ausgesprochen optimistische, vielleicht sogar utopisch-überhöhende Rahmung von Open Science (Dickel und Franzen 2015; Tkacz 2012) und Open Humanities maßgeblich speist.

                
Im interdisziplinären Projekt „Digitalisierung als Disruption von Wissenssystemen – Opening Knowledge“ (DiaDisK; Laufzeit 10/2021 – 03/2025) im Rahmen des EXU-Verbundes „Disruption and Societal Change“ an der TU Dresden fragen wir nach den disruptiven Auswirkungen der Digitalisierung in den für die Wissensgesellschaft zentralen Institutionen Universität, Bibliothek und Schule. Wir gehen davon aus, dass sich gerade im Prinzip der Offenheit und seiner diskursiven Verhandlungen Deutungsmuster manifestieren, welche die Disruptionen, die digitale Technologien für die traditionellen Routinen in Bildung und Wissenschaft mit sich bringen, positiv als Chancen, Transformationen usw. rahmen (Koch, Nanz, und Pause 2016, 19). Diese Deutungsmuster nehmen wir in einem der linguistischen Arbeitspakete des Projektes, das sich den Open-Science-Aktivitäten der Sächsischen Landesbibliothek – Staats- und Universitätsbibliothek Dresden (SLUB) widmet, aus einer korpuslinguistischen Perspektive in den Blick. Wir untersuchen, welche komplexen diskurssemantischen Profilierungen des Begriffs der Offenheit sich in diesem konkreten Diskurs nachweisen lassen, die seinen Schlagwortcharakter (Schröter 2011) grundieren. Damit soll eine Schärfung des Begriffs ermöglicht werden, die es erlaubt, die oft strikt nach Pro und Kontra geführten Debatten über Openness differenzierter zu führen, nicht zuletzt in wissenschaftspolitischen Kontexten. 

            

            

                
Daten und Methoden

                
Grundlage unserer Analysen bilden verschiedene Publikationen (Forschungsbeiträge, Geschäftsberichte, aber auch Blogbeiträge und Tweets) sowie Dokumente (Drittmittelanträge und Strategiepapiere) aus dem Umfeld der SLUB, die dezidiert aus der Perspektive der Institution formuliert sind. Wie prominent im Strategiepapier SLUB 2025 festgehalten ist, positioniert sich die SLUB proaktiv in der deutschsprachigen Bibliotheks- und Wissenschaftslandschaft als „Motor für offene Wissenschaft und Gesellschaft“ (Bonte und Muschalek 2019). Sie nutzt in ihren Selbstdarstellungen strategisch die vielfältigen, zumeist positiven Konnotationen des Begriffs(feldes) der Offenheit, weshalb sich diese für unsere Fragestellung besonders gut eignen.

                
Das laufend zu erweiternde Korpus (1,59 Millionen Tokens, Stand Juli 2022) stellen wir im Projekt in morphosyntaktisch annotierter Form über verschiedene digitale Analyseumgebungen wie die Korpusanalyseplattform CQPweb (Hardie 2012) und die kollaborative Annotationsumgebung INCEpTION (Castilho u. a. 2018) zur Verfügung. Für die Analyse nutzen wir zum einen korpuslinguistische Verfahren der datengeleiteten diskurssemantischen Analyse wie Kollokations- und Ngram-Analysen (Bubenhofer 2017), aber auch Word Embeddings als Verfahren der distributionellen Semantik mit der Software word2vec (Mikolov u. a. 2013; Kozlowski, Taddy, und Evans 2019), um die Gebrauchsprofile einschlägiger Lexeme zu erschließen. Zum anderen wählen wir mit der Frame-Semantik (Ziem 2020) einen stärker theoriegeleiteten Ansatz. Dabei handelt es sich um eine semantische Theorie, die die Bedeutung von sprachlichen Ausdrücken in Bezug auf das in untereinander vernetzten Frames organisierte Weltwissen der Sprechenden beschreibt. Frames sind dabei schematische Repräsentationen von Situationen oder Konstellationen, vor deren Hintergrund dann sprachliche Ausdrücke verstanden werden (Busse 2012). Ausgehend von der lexikographischen Ressource FrameNet (
                    
https://framenet.icsi.berkeley.edu/
) annotieren wir im Textmaterial die semantischen Valenzen einschlägiger Ausdrücke und der durch sie evozierten Frames, so dass die unterschiedlichen Lesarten etwa von 
                    
offen
 und die assoziierten Formulierungsmuster präzise erfasst werden können.
                

            

            

                
Erste Ergebnisse

                
Erste Ergebnisse zeigen, dass im untersuchten Diskurs das Begriffsfeld der Offenheit systematisch zwischen einer eher technisch auf digitale Daten und ihre Zugänglichkeit bezogenen Lesart, einer auch politisch aufgeladenen, auf Partizipation und Inklusion abzielenden Lesart sowie einer auf Offenheit als epistemische Tugend abzielenden Lesart changiert. Typische Kollokate (Assoziationsmaß Log Dice) von 
                    
offen
 sind etwa 
                    
Daten
, 
                    
Schnittstelle
 und 
                    
Standards
 im Sinne der technischen Lesart, 
                    
Kreativraum
 und 
                    
Austausch
 im Sinne der partizipativen Lesart, aber auch Abstrakta wie 
                    
Wissen
 und 
                    
Neugier
. Paarformeln und Aufzählungen wie 
                    
offene und freie Wissensgesellschaft
 oder 
                    
Offenheit, intellektuelle Freiheit und Redlichkeit
 zeigen, dass das Begriffsfeld der Offenheit mit anderen Schlagwörtern in Interaktion gebracht und so zusätzlich semantisch als ethisch gehaltvolle Zielnorm aufgeladen wird. Auch die Berechnung von Word Embeddings weist in diese Richtung. Als sog. Nearest Neighbors von 
                    
offen
, die im untersuchten Diskurs also semantische Ähnlichkeit aufweisen, werden 
                    
nachhaltig
, 
                    
transparent
, 
                    
interdisziplinär
, 
                    
nachnutzbar
, aber auch 
                    
kreativ
, 
                    
vernetzt
, 
                    
modern
 und 
                    
innovativ
 ausgegeben (Abb. 1); zu 
                    
Offenheit
 werden 
                    
Transparenz
, 
                    
Verbreitung
, 
                    
barrierefrei
, aber auch 
                    
demokratisch
 ausgegeben (die Visualisierung des Modells ist unter 
                    
https://kurzelinks.de/diadisk
 öffentlich zugänglich; auch auf Teile der Korpora kann auf Anfrage Zugriff gewährt werden).
                

                

                    

                    
Abb. 1

                

                
Für die framesemantische Annotation bietet sich u.a. der Frame Openness an, der auf das Moment der Zugänglichkeit abzielt und sowohl wörtliche als auch metaphorische Verwendungen zulässt. Dabei zeigt sich in einer Pilotierung am Beispiel der Forschungsbeiträge im Korpus (51 Texte), dass die Frame-Elemente THEME (für wen ist etwas offen?) und BARRIER (was verhindert potentiell den Zugang?) nur in 11% bzw. 3% der insgesamt 72 Fälle explizit besetzt werden, so dass der Begriff der Offenheit vage gehalten und so seine freien Assoziationspotenziale besonders gut entfalten kann (Abb. 2).

                

                    

                    
Abb. 2

                

            

            

                
Ausblick

                
Unsere Ergebnisse versprechen einen detaillierten und empirisch gestützten Blick auf das für die Digital Humanities so bedeutsame Prinzip der Offenheit weniger in seinen technischen als in seinen diskursiven Aspekten. Im Projekt werden unsere linguistischen Analysen ergänzt werden durch psychologische, experimentelle Erhebungen individueller Konstruktsysteme (Kelly 2005) von Akteur:innen im Feld der Open Science, die wir mit unseren Befunden zu diskursiven Deutungsmustern triangulieren werden. Zudem werden wir die Korpusgrundlage auf Publikationen anderer Akteur:innen im bibliothekarischen Diskurs ausweiten.

            

        

            
Der visuell-materiellen Gestaltung von historischen Dokumenten und insbesondere von Briefen kommt in der Forschung ein hoher Stellenwert zu. Visuell-materielle Eigenschaften wie die Papier- und Stiftfarbe, die Anordnung von Schrift auf der Seite und die Schriftrichtung geben Auskunft über historische Bedingungen und konkrete Umstände des Schreibens und der Schreibenden sowie, im Fall von Briefen, über das Verhältnis der Schreibenden zu den jeweiligen Adressat:innen (vgl. Baasner 2008; Wiethölter/Bohnenkamp 2010; Henzel 2020; Lukas/Osthof 2016).

            
Obwohl die digitale Repräsentation historischer Dokumente in digitalen Editionen und Archiven entscheidende Vorteile hinsichtlich der Darstellung visuell-materieller Charakteristika bietet (vgl. Bohnenkamp-Renken 2013; Radecke 2015), spielt deren Analyse in computergestützten Verfahren noch immer eine untergeordnete Rolle. Die meisten quantitativen Methoden und hierbei genutzten Tools beziehen sich auf den Inhalt und/oder den Schreibstil literarischer und historischer Texte, die bibliografischen Codes (McGann 1991: 77) bleiben jenseits ihrer Repräsentation als Bild-Digitalisate in digitalen Editionen bei der Analyse und Interpretation meistens unberücksichtigt.

            
Im Rahmen des geplanten Vortrags soll ein Ansatz zur quantitativen, Algorithmen-gestützten Erschließung visuell-materieller Charakteristika in einem Korpus von Briefen aus dem Zeitraum ‚Deutschland um 1900‘ präsentiert werden. Hierbei wollen wir zeigen, wie sich anhand verschiedener (teil-)automatisiert ermittelbarer Werte zur optisch erfassbaren Gestaltung der Briefe inhaltliche Aussagen sowohl über konkrete Dokumente sowie deren Schreibende und Adressat:innen machen als auch Erkenntnisse über die historischen Dynamiken der Textsorte ‘Brief’ gewinnen lassen. Denn zum einen handelt es sich bei Briefen um einen Dokumententyp, der spätestens seit dem 18. Jahrhundert spezifischen Kodierungen unterworfen ist, bei denen die Ebenen des sprachlichen Ausdrucks und der visuell-materiellen Gestaltung komplex ineinandergreifen (vgl. Baasner 2008). Zum anderen zeichnet sich gerade der untersuchte Entstehungszeitraum der Briefe durch die Lockerung kommunikativer Etikette aus, was im Rahmen des Briefformats zunehmende Spielräume zur individuellen Ausgestaltung eröffnet (vgl. Ehlers 2004).

            
Bei dem analysierten Korpus handelt es sich um einen Ausschnitt aus dem ca. 35.000 Briefe umfassenden Dehmel-Archiv der Staats- und Universitätsbibliothek Hamburg (SUB), die aktuell im Projekt 
                
Dehmel digital
 wissenschaftlich erschlossen werden.
 Die Briefe richten sich an den um 1900 berühmten, 1920 verstorbenen Dichter Richard Dehmel und stammen von verschiedenen anderen Künstler:innen wie Rainer Maria Rilke, Stefan Zweig, Else Lasker-Schüler, Detlev von Liliencron und Peter Behrens, die in unterschiedlich engem Kontakt zu Dehmel standen und deren Erfolg als Künstler:innen zum Zeitpunkt der Korrespondenzen ebenfalls verschieden groß war. Weiterhin enthält das Korpus auch Briefe von Vertreter:innen des Literaturbetriebs, die mit Dehmel über Publikationsvorhaben, geplante Veranstaltungen oder gemeinsame Projekte verhandeln.
            

            
Wir wollen in unserem Beitrag zeigen, wie sich anhand der visuell-materiellen Gestaltung der Briefe Aussagen über die Lebensumstände der Briefschreibenden, deren Beziehung zu Dehmel sowie die Charakteristika epistolarer Kommunikation um 1900 und die Veränderung der Normen der Briefschreibung ableiten lassen, welche sich in dieser Zeit vollziehen. 

            
Die Grundlage für die maschinenlesbare Erschließung visuell-materieller Eigenschaften der Dokumente bilden die Strukturinformationen auf der Briefseite. Diese werden im Rahmen der Layoutanalyse, einem Teilschritt der HTR (Handwritten Text Recognition), semi-automatisch mittels OCR4all
 (vgl. Reul et al. 2019) erfasst. Hierbei können Strukturen wie der Seitenspiegel, der Haupttext, Grußformeln, Briefköpfe etc. entweder manuell ausgezeichnet und typisiert oder ein automatisch generierter Vorschlag gezielt korrigiert werden, wobei manuell geprüfte Daten als Trainingsbeispiele für die fortlaufende Verbesserung der algorithmischen Methoden herangezogen werden können. Aus den so erzeugten Daten können anschließend die Informationen über die visuell erfassbaren Merkmale der Briefe automatisiert extrahiert werden, um im Rahmen statistischer Analysen Zusammenhänge zwischen Layout und Briefinhalt aufzudecken (vgl. Busch/Hegel 2017; Hurlbut 2013). So lassen sich z.B. statistische Mittelwerte für den Weißraum der Briefe einzelner Korrespondenzpartner:innen berechnen und miteinander vergleichen oder die Papierfarbe eines Briefs ins Verhältnis zu den innerhalb des Korpus üblichen Färbungen setzen. 
            

            
Die von uns untersuchten Merkmale lassen sich grob in zwei Kategorien unterteilen: Erstens gibt es grundsätzlich vorhandene visuell-materielle Eigenschaften wie das Format der Briefbögen, das Verhältnis von Weißraum und Textraum sowie der Abstand zwischen Grußformeln und Textblock, die für die epistolare „Respektsemiotik“ (Ehlers 2004: 21) von großer Aussagekraft sind. Diese können direkt aus den oben erwähnten Auszeichnungen abgeleitet bzw. berechnet werden. Ebenfalls in den Bereich der grundsätzlichen Eigenschaften gehören die Papier- und Stiftfarbe, deren „Auswahl und Wirkungsweise [...] in enger Abhängigkeit – historisch wandelbarer – ökonomischer, kultureller, sozialer, ästhetischer u.a. Faktoren sowie des jeweiligen Inhalts und der Funktion des Briefs“ stehen (Henzel 2020: 222). Die computergestützte Identifikation dieser Merkmale erfordert allerdings im Vergleich einen etwas größeren Aufwand: Zunächst wird das originale Farbbild in ein Binärbild umgewandelt, sodass in den zuvor ausgezeichneten Textregionen die Vordergrundpixel weitestgehend der Schrift und die Hintergrundpixel weitestgehend dem unbeschriebenen Papier entsprechen. Nach dem Ausschließen von Übergangspixeln, um Störeffekte zu minimieren, werden die Entsprechungen im Farbbild gesammelt und der jeweiligen Klasse zugewiesen. Abschließend wird separat die durchschnittliche Schrift- und Papierfarbe berechnet, indem zunächst je ein Mittelwert für die drei Farbkanäle Rot, Grün und Blau gebildet wird und diese zu je einer Farbe kombiniert werden.

            
Neben diesen generellen visuell-materiellen Eigenschaften beziehen wir zweitens spezifische Charakteristika wie Briefköpfe, Abbildungen und Zeichnungen in unsere Analysen mit ein, die nur in einem Teil der Dokumente enthalten sind und bei denen vorerst lediglich erfasst wird, ob sie vorhanden sind oder nicht. Diese Erfassung zielt darauf, die Dokumente anschließend nach den entsprechenden Charakteristika filtern zu können, um auf breiter Basis Aussagen über die Verbreitung und die konkreten Eigenschaften der Gestaltungsmittel machen zu können.

            
Zwei Beispiele sollen im Folgenden die dargestellte Vorgehensweise sowie die Aussagekraft der Ergebnisse der visuell-materiellen Analysen exemplarisch illustrieren.

            

                

                
Abb. 1: Brief von Heinz Möller an Richard Dehmel vom 16.02.1903, Dehmel-Archiv der SUB, HANSb18474, S. 1

            

            
            
Der oben abgebildete Brief des Leipziger Herausgebers Heinz Möller vom 16. Februar 1903 ist offensichtlich auf eigenem Briefpapier verfasst. Wie anhand des Briefkopfs mit den Initialen des Verfassers zu erkennen, tritt Möller im eigenen Namen auf und handelt nicht im Auftrag einer Organisation. Des Weiteren orientiert sich die Gestaltung des Briefs geradezu prototypisch an den Regeln der Respektsemiotik: Das Ausmaß der textfreien Weißräume an den Briefrändern und zwischen den einzelnen Briefteilen (oberer Briefrand und Anrede, Anrede und Textblock) und das äußerst sorgfältige Schriftbild bringen auf visuell-materieller Ebene die Hochachtung des Schreibenden gegenüber dem angeschriebenen Richard Dehmel zum Ausdruck und korrespondieren dabei mit verbalsprachlichen Formeln wie „Hochverehrter Herr“ und „In aufrichtiger Verehrung und Dankbarkeit“. Insbesondere im Vergleich mit dem zweiten Beispiel wird deutlich, dass gerade im Rahmen geschäftlicher Korrespondenz auch zu Beginn des 20. Jahrhunderts die seit der Antike in stetig aktualisierten „Briefstellern“ verbreiteten formalen Regeln zur Abfassung von Briefen noch Gültigkeit besaßen (vgl. Schiegg 2020). Zugleich lässt sich aus der Gestaltung eine Aussage über das Hierarchieverhältnis der beiden Korrespondenzpartner zueinander ableiten: Möller wendet sich hier, wie gesagt in eigener Sache, an Dehmel als „Obmann“ des Kartells lyrischer Autoren, um einen Nachlass für die Honorare einer von Möller geplanten Lyrik-Anthologie zu erbitten. Der Umstand, dass es sich hier um ein Bittschreiben handelt, spiegelt sich eindrücklich in der visuell-materiellen Gestaltung des Dokuments.

            

                

                
Abb. 2: Brief von Peter Behrens an Richard Dehmel vom 23.09.1903, Dehmel-Archiv der SUB, HANSb313015, S. 1

            

            
            
Das zweite Beispiel, der Brief des Kunsthandwerkers und Designers Peter Behrens an Richard Dehmel vom 23. September 1903, zeigt eine andere Form der signifikanten Individualisierung von Briefpapier: Behrens’ Briefbogen enthält zwar keinen aufgedruckten Briefkopf, die farbliche Gestaltung, das lilafarbene Briefpapier hebt ihn aber sehr stark aus dem Feld der breiteren Masse von Briefen aus dem untersuchten Zeitraum hervor und macht ihn, etwa in einem Stapel von Briefen, sofort als einen Brief des Verfassers Behrens erkennbar. Darüber hinaus weist die Gestaltung des Dokuments durchaus grundlegende visuell-materielle Charakteristika der Briefschreibung auf, die im Rahmen einer quantitativen Analyse als Eigenschaften des etablierten Brieflayouts der Zeit um 1900 ermittelt werden können: Es besteht, wie für den Brief von Möller festgestellt, ebenfalls ein gewisser Abstand zwischen dem oberen Blattrand und der Anrede sowie zwischen Blattrand und Brieftext, die Anrede ist, wiederum analog zum Brief Möllers, zentriert und der eigentliche Brieftext als Block gesetzt. Im Vergleich mit dem ersten Beispiel zeigen sich allerdings auch deutlich messbare Differenzen, die Rückschlüsse auf das Verhältnis der Korrespondenzpartner erlauben. So besteht ein deutlich geringerer Abstand zwischen oberem Blattrand und Anrede und kein gegenüber dem sonstigen Zeilenabstand vergrößerter Abstand zwischen Anrede und Brieftext. Hinzu kommt eine Wort-Einfügung über der Zeile, die ebenfalls einen Bruch mit der klassischen epistolaren Etikette darstellt und von einer größeren Vertrautheit der Korrespondenzpartner zeugt. Wiederum spiegelt hier die visuell-materielle die verbalsprachliche Gestaltung. Peter Behrens war nicht nur künstlerisch mit Richard Dehmel verbunden, sondern auch ein guter Freund, was sich in der Anrede als „Lieber Richard“ niederschlägt.

            
Die beiden Beispiele zeigen andeutungsweise, wie die automatisierte, quantitative Auswertung der visuell-materiellen Gestaltungsformen in einem Briefkorpus dazu beitragen kann, bereits vor der genauen inhaltlichen Sichtung der Briefe Hypothesen über das Verhältnis der Korrespondenzpartner:innen und den Zweck der Kommunikation anzustellen sowie darüber hinaus grundsätzliche Charakteristika epistolarer Kommunikation in einem bestimmten Zeitraum zu erschließen.

        

            
Jean Paul (1763–1825) zählt zu den bedeutendsten Schriftstellern der deutschen Literatur um 1800 und war ein überaus produktiver und geistreicher Briefeschreiber, der mit bekannten Persönlichkeiten wie Heinrich Jacobi, Caroline und Johann Gottfried Herder, Charlotte von Kalb und Rahel Levin Varnhagen korrespondierte. Die Briefe Jean Pauls erschienen bereits Mitte des 20. Jahrhunderts in der Historisch-kritischen Ausgabe (Berend 1952–1964); Anfang des 21. Jahrhunderts folgten die Briefe an Jean Paul (Begemann et al. 2003–2017), ebenfalls im Druck. Seit 2018 ist Jean Pauls Briefkosmos auf dem Weg in die digitale Welt: Die 5562 Von-Briefe, die zunächst buchzentriert retrodigitalisiert
 und anschließend briefzentriert retrokonvertiert wurden,
 sind seitdem auf 
                
Jean Paul – Sämtliche Briefe digital
 verfügbar (Miller et al. 2018–2022). Daneben sind derzeit (Dezember 2022) bereits 1479 Dokumente des sich noch im Aufbau befindenden ‚born digital‘-Korpus der Umfeldbriefe erschienen,
 das die Korrespondenz von Familie, Freundinnen und Kolleginnen des Schriftstellers umfasst.

            

            
Aus methodisch-technischer Perspektive, setzt die Edition mit der Verwendung von XML/TEI und dem Basisformat des Deutschen Textarchivs (DTA 2011–2020) sowie der Anreicherung mit Normdaten (GND, GeoNames) auf Standards. Im Zeichen von ‚Open Data‘ erscheinen die XML/TEI-Dokumente der Briefe unter Creative Commons-Lizenz (CC-BY-SA 4.0), und zwar in drei Publikationsmodi, die verschiedene Funktionen hinsichtlich ihrer Nutzung erfüllen:

            
(1) Zur 
                
Datenlektüre
 bzw. zum Abgleich zwischen einem Brieftext in HTML und den zugrundeliegenden Daten kann man jedes Dokument innerhalb der digitalen Edition einzeln als XML/TEI herunterladen. Die editorische Arbeit bzw. das ‚Wissen‘ in den Daten wird so transparent, nachvollziehbar und vollumfänglich zugänglich gemacht, da die Komplexität der Kodierung, wie in den meisten digitalen Editionen, nicht vollständig im User Interface abgebildet wird (Neuber 2023). 
            

            
(2) Zur 
                
maschinellen Interaktion
 über technische Schnittstellen (Witt 2018) bietet die digitale Edition derzeit verschiedene BEACON-APIs und eine CMIF-API, wodurch u. a. die 
                
Deutsche Biographie
 und 
                
correspSearch
 (Dumont et al. 2021) Informationen der Edition aggregieren.
 Gleichzeitig bezieht die digitale Edition über die erfassten Normdaten auch selbst Informationen von Schnittstellen, beispielsweise Koordinaten von 
                
GeoNames
 zur Generierung von Karten und Bild-URLs mit Portraits von 
                
Wikimedia Commons
.
            

            
(3) Zur 
                
Nachnutzung bzw. Re-kontextualisierung 
der Datensätze stellt die Edition die Brieftexte als Paket auf GitHub und Zenodo bereit (Neuber 2022a), wodurch die Daten archiviert und versioniert sowie mit einer DOI zitierbar sind. Durch diese Art der Datenpublikation wurde das Korpus der Briefe von Jean Paul bereits mehrfach jenseits der Edition in anderen Kontexten nachgenutzt: im 
                
Digitalen Wörterbuch der Deutschen Sprache
 als historisches Korpus (2022), im 
                
CorpusExplorer
 als korpuslinguistische Ressource
 (Oliver 2018) und auf Twitter als Bot @jeanpaultoday
 (Neuber 2022b). So wird durch die offene Bereitstellung der Daten Forschung jenseits der Edition gefördert und die Brieftexte einem gänzlich neuen Publikum zur Verfügung gestellt.
            

            
Der Beitrag, der die Publikationsmodi der Jean Paul Briefedition und ihre jeweilige Funktion illustriert, ist für die DHd-Konferenz höchst relevant, da ‚Open Data‘ im Editionskontext immer noch eher die Ausnahme als die Regel ist. Aus Greta Franzinis Editionenkatalog (2016–2022) geht hervor, dass von 320 digitalen Editionen lediglich ~27% CC-Lizenzen verwenden, ~23% TEI-Daten zum Download bereitstellen und ~5% APIs anbieten. Die Zahlen sind bedauerlich, da Daten das primäre Forschungsergebnis digitaler Editionen sind: 

            
 [...] [I]n digital editions the encoded texts themselves are the most important long-term outcome of the project, while their initial presentation within a particular application should be considered only a single perspective on the data. Any given view will be far from unique or canonical, as different usage scenarios call for different presentations (Turska et al. 2017, §4).

            
Im Kontext der Jean Paul-Edition gelten die Daten den Herausgeberinnen als 
                
Primärpublikation
, die in der digitalen Edition im Web ihre Kernpräsentation, nicht aber ihre einzige (Re-)Präsentationsform finden müssen. Die Maßnahmen für offene Daten zielen daher auf ein Höchstmaß an Transparenz, Interoperabilität und Nachnutzbarkeit, um einer Nachnutzung durch Mensch und Maschine gerecht zu werden (Baillot und Busch 2021).

            

        

            

                
Einleitung

                
Zu den Anstrengungen der Europäischen Union für die Umsetzung von 
                    
Open Sciences
 gehört auch eine Klärung und Förderung der Reproduzierbarkeit und der Sicherung von Integrität. Unter Reproduzierbarkeit wird dabei im 
                    
Scope Report
 der Europäischen Kommission die Wiederholbarkeit des Forschungsprozesses mit denselben Daten und Methoden verstanden (Directorate-General for Research and Innovation 2020). Für die Autor*innen des Reports stellt die Reproduzierbarkeit einen spezifischen Fall der Wiedernutzbarkeit (Re-Use) von Forschungsdaten dar. Aus unserer Sicht bilden jedoch die strikte Reproduzierbarkeit und eine offene Wiedernutzung Szenarien, die in der Praxis zu widersprüchlichen Anforderungen an die Kuration von Forschungsdaten sowie ihrer Ausgabe über graphische Nutzeroberflächen und APIs führen. Im Verhältnis mit der Datenintegrität entsteht dabei ein nicht vollständig aufzulösendes Dilemma, das Versuche der Aushandlung aber in eine produktive Spannung überführen können. Dies möchten wir verdeutlichen, indem wir einerseits unseren Gebrauch der Begriffe Re-Use, Neuausrichtung und Datenkuration präziseren, und die Problematik andererseits an einem Fallbeispiel aus der Forschungsarbeit des Deutschen Forums für Kunstgeschichte Paris (DFK Paris) veranschaulichen, für das wir nach Wegen gesucht haben, zwischen den Polen Reproduzierbarkeit und Re-Use zu vermitteln. Es handelt sich um die Aufbereitung und neue Präsentation einer 20 Jahre alten Datenbank zur wechselseitigen Rezeption des Kunstgeschehens in Texten der Kunstkritik aus Deutschland und Frankreich zwischen 1870 und 1960 (DFKV) (DFK Paris 2022b).
                

            

            

                
Fallstudie

                
Von 1999 bis 2005 wurden in drei aufeinanderfolgenden Förderungen Quellenanthologien, Aufsätze, eine Monografie und drei bibliographische Datenbanken zur deutsch-französischen Kunstrezeption erstellt (DFK Paris 2022c), die den Blickwinkel der in den 1990er und 2000er Jahren sehr einflussreichen Kulturtransferforschung einnahmen (Espagne &amp; Werner 1988; Espagne 1999; Gaethgens 2009). Die Datenbanken waren technologisch und im Datenmodell einheitlich angelegt, während sich die Verschlagwortung jeweils spezifisch entwickelte. Insgesamt beinhalten sie knapp 6800 mehrheitlich kommentierte und verschlagwortete Hinweise auf Artikel, Meldungen und Notizen in 314 verschiedenen Reihen deutscher, beziehungsweise französischer Kunstzeitschriften und wenigen zeitgenössischen Buchpublikationen. Ziel war es, Kunsthistoriker*innen ein Hilfsmittel zur wissenschaftlichen Arbeit anzubieten. Ab Winter 2004/2005 standen sie offen online zur Verfügung und sind bis 2016 über die Webseite des DFK Paris auffindbar gewesen. 2019 erfolgte ein erster Relaunch der zwischenzeitlich in eine MySQL Datenbank migrierten Datenbanken. Von 2021 bis 2022 wurden die Daten umfangreich kuratiert und im Juni 2022 neu veröffentlicht (DFK Paris 2022a). Ausgangspunkt war die mangelhafte Nutzbarkeit der öffentlichen Datenpräsentation, die aus der mehrfachen Migration hervorgegangen war und das Verständnis für die Zusammensetzung und Bedeutung der Daten minderte. Das Webangebot führt heute ein einstiges Werkzeug fort, dessen zugrundeliegende Forschungsfrage des Kulturtransfers inzwischen aufgrund der Weiterentwicklung hin zur Untersuchung von Mobilität und Migration nicht mehr in gleicher Weise gestellt wird. 

            

            

                
Begrifflichkeiten: Re-Use, Datenkuration und Neuausrichtung

                
Um die von uns verwendeten Begrifflichkeiten zu schärfen und Divergenzen zu anderen Definitionen vorzubeugen, sei eine Erläuterung des hier zugrunde gelegten Verständnisses der Begriffe 
                    
Re-Use
, 
                    
Neuausrichtung
 und 
                    
Datenkuration
 vorangestellt. Re-Use beschreibt die erneute, offen gedachte Nutzung von Daten in der Forschung zur Beantwortung einer neuen Fragestellung. Dies kann die Rekombination mit anderen Daten beinhalten und ist ein wesentliches Ziel der Bemühung um die Anwendung der FAIR Prinzipien (Huie et al. 2021). 
                    
Als Datenkuration möchten wir hier eine Gruppe von Aktivitäten verstehen, die an der Daten-haltenden Institution angesiedelt ist, und mit dem Ziel, eine verständige Nachnutzung durch Dritte zu ermöglichen, durchgeführt wird. Notwendige Voraussetzung sind dabei die Dokumentation und Kenntnis der Erzeugung, Prozessierung und Anzeige der Daten, wie es Flanders und Muñoz aus Perspektive der Geisteswissenschaften zusammengestellt haben (Flanders u. Muñoz 2015). Dabei weisen die von Kim und Koh (Kim u. Koh 2021) herausgegebenen Forschungen zur Geschichte von Digital Humanities-Projekten darauf hin, dass unter „Erzeugung“ wie „Prozessierung“ ein Zusammenspiel von theoretischer Ausrichtung, methodischer Vorgehensweise und organisatorischen wie institutionellen Faktoren zu betrachten ist. Die Dokumentation nach Flanders und Muñoz ist die Voraussetzung, um Daten überhaupt auf eine neue Verwendung hin aufzubereiten. Die Herausforderung liegt nach Woodall (2017) dann darin, die Eignung der Daten für diesen neuen Anwendungsfall bestimmen zu können und sie gezielt daraufhin zu entwickeln, um Fehlinterpretationen vorzubeugen. Im Idealfall sollte die Datenkuration darauf gerichtet sein, innovative Forschungen zu ermöglichen und die Daten entsprechend für möglichst viele Ansatzpunkte öffnen, beispielsweise durch Verknüpfung mit Normdaten. 
                    
Mit dem Begriff der Neuausrichtung schlagen wir eine spezifische Auslegung der Datenkuration vor, die sich diesen Herausforderungen auch auf Ebene der Benutzeroberfläche widmet, wie es die Medienwissenschaftlerin Drucker (2021, 78) für die Aufbereitung von Forschungsdaten in den Geisteswissenschaften angeregt hat. Wir erweitern das Verständnis von der Aufbereitung damit inhaltlich gegenüber dem vorher in den 
                    
Data Sciences
 auf die Daten gerichteten Fokus (Woodall u. Wainman 2015) und nähern es dem 
                    
Refashioning
 nach Bolter und Grusin (1999, 45 f.) an. Ziel ist es also, einen (alten) bestehenden Datensatz visuell und textuell so zu vermitteln, dass dessen Beschaffenheit für den Re-Use verständlich wird (bspw. durch eine entsprechende Suchmaske). Indem die Neuausrichtung durch ihre spezifische kuratorische Ordnung und Anreicherung der Daten jedoch bereits Beispiele der Weiternutzung, Interpretation und Verknüpfung anbietet, nimmt sie ein ambivalentes Verhältnis sowohl zur Maxime der weitmöglichsten Öffnung der Daten zum Re-Use als auch zur historischen Gewachsenheit der Daten ein. Diese gilt es wiederum zu kommunizieren.
                

                

                    

                    
Die verschiedenen Nutzungsansprüche einer republizierten Datenbank im Vergleich zur ursprünglichen Zielsetzung am Beispiel der Datenbanken DFKV (Grafik: Klara Niemann, 
                        
CC BY 4.0
).
                    

                

            

            

                
Vom Dilemma zwischen Neuausrichtung und historischer Integrität

                
Als Maßnahmen der Datenkuration möchten wir aus unserem Fallbeispiel die semantische Anreicherung von Daten durch die Referenzierung mit Normdaten und die Einbeziehung von digitalen Surrogaten des Quellmaterials für Datenbanken der DFKV heranziehen. Auf diese Weise wurden von 9076 in den Datenbanken DFKV genannten Personen (Autor*innen, Künstler*innen, Kurator*innen und weitere) 4686 (52 %) mit Normdaten (Getty Union List of Artists Names (ULAN), Gemeinsame Normdatei (GND), 
                    
notices d’autorité
 der Bibliotheque nationale de France (BnF)) oder Wikidata referenziert. Dies hat zur Entdeckung von insgesamt 603 Personennamen geführt, die entweder in mehr als einer der Datenbanken auftreten oder auch unbemerkt jeweils mit verschiedenen Namensvarianten eingetragen wurden. Diese Varianten sind meist in den Quellen angelegt, wenn etwa Vornamen und Adelstitel in Französisch oder Deutsch übersetzt wurden. Die Gewohnheit, in den Zeitschriften Namen zu übersetzen oder auch Umlaute und Akzente anzupassen, ist ein Hinweis zur ursprünglichen Rezeption. Dass diese verschiedenen Schreibweisen bei der Datenerfassung in den 2000er Jahren übernommen, aber nicht mit übereinstimmenden Personen assoziiert wurden, ist wiederum eine wichtige Information zur Einschätzung der Qualität der Daten. Darüber hinaus konnten einige Aliasnamen aufgedeckt werden, die den Erstbearbeiter*innen nicht bekannt waren.
 Zusammengefasst beträgt der Anteil an Dubletten damit rund 9 %. Für 2548 der 5735 in der Datenbank beschriebenen Zeitschriftenbeiträge (Zeitraum vor 1940) konnte eine Verlinkung auf ein Digitalisat in den Angeboten der Universitätsbibliothek Heidelberg oder der BnF erstellt werden. Wo ein Dateneintrag zusammenfassend auf mehrere verschiedene Zeitschriftenbeiträge verweist, musste die Datengrundlage erweitert werden, sodass die Gesamtanzahl bibliografischer Attribute in der Datenbank durch die Kuration von 5948 auf 6194 angestiegen ist. 
                

                
Im Sinne der Neuausrichtung sollten die Mehrfachnennungen der Personen zusammengefasst werden und die Qualität der Daten einschätzbar sein, aber zugleich die historisch bedingte Ambiguität erhalten bleiben. Dieses Dilemma zwischen der Neuausrichtung und dem Erhalt der ursprünglichen Datenbanken als Artefakt hat uns zur Frage geführt, was die historische Integrität dieser Daten ausmacht. Medienarchäologische Studien und Datenzentren haben von unterschiedlichen Ausgangspunkten ausgehend wahlweise die vollständige Emulation oder die Erhaltung der Präsentationsschichten bei einem Wechsel der zugrundeliegenden Technik vorgeschlagen (Waelder 2017; Steiner et al. 2022). Im Falle der Datenbanken der DFKV sind jedoch zum einen bereits unterschiedliche Softwares und Ansichten zur Dateneingabe und für die Ausgabe im Internet verwendet worden, sodass mehrere Versionen emuliert werden müssten. Zum anderen hat sich aus den Befragungen von ehemaligen Mitarbeiter*innen zum Gebrauch der Datenbanken in den 2000er Jahren ergeben, dass die Software als solche kaum wahrgenommen wurde und weder Funktionen zur Filterung noch des Exports später beschrieben werden konnten.
 In dieser Situation haben wir uns entschlossen, nicht die historische Integrität der Datenbanken als Ganzes zu betrachten, sondern auf die Ebene der einzelnen Datensätze zu gehen und sie abstrakter als einen definierten Zustand der Daten anzusehen.
 Dadurch sind wir dazu gelangt, die Datenbanken orientiert an CIDOC CRM als Konvolute von Dokumenten (E31; Bekiari et al. 2022, 83 f.) zu verstehen, deren Erzeugung wie auch Anreicherung diskrete Ereignisse (E5; Bekiari et al. 2022, 63 f.) ihrer Objektgeschichte bilden. Mit den Ereignissen sind ein Zeitraum (1999–2004 und 2021–2022), die ausführenden Personen, die Art der Aktivitäten und damit die Zusätze und Streichungen beschreibbar. 
                

                

                    

                    
Das Dilemma zwischen Integrität und Nutzbarkeit der Daten am Beispiel der Datenbanken DFKV (Grafik: Klara Niemann, 
                        
CC BY 4.0
).
                    

                

            

            

                
Gestaltung des GUI

                
Ausgehend von der Idee, die Zustände und Anreicherungen der Daten selbst erfahrbar zu machen, haben wir das GUI gestaltet (Niemann 2021). Es ist in drei Funktionsbereiche aufgeteilt: die Übersichtsseite zur Suche und Auswahl der in verkürzter Ansicht gezeigten Dateneinträge, die vollständige Ansicht der einzelnen Dateneinträge und eine Merkliste mit individuell von den Nutzer*innen ausgewählten Einträgen. 

                
In der vollständigen Ansicht der einzelnen Dateneinträge wurde mit Farbhintergründen und Schichten gearbeitet, die die Inhalte den verschiedenen Phasen der Datenbanken und ihrer Bearbeitung zuordnen.
 Auf neutralem Grund sind bibliografische Angaben und Textauszüge angelegt, die faktische Informationen zum aufgenommenen Quelltext liefern. Farbig hinterlegt sind die Schlagworte, Kommentierungen und weitere Anreicherungen der Datenerfassung und somit des ersten objektgeschichtlichen Ereignisses. Als Widgets können für die Autor*innen und die als genannte Personen angegebenen Namen die Referenzierungen auf Normdaten oder Wikidata und Hinweise auf weitere in der Datenbank vorhandene Schreibweisen aufgerufen werden. Sie bilden somit das Ereignis der Kuration ab. In gleicher Weise sind Informationen zur Zeitschrift bei der BnF oder der GND aufrufbar. Als Fly-in schiebt sich von rechts über die Grundebene ein Widget mit Erläuterungen zur Nutzung, die anhand von Symbolerklärungen die Hintergründe der objektgeschichtlichen Stationen und die Entscheidungen der Kuration transparent vermitteln. Als äußerste Schicht kann jeder Dateneintrag als JSON in einem weiteren Widget aufgerufen werden, um die Anreicherungen und Verknüpfungen der Informationen per IDs (als Spuren der ursprünglichen Erfassung in einer relationalen Datenbank) nachzuvollziehen. Insgesamt stellen diese Gestaltungsentscheidungen eine Reaktion auf den Anspruch der Nachvollziehbarkeit und damit der Reproduzierbarkeit dar. 
                

                
Die Suchfunktionen auf der Übersichtsseite sind hingegen auf den Re-Use ausgerichtet. Die verschiedenen Optionen, über die Datenbankzugehörigkeit, mit dynamischen Filtern in vorgegebenen Kategorien, dem interaktiven Zeitstrahl oder der Freitextsuche zu suchen, regen eine Entdeckungstour durch die Daten an, bei der die Nutzer*innen weniger über eine spezifische Suche als ein Schweifen in das Material einsteigen. Um dies zu fördern, kann man sich auf der bereits beschriebenen Vollanzeige der Dateneinträge außerdem horizontal per Klick von einem zum nächsten bewegen. Die digitalen Surrogate werden schließlich über ein Icon aufgerufen und öffnen sich in einem IIIF-Viewer, der sich in einem neuen Browsertab öffnet. Indem die Manifeste der Zeitschriftenbände verknüpft sind, können die referenzierten Artikel und Beiträge vollständig gelesen werden, in dem Band geblättert werden und weitere von den Bibliotheken erstellte Metadaten aufgerufen werden. 

            

            

                
Umsetzung in der Datenmodellierung 

                
Nicht implementiert haben wir eine vorerst prototypische Umsetzung dieser Schichtung in einem Datenmodell, das wir mit dem Linked Art Data Model erstellt haben (Klammt 2022). Das Linked Art Data Model (LADM) ist eine Anwendung des CIDOC CRM (Linked Art), das den Re-Use von Kulturdaten unterstützen möchte. Anders als LIDO XML geht es dabei nicht um ein Transferformat, mit dem Metadaten verschiedener Quellen zusammengeführt werden können, sondern darum, Kulturdaten zu einfach nutzbaren Linked Open Data zu machen. Dieser Fokus spiegelt sich auch in der Wahl von JSON als Dokumentformat. In einem ersten Prototyp haben wir die einzelnen Dateneinträge als Informationsobjekt modelliert, das auf meist einen Zeitschriftenbeitrag referenziert, dessen bibliografische Angaben mit dem LADM ausgedrückt werden können. Jeder Dateneintrag hat als Ereignisse seine Erstellung und die Kuration eingetragen, mit den Zeiträumen und den jeweils verantwortlichen Projektleitern. Über verschiedene definierte Eigenschaften sind die Verlinkung zum IIIF-Manifest, die Kommentierungen und Verschlagwortung genauso wie die Referenzierung auf Normdaten im Zuge der Neuausrichtung als LOD erklärt. Die Verwendung des Models für die Beschreibung von Forschungsdaten liegt außerhalb seiner ursprünglichen Intention. Sie erlaubt aber auf Datenebene transparent zu dokumentieren, welche Maßnahmen zur Ausrichtung der Daten auf den Re-Use ausgeführt wurden. Gleichzeitig können diese Veränderungen reversibel eingeschrieben werden.

            

            

                
Resümee

                
Im Prozess der Neuausrichtung der DFKV-Datenbanken durch verschiedene Anreicherungsprozesse und das Einbetten in ein neues GUI sahen wir uns in der Praxis mit der Aufgabe einer offen gedachten Reproduzierbarkeit gegenübergestellt. Reproduzierbarkeit ist dabei sowohl der Weiterverwendung als auch der Integrität der Daten verpflichtet. Wenn es darum geht, geisteswissenschaftliche Datenbanken langfristig zu erhalten, heißt das, diese auch jenseits der Langzeitarchivierung im Sinne der dynamischen Entwicklung der Forschungsfragen anschlussfähig an die wissenschaftliche Community zu halten. Entscheidungen der Datenkuration müssen in diesem Prozess individuell entsprechend des Einzelfalls, aber immer respektive der Geschichte und konkreten Beschaffenheit der Daten getroffen werden. Versteht man alle Eingriffe und Veränderungen als spezifische Ereignisse der Datenhistorie, gilt es, diese für den Re-Use transparent zu kommunizieren. Im Angesicht der ersten, nicht mehr abrufbaren Datenbanken aus den späten 1990ern und frühen 2000ern wird der Handlungsbedarf in diesem Bereich deutlich.

                

                    

                    
Datensätze als digitales Objekt, das durch Erstellung und Kuration geformt wird (Grafik: Anne Klammt; Lizenz: 
                        
CC BY 4.0
).
                    

                

            

        

            

                

                    
Einleitung
                

                
Die wohl bedeutendste Struktur der zeitgenössischen akademischen Philosophie ist die Trennung zwischen kontinentaler und analytischer Philosophie.

                
Obwohl wenig Einigkeit über die eigentliche Natur der Trennung besteht – ist es eine methodologische (vgl. Petrovich und Buonomo 2018), thematische, linguistische (vgl. Hobbs 2014), oder doch nur eine soziale? – spielt sie eine wichtige Rolle im philosophischen Berufsleben, und wird nicht nur informell verhandelt, sondern schlägt sich in den Aufnahmekriterien von Fachzeitschriften, in den Entscheidungen von Berufungskommissionen und im Aufbau zahlreicher professioneller Vereinigungen nieder.

                
Klarheit über die Topologie dieser Trennung zu erreichen, ist dementsprechend von großem Interesse. Dennoch besteht in der Literatur eine ausgesprochene Uneinigkeit über eine Reihe von basalen Fragen: Wann hat die Spaltung ihren Anfang genommen (man vgl. die Darstellungen von e.g. Glock 2008; Critchley 1997; 2001)? Handelt es sich überhaupt um eine Trennung, oder eher um zwei Extreme am Rande eines Kontinuums? Hat sie überhaupt noch Bestand, oder hat sich die Kluft im einundzwanzigsten Jahrhundert weitestgehend geschlossen (Bieri 2007; Beckermann 2003; Hoche 2009)?

                

                    

                        

                        
Zusammensetzung des gereinigten Datensatz über hundertzehn Jahre. Wir beobachten eine sich massiv verstärkende Dominanz des englischen Materials über das Jahrhundert hinweg, die Teils den Datenquellen geschuldet, zu einem großen Teil aber auch aus den Bedingungen des modernen wissenschaftlichen Publizierens erwachsen ist.

                    

                

                
Diese Uneinigkeit ist nicht überraschend. Da die Konkretisierung der Trennung in eine Zeit exponentieller Zunahme des wissenschaftlichen Outputs seit den 1950er Jahren fiel (vgl. Bornmann und Mutz 2015, siehe auch Abb. 1), und zugleich von geografischen und sprachlichen Grenzen moderiert wurde, aber nicht mit diesen identifiziert werden kann, ist ihre Geschichtsschreibung mit außergewöhnlichen Herausforderungen konfrontiert, da sie sich nicht mit Hunderten, sondern eigentlich mit Hunderttausenden von heterogenen Quellen befassen muss, wenn sie Fragen nach der tatsächlichen disziplinären Meta-Struktur beantworten will, anstatt sich mit philosophischen Einzelschicksalen zu befassen.

                
In der vorliegenden Arbeit schlagen wir eine Methode zur Beantwortung solcher grenzüberschreitenden, globalen Fragestellungen vor. Mithilfe eines multilingualen Sprachmodells (PhilroBERTa), welches wir auf philosophischen Texten fein-tunen generieren wir Textvektoren von 288.546 philosophischen Texten aus den vergangenen hundert Jahren. In dem derart aufgespannten Vektorraum identifizieren wir die Achse welche der kontinental-/-analytischen Unterscheidung entspricht. Indem wir die Positionierung der einzelnen Artikel auf dieser Achse erheben, können wir eine erste quantitative Einschätzungen der Topologie der analytisch/kontinentalen-Trennung vorschlagen.

            

            

                
Datensatz

                
Eine scharfe Umgrenzung des Gebietes der Philosophie, insbesondere eine, welche einem ganzen Jahrhundert und mehreren nationalen Philosophiekulturen, verpflichtet ist, ist ausgesprochen schwierig. Dementsprechend ist für die vorgelegte Arbeit der zugrundeliegende Datensatz so expansiv wie möglich gewählt worden. Der Datensatz enthält alle Texte welche in der JStor Datenbank unter der Rubrik ‘Philosophy’ archiviert worden sind (437.703 Rohtexte), sowie alle Abstracts aus dem 
                    
Web of Science (WOS)
, welche in den Rubriken ‘Philosophy’ und ‘History and Philosophy of Science’ angesiedelt sind, oder aus in der 
                    
PhilPapers-
Journal-Liste verzeichneten Publikationen stammen (188.794 Einträge).
                

                

                    
Da die Qualität der Rohdaten mäßig ist und die Fehlerquellen äußerst heterogen sind, verwenden wir einen Bulk-Labelling-Ansatz, bei dem alle Texte nach einem BOW-Modell encodiert werden und mit UMAP (McInnes, Healy, und Melville 2018)

                    
 

                    
kartographiert werden. Cluster verwendbarer Rohdaten werden in einem interaktiven Layout manuell selektiert. Dabei wurden Titeleien, publizierte Bibliographien, besonders ungenügendes OCR und nur partiell oder gar nicht erhaltene Artikel entfernt. JSTOR und WOS-Quellen wurden vereint und überschneidende Artikel angeglichen, was zu einem finalen Datensatz von 288.546 Artikeln führte.

                

            

            

                
Methode

                

                    
Die Modellierung multilingualer Textcorpora stellt seit längerem ein Problem für zahlreiche Bereiche der Digital Humanities dar 

                    
(Dombrowski 2020). Klassische Methoden, wie BOW-, Topic-, oder Wortvektormodelle stoßen hier an ihre Grenzen, da die von ihnen gelernten Repräsentationen hauptsächlich Unterschiede zwischen Sprachen als salienteste Muster erkennen, und das eigentliche übersprachliche Erkenntnisinteresse verdecken. Der aus diesen Problemen resultierende Fokus auf rein englischsprachiges Quellenmaterial ist unbefriedigend (Pitman und Taylor 2017; Galina Russell 2014). Multilinguale Sprachmodelle, wie z. B. xml-Roberta (Conneau u. a. 2020)

                    
 

                    
sind zwar in der Lage, dieses Problem zu lösen, indem sie deckungsgleiche Vektorräume für verschiedene Sprachen bereitstellen. Ihre Anwendbarkeit auf spezifische Forschungskorpora ist allerdings begrenzt, da die notwendige Wissensrepräsentation über den spezifischen Textgehalt nicht gegeben ist. 

                    
Das fine-tuning solcher Modelle auf den Forschungsdaten stellt hier allerdings eine Herausforderung dar, weil die dafür zur Verfügung stehenden Architekturen dazu tendieren, in multilingualen Trainingskorpora hauptsächlich Sprachunterschiede zu lernen und damit die Einbettungen ‚auseinanderzubrechen‘.

                

                

                    
Ein kürzlich vorgeschlagener Lösungsansatz, nämlich die automatische Übersetzung des gesamten Textcorpus (vgl. Malaterre und Lareau 2022; siehe auch Böhm, Alexander u. a. 2022)

                    
 

                    
ist vielversprechend für die Erforschung der thematischen Zusammensetzung von Korpora, aber 

                    
ungeeignet

                    
 für Anwendungen in denen die unterschiedlichen 

                    
Konnotationen

                    
 von Wörtern in unterschiedlichen Sprachen und Kontexten eine Rolle spielen. Weiterhin ist die automatisierte Überführung von anderen Sprachen in eine einzige Basissprache – Englisch – mit Blick auf den Wunsch nach einer Wertschätzung verschiedener Sprachkulturen nicht ideal.

                

                

                    

                        

                        
Karte des verwendeten Datensatzes. Jeder der kleinen Datenpunkte entspricht einem von 288.546 Artikeln, angeordnet anhand der Cosinusähnlichkeit ihrer Textvektoren. Einzelne Cluster sind mit (aus Platzgründen) jeweils zwei zufällig ausgewählten Sprachen anhand von mit tfidf-ausgewählten Keywords gelabelt.

                    

                

                

                    
In dieser Einreichung folgen wir dementsprechend einem alternativem Ansatz: Wir beginnen damit, zwei zur Erhebung von Textähnlichkeiten geeignete Sprachmodelle – ein 

                    
englischsprachiges

                    
 (

                    
paraphrase-distilroberta-base-v2

                    
) und ein multilingual vortrainiertes Modell (

                    
xlm-roberta-base, beide bereitgestellt von 

                    
Reimers und Gurevych 2019)

                    
 

                    
auf trainings-samplen aus unserem Beispielkorpus feinzutunen. Für das englischsprachige Modell greifen wir dabei auf GPL 

                    
(Wang u. a. 2022)

                    
 

                    
zurück, das multilinguale Modell tunen wir mit TSDAE 

                    
(Wang, Reimers, und Gurevych 2021)

                    
, da der für GPL benötigte query-generator nur auf Englisch verfügbar ist. Danach verwenden wir den Ansatz von 

                    
Reimers und Gurevych (2020

                    
) bei dem das englischsprachige Modell als ‘Lehrer’ verwendet wird, um mithilfe eines Korpus von Übersetzungen, die unterschiedlichen Sprachen in dem multilingualen Modell zur Deckung zu bringen. Wie Abb. 2 zeigt, gelingt das tatsächlich: Eine durch 

                    
UMAP

                    
 

                    
(McInnes, Healy, und Melville 2018)

                    
 

                    
auf der Grundlage von Cosinus-Ähnlichkeiten zwischen Texteinbettungen erstellte Karte unseres Korpus weist keine separierten Cluster von Einzelsprachen auf. Gleichzeitig ist das TSDAE-pretraining über die Angleichung der Sprachen hinweg erhalten geblieben: In einem Philosophie-spezifischen triplet-evaluation task, bei dem das Modell mit drei Passagen aus der 

                    
Stanford Encyclopedia of Philosophy 

                    
konfontiert war, in welchem zwei aus dem selben Artikel, die dritte jedoch aus einem der (mit einem einfachen BOW-Modell ermittelten) zwanzig thematisch ähnlichsten Artikel stammte, verbesserte sich die Fähigkeit, die zusammengehörenden Artikel zu ermitteln, um sieben Prozentpunkte.

                

                
Um von unserem Sprachmodell zu Antworten auf unsere Fragen nach der Struktur der analytisch-kontinentalen Kluft zu kommen, encodieren wir zuerst alle Texte in unserem Sample mit dem multilingualen Sprachmodell. 

                
Die analytisch-Kontinentale Trennung wird in philosophiehistorischen Werken häufig über die Angabe von paradigmatischen ReferenzautorInnen eingeführt (e.g. "Frege", "Russel", "Moore", Quine, Strawson, … und "Hegel", "Husserl", "Heidegger","Adorno",...). Wir sammeln solche Listen in der Literatur und wählen die am häufigsten genannten Autoren aus. Dann wählen wir zufällig 2000 Beispielartikel aus, welche Autoren aus der einen, aber nicht der anderen Gruppe zitieren, also tendenziell eher aus der analytischen, oder kontinentalen Ecke kommen. Das ist unsere 'seed'-Stichprobe. 
                    
Einem von Waller und Anderson (2021)

                    
 

                    
inspirierten Verfahren für die Modellierung von Polarisierung in Online-communities folgend, bilden wir den durchschnittlichen Differenzvektor zwischen den Einbettungen dieser 'seed'-

                    
A

                    
rtikel. Anschließend suchen wir nach 500 Paaren von ähnlichen Artikeln, deren Vektordifferenz diesem Differenzvektor am ähnlichsten ist, d.h. Artikelpaaren, die ein ähnliches Thema behandeln, aber deren Grundeinstellung einmal analytisch, einmal kontinental ist. 

                    
Ein solches Paar bilden beispielsweise 

                    
der analytischere Text:

                    
 '¿Hay una Filosofia de la Ciencia en el ultimo Wittgenstein?' (Moulines 1989), und die kontinentaler gelagerte Besprechung: 'Von Umgangskörpern, Vertikalspannungen, Responsivität und Musikphilosophie: Ludwig Wittgenstein im Spiegel neuerer Literatur' (Kroß 2012). Auf diese Weise ‚bereinigen‘ wir die ursprünglichen Differenzvektoren von nur thematischen Ungleichgewichten. 

                

                
Durch die Berechnung der Cosinus-Ähnlichkeit aller Artikelvektoren in dem Datensatz zu den kontinentalen und analytischen Artikeln in den ermittelten Artikelpaaren können wir so einen themenunabhängigen einzigen "Analytizität/Kontinentalität"-Score für jeden Artikel entlang der kontinentalen/analytischen Achse ableiten. Die Dichte-Verteilung aller Artikel auf diesem Score ist in Abb. 3.b dargestellt. 

                
Für die Validierung dieser von dem Modell berechneten Scores, haben wir eine Web-Applikation entwickelt, in der NutzerInnen, in unserem Fall PhilosophInnen, den Titel eines Artikel aus unserem Datensatz angezeigt bekommen und entscheiden müssen, ob der Artikel aus der kontinentalen oder analytischen Philosophie stammt. Die Applikation ist sowohl auf Desktop-PCs als auch auf mobilen Endgeräten verfügbar. Die erstellte Applikation wird in den kommenden Monaten gezielt einem multilingualen Fachpublikum zugänglich gemacht werden, sodass die aus dem Sprachmodell generierten Scores empirisch validiert werden können.

                
Um zu messen, wie sich die kontinentale/analytische Spaltung im Laufe der Zeit vergrößert/verkleinert hat, fitten wir eine Serie von verbundenen gaußschen Mischverteilungsmodellen auf den Datensatz. Unter der Annahme, dass der Datensatz tatsächlich durch die Wirkung zweier Prozesse, welche analytische und kontinentale Philosophie generieren, entstanden ist, geben uns diese Modelle die jeweilige zentrale Tendenz und Spannbreite dieser Prozesse an.

                

                    

                        

                        
Verteilung von Artikeln entlang des analytisch-Kontinentalen-Gradienten. (a) zeigt die Entwicklung des Gradienten im Verlauf der Zeit. Zentrale Tendenzen und Dezile sind einer Serie von verbundenen zwei-Komponenten Gaußschen Mischmodellen entnommen. Im ersten Drittel wäre allerdings ein ein-Komponenten-Modell vorzuziehen. Vier einzelne Philosophen-Karrieren sind anhand der fortschreitenden zentralen Tendenz der Werte ihrer Artikel auf dem Gradienten eingetragen. Man beachte insbesondere die Karriere Rortys von einem ursprünglich analytischen Philosophen, zu einem der wirkungsvollsten Proponenten kontinentaler Autoren im angloamerikanischen Raum. (b) Zufällig ausgewählte Beispiel-Titel entlang des analytisch-kontinentalen Gradienten, nebst Dichte-Verteilung über das gesamte Sample.

                    

                

            

            

                
Vorläufige Ergebnisse

                
Die Ergebnisse dieser Modelle sind in Abb. 3.a wiedergegeben. Wir beobachten, dass sich die beiden Verteilungen bis in die späten 1940er Jahre nahezu parallel entwickeln – und in der Tat suggerieren die statistischen Kennzahlen der Modellwahl, dass eine einzige Gauß-Verteilung die Daten in diesem Bereich besser beschreiben würde. Von 1950 bis 1960 beobachten wir hingegen ein scharfes Ausschwenken der analytischen Verteilung, gemeinsam mit einer Verkleinerung der Breite der Verteilung – also eine Konzentration und Konsolidierung analytischer Tendenzen in unserem Korpus, verbunden mit einer asymmetrische Polarisierung, die bis heute weitestgehend konstant zu bestehen scheint.

            

            

                
Diskussion

                
Diese Ergebnisse stehen im scharfen Kontrast zu früheren monolingualen Zitations-Studien, die eine Isoliertheit kontinentaler Philosophie identifiziert hatten (Noichl 2021). Die Erweiterung des Datensatzes um mehrere Sprachen und die damit einhergehende methodologische Komplexität stellt also in jedem Fall eine notwendige Grundlage für weitere Untersuchungen dar. Dabei hat der verwendete Datensatz noch nicht alle Möglichkeiten zu multilingualer Erweiterung ausgeschöpft: Die reichhaltige spanischsprachige OpenAccess-Kultur hat in unserer Untersuchung zum Beispiel noch nicht ausreichend Eingang gefunden. Wenn für die gestellte Fragestellung auch nicht zwingend notwendig, wäre eine Erweiterung über den europäischen Sprachraum hinweg wünschenswert.

            

        

            

                
Startpunkt Notensatzprogramme

                
 

                
Musikalische Notation ist das vordringliche Medium zur Musiküberlieferung, aber auch der alltäglichen musikalischen Praxis und ist durch ihre Textualität auch Gegenstand der Digital Humanities. Die wissenschaftliche Beschäftigung mit Musik wie auch die Kommunikation über Musik und musikalische Ideen ist in weiten Teilen der heutigen Musikkultur ohne eine Notenschrift nicht denkbar. Selbst die Interaktion zwischen Musizierenden und mit dem musikalischen Material findet vorrangig vermittels dieses Mediums statt. Dabei kann die Verschriftlichung von Musik unterschiedliche Formen annehmen, sei es die sogenannte Common Western Music Notation (CWMN), Notenschriften aus anderen Kulturen oder zeitlichen Epochen, die vielfältigen eher technischen Darstellungsformen in Software-Werkzeugen zur Musikproduktion oder die analytischen Visualisierungen aus dem Bereich des Music Information Retrieval (Khulusi 2020).

                
Trotz dieser Vielfalt gilt die Interaktion mit Musiknotation aber noch immer primär als Domäne klassischer Notensatzprogramme
 und wird jenseits dessen kaum tiefergehend thematisiert. Die Funktionen von Notensatzprogrammen umfassen in erster Linie die Eingabe und das Editieren von Notenmaterial nach der CWMN sowie das Platzieren im Layout für den Druck. Sie sind damit Textbearbeitungsprogrammen nicht unähnlich. Ihre Benutzeroberflächen folgen ähnlichen Gestaltungskonzepten, visualisieren das interaktive Notenblatt im Bildzentrum und umgeben es mit vielfältigen Werkzeugpaletten und Modusschaltern, z.B. um Noten zu schreiben, editieren und abzuspielen. Es sind klassische WYSIWYG
-Desktopanwendungen, die für eine filigrane Maus- und Tastatursteuerung konzipiert sind. Diese etablierten Konventionen der Interaktion in einer Desktopumgebung stoßen bei den vielfältigen Anwendungsszenarien von Musiknotation jedoch an ihre Grenzen. Dies geht über das bloße Erstellen, Anzeigen und Ausdrucken der Noten hinaus. Der Abschnitt „Vielfalt der Anwendungsszenarien“ wird dieses Spektrum überblickshaft umreißen. Abschnitt 3 „Interaktion jenseits von Notensatz“ wird den Fokus entsprechend weiten und Interaktionskonzepte und Technologien aus angrenzenden Forschungs- und Anwendungsgebieten im Bereich Musik in den Blick nehmen. Leere Flecken auf der „Forschungslandkarte“ werden im Abschnitt „Bestimmung des Design Space“ herausgearbeitet und anhand von Beispielen verdeutlicht.
                

                
 

            

            

                
Vielfalt der Anwendungsszenarien

                
Die heute etablierten graphischen Darstellungs- und Interaktionsformen für Musik möchten wir zunächst nach dem Anwendungsbezug wie folgt systematisieren und ihre jeweiligen Besonderheiten herausarbeiten: 
                    
Erstellung
, 
                    
Musizieren
, 
                    
Präsentation
 und 
                    
Gaming
. 
                

                

                    
Erstellung:
 Neben der manuellen Noteneingabe spielt der Notenscan eine immer wichtigere Rolle. Solche Scans werden mittels Optical Music Recognition-Verfahren in editierbaren Notentext übersetzt (Calvo-Zaragoza 2020). In der historisch-kritischen Musikedition liegen der Erstellung meist mehrere Quellen zu Grunde, die nicht nur (digital) erfasst, sondern auch verglichen werden. Werkzeugen für vergleichende Ansichten kommt dabei eine zentrale Rolle zu (Kepper 2007, Waloschek 2019). Grundsätzlich fanden diese Arbeitsprozesse seitens der Interaktionsforschung bislang kaum Beachtung.
                

                
Es ist vor allem aber die Kreativarbeit, welche von spontaneren und direkteren Eingabemöglichkeiten zur Erstellung profitieren würde. Beim Komponieren entstehen Skizzen nicht notwendigerweise nur am Schreibtisch oder Klavier, sondern in Alltagssituationen, in denen allenfalls ein Smartphone gerade zur Hand ist. Bei Größeren Werken geht oft eine Planung der Formteile und ihrer Proportionen voraus, die dann in beliebiger Reihenfolge oder auch parallel ausgearbeitet werden. Notensatzprogramme können solchen nichtlinearen, kreativen Prozessen aber kaum gerecht werden.

                

                    
Musizieren:
 Im Ensemble spielen die Musizierenden oft nicht aus der Partitur, sondern aus Einzelstimmen, und sehen damit nicht, was ihre Mitmusizierenden spielen. In der Bandmusik und in improvisatorischen Kontexten reduziert sich auch das eigene Notenmaterial weiter auf Lead Sheets oder nur mündliche Absprachen zu Akkordfolgen und Tempo. Die Kommunikation beim Ensemblespiel geschieht über den vermittelnden Dirigenten, Sichtkontakt, Bewegungsgesten und das eigene Gehör. Das Notenmaterial ist in dieser Konstellation allerdings kein starres Objekt, das nur noch gelesen wird. Das wird vor allem bei den zahlreichen Eintragungen der Musizierenden deutlich: Interpretationsanweisungen, Hinweise zur Kommunikation, Ergänzungen und Veränderungen von Noten. Bei vernetzten, digitalen Noten könnten diese auch mit den anderen geteilt werden. Maus und Tastatur sind in diesem Szenario nicht praktikabel. Für das Weiterblättern kommen daher Pedale und Taster zum Einsatz. Im Idealfall hört das Gerät sogar mit, führt ein Audio-to-Score Alignment aus und blättert vollautomatisch weiter.
                

                
Auf der Seite der Musikproduktion werden Eintragungen in Partituren gezeichnet und dienen dazu, Takes im musikalischen Kontext zu verorten, gelungene oder weniger gelungene Stellen zu annotieren und den Schnittplan zu erstellen. Dabei interagieren die Noten mit den Aufnahmedaten in einer Digital Audio Workstation (DAW) (Waloschek 2017).

                

                    
Gaming
: Visualisierungen sind oft stilisierte und abstrahierte Ableitungen klassischer Musiknotation. Diese können, ebenso wie die erklingende Musik, fest vorgegeben und unveränderlich sein, wobei sie dann Tempo und Rhythmus von Geschicklichkeitsübungen diktieren. Sie können aber auch interaktiv sein, sodass die Spielenden durch ihre Interaktion Einfluss auf die Musik nehmen, sie spielend erzeugen oder arrangieren (Berndt 2011).
                

                

                    
Präsentation:
 Dies zielt vor allem darauf ab, neue Zugänge zum Verständnis der Musik zu schaffen. In den Anwendungsgebieten Musikwissenschaft, Music Information Retrieval und Musikvermittlung gilt die Interaktion daher vor allem der Annotation von Analyseergebnissen. In Videos werden Notentext und klingende Musik synchron dargestellt, um ein mitlesendes Hören zu ermöglichen. Einige YouTube-Kanäle inszenieren ihre Musikanalysen in Form aufwendig angefertigter, annotierter Partiturreduktionen
. Im schulischen und akademischen Musikunterricht findet sich das Notenbild großformatig auf der (digitalen) Tafel wieder, wo kurze Notentexte verfasst, editiert, angehört und nachgespielt werden. Da meist klassische Notensatzprogramme an die Wand projiziert werden, fällt auf, dass hierfür immer wieder zu Maus und Tastatur gegriffen werden muss, also nicht mit dem Tafelbild direkt interagiert wird. In rein virtuellen und hybriden Unterrichtsformen betreiben die Beteiligten jeweils lokal ein Notensatzprogramm, können zwar Bild und Ton mit den anderen teilen, nicht aber gemeinsam am selben Notentext arbeiten. Hierfür lohnt ein Blick in Museen, wo musikbezogene Medien zumeist von mehreren Besuchern gleichzeitig erlebt und bedient werden können, sei es an großen Multitouch Displays, Tabletops, mittels Freihandgesteninteraktion (Berndt 2016) oder in raumgreifenden Klanginstallationen (Berndt 2019).
                

                
Die meisten der hier aufgeführten Anwendungsszenarien erfordern aber mehr als nur neue Funktionen innerhalb der etablierten und durch ihre Konventionen geprägten Notensatzprogramme. Sie erfordern ein unverstelltes Neudenken von interaktiven Zugängen zum Medium Notentext. 

            

            

                
Interaktion jenseits von Notensatz

                
 

                
Interaktionen mit Musiknotation lassen sich nur schwer von ihren musikalischen Realisationen trennen. Daher überrascht es nicht, dass die Exploration von neuen Eingabemodalitäten vor allem in der künstlerischen Forschung geschieht. Hier sind meist die schnelle und unmittelbare Eingabe von Noten Hauptaspekt der Betrachtung. Der Fokus auf die künstlerische Forschung konnte vor allem durch eine Institutionalisierung der „New Interfaces for Musical Expression“ (NIME) geschehen. In einer Analyse der Texte der gleichnamigen Konferenzen der letzten 20 Jahre zeigt sich, dass diese vor allem neuartigen Musikinstrumenten, Aufführungen und elektronischen Kontrollmöglichkeiten gewidmet sind (Fasciani 2021). Viele dieser Controller, Software und Interaktionsformen sind jedoch meist schwer generalisierbar und werden nur durch ein Werk oder eine Menge an Werken desselben Künstlers exemplifiziert. Solche Formen sind etwa die Synthese von Noten durch Sensoren am Körper oder Objekte, wie etwa Taktstöcke. Dadurch können aus einer vorher festgelegten Geste Notationen entstehen, die dann von Maschine oder Musizierenden umgesetzt werden. Eine Geste wird übersetzt in musikalische Parameter, wie z.B. Tonhöhe, Dynamik etc., und ermöglichen abstraktere Interaktionen als Notensatzprogramme es erfordern. Es entstehen live notierte Abschnitte, welche als eine Reihe von musikalischen Gesten dargestellt werden. Dabei macht es keinen Unterschied, ob diese Notationen in Form von CWMN oder anderer graphischer Elemente dargestellt werden (Frame 2022). Darüber hinaus sind aktionsbasierte Notationen gestische Anweisungen, die so dargestellt werden, dass sie direkt und ohne Kenntnis von anderen Notationssystemen ausgeführt werden können. Sie werden im Voraus erstellt und ähneln in ihrem Mapping Tabulaturen, erweitern sie aber durch stilisierte Animationen von Hand- oder Körperbewegungen, welche den entsprechenden Klang erzeugen sollen (Clay 2010). Diese Bewegungen können elektronisch analysiert werden, um z.B. automatisch „weiterzublättern“ (Dori 2020).

                
Als Erweiterung der sensorbedingten Interaktion kann man die Nutzung von Virtual Reality (VR) verstehen. Die Immersion der Nutzenden geschieht durch die Übertragung der körperlichen Bewegung in einen simulierten, dreidimensionalen Raum. Bewegungen können in den realen Raum abgebildet werden und können, z.B. als Nachzeichnungen dieser Bewegungen, Ausgang einer Notation für Musiker*innen sein (Santini 2022). Dafür benötigt es u.a. Controller, welche das Greifen und Zeigen ersetzen. Ebenso muss die Körperposition verfolgt werden können, um eine Beziehung zu den simulierten Objekten herzustellen. Solches Greifen wird auch in Anwendungen zu Lernszenarien mit haptischen und räumlichen Komponenten genutzt, in welchen Nutzer Noten oder Harmonieverläufe direkt vertikal und horizontal anordnen und so allein durch die Kopfbewegung einen direkten Überblick über das Geschaffene gewinnen können (Shvets 2022).

                
Im Unterschied dazu wird in der Augmented Reality (AR) kein Raum simuliert, sondern Informationen im realen Raum ergänzt. Neben spezialisierten Geräten, wie der HoloLens,
 sind auch viele Smartphones in der Lage, das Livekamerabild zu analysieren und mit Augmentierungen anzureichern. Beispiele für die AR-Nutzung gibt es im Bereich der Instrumentallehre. Darstellung der Noten können als Pianorolle direkt über der Klaviatur dargestellt werden und ermöglicht so das gleichzeitige Beobachten von Notation, Fingern und Instrument (Kim-Boyle 2022).
                

                
Jenseits vom Notensatz erhält die Notation also immer mehr räumliche Bedeutung. Durch Web-Technologien wird auch die Entfernung zwischen den Zusammenspielenden beliebig. So können etwa durch Interaktionen der Dirigierenden Notationen direkt auf Displays im Orchester verteilt werden (Andersen 2022), oder es ermöglicht den Musizierenden sich über verschiedene Geräte sowohl in Proben- als auch in Aufführungssituationen zu synchronisieren (Bell 2017, Bell 2021).

                
Die realweltliche Interaktion mit dem Notenblatt auf dem Notenpult findet über Handgesten (z.B. Umblättern des Notenblattes, Zeigen) und Stift (z.B., Schreiben von Noten, Ergänzen von Vortragsanweisungen) statt. Vor diesem Hintergrund kommt auch der mittlerweile sehr umfangreichen Forschung zu Touch- und Stiftinteraktion - und im genannten Nutzungsszenario insbesondere auf Tablets - eine große Relevanz zu (Baró 2019).

                
In den Besprechungen der NIMEs werden allerdings nichtkünstlerische Bereiche oft vernachlässigt. So werden z.B. in der Pädagogik musikbezogene Interfaces auch als Mittel genutzt, um das Lernen in anderen Domänen zu erleichtern. Das „Computational Music Thinking“ (Repenning 2019) ist ein Ansatz, um informatische und musikalische Konzepte zu verknüpfen und Wissen aus dem jeweils anderen Bereich zu übertragen. Grundlegend dafür ist die Überzeugung, dass musikalische und programmatische Muster (
                    
Patterns
) übersetzbar seien. Daraus entstehen Programmierumgebungen, die durch die Abstraktion der musikalischen Elemente eher visuellen Programmiersprachen, wie Max/MSP
 ähneln. Die Interaktion ist hier aber klar auf Maus und Tastatur ausgerichtet.
                

                
Als Ziel der Interaktionsforschung sticht also deutlich die künstlerische Aufführung hervor. Für deren Verwendung in Forschungsbereichen der Digital Humanities möchten wir sie in einem möglichen Design Space einbetten. 

            

            

                
Bestimmung des Design Space

                
Als Design Space verstehen wir den Raum, in dem die Elemente vorhanden sind, die für die Konstitution eines Forschungsbereiches herangezogen werden können. Der Raum sei hier aber nicht nur als Metapher verstanden. Als Instrument der Positionsbestimmung im Design Space kann die Kombination der Elemente als Vektor dargestellt werden. Abbildung 1 zeigt nur eine flache Darstellung der verschiedenen Dimensionen, welche in verschiedenen Farben kodiert sind. 

                
Die Auswahl der Elemente basiert auf der Analyse von Interaktionsformen mit musikalischer Notation. Die Dimensionen werden definiert durch die Objekte der Interaktion, der Granularität ihrer Interaktionsgegenstände, des musikalischen und sozialen Kontexts der Nutzung, sowie Fragen nach verschiedenen Darstellungsformen und Interaktionsgeräten. Diese Auflistung ist beileibe nicht vollständig, geben aber eine Orientierung und Klassifikation aktueller und zukünftiger Forschungsfelder.

                
In klassischen Musiknotationsprogrammen interagieren die Nutzenden mit Notationszeichen auf der logischen Ebene nach der Klassifikation von Maxwell (Maxwell 1981): Es können nur syntaktisch wohlgeformte Notationen erstellt werden. Die kleinste Granularität der Interaktion ist hier also die Manipulation von Musikzeichen auf der logischen Ebene. In handgeschriebenen Notationen finden sich allerdings manchmal Verletzungen dieser „Logik“, zum Beispiel durch Auslassungen, abgekürzte Notationen oder absichtliche Überschreitungen. Daher erlaubt das bei digitalen historisch-kritischen Ausgaben häufig benutzte MEI-Format (Hankinson 2011) die Modellierung von Inhalten u.a. auf der grafischen Ebene, um damit auch Merkmale von Handschriften nachbilden zu können. Eine weitere Ebene tiefer kann es auch von Interesse sein, mit den grafischen Grundelementen des Notentextes als Vektorgrafik zu interagieren, um besondere gestalterische Ziele umzusetzen. Andererseits ist es auch sinnvoll mit Notentexten auf höheren Granularitätsstufen zu interagieren: Beispielsweise werden in digitalen Musikeditionen häufig verschiedenen Quellen taktweise verlinkt, um Ähnlichkeiten und Abweichungen zu erkennen. Interfaces sollten auf unterschiedlichen Stufen bedienbar sein, unter anderem um graphische Primitive, Musikzeichen auf der grafischen oder logischen Ebene, Takte, Stimmen, von Anwender*innen ausgewählte Teile bis hin zu Musiksammlungen zu manipulieren.

                
Solche Interfaces werden auch in vielfältigen sozialen Kontexten verwendet, ob gemeinsam oder allein, beim Einstudieren eines Stückes, in einer Probe, bei der gemeinsamen Arbeit an einem Notentext oder im Musikunterricht. Auch der Musikstil hat einen Einfluss auf das Interface Design, insbesondere welche Darstellungsformen zum Einsatz kommen: Wenn improvisiert wird, könnten Lead Sheets zum Einsatz kommen. Bei Neuer Musik oder in NIME-Performances mit Controllern und Live-Elektronik, kommen meist grafische Partituren zum Einsatz. Piano Roll und auf Spektrogramm basierende Darstellungsformen können unabhängig von musikalischen Vorkenntnissen in verschiedenen Stilen genutzt werden.

                
Zuletzt spielen auch die eingesetzten Interaktionsgeräte und -technologien eine Rolle. Noch vor WIMP
-basierten Interfaces für PCs und Laptops, sind heute Touch-Interfaces für Smartphones und Tablets das wichtigste Medium für den Umgang mit Noten, während Interfaces für AR/VR sich noch in einem experimentellen Stadium befinden.
                

                
Ein Beispiel: Die Elemente sind aus den jeweiligen Kategorien frei kombinierbar. So ließe sich z.B. ein Vektor erstellen, der alle fünf Kategorien umfasst: [AR, Sammlungen, Skizzenhaft, NIME, Lehre]. In diesem Fall könnte man sich eine AR-Anwendung (vielleicht auf einem Smartphone) vorstellen, welche auf vordefinierte Sammlungen angewandt werden könnte. Die Skizzen die daraus automatisiert erstellt werden, wären Grundlagen für Performances mit einem NIME, welche in einem Lehrkontext (im Musikunterreicht oder an einer Musikhochschule) besprochen werden. Beiträge dazu würden beispielsweise die kreative Aneignung von klassischer Musik durch AR erforschen. Verkleinert man den Vektor und lässt den sozialen Kontext „Lehre“ und das Interaktionsmedium „AR“ heraus, so hätte man eine Fokussierung auf Sammlung beispielsweise als Grundlage generativer Musik oder für die Nachmodellierung aktueller kompositorischer Prozesse. Bei der Aufführung bietet sich darüber hinaus eine Aufzeichnung als Forschungsgegenstand an, die selbst weiteren digitalen Analysen unterzogen werden kann. 

                
Mit diesem Paper haben wir die vielfältigen Perspektiven für Interaktionen mit Musiknotationen analysiert. Dabei haben wir auch gezeigt welches Innovationspotential hier noch offen liegt – ein lohnendes Feld für zukünftige Forschungen und Entwicklungen.

                
​​

            

        

            

                
Motivation und Einleitung

                
In Wissenschaftsdisziplinen wie den digitalen Geistes- und Sozialwissenschaften „Digital Humanities“ (DH), besteht ein großes Interesse daran, umfangreiche Mengen von Text, z.B. aus historischen Zeitschriften (Purschwitz 2018, 109–142), sozialen Medien (Stier u. a. 2017, 1365–1388) oder Zeitungen, hinsichtlich verschiedener Fragestellungen auszuwerten. Dabei ist oft eine Kombination von qualitativen Analyseschritten mit quantitativen Auswertungen gefragt (Stulpe und Lemke 2016, 17–61).

                
Eine ausschließlich manuelle Bearbeitung ist angesichts des Umfangs und des daraus resultierenden Aufwands oftmals ausgeschlossen. In diesen Fällen ermöglicht es eine (teil-)automatisierte computerlinguistische Verarbeitung dennoch Analyse und Auswertung durchzuführen.

                
Ziel dieses Dissertationsvorhabens ist es daher geeignete Methoden zu entwickeln und in eine moderne Sprachtechnologieplattform zu integrieren, die es Wissenschaftlern aus den DH ermöglicht, selbstständig große Textkorpora mit Hilfe (teil-)automatischer Verarbeitungsschritte aus der CL vorzubereiten und hinsichtlich vielfältiger Fragestellungen auszuwerten. Dies umfasst lexikalische wie semantische Suchfunktionen, intuitive Annotationsfunktionen, AI- bzw. Human-in-the-Loop Funktionalitäten zur (teil-)automatischen Skalierung von Annotationen auf große Korpora, über klassische syntaktische Anreicherungen hinausgehende inhaltliche NLP-Methoden wie Koreferenzausflösung und Zitaterkennung sowie qualitative und quantitative Analysemöglichkeiten.

            

            

                
Forschungsstand

                
Verwandte Software aus der Computerlinguistik wie 
                    
brat
 (Stenetorp u. a. 2012, 102–107), 
                    
WebAnno
 (Eckart de Castilho u. a. 2016, 76–84), 
                    
INCEpTION
 (Klie u. a. 2018, 5–9) oder 
                    
TextAnnotator
 (Abrami u. a. 2020, 891–900) sind vorrangig für linguistische Annotationen gedacht, um annotierte Korpora zu erstellen; weniger jedoch für die inhaltliche Bearbeitung einer DH-Forschungsfrage mittels Suche, Annotation, Aggregation und Analyse.
                

                
Für spezifische Recherchezwecke existieren computerlinguistische Anwendungen wie 
                    
SiNLP
 (Crossley u. a. 2014, 511–534) zur Diskursanalyse, 
                    
ALCIDE
 (Moretti u. a. 2016, 100–112) zur Analyse von historischem und politischem Diskurs, 
                    
new/s/leak
 (Wiedemann u. a. 2018, 313–322) zum Entdecken berichtenswerter Geschehnisse in großen Textkorpora sowie 
                    
LawStats
 (Ruppert u. a. 2018, 212–222) zur Suche und Analyse von Revisionen des Bundesgerichtshofs. Diese Anwendungen sind jedoch weniger geeignet andere Fragestellungen zu bearbeiten.
                

                
In den Sozialwissenschaften gängige qualitative Analysetools wie 
                    
MaxQDA
 oder 
                    
atlas.ti
 verfügen über Annotations- und qualitative Analysefunktionen, sind aber proprietär, erlauben kaum Kollaboration und bieten keine modernen NLP-Methoden.
                

                

                    
Recogito
 (Simon u. a. 2017, 111–132) und 
                    
CATMA
 (Gius u. a., 2022) sind intuitive Annotationsanwendungen für die DH mit Kollaborationsfunktionen. 
                    
Recogito 
setzt 
                    
e
inen Fokus auf Orte und Integration in eine Karte mittels automatischer Erkennung von Entitäten. 
                    
CATMA
 bietet konfigurierbare Annotationsschemata und vielfältige Analysefunktionen. Beide verfügen jedoch nur über eine bescheidene bzw. keine Suche, wenig Unterstützung für große Korpora und keine Möglichkeit zur Skalierung manueller Annotationen.
                

                

                    
WebLicht
 (Hinrichs u. a. 2010, 25–29) integriert enorm viele NLP-Module, die zu individuellen Pipelines zusammengefügt werden können.
                    
 

                    
Nopaque
 (Universität Bielefeld 2022) unterstützt historische Dokumente mittels OCR und syntaktische Analysen anhand Keyword-In-Context-Suche auf Basis
                    
 
gängiger NLP-Modelle und individueller NLP-Pipelines. Beiden fehlen jedoch Annotationsmöglichkeiten, Kollaborationsfunktionen, Dokumentensuche sowie Möglichkeiten zur quantitativen Analyse großer Korpora.
                

                

                    
ILCM
 (Niekler u. a. 2018, 1313–1319) ist eine Textmining-Umgebung für die datengetriebene Forschung auf der großen Textmengen mittels statistischer Analysen. Es fehlen jedoch moderne NLP-Modelle sowie interaktive Funktionalitäten wie Annotation oder Suche.
                

            

            

                
Forschungsvorhaben und Forschungsfragen

                
Das Forschungsvorhaben steht unter der übergreifenden Forschungsfrage: Wie kann eine digitale Arbeitsumgebung entwickelt werden, welche undogmatisch die Anwendung von DH-Methoden durch moderne NLP-Methoden für Suche, Annotation, Skalierung, Aggregation und Analyse auf großen Korpora unterstützt?

                
Das Ziel ist es geeignete Methoden für eine Sprachtechnologieplattform zu entwerfen, die intuitiv aus den DH genutzt werden kann, um manuelle Arbeit mit Textdokumenten automatisch auf große Korpora zu skalieren. Dies umfasst u. a. eine semantischen Suche um ähnliche Aussagen zu einer bestimmten Textpassage im gesamten Korpus zu finden, die Möglichkeit gefundenen Textpassagen qualitativ manuell zu analysieren oder automatisch zu aggregieren für quantitative Auswertungen. Im Zusammenspiel von Entity-Linking, Koreferenzauflösung, Zitaterkennung und Auffinden ähnlicher Textpassagen soll eine Skalierung manueller Annotation bzw. Kodierung von Textspannen auf den gesamten Korpus ermöglicht werden, indem Annotationen einzelner Textstellen auf passende Stellen gesamten Korpus angewandt wird.

                
Wie können dem aktuellen Stand der Forschung entsprechende computerlinguistische Methoden undogmatisch und intuitiv für die Bearbeitung verschiedener DH-Fragestellungen nutzbar gemacht werden?

                
Zum aktuellen Stand der Forschung zählen kontextualisierte Embeddings wie 
                    
BERT
 (Devlin u. a. 2019, 4171–4186), die jedem Wort und (Ab-)Satz eine Bedeutung in Abhängigkeit des Kontexts anhand der Position in einem hochdimensionalen Vektorraum zuordnen. Wie lässt sich auf dieser Basis eine semantische Ähnlichkeitssuche zum Auffinden verwandter Aussagen mit variierender Länge entwickeln? Dabei ist zu klären, welche Lösungen für die rechenintensiven Operationen bei der Ähnlichkeitssuche mit großen Korpora skalieren.
                

                
Um diese NLP-Methoden nutzbar zu machen, werden sie in eine webbasierte Benutzeroberfläche integriert, die das Durchsuchen, Annotieren, Skalieren, Aggregieren und Auswerten großer Korpora mittels der zuvor beschriebenen computerlinguistischen Funktionalitäten unterstützt. Ferner wird geeignete Softwarearchitektur entworfen, welche die Integration bestehender und zukünftiger CL-Softwarebibliotheken ermöglicht.

            

        

            
Eines der Hindernisse, die der freien Weitergabe von Forschungsdaten im Sinne der Open-Science-Bewegung im Wege stehen, ist das Urheberrecht (UrhG). Dieses erschwert die Einhaltung der guten wissenschaftlichen Praxis und der FAIR-Prinzipien (Wilkinson et al. 2016) insbesondere bei Forschung zu zeitgenössischen Texten. Für urheberrechtlich geschützte Texte besteht bisher nur die Möglichkeit, sog. abgeleitete Textformate (Schöch et al. 2020) für „non-consumptive access“ (Organisciak und Downie 2021) zu veröffentlichen, etwa in Form von Frequenzlisten. Geisteswissenschaftliche Forschungsprojekte sind allerdings häufig auf die Verfügbarkeit von textuellem Kontext angewiesen, der erst eine angemessene Interpretation der Daten erlaubt. Um die Nachnutzbarkeit von Textdaten in dieser Hinsicht zu unterstützen, wurde im Projekt XSample
 ein Workflow entwickelt, der auf das Recht zur Weitergabe von Textauszügen aufbaut, und dabei ermöglicht, die Auszüge auf das eigene Forschungsanliegen hin zu optimieren.
            

            

                

                    
Rechtslage 
                

                
Im deutschen UrhG unterliegen Texte bis 70 Jahre nach dem Tod der Autor*innen dem urheberrechtlichen Schutz, der die Vervielfältigung und die öffentliche Zugänglichmachung von Texten erheblich einschränkt. Hiervon sind vor allem zeitgenössische Texte betroffen, die aus diesem Grund womöglich gar nicht erst als Gegenstand von Forschungsprojekten in Erwägung gezogen werden. Seit 2018 ist zumindest die Verwendung von urheberrechtlich geschützten Texten zu Zwecken des Text- und Dataminings durch §60d UrhG legitimiert (vgl. Raue 2021). Die Nachnutzung der Daten nach Abschluss eines Projektes ist aber weiterhin nur unklar geregelt (vgl. Kleinkopf et al. 2021, Andresen et al. 2022). 

                
Der XSample-Workflow kombiniert den §60d UrhG mit § 60c Abs. 1 Nr. 1 UrhG, der es erlaubt, bis zu 15 Prozent von Werken und auch vollständige Werke geringen Umfangs zu Zwecken der nicht-kommerziellen wissenschaftlichen Forschung zu vervielfältigen und an bestimmt abgegrenzte Personenkreise für deren eigene wissenschaftliche Forschung weiterzugeben. 

            

            

                

                    
Weitergabe von Auszügen im XSample-Workflow
                

                
Die Weitergabe von nur 15 Prozent eines Textes erscheint auf den ersten Blick nicht hinreichend. Um die Nützlichkeit dieser Textauszüge zu maximieren, wird im XSample-Workflow über eine Benutzeroberfläche eine gezielte Textauswahl unterstützt. So können Forschende die Textauswahl genau auf die eigenen Forschungsanliegen zuschneiden. Dafür werden auch im Korpus enthaltene Annotationen berücksichtigt, sodass bei Interesse an einem bestimmten Phänomen systematisch die mit den relevanten Kategorien annotierten Textstellen extrahiert werden können (vgl. Gärtner 2020). Die Möglichkeiten und Grenzen des Auszugskonzepts sind am Beispiel zweier Anwendungsfälle aus der Literaturwissenschaft und Linguistik erprobt worden (vgl. Andresen et al. 2022). Sie zeigen beispielsweise, dass viele Forschungsfragen davon profitieren, wenn abgeleitete Textformate, die quantitative Analysen auf dem Gesamtkorpus ermöglichen, und Auszüge, die die qualitative Interpretation erlauben, kombiniert werden.

                

                    

                    
Abb. 1: Der XSample-Workflow im Überblick.

                

                
Abbildung 1 fasst das Auszugskonzept zusammen: Forscher*innen übermitteln ihre (annotierten) Forschungsdaten an Forschungsinfrastruktureinrichtungen (z. B. wissenschaftliche Bibliotheken), die diese verwalten. Nachnutzer*innen können Zugriffsanfragen an die Infrastrukturbetreiber stellen, um Auszüge aus den Forschungsdaten zu erhalten.

                
Der in Abbildung 1 beschriebene Workflow wurde im Rahmen des XSample-Projekts prototypisch in Form eines Webservices implementiert.
 Die urheberrechtlich geschützten Korpusdaten liegen hierbei sicher in einem Dataverse
-Repositorium. Öffentlich auffindbare Metadaten ermöglichen Nutzer*innen den Einstieg in den XSample-Workflow und leiten auf einen gesonderten Server weiter, wo die eigentliche Auszugserstellung stattfindet. Hierfür stehen verschiedene Verfahren zur Auswahl, die dabei helfen können, die Nützlichkeit der Auszugsdaten für die eigenen Bedarfe zu erhöhen. Für die anfragebasierte Auszugserzeugung setzt unsere Implementierung auf ein Java-Framework für Korpusanfragen (Gärtner 2020), wodurch gezielt nach bestimmten annotierten Phänomenen gesucht werden kann, die dann als Grundlage für die individuelle Auswahl der Auszüge dienen. Während des gesamten Prozesses haben Nutzer*innen keinen direkten Zugriff auf die geschützten Daten und erhalten vor dem Download ihres individuellen Auszugs lediglich grafische Veranschaulichungen der Auszugskomposition basierend auf dem gewählten Verfahren und/oder den Suchergebnissen (vgl. Andresen et al. 2022). 
                

            

            

                

                    
Fazit
                

                
„Open Access“ ist und bleibt die Zielvorstellung für offene und reproduzierbare Forschung auch in den digitalen Geisteswissenschaften. Das hier vorgestellte Auszugskonzept stellt demgegenüber eine „Behelfslösung“ dar, um die Nachnutzung urheberrechtlich geschützter Daten zu ermöglichen und der Tendenz entgegenzuwirken, diese Texte per se aus Forschungsprojekten auszuschließen (vgl. Gärtner et al. 2021). Die Lösung ist an der Forschungspraxis der digitalen Literatur- bzw. Geisteswissenschaften ausgerichtet, für die andere „verfremdende“ Verfahren wie abgeleitete Textformate (Schöch et al. 2020, Organisciak und Downie 2021) nur eingeschränkt nachnutzbar sind. Das Auszugskonzept ermöglicht hingegen eine größere Nähe zum Text, indem die ursprüngliche Textgestalt beibehalten wird, die für die Interpretation der Daten häufig unabdingbar ist. Darüber hinaus wird der rechtliche Rahmen durch die individuelle Auszugsgenerierung optimal ausgeschöpft und den individuellen Forschungsinteressen angepasst.

            

        

            
OPERAS
, Open Scholarly Communication in the European Research Area for Social Sciences and Humanities, ist eine verteilte europäische Forschungsinfrastruktur für die offene wissenschaftliche Kommunikation in den Sozial- und Geisteswissenschaften und arbeitet an innovativen, auf Nutzer:innen oder Institutionen zugeschnittenen Services. (Maryl et al. 2020) Als solche begleitet OPERAS die Entwicklung dieser Angebote sowohl auf den nationalen Ebenen der sogenannten Core-Mitglieder in Gestalt der National Nodes, die den Kontakt zum eigenen nationalen Bezugsrahmen herstellen, als auch auf der europäischen Ebene über die OPERAS eingegliederten Special Interest Groups (SIGs)
, in denen sich die OPERAS-Community über zentrale Aspekte wie Best Practices im Bereich Open Access und Fragen von Multilingualism für OPERAS austauschen kann. Das BMBF-Projekt OPERAS-GER hat unter dem Dach der Max Weber Stiftung die Rolle eines National Node für Deutschland inne und trägt somit als nationales Projekt zur Vernetzung von OPERAS mit der deutschen Wissenschaftslandschaft bei. Durch den Aufbau der nationalen Kontaktstelle wird ein Beitrag zur nachhaltigen Verknüpfung europäischer und nationaler Forschungsinfrastrukturen geleistet. Als europäische Infrastruktur ist OPERAS als Ganzes seit 2021 Teil der ESFRI Roadmap.
 (Hrušák et al. 2021, 189)
            

            
Die Services und der Aufbau von OPERAS richten sich zum einen nach den Bedarfen des akademischen Umfeldes, zum anderen nach den Problemen und Hindernissen bei der Umsetzung einer offenen Wissenschaftskultur in den Sozial- und Geisteswissenschaften. Zwei wesentliche Probleme sind dabei die Fragen nach der Qualitätssicherung von Publikationen, die Open Access publiziert werden und die Problematik der Auffindbarkeit bzw. des Zugriffs auf Open-Access-Publikationen über disziplinäre und sprachliche Grenzen hinweg (Bennett, 2013, 169; Guédon 2019, 30-33). Diese Herausforderungen werden von OPERAS angenommen und durch die Entwicklung der Services GoTriple
, im Projekt TRIPLE, Transforming Research through Innovative Practices for Linked Interdisciplinary Exploration und PRISM
, Peer Review Information Service for Monographs, beide von OPERAS entwickelt, zugleich als Chance zur Innovation begriffen (Balula and Leão 2021, 96). PRISM, basierend auf DOAB
, Directory of Open Access Books, bietet Zugang zu Peer-Reviews von Open-Access-Publikationen. GoTriple, der innovative Discovery-Service von OPERAS, soll die Sichtbarkeit von Forschenden und Open-Access-Literatur verbessern. Außerdem entwickelt OPERAS drei weitere Services, wie den Metrik-Service, der Statistiken über die Nutzung von Publikationen aus dem Open Access, zunächst basierend auf DOAB, bietet
. Das Publikationsservice-Portal wiederum soll zukünftig unterschiedliche Publikationsmodelle von OPERAS-Partnern für User übersichtlich aufführen. Schließlich unterstützt COESO
, Collaborative Engagement on Societal Issues, die Etablierung von Citizen Science in den Sozial- und Geisteswissenschaften mit der Entwicklung von 10 Pilotprojekten und der VERA-Plattform
, Virtual Ecosystem for Research Activation. 
            

            
Die OPERAS-Services PRISM und GoTriple nehmen das Problem der Qualitätssicherung und der Auffindbarkeit ins Visier. Für die Wissenschaft ist insbesondere das Vertrauen in die Zertifizierung von Forschungen von zentraler Bedeutung, Open-Access-Publikationen haben in diesem Zusammenhang den Nachteil, dass manchen Forschenden die Hürden zur Publikation zu niedrig erscheinen und zugleich die Peer-Review-Verfahren bei Open-Access-Publikationen nicht transparent erschienen. Zudem werden Open-Access-Formate noch immer nicht als Standard von manchen Herausgebern und Verlagen begriffen.

            
Zur Sicherstellung von Peer-Review-Standards für Open-Access-Monographien wird daher PRISM entwickelt und über DOAB bereitgestellt. Der Release einer ersten Vollversion ist für Herbst 2022 vorgesehen und wird zukünftig über DOAB und die European Open Science Cloud verfügbar werden. Damit werden Peer-Reviews von Open-Access-Monographien für Nutzer zentral zugänglich gemacht werden, um so die notwendige Transparenz über die Ergebnisse der Evaluation von Publikationen herzustellen, wobei Einsicht in die durchlaufenen Peer-Reviews gewährt werden kann. PRISM bietet eine leichte Integration in Bibliothekskataloge und unterstützt Metadatenformate wie MARC21, MARCXML, CSV, RIS und ONIX XML mit OAI-PMH Harvesting. Die Peer-Reviews sind dabei direkt Teil der Metadaten und frei verfügbar.

            
GoTriple als ein weiterer Service bietet eine ganze Reihe nützlicher Features zur Arbeit mit Open-Access-Publikationen an und stellt sich dabei dem Problem, dass die Auffindbarkeit von Open-Access-Veröffentlichungen durch die institutionellen und sprachlichen Grenzen und durch die Pluralität von Plattformen negativ beeinflusst werden kann. Daher wird die Plattform GoTriple als gebündelter Zugriffspunkt auf Publikationen und Daten zu Forschungen entwickelt. Als Discovery Service verfügt GoTriple über eine gleichzeitige mehrsprachige Suchfunktion in 11 europäischen Sprachen - Englisch, Französisch, Spanisch, Portugiesisch, Deutsch, Italienisch, Polnisch, Griechisch und Kroatisch, sowie Slowenisch und Ukrainisch. Auf diese wird bei einer Suchanfrage zu einem Thema direkt der gesamte unterstützte Raum, über die unterschiedlichen Sprachen hinweg, erfasst. Die Plattform wird von Huma-Num
 bereitgestellt, eine Vollversion soll 2023 erscheinen. Neben der multilingualen Suche bietet GoTriple weitere integrierte Features an, die den Discovery Service ergänzen: So können auch Daten, die Profile von Forschenden (es gibt hier die Möglichkeit eigene Nutzerprofile anzulegen, sowie über das Trustbuilding System zu vernetzen) und Projekte gesucht werden. Diese Möglichkeiten der Vernetzung mit anderen Forscher:innen und Projekten über GoTriple werden durch das Annotationstool Pundit zur gemeinsamen Ergebnissicherung und ein Crowdfunding-Angebot über die Webseite WeMakeIt ergänzt. Zum aktuellen Zeitpunkt speist sich GoTriple aus den folgenden Repositorien und Datenquellen: DOAJ (Directory of Open Access Journals), EKT (Greek National Documentation Centre), OpenAire (Open Access Infrastructure for Research in Europe)
, Isidore und CORDIS (Operational Potential of Ecosystem Research Applications). 
            

        

            
Die großen geisteswissenschaftlichen Langzeitvorhaben im deutschen Akademienprogramm umfassen auf Jahrzehnte angelegte Forschungsprojekte, die in der Vergangenheit analog konzipiert wurden und in der Gegenwart vor der Herausforderung stehen, nicht nur ihre Methoden sondern auch ihre Forschungsergebnisse und Publikationsformen einer digitalen Transformation zu unterziehen. Als ursprünglich analoges Langzeitvorhaben, mit nachträglich ergänzter digitaler Komponente, ist das Corpus Vitrearum Medii Aevi (CVMA) Deutschland ein Paradebeispiel für die kontinuierliche Entwicklung von der Buchreihe über das ergänzende digitale Bildarchiv bis hin zum Aufbau von 
                
born-digital
 Publikationen. Unter dem Namen ‘Glasmalereien im Kontext’ verlinken Mitarbeitende des Forschungsvorhabens entkontextualisierte Einzelfotografien von Kirchenfenstern, machen die Fenster in ihrem räumlichen Kontext digital erfahrbar und reichern sie mit Linked Open Data an. Der vorliegende Beitrag soll das Vorgehen bei der Entwicklung des Formats aufzeigen und argumentiert darüber hinaus, dass solche offenen digitalen Publikationsformen ein zentraler Beitrag der Digital Humanities zur Weiterentwicklung traditioneller Vorhaben und zukunftsgerichtete Anbindung ihrer Forschungsergebnisse sein können.
            

            

                

                    
Ausgangslage: Das entkontextualisierte Bildarchiv
                

                
Das CVMA Deutschland ist ein interakademisches Forschungsvorhaben mit Arbeitsstellen in Freiburg (Akademie der Wissenschaften und Literatur Mainz) sowie Potsdam (Berlin-Brandenburgische Akademie der Wissenschaften). Die Aufgabe des Corpus ist die Erfassung, Dokumentation und Katalogisierung von Glasmalereien des Mittelalters und der Frühen Neuzeit (bis etwa 1550) in Deutschland. Das CVMA Deutschland ist Mitglied des 1952 gegründeten Internationalen Corpus Vitrearum, welches sich aufgrund der erheblichen Verluste durch die beiden Weltkriege zum Ziel gesetzt hat, die Glasmalereien des Mittelalters vollständig fotografisch zu erfassen und nach festen Richtlinien zu dokumentieren. Die Forschungsergebnisse werden auf Grundlage der auf internationaler Ebene vereinbarten Richtlinien (Comité international d’histoire de l’art und Union académique internationale 2016) als Katalogtexte in den Corpusbänden publiziert. Dabei entsteht neben einem umfassenden schriftlichen Werk ein umfangreiches kuratiertes Bildarchiv, welches in Deutschland seit 2015 über eine gemeinsame Plattform strukturiert online verfügbar gemacht wird (CVMA Deutschland o. J. a). Mit Metadaten angereicherte Bilddaten dienen als nachnutzbares digitales Instrument für die wissenschaftliche Arbeit.

                
Die Rolle des Bildes als Repräsentation des Kunstwerkes ist je nach Publikationsform unterschiedlich. Im gedruckten Corpusband präsentiert sich die Abbildung als visuelle Argumentation mittels Gegenüberstellungen und vergleichenden Anbringungen inhaltlich gleichberechtigt zum Text. Beide Kommunikationsformen ergänzen sich zu einer umfassenden Darlegung der wissenschaftlichen Erkenntnisse. Digitale Bildarchive stehen im Vergleich zu klassischen Bucheditionen vor Herausforderungen auf zwei Ebenen: der des potentiellen Nutzer:innenkreises eines über das Internet frei zugänglichen und vernetzten Bildarchivs, der im Gegensatz zu dem oft engen und bildungsbürgerlichen Nutzer:innenkreis der gedruckten Corpusbände steht, wo traditionell Kontextwissen durch die Gesamtschau innerhalb eines Bandes abgedeckt wird. Online-Bildarchive sprechen in der Regel breitere Zielgruppen an, Webseiten bedienen veränderte Lese- und Betrachtungsmethoden (cf. Hyman, Moser und Segala 2014, 37). Herausforderungen stellen sich genauso auf der Ebene der nun vom Werk- als auch vom Textkontext entfremdeten Abbildung, die ihre ästhetische wie wissenschaftliche Bedeutung im Online-Bildarchiv neu manifestieren muss.

                
Das Online-Bildarchiv des CVMA Deutschland (CVMA Deutschland o. J. a) versammelt bereits mehr als 7.000 Abbildungen aus der Buchreihe sowie zusätzliches, bisher unveröffentlichtes Material und wird von den beiden Arbeitsstellen konstant um weitere Bestände erweitert. Dabei stehen Detailaufnahmen einzelner Scheiben, zugehörige Fensterdarstellungen oder Montagen komplexer Bilderzählungen nicht zwangsläufig nebeneinander. Die Sortierung der Galerieansicht orientiert sich zunächst an einer Datenlogik, nicht an ihrem Inhalt. Über diesen Zugang fehlen übergreifende Informationen zum Nachvollziehen der Zusammenhänge zwischen Einzelbildern und ihrer wissenschaftlichen Einordnung. Der Workflow einer Veröffentlichung über das Online-Bildarchiv lässt die Möglichkeit, Abbildungen semantisch zu Objekten zu gruppieren und sie innerhalb der Architektur bzw. der Räume zu verorten, in denen sie sich befinden, als Desiderat zurück. 

            

            

                

                    
Grundlage: Nachhaltiges und interoperables Metadatenmanagement
                

                
Grundlage für die Erweiterung und Weiterentwicklung des Online-Bildarchivs auf ein 
                    
born-digital
 Publikationsformat ist die einheitliche Anlage von Metadaten entlang internationaler Standards. Die Grundsätze des nachhaltigen und interoperablen Metadatenmanagements müssen nach der Langzeitverfügbarkeit der Daten und damit auch nach der sie zugänglich machenden Soft- und Hardware fragen. Die Antwort zielt dabei auf Standards, die von vielen geteilt, verstanden und weiterentwickelt werden. Auch eine interoperable und nachhaltige Datenmodellierung, die unabhängig von ausführenden und alternden Softwarelösungen lesbar bleiben möchte, basiert auf standardisierten Ausdrücken und generischen Ansätzen der Formulierung von Beziehungen.
                

                
Die Integration von Metadaten in die Bilddateien ist ein maßgeblicher Bestandteil der Strategie ihrer Langzeitarchivierung und Auffindbarkeit. Das CVMA orientiert sich dabei am Standard für PDF-Dokumente, der 
                    
Extensible Metadata Platform
 (XMP), seit 2012 ISO Standard ISO 16684-1 (ISO 2019). Die Metadaten werden hierbei in den Header der Bilddateien geschrieben, so dass die digitale Ressource und ihre Metadaten eine Einheit bilden. Die CVMA-XMP-Metadatenspezifikation (CVMA Deutschland 2016)vereint administrative, beschreibende und technische Metadaten. Sie nutzt weit verbreitete Standards nach, z. B. Dublin Core, IPTC und wurde um einen CVMA-spezifischen 
                    
namespace 
erweitert, mit dem die genuin glasmalereispezifischen Informationen erfasst werden. Die Bilddateien enthalten auf diese Weise ihre relevante Metadaten und können durch Dritte heruntergeladen werden, wobei die Einheit der digitalen Fotografie und ihrer Metadaten eine umfassende Nachnutzbarkeit über die visuelle Information hinaus gewährleistet. 
                

                
Die Auszeichnung der Abbildungen mit Normdaten nutzt nicht nur der Verlinkung von Informationen mit außenstehenden Repositorien, sondern dient auch der internen Strukturierung des Bildarchivs. So werden prospektiv alle Normdateneinträge, zur Zeit zumindest die IconClass Notationen sowie die Personendaten, neben den eigentlichen Bilddateien als Ressourcen des Archivs betrachtet und prozessiert. Jede Ressource erhält durch die automatisierte Vergabe von Uniform Ressource Identifiern (URI) eine CVMA-interne ID. Diese erlaubt eine eindeutige Referenzierung innerhalb des Bildarchivs und auch nach außen. Über unterschiedliche Serialisierungen können diese Ressourcen für verschiedene Bedarfe ausgegeben werden. JSON-, RDF- und TTL-Formate erlauben eine Einbindung der Ressourcen in weitere Datenkontexte, die HTML-Serialisierung dient in erster Linie einer visuellen Erfassung der Ressourcen über den Browser. Für die Bilddaten ist dies die Einzelansicht der Abbildung im Online-Bildarchiv mit einer Auswahl an relevanten Metadaten, die über Kartenansicht und begleitender Textanzeige ein grundlegendes Informationsset zum dargestellten Werk bietet. Für Iconclass-Notationen umfasst dies die Darstellung aller Bildressourcen aus dem Online-Bildarchiv, die mit jener spezifischen Notation ausgezeichnet sind.

            

            

                

                    
Erweiterung: Das Cultural Heritage Framework
                

                
Das Ziel der Strukturierung des Bildarchivs lautet aber nicht, weitere Sammlungen zu erzeugen, sondern die Daten durch eine komplexere semantische Verknüpfung zu modellieren. Dafür kommt das an der Digitalen Akademie in Mainz entwickelte Cultural Heritage Framework, kurz CHF, zum Einsatz (Schrade 2017).

                
Das Framework dient unterschiedlichen Projekten, die mit Objekterfassung im Bereich des kulturellen Erbes befasst sind, dazu, Digitalisate unterschiedlicher Provenienz zu einem Kulturerbeobjekt zu verknüpfen, mit (wissenschaftlichen) Texten anzureichern und als 
                    
born-digital
 Onlinepublikation zu veröffentlichen.
                

                
Das Datenmodell besteht aus mehreren Komponenten, die auf den jeweiligen Erfassungsbedarf des Projektes angepasst werden: ‘Artefakte’, ‘Entitäten’, ‘Ereignisse’, ‘Personen’ und ‘Orte’. Artefakte können dabei mit den digitalen institutionellen Sammlungen, zum Beispiel Ressourcen aus dem CVMA Online-Bildarchiv, verknüpft werden. Datierungen, geographische Angaben wie Koordinaten, Ortsangaben oder 
                    
Geonames Identifier
 und Stichwörter werden zwischen den entsprechenden Datenbanken, die diese Informationen speichern, sowie dem Datenmodell ausgetauscht. Es organisiert textuelle und nicht-textuelle Forschungsdaten. 
                

                
Für das CVMA wurde hieraus das auf der Webseite verfügbar gemachte Modul “Glasmalerei im Kontext” entwickelt. Das Publikationsmodell kann aus drei Perspektiven betrachtet werden. 

                
Aus der Sicht des Datenmodells strukturiert es Forschungsdaten zu Bauwerken, meist Kirchen, hierarchisch: vom Standort zum Gebäude, zum Gebäudeteil, zum Fenster, zur Einzelscheibe bzw. Montage. Spezielle Entitäten können mit weiteren spezifischen Ressourcen verknüpft werden. Die oberste Hierarchieebene sieht die Einbindung eines Grundrisses vor, während die unterste Hierarchieebene eine direkte Verlinkung zu den Bildressourcen des Online-Bildarchivs erlaubt. Auf allen Ebenen der Fenster- und Scheibendarstellungen wird eine Gegenüberstellung der Glasmalereien und ihrer Erhaltungsschemata ermöglicht, um mittelalterlich erhaltene Bestandteile und die Anteile möglicher Restaurierungsmaßnahmen visuell nachzuvollziehen. Damit wird das Publikationsformat zu einem wertvollen Forschungsinstrument.

                
Aus der Sicht der Abbildungen bietet das Modul ‘Glasmalereien im Kontext’ die Möglichkeit zur semantischen Strukturierung und kontextualisierten Darstellung der im Online-Bildarchiv publizierten Bildressourcen. So bietet sich dem Nutzer des Bildarchivs bei jeder über das Modul verlinkten Ressource automatisch der Weg in die Standortdarstellung. Eine Datenmodellierung über das Content Management System und dem dort aufgesetzten CHF wirkt sich also strukturierend auf das Online-Bildarchiv aus.

                
Aus der Sicht der Nutzenden schließlich eröffnet sich ein niedrigschwelliger Zugang nicht nur zu einzelnen Forschungsressourcen, wie im Bildarchiv, sondern auch zu den Forschungsergebnissen, die in Form von Bildmaterial und kurzen Texten als Microsites veröffentlicht sind. Hier gibt es in übersichtlichen Schritten immer tiefergehende Informationen von der Baugeschichte des Kirchenstandortes bis zu den Vorbildern einzelner in den Glasmalereien dargestellter Szenen.

            

            

                

                    
Ergebnis: Glasmalereien im Kontext – 
                    
Born-Digitals
 als semantische Datenmodellierung, offene digitale Publikationsform und Open Educational Resource
                

                
‘Glasmalereien im Kontext’ ist eine Kombination aus semantischer Datenmodellierung und deren Ausspielung als digitale Publikationsform (CVMA Deutschland o. J. b). Aufgeschlüsselt nach Standorten können Nutzer:innen hier Kirchenräume und ihren mittelalterlichen Glasmalereibestand über Text- und Bildmaterial erkunden. Die Notwendigkeit, Struktur in die entkontextualisiert vorliegende Bildsammlung zu bringen, hat im Ergebnis eigenständige 
                    
born-digital
 Publikationen aus vormals gedruckten Corpusbänden entstehen lassen, die sich in konservativer Modellierung an der Struktur der Printpublikationen orientieren und darüber hinaus auch innovative Modellierungen zulassen, die als niedrigschwellige aber hochwertige, aktualisierte und non-lineare Ergänzungen zu den gedruckten Werken veröffentlicht werden.
                

                
Für die nah am Corpusband formulierte Modellierung werden ausgewählte Standorte mit überarbeiteten und reduzierten Katalogtexten zu den jeweiligen Gesamtfenstern, Szenen und Einzelscheiben angeboten. Eine Kirche wird in Objektgruppen untergliedert, bei denen es sich um Gebäudeteile wie Chor oder Langhaus handelt. Die darunter liegenden Ebenen bilden die dort enthaltenen Objekte. Dies sind die Buntglasfenster, die in ihrer Gesamtdarstellung als Bildmontagen und im Detail als Einzelscheiben dargestellt sind. Beide Entitäten sind im Objekt enthaltene Objektteile.

                
Während eines mehrere Jahrzehnte umspannenden Forschungszeitraums ergeben sich unter Umständen für bereits bearbeitete Objekte neue Erkenntnisse oder Forschungsmeinungen, die der Revision bedürfen. Das born-digital Format ‘Glasmalereien im Kontext’ bietet die Möglichkeit, bereits gedruckte Katalogtexte und -abbildungen in den Corpusbänden zu ergänzen und zu erweitern, auf die vorangegangenen Texte bezug zu nehmen oder von ihnen Abstand zu gewinnen. So können bspw. neue Erkenntnisse zu bereits vor geraumer Zeit publizierten Forschungen einfließen und neu aufgefundene Standorte veröffentlicht werden. Hier zeigt sich die einzigartige Möglichkeit, aktuelle Forschungen zitierfähig und nachnutzbar zu publizieren.

                
Darüber hinaus bietet das generische Datenmodell aber auch die Möglichkeit, alternative Rekonstruktionsvorschläge für einen architektonischen Teilraum, einer Objektgruppe im CHF, vorzulegen. Hier werden Abbildungen nicht in ihrer aktuellen Anbringung im Kirchenraum kontextualisiert, sondern ihre vermutete Anbringungen zu unterschiedlichen Zeitpunkten in der Vergangenheit für einen Ort modelliert. Diese Nebeneinanderstellung unterschiedlicher Zustände bzw. Rekonstruktionen stellt einen innovativen Ansatz in der Forschung der mittelalterlichen Glasmalerei dar, der insbesondere in der universitären Lehre in Seminaren Anwendung findet.

                
Dabei ist das CHF in seiner Form als ‘Glasmalereien im Kontext’ zugleich Modellierungsumgebung und semantisches Tool und wird mit der Verfügbarmachung der Ergebnisse über das Internet als digitale Publikationsform sofort zur Open Educational Resource.

            

        

            

                
Digitale Editionen sind ein Kernbereich der Digital Humanities; sie machen historische Quellen zugänglich. Dabei werden computergestützte Methoden zur Umsetzung, Verbreitung und Erforschung von wissenschaftlich fundierten Quellenveröffentlichungen herangezogen. Digitale Editionen umfassen dabei textuelle, visuelle und ggf. auch quantitative Daten und erfordern oft spezielle Benutzeroberflächen, um domänenspezifische Forschungsfragen zu bearbeiten. Obwohl jedes Editionsprojekt seine eigenen spezifischen Anforderungen hat, lassen sich einzelne Schritte identifizieren, die für Editionsvorhaben generell notwendig sind. Das ist im weitesten Sinne die Digitalisierung der Quelle mit der Verwaltung von Bildern und Text, die Transkription, die Modellierung relevanter Textphänomene mittels adäquater Auszeichnungssprachen, die Annotation semantischer Informationen und Named Entities, die Erstellung von Indizes, sowie eine den FAIR-Kriterien entsprechende Publikation über das Web. In den letzten Jahren wurde eine Vielzahl an Tools entwickelt, die für all diese Schritte eingesetzt werden.

            

            

                
Editionen bauen in der Regel auf Bilddigitalisaten der Quelle auf, deren Erstellung und Zurverfügungstellung im Aufgabenbereich von Bibliotheken und Archiven liegt. Ein Zugriff darauf ist im Idealfall mittels iiif möglich.

            

            

                
Die Transkription von Texten kann manuell, über Crowdsourcing (z. B. FromThePage) oder automatisiert (z. B. Transkribus) durchgeführt werden. Relevant dabei ist, dass unabhängig vom Werkzeug die Umwandlung des transkribierten Textes in XML/TEI möglich ist. Im bereits modellierten XML/TEI werden schließlich weitere Annotationen durchgeführt. Auch hier gibt es wieder eine breite Auswahl an Werkzeugen, deren Verwendung von projektspezifischen Anforderungen und Benutzergruppen abhängig ist. Einige sind für spezielle Forschungsbereiche konzipiert, wie z. B. LaKomp, andere bieten eine grafische Oberfläche für Editor*innen ohne tiefgreifende XML/TEI-Kenntnisse (CATMA). Wieder andere kombinieren eine Benutzeroberfläche mit bestimmten Funktionalitäten, wie z. B. einer Registerfunktion (ediarum). In einigen Anwendungsfällen bietet es sich darüber hinaus an, reines XML/TEI im Oxygen XML Editor zu schreiben. 

            

            

                
Digitale Editionen produzieren Forschungsdaten und machen diese im Idealfall unter Einhaltung der FAIR-Prinzipien zugänglich. Dies erfordert die Einbindung von Normdaten oder kontrollierten Vokabularen. Werkzeuge hierfür können OpenRefine zur halbautomatischen Verknüpfung von Entitäten sein, oder Tools wie ba[sic]; stärker datenzentrierte Projekte verwenden Tools wie Fast Cat.

            

            

                
Die Veröffentlichung und Langzeitarchivierung kann schlussendlich über Repositories und Tools wie GAMS, ARCHE, teiPublisher oder ediarum.Web erfolgen. Für manchen Editionsvorhaben ist es sinnvoll, domänenspezifische Werkzeuge oder APIs, wie z. B correspSearch für Korrespondenzen, zu verwenden. 

            

            

                
Ziel des Projekts 

                
Digital Edition Creation Pipelines: Tools and Transitions (DigEdTnT)

                
 ist es, Best-Practice-Pipelines und Tutorials für ausgewählte Tools und deren Übergänge (=Transitions) zu erstellen, die bei der Wahl der Tools und der Arbeit mit Tools zur Erstellung digitaler Editionen helfen sollen. Denn an den Übergängen ergeben sich mitunter besondere Herausforderungen, wenn beispielsweise Ergebnisse aus Transkribus nach ediarum zur weiteren Annotation überführt werden sollen. Das Projekt setzt dabei insbesondere auf eine Community-basierte Auseinandersetzung mit vorhandenen Tools. Daher sollen in zwei Workshops Nutzer*innen und Entwickler*innen zusammengeführt werden, um Anwendungsfälle und Feedback gemeinsam zu diskutieren. Wie die Übergänge zwischen einzelnen Tools abgewickelt werden können, wird letztlich in Tutorials und Guidelines sowie in Code Snippets beschrieben, die wiederum in einschlägigen Kontexten (KONDE Weißbuch, DARIAH Campus, etc.) zugänglich gemacht werden. 

            

            

                
Das eingereichte Poster soll zum Projektstart von DigEdTnT die Diskussion zu diesem Thema eröffnen und interessierte Kolleg*innen sowie auch Toolentwickler*innen adressieren.

            

        

            
Die buddhistischen Höhlenkomplexe in der Region Kucha an der nördlichen Seidenstraße (Uigurisches Autonomes Gebiet Xinjiang, VR China) beherbergen beeindruckende Wandmalereien, die etwa aus dem 5. bis 10. Jhd. stammen. Die ersten Hinweise auf eine frühere buddhistische Kultur wurden zu Beginn des 20. Jahrhunderts entdeckt, woraufhin mehrere Länder Expeditionen in das Gebiet schickten, um die einst in der Region vorherrschende Religion zu erforschen. Es war eine Sensation, als verschiedene buddhistische Höhlenkomplexe entdeckt wurden. Damals wurden auch die ersten Fotografien vom Ist-Zustand der Höhlen angefertigt und Teile der Malereien aus den Höhlen entnommen und in die jeweiligen Nationalmuseen gebracht. Heute sind Fragmente der Wandmalereien über die ganze Welt verstreut, was eine Zuordnung zu den einzelnen Ursprungshöhlen sehr schwierig macht (Weitere Informationen: Yaldiz 1987; Popova 2008; Dreyer 2015).

            
Das hier vorgestellte Projekt hat es sich zur Aufgabe gemacht, die Wandmalereien in situ und die weltweit vorhandenen Einzelstücke zu dokumentieren, zu beschreiben und mit Hilfe von historischen Fotografien wieder in ihren ursprünglichen Kontext zu bringen.

            

            
Das Projekt bedient sich moderner Möglichkeiten der Digital Humanities, indem nicht nur eine umfangreiche textliche Beschreibung einzelner Szenen erfolgt, sondern auch die Bildinhalte der sich wiederholenden Darstellungen erfasst und mit digitalen Methoden angereichert werden. Zu diesem Zweck wird das digitale Bildannotationstool Annotorious
 (siehe Abbildung 1) genutzt, um die Inhalte direkt mit einer rund 1.000 Einträge umfassenden Taxonomie zu annotieren. Die erarbeiteten Forschungsdaten stehen online frei zur Verfügung.

            

            

                

                    

                    
Abbildung 1: Annotatieren mit Annotorious

                
Die Annotation von Objekten im Bild ermöglicht zwar eine wissenschaftliche Nachvollziehbarkeit der identifizierten Objekte, ist aber auch eine sehr umfangreiche und zeitaufwändige Aufgabe. Viele Bildinhalte wiederholen sich häufig in unterschiedlichen Zusammenhängen. Außerdem liegen manchmal mehrere Bilder eines Objekts aus verschiedenen Perspektiven vor oder es gibt Bilder aus der Zeit der Expeditionen, auf denen abgetrennte Teile noch in ihrem ursprünglichen Kontext zu sehen sind. Es besteht also die Notwendigkeit, manchmal sehr ähnliche oder gleiche Objekte mehrfach zu annotieren. Die Übertragung von Annotationen ist jedoch schwierig. Selbst wenn Fotos von denselben Objekten vorhanden sind, können unterschiedliche Blickwinkel und verschiedene Objektive dazu führen, dass die Bilder verzerrt sind. Es ist kaum möglich, diese Aufgabe mit herkömmlichen Computer-Vision-Methoden automatisch durchzuführen.
            

            
Aus diesem Grund wird im Rahmen des Projekts derzeit versucht, mit den bereits vorgenommenen Annotationen Region Based Convolutional Neural Networks (RCNNs)
 zu trainieren, um in Zukunft zumindest Teile der Annotation halbautomatisch (die Annotierenden werden die Möglichkeit haben, die gefundenen Regionen des RCNNs einzusehen und diese anzunehmen oder gegebenenfalls zu verbessern) durchführen zu können.
            

            
Bislang wurden RCNNs in den Digital Humanities vor allem zur Identifizierung, Lokalisierung und Ordnung von Objekten in Bildern eingesetzt (siehe z.B.: Howanitz et al. 2019; Arnold/Tilton 2019; Duhaime 2019; Duhaime 2019; Helm et al. 2021). Ihre Verwendung für eine halbautomatische Annotation ist zumindest zur Kenntnis des Autors dieses Posterproposals noch nicht umgesetzt worden. Da die Ränder automatisch erkannten Annotationen oft ausfransen oder nicht das gesamte Objekt erkannt wird,mag es auch gewagt sein, einen solchen Versuch zu starten.

            
Die Voraussetzungen des Kucha-Projekts sind sehr gut. Es existieren bereits über 9.000 Polygone, die in insgesamt etwa 12.000 Annotationen verwendet wurden (ein Polygon kann mit mehreren Elementen der Taxonomie verknüpft sein). Einige Objekte wurden mehre hundert Mal annotiert. Es gibt jedoch auch einige Probleme, die zu berücksichtigen sind. So gibt es beispielsweise zwei grundlegend verschiedene Arten von Bildern im Korpus: zum einen Fotografien (historische und moderne), zum anderen Zeichnungen der Malereien. Die Erkennung auf Fotografien dürfte deutlich schwieriger sein, da hier die Malereien oft in sehr schlechtem Zustand sind und selbst von einem geübten Auge nur schwer zu identifizieren sind.

            
Es wurden verschiedene Experimente durchgeführt, die die prinzipielle Nutzbarkeit von RCNNs für eine semi-automatisierte Annotation testen sollten. Diese werden im Poster näher präsentiert. In einem ersten Experiment wurden alle Trainingsdaten zusammen trainiert (mAP
                
IoU =0.75
 = 5.85
). Ein zweites Experiment teilte die Daten nach Bildarten auf (mAP
                
IoU =0.75
 = 3.40 für Fotos und mAP
                
IoU =0.75
 = 4.04 bei Zeichnungen). Es war dabei auffällig, dass die Zeichnungen bessere Resultate erzielten, die aber niedriger als die Resultate des ersten Experimentes waren. Die folgenden beiden Experimenten wurden deswegen vorerst auf Zeichnungen beschränkt. Das dritte Experiment konzentrierte sich auf Klassen mit mehr als 50 Annotationen, was zwar den Großteil der Klassen außen vor ließ, aber vielversprechende Ergebnisse erzeugte ( mAP
                
IoU =0.75 
= 23.06). Deswegen wurden in einem vierten Experiment alle Klassen von menschlichen Abbildungen zu einer Metaklasse zusammengelegt. Dieses letzte Experiment funktionierte besonders vielversprechend (mAP
                
IoU =0.75
 = 59.99; ein Beispiel kann in Abbildung 2 eingesehen werden). Zur Bewertung der Experimente kann der Output der Testbilder online eingesehen werden.
 Bis zur DHd im nächsten Jahr sollen einige weitere Experimente durchgeführt werden und ein erster Prototyp zur Anwendung kommen und im Rahmen des Posters präsentiert werden.
            

            

                

                    

                    
Abbildung 2: Beispiel-Output für Experiment 4

                

            

        

            

                
Listen begegnen uns im Alltagsleben in vielfältiger Form und ihre Erstellung und Nutzung kann als fundamentale Kulturtechnik erachtet werden (Adelmann 2021, 26). Gleichzeitig ist es ebendiese Fundamentalität von Listen, die sie für wissenschaftliche Untersuchungen lange Zeit irrelevant erscheinen hat lassen: „Listen werden zumeist unterschätzt, weil sie uns so einfach und selbstverständlich vorkommen“, halten Schaffrick und Werber (2017, 303) fest und fassen damit den Forschungsstand zusammen: Wenn auch einzelne Erscheinungsformen von Listen – nämlich jene in der Literatur (u.a. Mainberger 2003, Belknap 2004, Barton et al. 2022) und modernen digitalen Kontexten (u.a. Esposito 2017, Bubenhofer 2020) – zunehmend in den Fokus der Forschung rücken, ist die Textform von den meisten Disziplinen, wie der (historischen) Linguistik, trotz ihrer hohen Frequenz bislang nur in Ausnahmefällen thematisiert worden (z.B. Doležalová 2009, Waldispühl 2019).
            

            
Dieses ‚Übersehen‘ im doppelten Sinne betrifft insbesondere die Erforschung frühneuzeitlicher Presseprodukte: Obwohl sich in einer Vielzahl historischer Zeitungen (periodisch publizierte) Listen finden, wie die „Anzeige der hier angekommenen Personen“ in der 
                
Münchner Zeitung
, die „Lista aller Getaufften“ im 
                
Wien[n]erischen Diarium
 oder das „Verzeichnis der Verstorbenen“ in der 
                
Preßburger Zeitung
, wurden Texte dieser Art im Gegensatz zu Nachrichtenartikeln (z.B. Pfefferkorn, Riecke und Schuster 2017) oder Inseraten (z.B. Bendel 1998) bislang weder theoretisch systematisiert noch korpusbasiert auf ihre textuellen Eigenschaften hin untersucht – und dies obwohl sie für frühneuzeitliche Zeitungsherausgeber einen zentralen Bestandteil ihrer Produkte darstellen. So kündigt beispielsweise Johann Baptist Schönwetter die diversen Listen des
                
 Diariums
 bereits im Titelkopf von dessen erster Ausgabe als einen „besondern Anhang / Daß auch alle die jenige Persohnen / welche wochentlich allhier gestorben / hingegen was von Vornehmen gebohren / dann copuliret worden / ferner anhero und von dannen verreiset / darinnen befindlich“ (WD 08.08.1703, 1) an und auch Johann Michael Landerer verspricht im Avertissement zur 
                
Preßburger Zeitung
, sie „wird allezeit die Verstorbenen richtig anzeigen“ (PZ 14.07.1764, 5). Dass Listen in historischen Zeitungen derart prominent Erwähnung finden, deutet auf ihren besonderen Wert für das damalige Lesepublikum hin – und damit auf ihre hohe Forschungsrelevanz. Hinzu kommt, dass gerade sogenannte ‚kleine Texte‘ sich oftmals als semiotisch hochkomplex erweisen und von „kommunikativer (formaler wie inhaltlicher) Prägnanz“ gekennzeichnet sind (Klug 2021, 219).
            

            
Mit ebendiesem Potenzial von Listen (und listenartigen
 Texten) setzt sich die Dissertation auseinander, indem sie diese innerhalb von Zeitungen zwischen 1600 und 1850 empirisch verfolgt. Praktisch umgesetzt wird dies über Methoden einer multimodalen Korpuslinguistik sowie der Digital Humanities: Indem etwa in bereits bestehenden digitalen Zeitungskorpora (z.B. ANNO, DiFMOE, DIGITARIUM, DTA, impresso, Teßmann digital) nach spezifischen sprachlichen Ausdrücken im Volltext (z.B. „List(e|a)“, „Vert?z(a|e)ichni(s|ß)“) sowie strukturellen Annotationen (z.B. TEI-Elemente &lt;list>, &lt;item>) gesucht werden kann, lässt sich über ein effizientes Distant Reading eine erste ‚Liste von Listen‘ erstellen, welche die Zeitungstextsorte für diverse Disziplinen überschaubar und damit nutzbar machen soll.
            

            
Überdies wird das identifizierte Material – im Sinne eines Close bzw. Scalable Readings (Mueller 2020) – mithilfe digitaler Textanalyse-Tools, wie Voyant Tools oder CATMA, sowohl zeitungsspezifisch als auch -übergreifend auf rekurrente textuelle Muster verschiedener Ebenen befragt:



                
Listentypus/-paradigma (z.B. Sterbeliste, Ankunftsliste, Preisliste, Inventar)

                
Selektionsprinzipien (z.B. Exklusion spezifischer Stände, Inklusion ‚leerer‘ Items)

                
Ordnungsprinzipien (z.B. chronologisch, hierarchisch, alphabetisch, geographisch)

                
Einsatz typographischer Ressourcen (z.B. Einrückungen, Aufzählungszeichen, Zwischenüberschriften)

                
Sprachliche Muster (z.B. zunehmende Abkürzungsdichte, Parallelen/Unterschiede zwischen selbem Listentypus in verschiedenen Zeitungen)

                
Pragmatische Textfunktion (z.B. Dokumentation, Handlungsaufforderung)

            

            
Zudem sollen durch Case Studies zu ausgewählten Textzeugen insbesondere die spezifischen Herausforderungen und Potenziale der historischen Zeitungstextsorte ‚Liste‘ für die Digital Humanities herausgearbeitet werden. Einen ersten Schritt in diese Richtung stellt das von der Stadt Wien geförderte Projekt „Zu Gast in Wien – digitale Ansätze zur (semi-)automatischen Auswertung der Ankunftslisten des 
                
Wien[n]erischen Diariums
“ (PI: Nina C. Rastinger, 2022–2023) dar. Hierin werden für die zwischen 1703 und 1725 zweimal wöchentlich in der historischen 
                
Wiener Zeitung
 erschienenen Listen zur „Ankunfft derer Hoch= und niederen Stands=Personen“ zuerst über Transkribus hochqualitative Volltexte erstellt und diese dann auf sprachliche Muster hin analysiert, welche wiederum als Basis für eine (semi-)automatische Named Entity Recognition und Visualisierung der Daten auf historischen Stadtplänen Wiens dienen können:
            

            

                

                
Abbildung 1: Faksimile, annotierter Volltext und Karten-Darstellung eines exemplarischen Ankunftslisteneintrags des 
                
Wien[n]erischen Diariums

            

            

            
            
Erste Ergebnisse dieser Fallstudie demonstrieren, dass Listen in frühneuzeitlichen Zeitungen aufgrund ihrer makrotypographischen Gestaltung (u.a. vermehrte Zwischenüberschriften, Einrückungen) zwar eine Herausforderung für automatische Layoutanalysen bilden, ihnen durch ihre starke Strukturiertheit, ihren diachron konsistenten Aufbau und ihre hohe semantische Dichte aber ein besonderes Potenzial für (semi-)automatische Informationsextraktionsprozesse innewohnt – wodurch ihre Volltextdigitalisierung und korpuslinguistische Untersuchung letztlich auch auf dieser Ebene einen hohen Erkenntnisgewinn verspricht.

        

            
Der zweitägige Workshop bietet eine Einführung in die 3D-Bearbeitungssoftware Blender (Blender 3.3 LTS). Blender ist eine freie, offen zugängliche und kostenlose 3D-Grafiksoftware, die ursprünglich vor allem in der Videospielbranche reüssieren konnte. Doch mit dem Erstarken der digitalen dreidimensionalen (3D) Modellierung und Rekonstruktion im Bereich der Denkmalwissenschaften, Kulturwissenschaften, Geschichte und ähnlicher Disziplinen, findet sich zunehmend Befürwortung auch aus dem wissenschaftlichen Umfeld. Mit der 3D-Rekonstruktion werden Forschungsgegenstände und -ergebnisse räumlich greifbar und nachvollziehbar. Das Modell wird förmlich zum digitalen Wissensspeicher.

            

                
Zielsetzung des Workshops

                
Der Workshop soll eine Plattform für ein interessiertes Publikum bilden, das einen umfassenden Einblick in die Funktionsweise von Blender und die daraus resultierenden Workflows erhalten möchte. Vorkenntnisse in der 3D-Modellierung oder mit der Modellierungssoftware sind nicht erforderlich. Fachwissenschaftlich sind für die Einführung keine Grenzen gesetzt, da jede Disziplin und unterschiedliche Forschungsfelder der Digital Humanities eigene Aspekte mit einbringen können und die 3D-Softwarelösung Blender äußerst flexibel einsetzbar ist. Durch Erweiterungen und Zusatzkomponenten kann diese an das eigene, spezifische Arbeitsumfeld angepasst werden.

                
Die Teilnehmer*innen des Workshops erwartet ein umfassender Einstieg in die Thematik der 3D-Rekonstruktion, der 4D-Komponente und den aus den digitalen Technologien resultierenden Möglichkeiten für Forschung, Lehre und Kunst- und Kulturvermittlung. Durch die Erweiterung um die vierte Dimension lassen sich Veränderungen und Prozesse, z.B. zeitliche Abfolgen, darstellen. Neben einer an Praxisbeispielen orientierten Einführung in die 3D-Software Blender wird auch eine kritische Diskussion über die Grenzen und Möglichkeiten von 3D-/4D-Rekonstruktionen und -Modellierungen mit den Teilnehmer*innen geführt.

                
Am Ende der Veranstaltung soll jede*r Teilnehmer*in reflektiert mit der Thematik der digitalen 3D- und 4D-Modellierung umgehen können, die Grundfunktionen der Software Blender beherrschen und eine Vorstellung von dem Spektrum möglicher Forschungsfragen haben. Zu den Ergebnissen des Workshops gehören unterschiedliche Modelle, die den Teilnehmer*innen auch im Nachhinein zur weiteren Bearbeitung zur Verfügung stehen.

            

            

                
3D- und 4D-Methoden für die Digital Humanities

                
Methoden, die eine räumliche Visualisierung, eine Rekonstruktion von Orten oder die Darstellung historischer Verläufe und sukzessive Reihenfolgen in der Zeit ermöglichen, können für sämtliche historische Fragestellungen relevant werden. Über die Fragen der Rekonstruktion und Vermittlung von vergangenen Zuständen hinaus ist auch die grundsätzliche Frage nach dem epistemologischen Stellenwert von 3D- und 4D-Methoden betroffen: Welche Aspekte und Zusammenhänge fallen rein durch Visualisierung in den Fokus der Wissenschaft? Über eine genehme Hilfestellung für die Anschaulichkeit oder die Illustration bereits formulierter Thesen hinaus werden 3D- und 4D-Prozesse für bestimmte Konstellationen zum Mittelpunkt der Forschung selbst. Etwa, wenn historische Kausal-, Raum- und Größenverhältnisse erst durch die Anschauung ihre Prägnanz erhalten. Inwiefern wären die Drei- und Vierdimensionalität jedoch „Science Fiction“, so dass die Maximen der wissenschaftlichen Objektivität und der Quellenkritik berührt würden? Könnten die Ergebnisse von Modellierungssoftware unser Erkenntnisinteresse irritieren oder fehlleiten? Im Workshop werden an den jeweiligen Stellen im Rahmen der Lehr-Module diese, die Theorie der Wissenschaftlichkeit der digitalen Modellierung betreffenden, Themen diskutiert.

                
Virtuelle Modelle sind zunehmend Teil des Arbeitsprozesses für Forschungsfragen in den Digital Humanities. Es existieren unterschiedliche Methoden, die wiederum abhängig vom Gegenstand und Ziel des Modells sind. Neben Laserscans und photogrammetrischen Aufnahmen sind auch digitale Modellierungen eine Option. Im Gegensatz zum Scan und zur Photogrammetrie, bei denen nur existierende Objekte innerhalb eines Zustandes aufgenommen werden können, kann mit Hilfe der Modellierung ein Objekt gleichsam von Null auf erstellt oder ein bestehendes Modell ergänzt werden. Diese Methode kann zum Beispiel dann zum Einsatz gelangen, wenn ein nicht mehr bestehender Gebäudezustand dargestellt werden soll. Diese digitale 3D-Modellierung hilft bei der Visualisierung von Vergangenem. Wenn neben den optischen Gesichtspunkten der Modellierung auch zeitliche Komponenten wissenschaftlich betrachtet werden sollen, vermag sich die Dreidimensionalität um die vierte Dimension der Zeit zu erweitern. Im Vorfeld einer jeden Modellierung sollte ein kritischer wissenschaftlicher Diskurs stehen, um dem Modell so viele Informationen wie möglich geben zu können, jedoch nicht die Grenze zur freien Interpretation zu überschreiten.

                
Grundbegriffe der Theorie und Systematik der wissenschaftlichen Datenmodellierung, wie das System des „Level of Detail“ (LOD), werden ebenfalls in der Veranstaltung erklärt. Die fünf Detaillierungsgrade stufen die Genauigkeit des digitalen 3D-Modells ein: von LOD 0 mit dem sog. Geländemodell bis hin zu LOD 4, einer detaillierten Nachbildung von Innen- und Außenräumen eines Gebäudes (Münster 2019, 53). Durch diese Abstufungen wird die Datenökonomie, als Strategie im Datenlebenszyklus, gewährleistet. So kann beispielsweise eine Gebäuderekonstruktion unmittelbar in einen urbanen Kontext eingebunden werden, ohne eine umfassende Modellierung der Nachbarschaftsbebauung vorzunehmen. Weitere Gebäude werden entsprechend einer vereinfachten Kubatur in LOD 1 oder 2 modelliert.

                
Die durch die Software Blender ermöglichte Technologie kann als Baustein im Repertoire weiterer bestehender Technologien zur digitalen 3D- und 4D-Modellierung und -Rekonstruktion gelten. Die Beherrschung der im Workshop vorgestellten Technologien und die Kenntnis der vermittelten exemplarischen Workflows sollen als Erweiterung des Instrumentariums zur Forschungspraxis in den Digital Humanities zu verstehen sein.

            

            

                
Open Source Software ‚Blender‘

                
Blender entwickelte sich in den letzten Jahren immer mehr zu einer vielseitig einsetzbaren 3D-Softwarelösung. Heute führt an dieser freien und plattformunabhängigen Software für 3D-Modellierungen kaum ein Weg vorbei. Mit ihrem offenen Quellcode und der großen weltweiten Community, durch die Weiterentwicklung und Updates gesichert sind, erweist sie sich daher für den Bereich der Digital Humanities als empfehlenswert. Mit einer beständig wachsenden Community mit zahlreichen Online-Tutorials, Materialien oder gar ganzen Modellen, die vermehrt kostenlos genutzt werden können, wird die Hürde am Anfang auch für den ersten Einstieg niedrig gehalten. Auch fortgeschrittene Nutzer*innen profitieren von den zahlreichen Funktionen und Erweiterungen. Zudem wird Blender als Werkzeug für die Bearbeitung und Erstellung von 3D-Modellen von weiteren Programmen, wie Game Engines oder CAD-Software, unterstützt und erlaubt so einen vereinfachten Datenaustausch zwischen diesen Programmen. Für die wissenschaftliche Modellierung und Rekonstruktion liegen Funktionen wie der Import von Scan-Modellen oder 2D-Grundrissen die Verwendung nahe.

            

            

                
Ablauf des Workshops

                
Der Workshop findet an zwei Tagen mit jeweils vier Stunden Dauer statt. Im Workshop sollen Kenntnisse über die Grundfunktionen der 3D-Software Blender erlangt werden. Der erste Tag ist der Einführung in die Software gewidmet, am zweiten Tag folgen, aufbauend auf den vorangegangenen Modulen, weitere, fortgeschrittenere Übungen zu den Funktionen der Software.

                
Am ersten Workshop-Tag werden, nach einer allgemeinen, überblickshaften Einführung in Chancen, Methoden und Forschungsfelder mit Bezug zu 3D-Software, erste Schritte der 3D-Modellierung vorgeführt und in die Grundlagen von Blender eingeleitet. Im anschließenden Modul liegt der Schwerpunkt auf den Grundprinzipien und Techniken einfacher Modellierung: Grundgeometrien, erste Kolorierung, Speichern, Darstellung und Rendering. Im anschließenden Modul werden Use Cases für 3D-Rekonstruktionen präsentiert. Der erste Workshop-Tag schließt mit einer Einführung in einen Workflow für den Forschungsprozess, Modul „Workflow I“: 2D-/3D-Pläne bzw. -Karten, Skalierung und Georeferenzierung.

                
Am zweiten Workshop-Tag erfolgt, nach einer kurzen Rekapitulation der vorangegangenen Module, der zweite Teil der Einführung in den Workflow innerhalb von Blender, das Modul „Workflow II“: Modellierung von Architektur, Texturierung, das Mapping von Bildern, das Thema Beleuchtungssituation sowie Rendern, Animation und Speichern/Export von Dateien. Schließlich werden die 3D-Fragestellungen um die vierte Dimension erweitert. Abschließend findet eine Schlussdiskussion statt.

                
Grundsätzlich sollen die Teilnehmer*innen befähigt werden, ein Verständnis für mögliche Forschungsfragen aus dem 3D-/4D-Bereich zu entwickeln. An beiden Tagen finden Diskussionen und kritische Reflektionen der Potentiale und Grenzen der erlernten Technologien für Forschungsfragen der Digital Humanities statt.

            

            

                
Zur Organisation

                
Max. Teilnehmer*innenzahl: 15

                
Teilnehmer*innen benötigen einen eigenen Laptop mit Internetzugang und vorab installierter Blender-Software, Version 3.3 LTS (Download: Blender 2022a, Handbuch: Blender 2022b). Die Systemanforderungen können über die Blender-Webseite abgefragt werden (Blender 2022d). Für die Bedienung der Software wird zudem eine Maus mit integriertem Mausrad benötigt. Der Workshop findet an zwei Tagen mit je vier Stunden Dauer statt. Technische Ausstattung vor Ort: Beamer und WLAN.

                
Die Referent*innen stellen im Vorfeld des Workshops ein digitales Dossier mit Informationsmaterial und Daten auf GitHub (
                    
https://github.com/manuelhunziker/blendergoesdhd
) zur Verfügung.
                

            

            

                
Beitragende und Kontaktdaten

                

                    
Manuel Hunziker
 (manuel.hunziker@lmu.de) (ORCID: 0000-0002-4684-938X) ist wissenschaftlicher Mitarbeiter und Dozent für Digitale Archäologie am Department für Kulturwissenschaften und Altertumskunde der Ludwig-Maximilians-Universität München. Als Kulturinformatiker setzt er zudem für das Museum für Abgüsse Klassischer Bildwerke in München digitale Strategien zur barrierefreien Vermittlung und zur musealen Inszenierung um. Er studierte Angewandte Informatik sowie Klassische Archäologie an der Universität Heidelberg und Denkmalpflege an der Universität Bamberg. Sein aktueller Forschungsschwerpunkt liegt in der Anwendung und Entwicklung computergestützter Verfahren zur dreidimensionalen Dokumentation, Visualisierung und virtuellen Rekonstruktion in den archäologischen Wissenschaften.
                

                

                    
Waltraud von Pippich
 (waltraud.v.pippich@kunstgeschichte.org) (ORCID: 0000-0002-4555-2816) Forschungsprojekt zur 3D-Visualisierung von Farbräumen, Fraunhofer-Institut für Graphische Datenverarbeitung IGD Darmstadt, Forschungsprojekt „Rot rechnen“ zur digitalen Farbanalyse, Schwerpunkte in der Theorie der Digital Humanities, digitale Bild- und Farbanalyse, Methoden der Stilometrie von Bild und Text, Methoden der Visualisierung, Rechtsfragen in Forschung und Entwicklung, politische Ikonographie, digitale Kunstgeschichte.
                

                

                    
Berenike Rensinghoff
 (berenike.rensinghoff@hotmail.de) (ORCID: 0000-0002-2717-3409) arbeitet als Trainee an der Akademie der Wissenschaften und der Literatur Mainz in der Abteilung Digitale Akademie für das Akademienvorhaben Corpus Vitrearum Medii Aevi (CVMA) Deutschland. Sie ist ebenfalls Mitorganisatorin des 3D Hackathons 2022 Creating New Dimensions. Im Rahmen der Masterarbeit im Studiengang Digitale Denkmaltechnologien an der Universität Bamberg und der Hochschule Coburg erstellte sie digitale 3D-Modelle und -Rekonstruktionen des Badezimmers des Palais Beauharnais in Paris. Aktuell modelliert sie für das CVMA Deutschland Glasmalereien mit Blender.
                

            

        

            

                
Das Prinzip Synopse

                
Digitale Editionen stehen häufig vor der Herausforderung, verschiedene Zustände eines Dokuments oder auch Varianten eines Textes in einer gegenüberstellenden Ansicht, also synchron anzeigen zu wollen. Das erstere Szenario ist dokumentenbezogen und kann beispielsweise dem Vergleich von Digitalisat, diplomatischer Transkription, Lesetext, XML-Text oder Plaintext dienen (Abb. 1), während das andere Szenario textbezogen ist und der Vermittlung von unterschiedlichen Überlieferungszuständen oder textgenetischen Verwandtschaften dient (Abb. 2). Dem jeweiligen Dokument oder Text können ferner Kommentare, Annotationen, Apparate und Metadaten anbeigestellt werden.

                

                    

                        

                        
Abb. 1: Dokumentbezogene synoptische Darstellung. Exemplarischer Screenshot: Theodor Fontane, "Notizbuch A1".
                    
 Theodor Fontane: Notizbücher. Digitale genetisch-kritische und kommentierte Edition.

                    
(zugegriffen: 14. Dezember 2022).
                

                    

                

                
                

                    

                        

                        
Abb. 2: Textbezogene synoptische Darstellung mit flexiblen Spaltenmanagement. Exemplarischer Screenshot: Märchen "Rumpelstilzchen" (nach den 
                    
Kinder- und Hausmärchen
 der Gebrüder Grimm). 
                    
LERA – Locate, Explore, Retrace and Apprehend complex text variants.

                    
(zugegriffen: 14. Dezember 2022).
                

                    

                

                
                

                    
Interfaces mit mehrspaltigem Layout

                    
Interfaces digitaler Editionen adressieren diese Herausforderungen fast immer mithilfe eines mehrspaltigen Layouts. Die sogenannte Synopse (abgeleitet von griech. 
                        
syn-optikós
 "das Ganze zusammensehend") ist eine klassische Darstellungsform, bei der zueinander in Beziehung stehende Dokumente oder Texte in zwei oder mehr Spalten nebeneinander angezeigt werden (vgl. das Konzept der "Composite Design Patterns", dazu z. B. Javed/Elmqvist 2012). Die Beziehung zwischen den gegenübergestellten Ressourcen drückt sich in dem jeweiligen Punkt der Synchronisierung aus: Diese kann beispielsweise an Seiten, Absätzen oder auch an einzelnen Zeilen vorgenommen werden. Das Layout erzeugt aus diesen Synchronisations-Punkten die 'ersichtliche' Verbindung zwischen den abgebildeten Ressourcen (zum Prinzip der "Juxtaposition" und die Kombination mit anderen Abbildungsmethoden vgl. Gleicher et al. 2011). Hingegen ist in den Daten diese visualisierte Verbindung nicht immer explizit abgelegt, sondern kann auch in einem 'soften' Konstrukt bestehen, beispielsweise indem regulär aufgebaute IDs und konsekutive Nummerierung verwendet werden, um Digitalisat und XML-Dateien abzugleichen. Denkbar ist zudem, dass zwei nebeneinanderstehende Texte aus ein und demselben Dokument generiert werden, wie etwa eine diplomatische Umschrift mit einem aufgeschlüsselten Lesetext daneben, oder auch zwei Textfassungen auf der Grundlage eines textkritischen Inline-Apparats.
                    

                

                

                    
Divergenz der Ansätze

                    
Synoptische Ansichten werden aktuell fast ausschließlich individuell für die jeweiligen Editionsprojekte konzipiert und programmiert. Eine Ursache dafür ist darin zu sehen, dass die jeweils gegenüberzustellenden Inhalte von Projekt zu Projekt extrem unterschiedlich ausfallen können, sowohl in der Art ihrer Beziehung zueinander, als auch in der eigenen Ausprägung. Dokumentenzentrierte Ansätze wie z. B. im 
                        
Deutschen Textarchiv
 (Abb. 3) und in 
                        
DiScholEd
 (Abb. 4) stellen einzelnen Digitalisaten verschiedene Derivate der dazugehörigen XML-Codierung gegenüber.
                    

                    

                        

                            

                            
Abb. 3: Dokumentenzentrierter Ansatz. Exemplarischer Screenshot: Gottfried Keller, "Der grüne Heinrich", 
                        
Deutsches Textarchiv
. 
                        
 (zugegriffen: 14. Dezember 2022).
                    

                        

                    

                    
                    

                        

                            

                            
Abb. 4: Dokumentenzentrierter Ansatz mit nutzerseitiger Auswahl der Spalteninhalte. Exemplarischer Screenshot: E. T. A. Hoffmann, "Der Sandmann", hg. von Anna Busch. 
                        
Briefe und Texte aus dem intellektuellen Berlin um 1800
, hg. von Anne Baillot. 
                        
(zugegriffen: 14. Dezember 2022).
                    

                        

                    

                    
                    
Projekte mit Fokus auf Textgenese stellen indessen mehrere Textdokumente gegenüber und setzen diese in der Regel nach Strukturabschnitten (Kapitel, Absätze, Zeilen) in Beziehung – was einen völlig anderen Präsentationsansatz darstellt. In diesem Fall gehen die Ansätze zudem in divergente Richtungen, deren Pole einerseits durch eine platzintensive Rasterdarstellung wie z. B. in LERA (Abb. 2) und andererseits durch ein auf eine Spalte (!) verdichtetes Konzept wie z. B. in der "spinal edition" des 
                        
Frankenstein Variorums
 (Abb. 5) gebildet werden. Es besteht somit der Bedarf an einem allgemeinen Modell synoptischer Präsentationskonzepte, welches die Grundlage eines generischen Interface-Frameworks für Digitale Editionen bilden könnte.
                    

                    

                        

                            

                            
Abb. 5: Verdichtung der Synopse auf eine einzelne Spalte. Exemplarischer Screenshot: 
                        
Frankenstein Variorum
, hg. von Elisa Beshero-Bondar, Rikk Mulligan und Raffaele Viglianti. 
                        
(zugegriffen: 14. Dezember 2022).
                    

                        

                    

                    
                    
Des Weiteren erlauben bestehende Ansätze bereits die nutzerseitige Auswahl der Spalteninhalte wie bei DTA (Abb. 3) und 
                        
DiScholEd
 (Abb. 4), teils sogar mit der zusätzlichen Möglichkeit des Spaltenmanagements wie bei den 
                        
Fontane Notizbüchern
 (Abb. 1) und im generischen 
                        
EVT Viewer
 (Abb. 6). Dies ist insbesondere mit steigender Zahl der Spalten notwendig (vgl. LERA, Abb. 2).
                    

                    

                        

                            

                            
Abb. 6: Synoptische Darstellung mit flexiblen Spaltenmanagement, hier im EVT Viewer [Abk. für Edition Visualization Technology]. Exemplarischer Screenshot: 
                        
Codice Pelavicino
, hg. von Enrica Salvatori und Edilio Riccardini. 
                        
(zugegriffen: 14. Dezember 2022).
                    

                        

                    

                    
                    
Als nicht nur technische, sondern vor allem auch konzeptionelle Herausforderung kommt noch Responsivität hinzu. Das Nutzungskonzept der Edition muss sich an unterschiedliche Endgeräte – und damit an die mögliche Zahl der sichtbaren (!) Spalten – anpassen können. Letzteres hat einen unmittelbaren Einfluss auf die Zugänglichkeit und Nutzbarkeit der Ressourcen. Dies hat außerdem zur Folge, dass die oben genannten divergierenden Präsentationskonzepte für die Gegenüberstellung verschiedener Textvarianten behandelt werden müssen, was schließlich auf ein Konzept hinausläuft, welches grundsätzlich von einer variablen Spaltenzahl ausgeht.

                    
Einen weiteren Aspekt bringt das Projekt TAPAS (Abb. 7) ein, welches dem Nutzer die Auswahl der dem Layout zugrundeliegenden XML-Renderingskripte erlaubt, die zudem jeweils responsiv agieren. Dies verdeutlicht nicht nur, dass die Grundlage für anpassbare Lösungen in der Trennung zwischen Daten- und Präsentationsschicht besteht, sondern gerade auch, dass dieser Umstand prinzipiell für Interfaces nutzbar ist, sofern die spezifische Abbildungslogik einmal formalisiert wurde. Der hier aufgezeigte Weg ist z. B. mithilfe von XSLT und ggf. ODD vollständig innerhalb des X-Technologiestacks umsetzbar (näheres bei Flanders/Hamlin 2013).

                    

                        

                            

                            
Abb. 7: Synoptische Präsentation von TAPAS. Exemplarischer Screenshot: François-Joseph Bérardier de Bataut, "Sixième entretien. Narration historique", 
                        
Essai sur le récit
, hg. von Christof Schöch. TAPAS. 
                        
http://www.tapasproject.org/berardier/files/sixi%C3%A8me-entretien-narration-historique
(zugegriffen am 14. Dezember 2022).
                    

                        

                    

                    
                

            

            

                
Die Software "Synopticon"

                
Das Software-Entwicklungsprojekt 
                    
Synopticon
 (vgl. Herbst 2022, auf GitHub verfügbar) verfolgt das Ziel, die typischen Anforderungen spaltenorientierter Interfaces digitaler Editionen in einer generischen Lösung zusammenzufassen, so dass es perspektivisch in bestehende Webseiten digitaler Editionen mit geringem Aufwand eingebunden werden kann. Auch die Einbindung in Editions-Frameworks wird verfolgt: So ist Synopticon bereits mit der modularen Architektur von 
                    
ediarum
 (Berlin-Brandenburgische Akademie der Wissenschaften 2022), das ebenfalls auf einen X-Technologiestack aufbaut, kombinierbar.
                

                
Um den unterschiedlichen Ansprüchen verschiedener Editions-Nutzungs-Konzepte zu entsprechen, können in der individuellen Konfiguration von Synopticon Beschreibungen der Text- und Dokumentenrelationen abgelegt werden. Dabei wird zwischen Textansichten (bspw. diplomatische Ansichten, konstitutive Lesetexte, Textvarianten etc.) und Zusatzinformationen (bspw. kritischer Apparat, Informationen zum Textumfeld etc.) unterschieden. Das Projekt ging hervor aus der Bachelor-Arbeit von Yannik Herbst (2021), die am Zentrum für Philologie und Digitalität (ZPD) der Universität Würzburg bereut wurde. Im Jahr 2022 erhielt das Projekt eine Förderung vom Universitätsbund Würzburg und bildet inzwischen eine Basiskomponente im Portfolio des ZPD.

                

                    
Herausforderungen

                    
Eine allgemeine Herausforderung stellt zunächst die Synchronisierung der Ansichten in Abhängigkeit von verschiedenen User-Interaktionen dar (Blättern, Scrollen etc.). Diese müssen projektspezifisch und in Abhängigkeit von dem jeweiligen Editionskonzept konfigurierbar sein, bilden also einen Teil der generischen Synopticon-Konfiguration. Zudem müssen dort weitere User-Interaktionen einbezogen werden, die ebenfalls eine Veränderung des Spalteninhalts hervorrufen oder sogar eine Resegmentierung des Textes zur Folge haben können (ähnlich LERA, vgl. Pöckelmann et al. 2022). Eine weitere Herausforderung besteht in den Konfigurationsmöglichkeiten der Spalten in Abhängigkeit von dem allgemeinen Nutzungskonzept der digitalen Edition. Daran knüpft sich zugleich die responsiv bedingte Variierbarkeit der sichtbaren Spalten und die jeweiligen Anpassungen des Nutzungskonzepts. Diese zahlreichen teils nutzer- teils projektspezifischen Anpassungen, die ermöglicht werden sollen, werfen Fragen der Referenzierbarkeit auf. Im Kontext von digitalen Editionen müssen Ansichten derselben reproduzierbar und zitierfähig sein.

                

                

                    
Umsetzung als Software-Architektur

                    
Für Frontend und Backend von Synopticon kommen unterschiedliche Open-Source-Frameworks zum Einsatz (Abb. 8). Im Frontend sorgt 
                        
Vue.js
 für die Organisation der verschiedenen Informationen und Strukturen. Auf Benutzerwunsch (bspw. Auswahl einer Textvariante) werden Anfragen an die von ediarum.Web gestellte Schnittstelle geschickt. Diese liefert als Antwort entweder HTML-Bruchstücke oder JSON, wobei die HTML Bruchstücke durch XSLT Transformationen ("Views") der zugrunde liegenden XML Dateien erzeugt werden.
                    

                    

                        

                            

                            
Abb. 8: Makrostruktur einer mittels Synopticon umgesetzten digitalen Editions-Webapplikation. Grafik: Yannik Herbst.

                        

                    

                    
                    
Das Frontend wurde mit dem JavaScript-Framework 
                        
Vue.js
 (You 2022) realisiert. Um zu ermöglichen, dass das Frontend in viele unterschiedliche Projekte aufgenommen werden kann, wurde es als Single-Page-Anwendung konzipiert, welche mittels einer JavaScript-, einer CSS- und einer Konfigurations-Datei in bestehende Webseiten eingebunden wird. Der modulare und auf wiederverwertbaren Komponenten aufbauende Ansatz von 
                        
Vue.js
 sowie die Konfiguration von 
                        
Synopticon
 ermöglicht es, projektspezifische Anforderungen mithilfe bereits bestehender Konfigurations- und Softwarekomponenten aus anderen Projekten zu realisieren. Somit wird der Weg für ein nachhaltiges und nachnutzbares Softwaredesign geebnet.
                    

                    
Als Framework kommt aktuell 
                        
ediarum.WEB
 (Fechner 2022) zum Einsatz, das im Bereich der digitalen Editionen bereits verbreitet ist. Dieses dient der Kommunikation zwischen Frontend und der XML-Datenbank 
                        
eXist
 (Meier 2022) des Backends und enthält die dazu notwendigen Schnittstellen und Konfigurationsmöglichkeiten. Außerdem beinhaltet es notwendige Konsistenzchecks und Error-Handling. Das Ökosystem von 
                        
ediarum
 (.BASE.edit, .REGISTER.edit, .WEB, .DB) bietet außerdem die Möglichkeit, den gesamten Workflow von Datenakquise bis hin zur Präsentation zu integrieren. Dieser Workflow, inklusive der Präsentation mittels Synopticon, wird derzeit in verschiedenen am ZPD betreuten Projekten erprobt.
                    

                

                

                    
Aktuelle Beispielprojekte

                    
Drei aktuelle Usecases decken diverse Synchronisations-Beziehungen ab. Ein Beispiel ist das DFG-geförderte Projekt 
                        
Narragonia 

                        
L

                        
atina
 (Baier/Hamm 2021), in dessen BMBF-Vorgängerprojekt 
                        
Narragonien 

                        
D

                        
igital
 (Burrichter/Hamm 2021) eine projektspezifische Synopse entwickelt wurde, die als Ausgangspunkt bzw. als Inspiration für die Entwicklung der generischen Lösung 
                        
Synopticon
 diente. Das Projekt erarbeitet eine kommentierte zweisprachige Hybridedition der beiden lateinischen "Narrenschiffe" von Jakob Locher (1497) und Jodocus Badius (1505), die verschiedene Druckausgaben, insbesondere Übersetzungen desselben Originalwerkes gegenüberstellt (Abb. 9) und die beiden "Narrenschiffe" mittels eines digitalen Kapitel-Kommentars interdisziplinär und vergleichend erläutert.
                    

                    

                        

                            

                            
Abb. 9: Synoptische Vergleichsansicht zweier Ausgaben desselben Originalwerkes im Projekt 
                        
Narragonien digital
 (Burrichter/Hamm 2021). Exemplarischer Screenshot: "Das Narren Schyff / La nef des folz du monde", 
                        
(zugegriffen: 14. Dezember 2022).
                    

                        

                    

                    
                    
Ein weiteres Beispiel ist das interdisziplinär ausgerichtete Akademieprojekt 
                        
Richard Wagner Schriften
, dessen Ziel es ist, erstmals die gesamte Hinterlassenschaft der rund 230 Texte Richard Wagners mit ca. 5.000 Seiten Umfang auf Basis aller überlieferter historischer Textzeugen philologisch zu erschließen und in einer vollständigen, historisch-kritischen Edition der Schriften mit umfassendem Kommentar zu bündeln. Dabei werden einem Text sowohl sein Apparat, sein Textumfeld und ggf. vorhandene Digitalisate gegenübergestellt (Abb.10).
                    

                    

                        

                            

                            
Abb.10: Projektspezifische Anpassung der durch Synopticon zur Verfügung gestellten Funktionalität für die Gegenüberstellung von Text mit einem dazugehörigen textkritischen Apparat. Exemplarischer Screenshot: Richard Wagner, "Le Freischutz". In 
                        
Richard Wagner Schriften
. 
                        
https://wagner-schriften.de
(zugegriffen: 14. Dezember 2022).
                    

                        

                    

                    
                

            

            

                
Zusammenfassung und Ausblick

                

                    
Synopticon
 bündelt mehrere verschiedene konzeptionelle Anforderungen spaltenorientierter Layouts digitaler Editionen. Vor dem Hintergrund zahlreicher Beispiele aus bereits bestehenden, projektspezifischen Lösungen betont der vorliegende Ansatz die Notwendigkeit einer verallgemeinerten Formalisierung von Interfaces bezüglich deren Layouts und deren Funktionalitäten. Hinsichtlich der Nachhaltigkeit der Infrastrukturen für digitale Editionen und deren Interfaces kann in einem solchen Modell eine Schlüsselfunktion bestehen.
                

                
Seitens des ZPDs wird 
                    
Synopticon
 bereits als Standardinstrument für synoptische Interfaces digitaler Editionen genutzt und kontinuierlich weiterentwickelt. Es wird angestrebt, im Rahmen zukünftiger Editionsprojekte sowohl die Funktionalität als auch die Interoperabilität von Synopticon zu erweitern: Perspektivisch soll 
                    
Synopticon
 zunächst modular kompatibel zur Editionssoftware 
                    
ediarum
 werden, bevor die Kombinierbarkeit mit anderen Frameworks weiter ausgelotet wird. Als weiterer konzeptioneller Bestandteil wird die generische Einbindung von Digitalisaten durch das 
                    
Image Interoperability Framework
 (IIIF) angestrebt. Des Weiteren zeigten Jänicke et al. (2017) bereits auf, dass synoptische Ansichten nicht nur Nutzungsszenarien des klassischen 
                    
close reading
 bedienen, sondern durchaus auch in der Lage sind, auch Distant Reading oder kombinierte Methoden zu befördern, so dass mit hinzukommenden Kooperationsprojekten eine weitere Flexibilisierung von 
                    
Synopticon
 in Aussicht steht. Ferner könnten zukünftig auch Schnittstellen zu Textanalyse-Tools (z. B. 
                    
Voyant
, LERA, etc.) und nach den Richtlinien der 
                    
Text Encoding Initiative
 (TEI) ausgezeichnete Textkorpora (ggf. via NFDI Text+) eingebunden werden.
                

            

        

            
Die AG “Greening DH” wurde 2021 ins Leben gerufen mit dem Ziel, das Bewusstsein der Verbandsmitglieder für ökologische Aspekte von Aktivitäten im Bereich der Digital Humanities (Forschung, Lehre, Projektmanagement, Softwareentwicklung etc.) zu schärfen. Neben konkreten Handlungsanalysen und -empfehlungen geht es der AG darum, die grundlegenden Veränderungen, die sich daraus für das Fach ergeben, epistemologisch zu begleiten. Der angedachte Workshop auf der DHd 2023 möchte beide Aspekte adressieren: Der erste Halbtag ist der Handlungsanalyse und Vermittlung von Best Practices auf individueller Ebene gewidmet, der zweite Halbtag geht institutionellen Handlungsspielräumen nach.

            

                
Herausforderungen der Klimakrise für die DH-Community

                
Während Forschende einerseits die aktuellen klimatologischen Entwicklungen als “Krise” begreifen und sich über den dringenden Handlungsbedarf einig sind (vgl. Leal Filho et al. 2021), hat zum anderen die Ausübung wissenschaftlicher Tätigkeit eine direkte und mutmaßlich weitreichende Auswirkung auf die Umwelt. Die Diskrepanz zwischen individuellen Überzeugungen und systemischen Anforderungen in Bezug auf die Wichtigkeit ökologischer Perspektiven beeinflusst alle Arbeitsschritte eines Projektes und wird häufig als Konfliktsituation wahrgenommen. Insbesondere in den DH gilt es kritisch zu hinterfragen, ob fachspezifische Technologien, die arbeitsökonomisch nachhaltig sein sollen, auch ökologisch nachhaltig sind und ob jeder wissenschaftliche Erkenntnisgewinn den damit verbundenen Energieverbrauch aufwiegt. Dieser Fragenkomplex stand in den DH bislang nicht im Fokus fachspezifischer Reflexionen und wird in der laufenden Diskussion um Sustainability (FAIR, CARE) allenfalls marginal berücksichtigt.

                
In der Forschungsliteratur wird die Thematik der ökologisch nachhaltigen Digitalisierung bereits seit einigen Jahren behandelt (dazu ausführlich Lange/Santarius 2018). So haben sich im Bibliotheksbereich Akteurs-Netzwerke wie Libraries4Future und Netzwerk Grüne Bibliothek formiert (dazu Hauke et al. 2013). Ein weiterer Diskursstrang behandelt das Gebiet der ökologisch nachhaltigen Softwareentwicklung (z.B. Gröger/Köhn 2014; Eder/Gallagher 2017; Höfner/Frick 2018). Daneben liegen Untersuchungen zum Energiebedarf von Datenzentren (exemplarisch: Schomaker et al. 2014) und Methoden zu umweltverträglicher Langzeitarchivierung (exemplarisch: Pendergrass et al. 2019) vor. Letzteres gilt es im Zusammenhang mit Minimal Computing und insbesondere Minimal Publishing (hierzu Holmes/Takeda 2019) zu betrachten, die in den DH steigende Beachtung erfahren. Für die fachübergreifende Thematik der umweltverträglichen Konferenzorganisation liegen spätestens seit dem “Zoom-Jahr” 2020 zahlreiche Untersuchungen vor (Stroud/Feeley 2015, Klöwer et al. 2020; Faber 2021; Glausiusz 2021). Ferner entstanden Frameworks zur Evaluierung der Nachhaltigkeit von Forschungseinrichtungen (Ferreboeuf et al. 2019; Mariette et al. 2021). Für den Bereich der DH sind aus dem Kontext der Digital Humanities Climate Coalition inzwischen erste Beiträge erschienen, die sich konkret mit dem ökologischen Impact der DH auseinandersetzen und konkrete Handlungsempfehlungen zusammenstellen (DHCC Information, Measurement and Practice Action Group und DHCC Toolkit Action Group 2022).

                
Auch bei Open Science kann man sich fragen, inwiefern das Streben nach universellem Zugang zu hochqualitativen Informationen (etwa Scans, Scripts) mit den ökologischen Verhältnissen auf Dauer kompatibel sind. Die Fragen der ökologischen Nachhaltigkeit in den DH muss auch im Kontext globaler sozial-ökonomisch ungleicher Ressourcenverteilung betrachtet werden, die insbesondere Fragen der Zugänglichkeit betreffen und neue Perspektiven auf die FAIR-Kriterien eröffnen: Sind Bereitstellungsangebote, die hohe Bandbreiten und leistungsstarke Servers und Clients erfordern, unter globalen und ökologischen Gesichtspunkten überhaupt als “fair” bewertbar? Welche Tugenden der Offenheit sind noch vertretbar, wenn die Bedingungen des Zugangs das Klima für einen wachsenden Teil der Weltbevölkerung unerträglich machen?

            

            

                
Ablauf

                

                    
World Café: Was macht die individuelle Forschungspraxis?

                    
Der erste Halbtag fokussiert auf die Forschungspraxis der Teilnehmenden hinsichtlich ihres ökologischen Fußabdrucks. Die Teilnehmenden werden eingeladen, ihre Forschungstätigkeit einer kritischen Analyse zu unterziehen. Sie reflektieren, an welchen Stellen in ihrem Arbeitsablauf Handlungsspielräume bestehen, die eine Reduktion des ökologischen Fußabdrucks ihrer Arbeit ermöglichen. 

                    
Nach diesem Einstieg werden drei einschlägige DH-Themen detaillierter betrachtet, zu denen bereits eine Handreichung der Digital Humanities Climate Coalition (s.o.) vorliegt, an welcher die Workshop-Veranstaltenden mitgearbeitet haben. Die Handreichung führt einschlägige Informationen zu ökologischen Aspekten digitaler Arbeitsprozesse zusammen.

                    
Das erste Thema beschäftigt sich mit der Projektorganisation und fragt, welche Aspekte zur Reduktion des ökologischen Fußabdrucks im Projektmanagement berücksichtigt werden sollten.

                    
Der zweite Themenblock nimmt die Nutzung von großer Rechenkapazität und Large Language Models in den Blick, auf denen Digital-Humanities-Forschung oft basiert. 

                    
Als drittes Thema vertiefen die Teilnehmenden die Potentiale des Minimal Computings. Dieser Themenblock thematisiert grundlegend die Verwendung von sparsamen technologischen Lösungen, die teils als konträrer Ansatz zu den erwähnten großen Rechenkapazitäten zu verstehen sind, insbesondere aber auf den Bereich der Bereitstellung abzielen.

                    
Zu allen drei Themen werden Lösungsansätze auf Grundlage der DHCC-Handreichung durch die Teilnehmenden erarbeitet. Anschließend werden die anfangs identifizierten Handlungsspielräume unter Berücksichtigung der erarbeiteten Szenarien gemeinsam diskutiert.

                    

                        

                            

                        

                    

                    

                        

                            

                            
Abbildungen: Labos 1point5, Infografiken zu CO2-Belastung durch Reisen zu Workshops und Konferenzen, Verkehrsmittel und Entfernungen im Vergleich. Quelle: 
                        
 – Lizenz: CC BY.
                    

                        

                    

                    
                

                

                    
Panel und Rollenspiel: Institutionelle Herausforderungen 

                    
Der zweite Halbtag beginnt mit einer Podiumsdiskussion, die die DH-Community und angrenzende Felder zusammenführt. Diskutiert werden soll die Umsetzung einer grünen Agenda auf zwei Ebenen: zum einen institutionell auf Ebene der Infrastruktureinrichtungen, die für DH-Forschung unerlässlich sind, zum anderen durch Vertretende anderer geistes- und sozialwissenschaftlicher Fächer bzw. Bereiche, die in der ökologischen Diskussion weiter sind als die Digital Humanities. Die Diskussion der Panelistinnen untereinander und mit den Workshop-Teilnehmenden soll dazu beitragen, institutionelle Spielräume zu identifizieren, in denen ein Paradigmenwechsel in der Praxis angestoßen werden kann.

                    
Es soll hier besonders der Frage nachgegangen werden, an welcher Stelle die DH-Community eigene Lenkungsmechanismen einsetzen könnte, um einen strukturellen Systemwandel in die Wege zu leiten: Was können die einschlägigen DH-Verbände (DHd, EADH, ADHO) und zukünftigen Konferenzausrichtende beitragen? Wie können Förderinstitutionen als Unterstützung erreicht werden? An welchen Stellen können Infrastrukturen ansetzen?

                    
Eingeladene Panelistinnen:

                    

                        
Dr. Magdalena Palica, Leiterin der Wissenschaftlichen Bibliothek der Stadt Trier

                    

                    

                        
Prof. Dr. Julia Affolderbach, Professorin für Nachhaltige Regional- und Standortentwicklung, Universität Trier

                        
Dr. Rabea Kleymann, ZfL Berlin, Vertreterin des DHd-Vorstands

                    

                    
Diese Diskussionsrunde möchte einen Impuls für eine ganzheitlich nachhaltige DH-Praxis geben und weitere Aktivitäten auf diesem Gebiet anstoßen. Die eingeladenen Expertinnen sind bewusst nicht ausschließlich aus den engeren Kreisen der DH-Community gewählt, sondern auch aus angrenzenden strategischen Feldern, um einen umfassenderen Austausch anzuregen sowie potentielle Synergien auszuloten. Außerdem ist vorgesehen, das Panel aufzunehmen und im Nachgang als Podcast auszustrahlen.

                    
Im Anschluss an diese Podiumsdiskussion wird ein Rollenspiel den Teilnehmenden die Gelegenheit geben, eine Perspektive der in einem Projekt involvierten Akteurinnen und Akteure einzunehmen. Den Teilnehmenden wird eine Funktion zugeteilt (Personal aus Universitätsleitung, DH-Forschung, DH-Studium, IT, etc.) und sie sollen sich im Gespräch mit den anderen einer Herausforderung stellen. Für jede Rolle werden Profil und Ziele vorgegeben. Es sind drei Rollenspiel-Runden (jeweils ca. 20 min) geplant, wobei die Rollen jedes Mal unter den Teilnehmenden neu gemischt werden. So sind die zu lösenden Aufgaben jedes Mal neu und die Teilnehmenden werden dazu angeregt, sich die jeweiligen Argumente aus verschiedenen Perspektiven anzuschauen und sich in verschiedene Handlungsspielräume hineinzudenken. Die drei angedachten Szenarien sind:

                    

                        
Top-Down: Das Direktorium verlangt den Entwurf einer Richtlinie, um den ökologischen Fußabdruck einer Abteilung / eines Instituts / einer Universität zu reduzieren, unter der Maßgabe des Qualitätserhalts von Forschung / Lehre / Infrastruktur.

                        
Bottom-Up: Mitarbeitende und Studierende engagieren sich für den Aufbau einer Nachhaltigkeitsgruppe, die sich innerhalb der Universität / Akademie für die nachhaltige Durchführung von Projekten und Bildungsveranstaltungen zu ökologischen Themen einsetzen soll und suchen Unterstützung in Verwaltung / Leitung.

                        
Worst Case (for now): Eine Hitzewelle von bis zu 45 Grad macht es unmöglich, den Betrieb von Forschung / Lehre / Rechenzentrum aufrecht zu erhalten: Welche Maßnahmen sollen getroffen und umgesetzt werden, um mit dieser Situation umzugehen?

                    

                    
Abschließend werden die gewonnenen Perspektiven, Fragen und Lösungsideen in einer gemeinsamen, moderierten Diskussion zusammengetragen. Individuelle und institutionelle Handlungsspielräume werden gegenübergestellt, passende Optionen für die Nachverwertung der Workshop-Resultate und zukünftige Perspektiven der “Green DH” erörtert.

                

            

        

            
Das DFG-geförderte Projekt 
                
Geschmacksbildung und Verlagspolitik
, eine Kooperation der Hochschule für Musik und Theater (HMT) Leipzig und der Sächsischen Landesbibliothek (SLUB) Dresden, erfasst die Geschäftsdaten von Leipziger Musikverlagen des 19. Jahrhunderts (Hofmeister, C. F. Peters, Rieter-Biedermann) in der eigens dafür gebauten Musikverlagsdatenbank (
                
mvdb
, https://musikverlage.slub-dresden.de). Zielsetzung des Projekts ist es, den Einfluss von Musikverlagen auf Musikgeschichte im weiteren bzw. auf Geschmacksbildungs- und Kanonisierungsprozesse im engeren Sinne zu verstehen. In vorliegendem Papier sollen zunächst Methoden und Zielsetzung des Projekts kurz erläutert werden. Abschnitte 2 und 3 werden dann die 
                
mvdb
 genau vorstellen, die das digitale Herzstück des Vorhabens bildet. Es werden sowohl ihre wesentlichen Funktionen erläutert, als auch die dafür nötigen Lösungen einiger typischer Probleme von Geisteswissenschaften im digitalen Medium vorgestellt. Als Ausblick werden abschließend einige erste Ergebnisse aus der Auswertung der Forschungsdaten präsentiert.
            

            

                
Einleitung: Projektvorhaben, Methode, Projektziele

                
Der Transfer der Musikwissenschaft ins Digitale ist unbestreitbar auf dem Vormarsch.
 Damit öffnen sich zwar neue wissenschaftliche Handlungsspielräume, diese müssen aber nun auch mit geeigneten Fragestellungen ausgefüllt werden. In der ‚verspäteten Disziplin‘ (Gerhard 2000), der „eine größere Widerständigkeit eigen“ ist (Unseld 2019, 172) gegenüber interdisziplinären Transfers, setzt sich in jüngerer Zeit vermehrt ein Bewusstsein dafür durch, dass Musikgeschichte losgelöst von den sozial-, wirtschafts- und kulturgeschichtlichen Kontexten ihrer Entstehung kaum noch hinreichend ist. Für das 19. Jahrhundert hat sich dies z.B. in Publikationen zu Verlags-, Salon-, und Repertoiregeschichte niedergeschlagen (Ballstaedt/Widmaer 1989, Beer 2000, Keym/Schmitz (Hg.) 2016, Gerber 2016, Beer 2020). Größere Repertoiregeschichtliche Forschungsvorhaben gibt es erst wenige (etwa Widmaier 1998, Hartmann-Enke 2022, auch https://performance.musiconn.de/). Das liegt unter anderem daran, dass hier einerseits geeignete Quellen vorliegen müssen, andererseits Datenmengen bearbeitet werden, die sich nur durch digitale Methoden hinreichend bewältigen lassen. Neben digitalen Musik- und Korpusanalysen (Neuwirth/Rohrmeier 2016) und digitalen Editionen besteht darin eines der großen Potentiale der Digital Musicology. Schon Helmut Loos hat darauf hingewiesen, dass es „im vordringlichen Interesse der Musikforschung [liegt], das bereitstehende innovative technische Potential der Gegenwart gewinnbringend zu nutzen und in die Erforschung eines bislang beinahe undurchdringlichen Gebiets [= Repertoireforschung, die Autoren] einzubringen“ (Loos 2010, 158). Oder in Worten von Laurent Pugin: „There are at least two key areas in which digital technology is transforming research: access and scale.“ (Pugin 2015, 1)
                

                
Hier knüpft das Projekt 
                    
Geschmacksbildung und Verlagspolitik
 an, indem es aus Geschäftsunterlagen die Verlagsprogramme dreier Musikverlage inklusive ihrer Wirtschaftsdaten erfasst. Aus ca. 20.000 Verlagsnummern lassen sich dann im Zeitraum von 1807 bis ca. 1945 Bewegungen des Musikalienmarkts als ‚distant reading‘ (Moretti 2016) der Geschmacksentwicklung rekonstruieren und sowohl zu den Agenden von Musikverlegern und Musikjournalisten als auch dem musikalischen ‚Kanon‘ in Beziehung setzen. ‚Distant reading‘ ist hier adaptiert zu verstehen und meint nicht die ursprüngliche korpusanalytische Konzeption nach Moretti, denn das Projekt arbeitet nicht mit Werk-Texten. Stattdessen wird das Repertoire anhand bibliographischer und verlegerischer Metadaten analysiert. Die Prämissen sind jedoch dieselben, weshalb die Begriffswahl adäquat erscheint: Im Kontrast zu bisherigen Perspektiven auf die Musikgeschichtsschreibung, die sich entlang einer vergleichsweise überschaubaren Zahl von (kanonischen) ‚close reading‘ Beispielen bewegt, wird hier ein weiter Blickwinkel eingenommen, der aufgrund des Verfahrens mehr Kompositionen erfasst, als eine Einzelperson je lesen könnte. Methodisch stellt sich das als ‚mixed methods‘-Zirkel dar: Deskriptive Statistiken erzeugen eine neue Lesart von musikgeschichtlichem Verlauf, erzeugen aber auch Erklärungsbedarfe, für die traditionelle Textquellen herangezogen werden. Andererseits lassen sich Annahmen über Musikgeschichte als Hypothesen statistisch am Material prüfen. Multivariate Statistiken sind ebenfalls möglich. Beispiele in Form erster Ergebnisse dafür werden am Schluss dieses Papiers vorgestellt.
                

            

            

                
Die Musikverlagsdatenbank 
                    
mvdb

                

                

                    

                    
Abbildung 1: Startseite mvdb

                

                
Die 
                    
mvdb
 ist „Herzstück“ des Projekts, weil sie einerseits der Datenerfassung und –organisation dient, andererseits aber auch die Plattform ist, auf der die Verlagsdaten der Forschungscommunity zur Verfügung gestellt werden. 
                

                
Technisch gesehen ist die 
                    
mvdb
 eine Erweiterung des PHP-basierten Content Management Frameworks TYPO3 (Version 9.5). Es bietet eine umfangreiche Programmierschnittstelle für Extensions, die solche Funktionen realisieren. Die Entwicklung von TYPO3-Extensions wird darüber hinaus durch Tools der TYPO3-Community wie den Extensionbuilder (Version 9.10) unterstützt. So konnten wir in verhältnismäßig kurzer Zeit einen Prototyp für unsere Forschungsumgebung realisieren. Sie besteht aus vier Komponenten (Abb. 2): (1) zur Normdatenverwaltung, (2) zur Terminologieverwaltung, (3) zur Errechnung von Infografiken und (4) einer Core-Komponente. Die Komponenten für Normdaten und Terminologie versorgen die Core-Komponente mit wesentlichen Metadaten. Letztere selbst dient der Erfassung und Präsentation der durch das Projekt erfassten Wirtschaftsdaten. Sie enthält ein Backendmodul mit grundlegenden Funktionalitäten zur Dateneingabe und Qualitätssicherung und mehrere Plugins, die verschiedene Teile des Frontends der 
                    
mvdb
 steuern. Dazu zählen u. a. ein Plugin, das die API bereitstellt, und ein Recherche-Plugin. Während die Core-Komponente projektspezifisch gestaltet ist, lassen sich die übrigen auch für ähnliche Projekte fortnutzen. Deshalb wird die komplette Code-Basis des Projekts frei zur Verfügung gestellt.
                

                

                    

                    
Abbildung 2: Die vier aktuellen Komponenten der mvdb (i. Uhrzeigersinn von links oben: API, Core, Terminologieverwaltung, Infografiken)

                

                
                
Im Projekt werden in der 
                    
mvdb
 aus den Geschäftsbüchern der Verlage Hofmeister (1807 bis ca. 1939), Rieter-Biedermann (1856 bis 1917) und Peters (1867 bis ca. 1940) vier Hauptinformationen händisch erfasst: 1) Verlagsnummer und 2) Titel der Ausgabe sowie 3) das Auflagedatum und 4) die Höhe der Auflage. Stand August 2022 sind etwa 30% der rund 20.000 Verlagsnummern erreicht. Händische Erfassung ist nötig, da OCR an den komplizierten handschriftlichen Quellen scheitert (vgl. unten Abb. 4). Außerdem bedürfen manche Unregelmäßigkeiten noch einer Interpretationsleistung durch die Bearbeitenden. Um die Daten für statistische und datenbasierte Verfahren anzureichern, wird jeder Verlagsartikel durch bibliographische Daten der Gemeinsamen Normdatei (GND, s. Behrens 2011) der Deutschen Nationalbibliothek ergänzt. Das bietet gegenüber eigener Auszeichnungen drei Vorteile: 1) Effizienz: Obwohl viele Datensätze von Bibliothekar*innen der SLUB für das Projekt neu angelegt werden müssen, sind etliche tausend Werkdatensätze bereits vorhanden. 2) Qualität: Für die GND gelten fundierte und erprobte Erfassungsstandards, zudem sind sie untereinander bereits reichhaltig vernetzt. 3) Gute wissenschaftliche Praxis: Aufgrund ihrer Standards gewährleisten die GND-Daten Interoperabilität und Nachnutzbarkeit. Dass dabei die Übertragung nicht immer reibungslos möglich ist, wird weiter unten nochmals thematisiert.
                

                
Für die musikwissenschaftliche Forschung leistet die 
                    
mvdb
 insbesondere drei Dinge:
                

                

                    
Aufbereitung und Archivierung

                    
Bei der Erstellung der verlinkten GND-Werk- und Personennormdatensätze werden die Informationen bibliothekarisch qualifiziert anhand von biographischen und bibliographischen Quellen, sodass die aus den sporadischen Geschäftsbucheinträgen hervorgehenden Verlagsartikel eindeutig identifiziert und kontextualisiert sind. Sofern es die Projektressourcen noch zulassen, ist auch geplant, die entsprechenden Quellendigitalisate und relevante Kontextquellen abzulegen. Nur am Rande sei erwähnt, dass die durch die 
                        
mvdb
 erfassten Daten von der SLUB unbefristet vorgehalten und archiviert werden.
                    

                

                

                    
Präsentation

                    
Über unser Frontend werden die Daten für die Community nutz- und nachschlagbar, sodass die Geschäftsdaten der Musikverlage einfach sichtbar und navigierbar sind. Die Recherche ist über das Recherche-Plugin auf der Datenbank katalogisch oder über eine Suchfunktion möglich. Im einfachsten denkbaren Fall kann die 
                        
mvdb
 über einfache philologische Fragen zur Identität einer Ausgabe wie z.B. Ersterscheinungsdatum oder Zahl der Titelauflagen beantworten. Zu den bibliographischen Mehrinformationen zu Werk und Personen lässt sich von jedem Verlagsartikeldatensatz einfach navigieren. Auch einige weitergehende ‚on-board‘-Visualisierungen zur Datenauswertung sind eingebaut, so werden beispielsweise in der Verlagsartikel- und der Personenansicht summarische Grafiken angeboten, die einen schnellen Überblick über die Konjunktur der Ausgabe bzw. aller Werke eine*r Komponist*in zulassen (Abb. 3).
                    

                    

                        

                        
Abbildung 3: Personengrafik Brahms mit einzelnen Werken und aufaddierter Gesamtkurve (Fünf-Jahres-Mittel, Screenshot mvdb)

                    

                    
                

                

                    
Nachnutzung

                    
Die Daten lassen sich über das API-Plugin mit Lizenz CC-BY ungefiltert für eigene Forschung exportieren. Sie können im JSON-Format mithilfe der Elasticsearch-Query-DSL abgefragt werden, wozu Clientmodule in allen gängigen Programmier- und Statistiksprachen existieren, von R und Python für eigene statistische Auswertungen bis zu PHP und Javascript zum dynamischen Einbinden in projektfremde Kontexte. Außerdem wird es der Community möglich sein, Daten aus ähnlichen Quellen zu ergänzen, sodass die Bildung einer weiteren Digitalen Inselressource verhindert wird. So kann die 
                        
mvdb
 perspektivisch ein digitaler Hub für musikwissenschaftliche Wirtschaftsdaten und bibliographische Informationen zu Verlagsausgaben des 19. Jahrhunderts werden.
                    

                

            

            

                
Problemlösungen

                
Bei einer vergleichsweise jungen Disziplin wie den digitalen Geisteswissenschaften gehören auch Lösungsansätze für methodische Probleme, wie sie nachfolgend vorgestellt werden, zu den wichtigen Ergebnissen.

                

                    
Was ist eine Ausgabe?

                    
Dass geisteswissenschaftliche Gegenstände gelegentlich Unschärfen aufweisen, die sich in den rigiden Strukturen des Digitalen nicht recht abbilden lassen, ist keine Neuheit (z.B. Kuczera u.a. 2019) und gilt auch für das musikalische Werk (Pugin 2015, 2). Bei der Einrichtung der 
                        
mvdb
 standen wir insbesondere vor dem Problem, dass ‚die Ausgabe‘ in der Verlagspraxis des 19. Jahrhunderts keine homogene Einheit darstellt.
 Aus dem losen Zusammenhang der Druckplatten ließen sich beliebig ganze Bände oder separat publizierte Einzelnummern drucken, die oft simultan erschienen. Häufig lässt sich beobachten, dass erfolgreiche Einzelnummern eines Bandes nach einigen Jahren als Ableger separat gedruckt wurden, weil diese von geringerem Umfang und damit günstiger und besser verkäuflich waren. Außerdem wurden etliche Werke in Einzelstimmen gedruckt, von denen die Stimmen einzeln und unterschiedlich oft verkauft wurden. Damit zerfällt ‚das Werk‘ in ein Konglomerat von losen Publikationsmodulen, die aber trotzdem kognitiv eine eindeutige Sinn- und Werkeinheit darstellen.
                    

                    

                        

                        
Abbildung 4: Nr. 1370 des Verlags Hofmeister, H. Marschner, Ouvertüre zum „Vampyr“ in Ausgaben für Orchester sowie Quartett einzeln, mit jeweils einzelnen Nachauflagen verschiedener Stimmen, D-LEsta 21072 F. Hofmeister, Nr. 43.

                    

                    
                    
Dem haben wir bei der Einrichtung des Datenmodells Rechnung getragen, indem jede Ausgabe als flexibler Zusammenhang von übergeordnetem Verlagsartikel (= „Makro“) und untergeordneten Teilartikeln (= „Mikros“) verstanden wird, die nur zusammen eine Ausgabe abbilden können. Was diese Aufteilung für die Auflagestatistiken bedeutet, loten wir durch verschiedene Bereinigungsverfahren noch aus, aber klar ist, dass sich Auflagezahlen einzelner Orchesterstimmen oder Nummern nur unter Vorbehalt zu einem ‚Gesamterfolg‘ einer Ausgabe aufaddieren lassen. Auch solcherlei Modellierungen erachten wir bereits als wichtige Ergebnisse, denn daraus lassen sich gleich mehrere Konsequenzen für die Digital Humanities und die Musikwissenschaft ziehen. Inhaltlich nämlich, dass die musikwissenschaftliche Auffassung des Werkbegriffs anhand dieser Erkenntnisse zu revidieren wäre. Damit wird die modulare Modellierung womöglich auch für andere Forschungsprojekte und insbesondere digitale Editionsvorhaben relevant. Formal bedeutet eine solche, von bisherigen Konzepten abweichende Modellierung wahrscheinlich auch, dass statistische und datenbezogene Kompetenz vermehrt Teil des fachlichen Methodenkanons werden müssen, um die richtige Interpretation der so abgelegten Daten zu gewährleisten.

                    

                        

                        
Abbildung 5: Auszug Datenmodell mvdb, Abbildung Makro-Mikro-Struktur

                    

                

                

                    
„Norm“-Daten?

                    
Das Potential von Normdaten als musikwissenschaftlichen Forschungsdaten wurde bereits elaboriert (Wiermann 2018, Bicher/Wiermann 2018), und auch deren Probleme benannt: Dateninkonsistenzen, Dubletten, Fehlende Datensätze, variable Tiefenerschließung und heterogene Differenzierungsgrade der Informationen (Wiermann 2018, 347–8). Selbst wenn sich die Fehlermagnitude durch die projektinterne Normdatenerfassung verringern lässt, sind GND-Daten auch in Bezug auf das Begriffssystem selbst problematisch. Eigentlich sollen dessen vertikale Ober- und Unterbegriffsrelationen in die 
                        
mvdb
 übernommen werden. Unter anderem stellte sich aber heraus, dass aufgrund der nicht eindeutig festgelegten Relationstypen grundsätzlich mehrere Oberbegriffe angegeben werden können – was eine begriffstheoretisch wünschenswerte Informationsablage in einer Baumstruktur verhindert. Daneben finden sich je nach Begriffsbereich sehr unterschiedliche Granularitäten, die bei unreflektierter Übernahme und ohne spezielle Maßnahmen quantitative Untersuchungen verzerren können. Ähnliche Probleme entstehen im Bereich der Instrumente, wo Besetzungen und Einzelinstrumente gleichberechtigt in Werkdatensätzen abgelegt werden dürfen. Schließlich werden Alternativbesetzungen in der Form ‚Klavier alternativ für Orchester‘ angegeben. Die tatsächliche Alternativbesetzung kann so nur intellektuell im Kontext der Hauptbesetzung erschlossen werden. Um diese Probleme zu bearbeiten, hat die mvdb ein eigenes Sachbegriffsmodul, das baumartig strukturierte Systeme von GND-IDs erfasst und Normdaten zur Benennung der Einzelknoten aus der Normdatei abfragt (s. Abb. 2). Diese Begriffssysteme können einheitlich granular strukturiert werden. Besetzungsknoten können einheitlich ausgezeichnet werden. Alternativbesetzungen werden automatisch erschlossen und sauber mit vollständiger Instrumentierung abgelegt. Auch dass es keine Versionierung der GND-Daten gibt, ist nicht unproblematisch bei der Vernetzung der Daten. Hier hoffen wir ebenfalls, dass unsere Lösungen einen gewissen Modellcharakter erhalten können: das Sachbegriffsmodul zur Anpassung von GND-Normdaten werden wir der Community als open source Repositorium zur Verfügung stellen.
                    

                

            

            

                
Ausblick auf Ergebnisse 

                
Da die Datenerfassung noch in vollem Gang ist, müssen erste Ergebnisse als Einblick genügen. Diese Ergebnisse sind zwar endgültig, aber in Bezug auf die Tiefe der Datenanalyse und Interpretation noch nicht repräsentativ für die Ambitionen des Projekts. Bereits abgeschlossen ist die Erfassung des vergleichsweise kleinen Programms von Rieter-Biedermann (ca. 2900 Nummern) von 1856 bis 1917. Eindeutig ist an der beinahe linear wachsenden jährlichen Gesamtproduktion (Abb. 6) erkennbar, dass der Verlag weder vom Auftreten der günstigen ‚Klassikerausgaben‘ nach 1867 (
                    
Collection Litolff
, 
                    
Edition Peters
) beeinträchtigt wurde, noch von Konkurrenztechnologien mechanischer Musikinstrumente, die um 1900 aufkamen (vgl. Heise 2022). Deutlich dagegen wird sichtbar, wie Kriege und Krisen den Absatz von Kulturgütern hemmten (s. etwa die Kriege 1870/71, 1914-1918).
                

                

                    

                    
Abbildung 6: Gesamtproduktion Rieter-Biedermann jährlich

                

                
                
Vergleicht man dies mit der Zahl der produzierten (Neu-)Ausgaben pro Jahr (Abb. 7) wird wiederum deutlich, dass Rieter-Biedermann etwa ab den 1890ern immer weniger einzelne Verlagsartikel pro Jahr, diese dafür aber in höheren Auflagen auf den Markt bringt. Ähnlich verhält es sich mit den Komponisten, die der Verlag jährlich neu ins Programm aufnimmt (Abb. 8). Anfangs nimmt der Verlag bis zu 37 neue Komponisten in einem Jahr auf, die Zahl reduziert sich aber bald drastisch. Neuaufnahmen erfolgen dann in immer kleiner werdenden Wellen, der Gesamttrend ist abnehmend. Es eröffnet sich also eine Schere zwischen Produktpalette und Produktionsmenge, die man – noch ohne die Identität des Verlegten genauer zu hinterfragen – als Selektionsprozesse erfolgreicher Werke deuten kann. Womöglich wird dadurch hier bereits die ‚invisible hand‘ (Winko 2002) der Kanonisierung ein stückweit sichtbar.

                

                    

                    
Abbildung 7: Ausgaben Rieter-Biedermann jährlich

                

                
                

                    

                    
Abbildung 8: Neu ins Verlagsprogramm von Rieter-Biedermann aufgenommene Komponisten pro Jahr 

                

                
                
Diese Tendenz steht wahrscheinlich auch in Zusammenhang mit dem Höhenflug von Johannes Brahms. Brahms ist im Gesamtvergleich der mit Abstand meistgedruckte Komponist im Verlagsprogramm von Rieter-Biedermann (Abb. 9), und das obwohl sein Hauptverleger eigentlich Simrock war. Brahms’ 
                    
Deutsches Requiem
 op. 45 ist wiederum mit einem guten Viertel aller Brahms-Drucke der ‚erfolgreichste‘ Verlagsartikel aus Rieter-Biedermanns Programm gewesen (Abb. 10), was das Werk wahrscheinlich nicht zu geringem Anteil den kriegerischen und nationalistischen Tendenzen des Kaiserreichs zu verdanken hat (s. Zunahme ab den 1890ern in Abb. 3). Das wiederum ist freilich bemerkenswert, da es die Erwartung unterläuft, die die Rangplätze 2 bis 10 bestätigen: dass die erfolgreichsten Verlagsartikel allgemein eigentlich Klavierstücke und Lieder für den häuslichen Gebrauch waren. Die Beispiele veranschaulichen, auf welche Weise die 
                    
mvdb
 und das Projekt ermöglichen, das Repertoire des 19. Jahrhunderts ‚aus der Distanz‘ zu entschlüsseln und im Lichte des Verlagshandelns neu zu beleuchten. Weitere solche Ergebnisse wären zu zeigen, würden aber den Rahmen sprengen. Der Fortschritt des Projekts kann stattdessen auf dem Portal der 
                    
mvdb
 transparent nachverfolgt werden. 
                

                

                    

                    
Abbildung 9: Jährliche Produktion der sechs meistgedruckten Komponisten im Verlag Rieter-Biedermann im Vergleich

                

                
                

                    

                    
Abbildung 10: Rangliste der 10 meistgedruckten Artikel im Verlag Rieter-Biedermann

                

                
            

        

            
Schon bald nachdem die ersten Coronavirus-Infektionsfälle auch in Deutschland bestätigt wurden, deutete sich an, dass die gesellschaftlichen Auswirkungen der Pandemie immens sein würden. Es war daher teilweise vorauszusehen, dass die Pandemie auch ihren Niederschlag in der Sprache finden würde. Und doch ist erstaunlich, wie weitreichend und tiefgreifend das Pandemiegeschehen und die gesellschaftlich-politischen Reaktionen Einfluss auf unseren Sprachgebrauch übten und üben, insbesondere auf der Ebene des Wortschatzes. Wir stellen zwei Ressourcen (OWIDplusLIVE und das zugrundeliegende Live-RSS-Korpus) vor, die einen explorativen Zugang zur Erforschung dieses Einflusses bieten. Zudem soll der sprachwissenschaftlichen Forschungsgemeinschaft ein Instrument an die Hand gegeben werden, auch andere sprachliche Entwicklungen in der Zukunft möglichst unmittelbar zu entdecken und anhand von Frequenzverläufen nachzuzeichnen. Das folgende Beispiel (Abb. 1) zeigt vier nacheinander gestellte Suchabfragen zu den Bi-Grammen: 
                
zweite
 (in blau), 
                
dritte
 (grün), 
                
vierte
 (gelb) und 
                
fünfte Welle
 (rot) [Stand: 26. September 2022].
            

            

                

                
Abb. 1

            

            
Das zugrundeliegende Korpus besteht aus Titeln und kurzen Einführungstexten (sog. RSS-Feeds) zu Artikeln aus (derzeit) 13 deutschsprachigen Online-Quellen (Details zu den Quellen und zur Quellenauswahl siehe Vorprojekt: Wolfer u. a. 2020). Das Korpus wird seit dem 01.01.2020 täglich erhoben und umfasste am 26. September 2022 ca. 84,1 Millionen Token. Die Daten sind auch in Form von täglichen Unigramm- (inkl. Wortarten-Tagging) und Bigramm-Frequenzlisten frei auf OWIDplus (
                
www.owid.de/plus/cowidplus2020
) verfügbar.
            

            

                

                

            
Abb. 2

                OWIDplusLIVE wurde mit dem Ziel entwickelt, eine flexible und performante Lösung unabhängig vom Gegenstand (COVID-19) zu bieten. Damit löst dieses Tool den zuvor erstellten Prototypen ‚cOWIDplus Viewer‘ (Wolfer u. a. 2020) ab. OWIDplusLIVE professionalisiert den Prototypen in folgenden fünf Bereichen: (1) zusätzliche Annotations-Layer, konkret: Lemma und Wortart (Part-of-Speech, POS), (2) größere N-Gramme (aktuell Tri-Gramme, aber auch die Möglichkeit N-Gramme größer 3 zu erfassen) sowie (3) die Möglichkeit zusätzlicher Visualisierungen. Dafür (4) wurden sowohl die webbasierte Oberfläche als auch das dahinterliegende Daten-Backend von Grund auf neu entwickelt. Die bestehende Feed-Verarbeitungspipeline konnte ohne größere Änderungen übernommen werden. Zentral für den Ansatz hinter OWIDplusLIVE ist die (5) gezielte Verzahnung von Technologien (Falk u. a. 2020; Banon u. a. 2022; You u. a. 2022), die es ermöglichen, die Anwendung einfach mit neuen Daten (und ggf. Analysemöglichkeiten) zu erweitern, die Berechnungen über mehrere Server zu verteilen, sowie Anfragen so effizient wie möglich zu verarbeiten. Alle im Projekt entwickelten Komponenten (API und Web-Frontend) stehen kostenfrei als OpenSource (unter der AGPL-3.0 Lizenz) zur Verfügung - siehe: 
                
https://github.com/notesjor/IDS.OWID.Plus.Live

            

            
Die Abfrage durch die Nutzer*innen erfolgt über eine webbasierte Oberfläche. Ein Großteil der Berechnungen und Visualisierungen findet im Browser der Nutzer*innen statt. OWIDplusLIVE ist verfügbar unter 
                
https://www.owid.de/plus/live-2021
. Die Oberfläche ist in drei Segmente eingeteilt, die im Folgenden benannt und weiter unten erklärt werden (siehe Abb. 2): (1) Der Abfragebereich. (2) Ein Bereich mit drei unterschiedlichen Visualisierungen. (3) Sowie die Detailansicht. 
            

            

                

                
Abb. 3

                Abb. 3 zeigt den Abfragebereich mit einer einfachen Suche nach Bi-Grammen auf unterschiedlichen Layern. Auf eine komplexe Such-Syntax wurde bewusst verzichtet. Platzhalter wie ‚?‘ und ‚*‘ sind jedoch möglich. Zuerst (1) wurde die Suchfenstergröße N=2 (Bi-Gramm) gewählt. An der ersten Position des Bi-Gramms wird auf dem Layer ‚Lemma‘ (2) nach ‚sprechen‘ (3) gesucht. An der zweiten Position wird auf dem POS-Layer (4) nach APPR (5) gesucht (APPR steht für die Wortart ‚Präposition; Zirkumposition links‘). Diese Abfrage ergibt somit Bi-Gramme wie „sprechen mit“, „spricht über“, „sprachen aufgrund“ usw.
            

            

                

                
Abb. 4

            

            
Abb. 4 zeigt, kompakt zusammengeschnitten, die aktuell verfügbaren Visualisierungen. Diese können links (siehe Abb. 4 – Markierung 1) gewählt werden. Zur Verfügung steht ein tagesbasierter Frequenzverlauf (2 – siehe auch Abb. 1), eine Kalenderansicht (3) und ein Sankey-Diagramm (4). Die Visualisierungen können über den unteren Bereich (5) angepasst werden. Es ist z. B. möglich, absolute und relative Frequenzen auszuwerten, eine Granulierung (Auswertung pro Tag, Woche, Monat, Quartal und Jahr) und davon abhängig eine Glättung zu wählen.

            

                

                
Abb. 5

            

            
Der Auszug der Detail-Ergebnisse im Suchverlauf (siehe Abb. 5) ermöglicht es, eine Teilmenge von Ergebnissen auszuwählen (1). Die gesamten Daten einer einzelnen Suchabfrage können über das Dreipunkt-Menü (siehe Bereich 2) als JSON, TSV und URL exportiert werden, um die Daten weiterzugeben bzw. auch um die Daten mit anderen Programmen auszuwerten und zu visualisieren. Außerdem ist es möglich, den gesamten Suchverlauf (siehe Bereich 3), also alle Suchabfragen, als JSON zu exportieren und einen gespeicherten Suchverlauf wiederherzustellen.

            
OWIDplusLIVE stellt bereits jetzt eine Ressource für die tagesaktuelle Analyse sprachlicher Daten in RSS-Newsfeeds deutscher Online-Presse dar. Trotzdem gibt es an einigen Stellen Potential zur Weiterentwicklung. So könnten die analysierten Zeitabschnitte noch flexibler gestaltet werden, um auch Entwicklungen zu erfassen, die kleinteiliger als ein Tag (z. B. für die Analyse von Social-Media-Sprachdaten) oder grobkörniger als ein Jahr (z. B. für diachrone Analysen) sind. Außerdem sind zusätzliche Visualisierungen denkbar, die unterschiedliche Blickwinkel auf die Daten ermöglichen würden.

        

            

                

                    
Gegenstandsbereich und Ausgangslage

                    
Die Aufgabe, kunst- und kulturhistorisch relevante Objekte digital zu dokumentieren, sie inhaltlich zu erschließen und über das Internet zugänglich und recherchierbar zu machen, stellt sich zunehmend nicht nur den “klassischen” Gedächtnisorganisationen wie Museen, Bibliotheken und Archiven. Auch kleinere Sammlungen, etwa an Universitäten (vgl. Koordinierungsstelle für wissenschaftliche Universitätssammlungen in Deutschland, Empfehlungen 2016) oder themenspezifische Forschungs- und Erschließungsprojekte (vgl. Kieven, Schelbert 2014) stehen vor der Aufgabe, Daten zu Kulturgütern strukturiert zu erfassen, sie nachhaltig zu speichern und sie dann – eben im Sinne von 
                        
Open Humanities, Open Culture
 - der Scientific Community und der interessierten Öffentlichkeit zur Nachnutzung zur Verfügung zu stellen (vgl. Schöch 2017).
                    

                    
Doch gerade ein solcher, wissenschaftsorientierter und zugleich Disziplinen übergreifender Ansatz ist mit zwei entgegengesetzten Problemen konfrontiert: Zum einen sind die zur Verfügung stehenden Ressourcen für die Beschaffung, Anpassung und Entwicklung von Infrastrukturen, für die inhaltliche Arbeit an Datenmodellen oder die Aufbereitung von Daten und Medien typischerweise sehr knapp bemessen. Zum anderen soll die digitale Erfassung hier oft deutlich mehr leisten als die klassische Objektdokumentation, da Wissensrepräsentation und Kontextualisierung eine größere Rolle spielen. Die traditionelle Vorstellung der Sammlungsdatenbank als eines abgeschlossenen, auf Vollständigkeit und Verbindlichkeit abzielenden digitalen Katalogs ist vor einem solchen Hintergrund zugleich unerreichbar und unzureichend.

                

                

                    

                        
I. Konzeptionelles. Offene Referenzen als Schlüssel zu einer zugleich mächtigen und offenen Datenstruktur
                    

                    
Interessante Perspektiven eröffnen in diesem Zusammenhang die Konzepte des Semantic Web bzw. der Linked Open Data: Durch die semantisch explizit definierte Verknüpfung öffentlich zugänglicher und über persistente Identifier (URI) adressierbarer Datenobjekte ergibt sich ein umfassender, offener Knowledge Graph, der sich aus den unterschiedlichsten epistemologischen Perspektiven befragen lässt. Allerdings ist dieses Konzept, aufs Ganze der digital verfügbaren Ressourcen von Museen, Archiven und Forschungseinrichtungen gesehen, bislang bestenfalls ansatzweise realisiert. Ein Beispiel sei genannt: Der Knowledge Graph des Konsortium NFDI4Culture ist als eines der ehrgeizigsten Projekte auf diesem Feld zunächst darauf beschränkt, die Ressourcen des Konsortiums als Linked Open Data auf der Basis einer Kultur-Ontologie zu beschreiben. (
                        

                            
https://nfdi4culture.de/resources/knowledge-graph.html

                        
). Er bewegt sich also noch auf der Metaebene, die Einbeziehung der Domänen selbst, von konkreten Kulturgütern bis hin zu allgemeinen Sachverhalten wäre der nächste Schritt. 
                    

                    
Es stellt sich also die Frage: Wie kann die eigene Sammlung Teil des universellen Knowledge Graph werden? Die Antworten erscheinen zunächst trivial: technisch durch die Publikation der Daten über eine Schnittstelle, konzeptionell durch die semantisch adäquate Datenmodellierung, bestenfalls nach einem zertifizierten Standard (etwa CIDOC CRM, ISO 21127:2014; LIDO oder EAD). 

                    
Nicht trivial sind allerdings, gerade im Hinblick auf die oben geschilderte Konstellation – forschungsbezogene Sammlungsdokumentation jenseits der klassischen Gedächtnisorganisationen mit sehr begrenzten Ressourcen –, die konkret und praktisch damit verbundenen Herausforderungen, insbesondere im Bereich der Datenmodellierung. Jedenfalls muss man zur Kenntnis nehmen, dass das Konzept der offenen und vernetzten Daten im Bereich der Sammlungsdokumentation noch zu einem sehr geringen Grad umgesetzt ist, obwohl nach unserer Einschätzung die Zielsetzung, Daten zu vernetzen, inzwischen bei vielen Verantwortlichen Zustimmung findet und zunehmend als Notwendigkeit verstanden wird. Auch ist oft die Bereitschaft zu erkennen, im Hinblick auf die eigene Arbeit umzudenken. Es fehlen aber die Perspektiven für eine Implementierung unter gegebenen infrastrukturellen Voraussetzungen.

                    
Zwar stehen für Datenbank-Lösungen, die über klassische, katalogisierende Objektdokumentation hinausreichen, die Kontextinformation und umfassende semantische Zusammenhänge abbilden und damit vernetzbar sind, heute etwa mit WissKI, ResearchSpace oder Wikibase (vgl. Müller 2022) ausgereifte Open-Source-Systeme zur Verfügung, um die erwähnten Konzepte zu implementieren. Insbesondere kleineren Sammlungen und Forschungsprojekten, denen unser besonderes Interesse gilt, erscheint die notwendige Kombination aus Systemwechsel, Datenmigration und semantisch anspruchsvoller und normgerechter Datenmodellierung jedoch oft als Überforderung.

                    
Dessen ungeachtet wäre prinzipiell zu fragen, ob die Ausstattung einzelner Institutionen oder Projekte mit hochkomplexen Datenbank-Management-Systemen ein uneingeschränkt erstrebenswertes Ziel ist. Man kann darin auch eine Sackgasse sehen: Die Planung, Implementierung, Wartung und Aufrechterhaltung dieser Großinfrastrukturen binden in hohem Maße Ressourcen und Aufmerksamkeit, während der reale Mehrwert von Nachnutzbarkeit und Vernetzung erfahrungsgemäß gerne etwas aus dem Blick gerät. Eher modular strukturierte, Agilität befördernde Ansätze, die wir zunächst und vor allem in Betracht ziehen, weil sie sich mit geringerem Ressourceneinsatz realisieren lassen, bieten hier aus unserer Sicht eine überlegenswerte Alternative oder zumindest Ergänzung. Der Fokus könnte sich von einer Optimierung der Vernetzbarkeit hin zu einer Vernetzung bereits in der Datenerfassung und -erschließung verschieben.

                    
Ein weiterer Punkt: Die gerade im kulturhistorischen Bereich relevante und zumeist angestrebte Vernetzung mit globalen Wissensbeständen konnte damit noch nicht erreicht werden.

                    
Wir setzen da an, wo auch traditionelle Datenbanken seit langem punktuell vernetzen. Bekanntlich bilden Normdaten den ersten Schritt des Verweisens über den eigenen Katalog hinaus. Normdaten wurden im Bibliothekswesen entwickelt, um die einheitliche Ansetzung von Elementen des Katalogs zu garantieren, die in mehr als einem Eintrag und nicht nur im einzelnen (Bibliotheks-)Katalog vorkommen (
                        
Woitas 2013

                        
)
. Klassische Anwendungsfelder sind die Ansetzung von Personen- und Ortsnamen. Zunächst waren Normdaten damit primär Schreibregeln. Das Prinzip wurde erweitert auf die Normierung von Begriffen und deren Einordnung in klassifikatorische Systeme: Als Klassifikationen oder Thesauri – etwa bei der Schlagwortnormdatei (SWD) – erfüllen Normdaten erweiterte semantische Funktionen bei der inhaltlichen Erschließung.
                    

                    
Indem die Normansetzungen zentral gehalten werden und über einen persistenten Identifier stabil referenzierbar sind, werden sie zu Referenzdaten. Sie erzielen neben der Sicherung von Konsistenz und Qualität der Daten einen klassischen Normalisierungseffekt (im Sinne des Entity Relationship Model von E. F. Codd), nur dass die „normalisierenden“ Entity-Relationen über die eigene Datenbank hinausgreifen: Die Daten zur referenzierten Entität müssen (im Prinzip) nicht mehr selbst angelegt und gepflegt werden, es reicht die ID der Normansetzung, in der Praxis ergänzt durch importierte oder begleitende Kerndaten (vgl. zur Normdatennutzung im Kulturbereich Kailus, Stein 2018, Kett et al. 2019).

                    
Damit beschränkt sich die Funktion des Verweisens nicht mehr auf die Funktion der Normierung, sondern eröffnet zusätzliche Aspekte. Nach Ansicht der Autoren sollte auch daher eher von Referenzdaten gesprochen werden. Wir identifizieren mindestens die folgenden Referenzdatentypen:



                        
kontrollierte Vokabulare: Ziel ist die Vereinheitlichung der Fachterminologie und die Sicherstellung semantischer Interoperabilität, d.h. dass in unterschiedlichen Ressourcen vom Gleichen die Rede ist, wenn derselbe 
                            
Begriff 
verwendet wird.
                        

                        
Identifikationsfunktion, Disambiguierung: Anspruch, bei Kulturgütern, Monumenten und Orten unverwechselbar deutlich zu machen, von welcher 
                            
Entität 
die Rede ist.
                        

                        
Taxonomien, Thesauri und Ontologien: Einrichtung einer begrifflich-inhaltlichen Ordnung der Gegenstandsbereiche, auch mit dem Anspruch, das weitere Konzept des Gegenstands zu erfassen.

                        
Lexikalische Anreicherung: zum Beispiel erweiterte biographische Angaben zu referenzierten Personen; historische Kontextualisierung von Gegenständen und Sachverhalten.

                        
Lokalisierung, Georeferenzierung als Sonderfall einer quantitativ definierten Verortung.

                    

                    
Derart konzeptualisiert, differenzieren sich die Funktionen, die Referenzdaten leisten können: Neben Normierung und Normalisierung spielen auch Identifikation, Erschließung und Anreicherung eine Rolle – und all dies in einer potentiell unendlich und für alle Anwendungsperspektiven skalierbaren Struktur, die daher auch offen sein muss für die möglichst umfangreiche Beteiligung der wissenschaftlichen Community.

                    
Für die gestellte Aufgabe der Anreicherung und Öffnung einfacher Objektkataloge eröffnen die Referenzdaten in diesem erweiterten Verständnis interessante Perspektiven. Denn sowohl ihre strukturierte und stabile semantische Bestimmung als auch die Vernetzung findet (etwas pauschalisiert und idealisiert gesagt) auf Seiten der referenzierten Repositorien statt und muss nicht (nur) in der eigenen Datenhaltung geleistet werden. Die Realisierung eines semantischen Mehrwerts – Explizität, Präzision, Eindeutigkeit, und damit: Eignung für die Vernetzung der eigenen Daten – geht also theoretisch sogar einher mit einem Effizienzgewinn in der eigenen Datenhaltung. Die Umsetzung stellt jedoch neue Herausforderungen: Die Implementierung der Referenzierung von Daten in unzähligen Repositorien muss bewältigt werden. Vorrangiges Ziel muss es also sein, die Komplexität der vielfältigen Referenzierungen zu reduzieren.

                    
Die Nutzung von Wikidata (https://www.wikidata.org, vgl. 
                        
Vrandečić, Krötzsch
 2014, Müller-Birn et al. 2015), der unter einer freien Lizenz verfügbaren Wissensdatenbank der Wikimedia-Familie, erscheint uns hier als der vielversprechendste Ansatz. Wikidata erfüllt als Infrastruktur und hinsichtlich des Datenbestandes einen erheblichen Teil der Anforderungen, die in unserem Bereich an Referenzdaten gestellt werden, und ist ein bedeutender Knoten im Linked Data Web (vgl. Erxleben et al. 2014).
                    

                    
Entscheidend ist zunächst, dass Wikidata nicht nur Funktionen traditioneller Normdaten erfüllen kann (vgl. 
                        
Voß et al. 

                        
2014): 
Wikidata bietet mit seinem aus Items und Properties aufgebauten Datenmodell eine logische Struktur, mit den darin erfassten Daten einen der größten globalen Wissensbestände (vgl. die DHd-Beiträge Schelbert 2017 und Müller-Birn et al. 2018; zu den allgemeinen Potentialen von Wikidata für den Kulturerbesektor vgl. Krötzsch 2016, Poulter 2017, 
                        
Schmidt et al. 2022
). Darüber hinaus steht mit der zugrundeliegenden Software-Suite Wikibase auch eine konkrete Arbeitsplattform frei zur Verfügung, die in einer eigenen Implementierung genutzt werden könnte, was im Zusammenhang dieses Projekts jedoch nicht vorgesehen ist (anders als etwa in den Projekten Rhizome und ArtBase, vgl. Rossenova 2021).
                    

                    
Unser Lösungsansatz sieht vor, von der eigenen Datenhaltung zunächst lediglich auf Wikidata zu referenzieren. Die Nutzung der Daten aus Wikidata kann von dieser Basis aus flexibel erfolgen: Das Konzept unseres Projekts verlangt keine 
                        
Festlegung auf bestimmte Daten. Eine Option ist, Wikidata-Referenzen nur zu verwenden, um die Durchsuchbarkeit zu verbessern, eine naheliegende andere ist der indirekte Zugriff auf klassische Normdaten (Statements der Rubrik „Identifiers“), möglich ist etwa auch die Kategorisierung von Objekten (bspw. nach übergeordneten Kategorien, die in Wikidata festgehalten sind, bspw: Ort → Land).

                    

                    
Der Aufwand ist selbstverständlich skalierbar. Aus der geschilderten Ausgangslage (reiche Sammlungen, wenige Ressourcen) richtet sich unser Augenmerk aber (zunächst) auf möglichst einfache Lösungen mit dennoch beachtlichem Ertrag. Die Beurteilung der Erschließungsleistung misst sich bei unserem Ansatz nach dem Verhältnis von Aufwand und erzieltem Mehrwert für Erschließung und Vernetzbarkeit.

                    
Der notorisch schwierigen und schwierig einzuschätzenden “Datenlage” in Wikidata begegnen wir, indem wir uns auf Daten konzentrieren, die mit einer gewissen Zuverlässigkeit vorhanden sind, sowie durch die Einbeziehung von Daten aus den sekundär referenzieren “klassischen” Normadatensätzen (GND etc.). 

                    
Auch wenn wir aus dem an sich viel reicheren Daten in Wikidata nur relativ basale Teilmengen verwenden, ergeben sich für die Erschließung des Materials erhebliche Verbesserungen, wenn man etwa für die auf einem historischen Lehr-Dia dargestellte Kirche auch Benennungsvarianten, die landessprachliche Bezeichnung, die Konfession, das Bistum oder die Geokoordinaten des Ortes für die Erschließung (aus Sicht der Nutzer*innen: für das Suchen, Filtern, Sortieren) zur Verfügung hat.

                    
Die Komplexität der Daten in Wikidata (z.B. Unschärfe der Datierung), die nicht im eigenen System abgebildet werden kann oder soll bzw. nicht vollständig in der Suche wirksam gemacht werden kann, stellt eine Herausforderung dar, der zunächst pragmatisch mit Hinweisen (Disclaimer) begegnet wird. Die darüber hinaus mögliche Implementierung entsprechend komplexerer Lösungen für Datenhaltung, -suche und -visualisierung ist nicht Gegenstand unseres Vortrags.

                    
Ein weiterer Aspekt ist die Frage nach der Form der Anbindung und damit der Aktualität der verwendeten Referenzdaten aus Wikidata. Hierbei soll nach folgenden Prinzipien vorgegangen werden, die wir im Fallbeispiel im Einzelnen ausführen.



                        
Eine Aktualisierung, also der wiederholte Abgleich mit den Referenzdaten, soll grundsätzlich stattfinden (können). 

                        
Es muss steuerbar sein, welche Daten aktualisiert werden sollen, ob die selbst eingepflegten Daten Priorität vor Daten haben, die über die Referenzierung gewonnen wurden.

                    

                    
Dem wichtigen Aspekt der Transparenz der Datenprovenienz wird durch die Kennzeichnung der aus Wikidata bezogenen Daten im Frontend Rechnung getragen. 

                

                

                    

                        
II. Fallbeispiel. Die kunsthistorische Lehrbildsammlung Berlin
                    

                    
Die Lehrbildsammlung des Instituts für Kunst- und Bildgeschichte der Humboldt-Universität eignet sich in zweifacher Hinsicht, um diese Konzeption prototypisch zu erproben. Zum einen sind die skizzierten Limitierungen hier gegeben: Die Daten werden im Medienrepositorium der Universität verwaltet, das als Digital-Asset-Management-System (auf der Basis von ResourceSpace, https://www.resourcespace.com/) mit einer relativen flachen Datenstruktur an sich wenig geeignet ist, um einen Linked-OpenData-Ansatz zu realisieren. Zum anderen ist bei der Erschließung von kunsthistorischen Bildobjekten die Vernetzung mit weiteren Daten zum Kulturerbe inhaltlich besonders vielversprechend, da diese bekannte, singuläre Artefakte abbilden (vgl. Schelbert 2022).

                    
Die kunsthistorische Lehrsammlung entstand mit der Gründung des Fachs seit den 1870er Jahren und stellt trotz erheblicher, teils kriegsbedingter Verluste eine der größten und reichhaltigsten ihrer Art dar (Haffner 2007, Schelbert 2018). Die Fotografien verschiedener Formate und Techniken (Papierabzüge, Glas- und Planfilmnegative, Glas- und Filmdiapositive) verweisen auf Kulturgut und Sachverhalte von großer zeitlicher und räumlicher Bandbreite. Sie interessieren außerdem in hohem Maß hinsichtlich fach-, institutions- und sammlungsgeschichtlicher Bezüge, wodurch sich die Herausforderungen der Referenzierung in besonderer Weise stellen. Es ist eine hohe Zahl an Digitalisaten vorhanden, denen die inhaltliche Erschließung fehlt, und es kann aufgrund der infrastrukturellen Bedingungen nur mit einfachen Datenbanksystemen und einfachen Datenmodellen gearbeitet werden.

                    
Wikidata wird an dieser Stelle bereits bisher als Schlüssel sowohl zur Anreicherung als auch für die Anschlussfähigkeit der Digitalisate eingesetzt. Anstelle der Anlage komplexer Stammdaten werden die grundlegenden Entitäten (Personen, Geographica, Werke, Körperschaften) mit Wikidata-ID versehen und so mit den entsprechenden Wikidata-Items verknüpft. Potentiell kann das referenzierte Wikidata-Item die oben genannten Funktionen der Referenzierung erfüllen: Es identifiziert, disambiguiert, verweist auf andere Referenz-Repositorien, enthält Geodaten und in seinen Statements zahlreiche relevante Daten.

                    
Allerdings sieht das Datenmodell im verwendeten DBMS ResourceSpace keine Einbindung von Referenzierungen vor. Ersatzweise sind die Wikidata-IDs deshalb lediglich in den Text der jeweiligen Metadatenfelder geschrieben. Da sie in ihrer Struktur (Q + Nummer) leicht zu identifizieren sind, sind sie sie prinzipiell maschinell verwendbar, jedoch nicht bzw. nur sehr mühsam in den von ResourceSpace vorgesehenen Bedienelementen.

                    
Es stellt sich also die Frage, wie die in Wikidata enthaltenen Informationen auch in der konkreten Anwendung praktisch nutzbar gemacht werden können. Hier bedarf es einiger infrastruktureller Schritte, die wir in einem Pilotprojekt u.a. im Rahmen des Projekts Digitales Netzwerk Sammlungen der Berlin University Alliance (https://www.ub.hu-berlin.de/de/ueber-uns/projekte/digitales-netzwerk-sammlungen/projekt-digitales-netzwerk-sammlungen) erprobt haben.

                    
Die technische Konzeption, an der wir uns orientieren, basiert auf den Prinzipien einer Separation of Concerns und der zeitlichen Entkopplung von Nutzung und Aggregation der (Referenz-)Daten. Gerade im Hinblick auf die genannten infrastrukturellen Limitierungen kann es nicht darum gehen, Sammlungsdatenbanken neu zu konzipieren oder in ihrer Struktur an die anspruchsvolle Referenzdatenvernetzung anzupassen. Wir ergänzen vielmehr die eingesetzten Systeme “von außen”, also jenseits der Schnittstelle (im Idealfall ein dynamisch abfragbarer Endpoint, notfalls ein händischer Datenexport) mit einer modular aufgebauten Middleware (Data processing und data aggregator tools auf der Basis von Node js; sekundärer Datenspeicher mit API), über die die semantisch reiche Verknüpfung geleistet wird. Schon auf der Ebene von Wikidata als Referenzdaten-Hub sind diese Operationen komplex und umfangreich, die Einbeziehung der dort referenzierten primären Repositorien (z.B. GND, Getty AAT, Iconclass …) steigert die technischen Anforderungen, die sich aus den Verknüpfungsoperationen ergeben, weiter. 

                    
Auch hier suchen wir die Lösung nicht in der Skalierung, sondern in der Modularisierung. Module, die bei der Nutzung, etwa bei der Suche nach relevanten Objekten, beim Filtern und Sortieren, angesprochen werden, arbeiten mit bereits hochgradig aggregierten Daten, die praktisch instantan zur Verfügung stehen. Der Prozess der Aggregation läuft dagegen periodisch bzw. durch Triggerung aus der Datenhaltung, also zeitlich entkoppelt von der Nutzung ab. Mit anderen Worten: Der Umsetzungsvorschlag sieht keine dauerhafte Übernahme von Daten aus Wikidata vor. Vielmehr handelt es sich um eine temporäre, asynchron-dynamische Übernahme von Daten aus Wikidata zum Zweck der Suche bzw. zum Zweck der Anzeige angereicherter Daten in einem entkoppelten Frontend (hier realisiert in Vue.js). 

                    
Die Middleware, in die periodisch Datenabzüge aus Wikidata und der Quelldatenbank eingespielt werden, wird lediglich aus Performanzgründen eingesetzt, da eine reine on-the-fly-Lösung aufgrund der notwendigen kaskadierenden Abfragen in Wikidata nicht praktikabel wäre. Im Fallbeispiel der kunsthistorischen Bilddatenbank werden aus Wikidata Daten für abgebildete Werke in die Middleware übernommen, um auf dieser Basis schnelle Abfragen und Darstellungen von zusätzlichen Daten zum jeweiligen Bildobjekt zu erzeugen. Das Ausspielen der eigenen Daten nach Wikidata zur Korrektur und/oder Ergänzung des dortigen Datenbestandes wäre ein reizvoller weiterer Schritt im Sinne einer echten Vernetzung, den wir aber noch nicht gegangen sind.

                    
Eine Vorstellung vom Aufbau einer nach diesen Prinzipien konzipierten Architektur vermittelt das folgende Schema :

                    

                        

                        
Abb. 1: Wikidata-basierte Anreicherung und Erschließung von Sammlungsdaten - Systemarchitektur des Pilotprojekts (Grafik M. Müller)

                    

                    
                

            

        

            

                

                    
30 Mio. Token, 140.000 Blogposts, über 200 aufbereitete Blogs
                

                
Bereits seit Ende der 90er Jahre gewannen Weblogs als Medium zur öffentlichen Darstellung unterschiedlicher Themen und Inhalte immer mehr an Popularität. Findige Literatur- und Kulturschaffende zögerten nicht lange, um das neue Medium auch für literarische Zwecke umzunutzen.

                    
Blogs wie 

                    
Die Dschungel. Anderswelt

                    
von Alban Nikolai Herbst
, 
                    
Abfall für alle

                    
von Rainald Goetz
, Wolfgang Herrndorfs 
                    
Arbeit und Struktur

                    
oder das kollaborativ betriebene Blog 

                    
Die Riesenmaschine

                    
werden seither zwar regelmäßig als Gegenstand wissenschaftlicher Unters
uchungen herangezogen (vgl. Fassio 2021, Giacomuzzi 2008, Knapp 2014, Knapp 2012), wie viele andere Formen von Literatur im Netz im Vergleich zu ihren genuin analogen Pendants jedoch immer noch durchaus stiefmütterlich behandelt. Hinzu kommt, dass sich die Verfasser:innen dieser Blog-Studien in erster Linie klassisch hermeneutisch-literaturwissenschaftlicher Methoden bedienen und nur in seltenen Fällen auf computergestützte Analysemethoden und -werkzeuge zurückgreifen (vgl. zuletzt: Fassio 2021), obwohl die Weblogs schon ihrem Begriff nach born-digital sind und damit eine ‚digitale‘, computergestützte Form der Analyse zunächst auf der Hand zu liegen scheint.

                

                
In dem Beitrag zur Jahrestagung der DHd 2022 (Blessing et al 2022) haben die Autor:innen des vorliegenden Beitrags bereits am Beispiel des u. a. von der Autorin Kathrin Passig ins Leben gerufenen Techniktagebuch
 verschiedene computergestützte Möglichkeiten sowie geeignete Werkzeuge für die Analyse literarischer Blogs vorgestellt. Der in der End-Anwendung später kaum sichtbare Aufwand an textuellen Preprocessing-Schritten, der bereits an diesem vermeintlich einfachen Fallbeispiel offenbar wurde, gepaart mit den Reaktionen und dem großen Interesse aus der wissenschaftlichen Community an der grundlegenden Methodik zum Umgang mit WARC-Dateien, lassen bereits die Gründe erahnen, weshalb in der Blog-Forschung digitale Methoden bislang vergleichsweise selten zum Einsatz kamen. Im hier nun vorliegenden Beitrag sollen – als Fortsetzung und Erweiterung des vorangehenden Beitrags auf der DHd 2022 – nicht nur exemplarisch ebendiese Herausforderungen, sondern vielmehr auch Lösungsmöglichkeiten im Umgang mit (archivierten) Blogs aufgezeigt werden. Im Fokus der Untersuchung steht dabei nicht mehr nur ein einzelnes Blog, sondern vielmehr ein insgesamt aus über 200 aufbereiteten Blogs mit ca. 140.000 Blogposts und 30 Millionen Token bestehender Fundus literarischer Blogs. Ebensolche literarische Weblogs sammelt die Bibliothek des Deutschen Literaturarchivs Marbach neben literarischen Zeitschriften und Objekten der Netzliteratur bereits seit 2008.
 Bislang erfolgt die Bereitstellung dieser Sammlungsobjekte über die Plattform Literatur-im-Netz
, 2023 werden diese zusammen mit der hier vorgestellten Ressource über das SDC4Lit-Repositorium bereitgestellt.

                

            

            

                
Bausteine
                    
 
von
                    
 
Weblogs
                

                
Typische Bausteine in Blogs sind Blogposts als (meist datierte) Inhaltseinheiten verschiedenster Länge und unter Verwendung unterschiedlicher Modalitäten wie Text, Bild, Animation, Video, Referenz (z. B. Hyperlinks), etc. Weiterhin finden sich auf der Ebene der Blogposts oft Kommentarformulare und Kommentare sowie eine Etikettierung von Einträgen in Form von Tags, die der Gruppierung und Beschreibung der Einträge dienen und sich auf Themen, Stimmungen, Autoren, etc. des Blogposts beziehen können (vgl. zu den Bausteinen von Blogs: Ernst 2010, 286f.). Zusätzliche Elemente wie Übersichtsseiten, die als Einstiegsseiten der jeweiligen Blogs die einzelnen Blogposts (zusätzlich) in einer bestimmten Reihenfolge auflisten, oder Archivseiten, die den Nutzer:innen einen Zugang zu älteren Blogposts ermöglichen sollen, prägen die Struktur der Blogs und damit ggf. deren Analyse.

                

                    

                    

                    
Abb.1: Links: Übersichtsseite (Home) mit mehreren Blogposts. Rechts: Seite des ersten Blogposts mit Tags, Kommentaren und Kommentarfunktion. Beispiele aus "Logbuch Isla volante : Bilder und Texte von der Insel". Spiegelung 2017.03.17_01, URN: urn:nbn:de:bsz:mar1-dd001-fe31dc38-7c43-4da2-ae3d-fd9619f88ea45.

                

                

                
                
Die Spiegelungen der Blogs, die am DLA durchgeführt werden, erfolgen zunächst mit einem Crawling-Vorgang, der der Hyperlinkstruktur im Blog folgt und die clientseitig (wie durch einen Browser) empfangenen Daten gemeinsam mit einigen Metadaten zum Crawlingprozess im Web ARChive- oder kurz: WARC-Format ablegt. Das aus dem ARC-Format des Internet Archive weiterentwickelte Archivformat hat sich inzwischen als internationaler Standard für die Archivierung von Webinhalten etabliert (IIPC, n.d.). Beim Crawling können Inhalte, die ggf. nicht Teil des Blogs sind, wie z. B. zufällig eingebundene Werbeanzeigen oder externe Inhalte, auf die von diesen Werbeanzeigen verwiesen wird, Teil des Archivobjekts werden. Aber auch bezüglich der tatsächlichen Blog-Elemente sind im Archivobjekt Inhalte (z. B. bestimmte Textpassagen) so oft abgelegt, wie der Crawler ihnen auf verschiedenen Seiten begegnet ist. Der Inhalt eines Blogposts kann an verschiedenen Stellen für den Crawler erreichbar sein: auf der Übersichtsseite, auf der eigenen Seite des Posts, auf der Seite jedes Schlagworts, mit dem der Post versehen ist, im Archiv, etc. Aber auch Textpassagen aus Strukturelementen wie Kopf- und Fußzeilen führen zu mehrfachen Vorkommen von Textpassagen oder Begriffen.

                
Werkzeuge, die Strukturelemente ausblenden und (Text-)Duplikate erkennen, sind daher bei der Vorverarbeitung der Daten für Analysen notwendig, arbeiten oft aber statistisch und müssen ggf. auf jedes zu untersuchende Blog neu angepasst werden, was im Spannungsfeld mit dem maschinell unterstützten Distant Reading steht. Im Folgenden wird daher das Vorgehen bei der Aufbereitung der Blogs beschrieben, das notwendig ist, um die Texte auch für quantitative Analysen verfügbar zu machen.

            

            

                
Aufbereitung der Blogs als Ressource für quantitative Analysen

                
Die am DLA archivierten Blogs sind nicht nur inhaltlich und stilistisch sehr heterogen, sondern auch in Bezug auf die technische Umsetzung. In den meisten Fällen werden bekannte Blog-Hoster wie wordpress (40%), twoday (15%), blogger, blogspot usw. verwendet. Diese Blog-Hoster wiederum setzen Content Management Systeme (CMS) ein, die die Blogpost-Erstellung, -Verwaltung sowie die Blogdarstellung sowohl für die Blogverfasser:innen, als auch für die Blogleser:innen vereinfachen.

                

                
Für die Analyse des Inhalts eines Blogs sind nur die Blogposts relevant, da alle Übersichtsseiten (Home, Tags, Categories) eines Blogs automatisch daraus generiert werden. Darüber hinaus hat sich gezeigt, dass beim Erstellen von WARC-Crawls auch einiges an ‚Beifang‘, also Webseiten, die nicht zum Blog gehören, aber für das Abspielen teilweise nützlich sein könnten, mit-archiviert werden, was wiederum bei der Aufbereitung der Blogs beachtet werden muss. Die hier beschriebene Umsetzung zielt darauf ab, ein sauberes Textkorpus für jedes Blog zu extrahieren und alle irrelevanten und redundanten Inhalte zu ignorieren.

                
Bei der Aufbereitung werden Verfahren aus dem Information Retrieval eingesetzt (vgl. Manning, Raghavan und Schütze 2008, 443–459). Das Besondere am aktuellen Datensatz liegt darin, dass die Daten bereits im WARC-Format vorliegen. Cormack, Smucker und Clarke (2011) haben bereits gezeigt, wie bei sehr großen Web-Crawls
                    

                    
, die als WARCs vorliegen, relevante Informationen extrahiert werden können. Unser Szenario unterscheidet sich dahingehend, dass hier eine klare Definition der gesuchten Information 
                    
–
 in Form von Blogposts 
                    
–
 existiert. Dies erfordert einen feingranularen Ansatz.
                

                
Die Umsetzung läuft in mehreren Schritten: In Schritt 1 werden alle HTML-Seiten des Blogs aus den WARCs extrahiert und erkannte Fremdinhalte entfernt. Schritt 2 erkennt das verwendete CMS-System. Schritt 3 basiert auf einem Regelsystem, das für jedes Blog die Posts ermittelt. Die URL-Pfade der Blogs sind ein Hauptmerkmal, welches in den Regelsystemen verwendet wird. In Schritt 4 werden zu jedem Post Text- und Metadaten mittels trafilatura
 extrahiert. Alle Schritte sind als Jupyter-Notebooks implementiert und werden zusammen mit der Ressource veröffentlicht. Nach der Aufbereitung liegen die Daten im JSON- und txt-Format vor. Ein JSON-Datensatz enthält Text und Metadaten und kann z. B. direkt in Analyse-Tools importiert werden. Zum Aufbereitungsprozess gibt es Logfiles, die für die iterative Verbesserung des Prozesses ausgewertet werden.
                

                
Da in allen vier dieser Umsetzungsschritte Anpassungen notwendig sind, gehen Weiterentwicklung und Optimierung der Aufbereitung immer einher mit dem Einsatz von Analysewerkzeugen. Bei der Verwendung solcher Werkzeuge können systematische Auffälligkeiten sichtbar werden, wodurch beispielsweise noch vorhandene Redundanzen zum Vorschein kommen. Ein Werkzeug, mit dem große Textkorpora durchsucht und auf grammatikalische sowie syntaktische Strukturen hin untersucht werden können, ist CQPweb

                    
(
Hardie 2012, 380–409). Ein Auszug aus einer Beispielanfrage, in der 
                    
–
 angelehnt an eine mögliche Forschungsfrage zu Formen der Erinnerung 
                    
–
 nach dem Zeichenstring „erinnere mich“ gesucht wird, bringt redundante Textstellen zutage (Abb. 2). Dieser Hinweis fließt in die Optimierung der Regeln in den Aufbereitungsskripten ein.
                

                

                    

                    

                    
Abb. 2: Auszug aus einer CQPweb-Testinstanz mit dem Suchbegriff „erinnere mich“.

                

                

                
                
Das Durchsuchen und Abspielen von WARC-Dateien wird durch die Web-Applikation SolrWayback
 ermöglicht. In der Ergebnisliste zur Suche nach dem Begriff „techniktagebuch“ verweisen die ersten Treffer alle auf Twitter (Abb. 3). Der Inhalt des Tweets bleibt dabei gleich, jedoch steht das Webinterface in verschiedenen Sprachen zur Verfügung. Insgesamt werden bei der Suchanfrage knapp 50 verschiedene Sprachen für die Twitterseite gefunden.
                

                

                    

                    

                    
Abb. 3: Auszug aus der Trefferliste einer SolrWayback-Testinstanz mit der Suchanfrage „techniktagebuch“ und dem Beginn der CSV-Trefferliste.

                

                

                
                
Der Erfolg bei der Aufbereitung lässt sich anhand der Zahlen in Tabelle 1 ablesen. Hier wird auch die Heterogenität der Blogs sichtbar: Die Aufbereitungsschritte haben je nach Blog unterschiedlich starke Auswirkungen. Während der Unterschied der Anzahl von WARC-Records und HTML-Seiten deutlich macht, wie viele externe Inhalte und Nicht-HTML-Inhalte entfernt werden müssen, zeigt sich an der Differenz zwischen der Anzahl an HTML-Seiten und der Anzahl an Blogposts, wie hoch die Redundanz durch generierte Übersichtsseiten ausfällt.

                

                    

                        
Blog

                        

                            
WARC-Records

                        

                        
HTML-Seiten

                        
Blogposts

                        
Token

                    

                    

                        

                            
Die.Dschungel.Anderswelt

                        

                        
123.859

                        
 65.014

                        
14.106

                        

                            
7,8 Mio

                        

                    

                    

                        

                            
Techniktagebuch

                        

                        
310.131

                        
 127.532

                        
 5.233

                        

                            
1,6 Mio

                        

                    

                    

                        

                            
Lux autumnalis

                        

                        
 11.121

                        
 6.426

                        
 3.092

                        

                            
1,3 Mio

                        

                    

                    

                        

                            
Henrikes Tagebuch

                        

                        
499.878

                        
168.856

                        
 861

                        

                            
0,2 Mio

                        

                    

                    
Tab. 1: Übersicht der resultierenden Ressourcengrößen nach der schrittweisen Aufbereitung für vier Beispielblogs.

                

            

            

                
Evaluation

                
Die Zahlen zur Ressourcengröße aus Tabelle 1 veranschaulichen, dass die Aufbereitung der Blogs nicht vollständig manuell validiert werden kann. Trotzdem ist eine Aussage über die Qualität der aufbereiteten Blog-Daten wichtig. Zu diesem Zweck wurde ein Annotations-Jupyter-Notebook entworfen, mit dessen Hilfe automatisch aus den jeweiligen CMS-Familien wie beispielsweise Wordpress oder Blogger repräsentative Datenmengen entnommen werden, die anschließend von mehreren Annotator:innen bewertet werden. Im ersten Schritt geht es um das Erkennen der Blog-Posts in Abgrenzung zu Übersichtsseiten oder weiteren Seiten wie beispielsweise dem Impressum. Wenn es sich bei der Seite um einen Post handelt, dann wird im zweiten Schritt die Qualität der Metadaten- und Textextraktion bewertet. Dabei wird u. a. darauf geachtet, ob Datum und Überschrift richtig extrahiert wurden und ob der extrahierte Text vollständig ist oder beispielsweise Teile aus der Seitennavigation enthalten sind.

                
Es hat sich gezeigt, dass die beschriebene manuelle Bewertung auch für Menschen nicht immer trivial ist. Daher ist der Annotationsprozess aktuell noch nicht abgeschlossen. Die Evaluationsergebnisse werden jedoch mit dem Release der Daten bereitgestellt.

            

            

                

                    
Nutzungsszenarien für die aufbereiteten Blog-Daten
                

                

                    

                    
Nach den oben beschriebenen Aufbereitungsschritten sind die 
daraus resultierenden Daten (nahezu) frei von Dubletten und Fremdinhalten, sodass sie für verschiedene Formen der Textanalyse sowie für weitere Prozessierungsschritte verwendet werden können. Dazu gehören korpuslinguistische Untersuchungen, wie sie mit CQPweb durchgeführt werden können, das Erstellen von Sprachmodellen (z. B. embeddings, topic-models) oder automatische Keyword-Erkennung. Auch die Untersuchung der in den Blogposts enthaltenen Metadaten (Datum, Autor:in, Verschlagwortung) kann für die Forschung von Interesse sein. Beispielhaft erwähnt sei an dieser Stelle die Verschlagwortung der einzelnen Blogs durch die jeweiligen Blog-Autor:innen, die mit End-Anwendungen wie Keshif
 erst auf Basis der aufbereiteten Blog-Daten überhaupt sinnvoll visualisiert und analysiert werden können (vgl. Abb. 4).
                

                

                    

                    

                    
Abb. 4: Screenshot aus einer Keshif-Testinstanz. Rechts werden alle Blogs hervorgehoben, in denen das Schlagwort Kunst vergeben wurde, links befindet sich eine Liste aller Schlagworte.

                

                

                
                
In Blessing et al (2022) wurde am Fallbeispiel des Techniktagebuchs bereits exemplarisch aufgezeigt, welche komplexeren computergestützten Analysen durch die Verwendung der extrahierten Text- und Metadaten vorgenommen werden können. Unter anderem wurde bereits untersucht, welche Zusammenhänge zwischen Inhalt 
                    
–
 repräsentiert durch automatische Keyword-Erkennung und die Verschlagwortung durch die Autor:innen 
                    
–
 und Form bzw. Sprache erkennbar werden.
                

            

            

                
Schätze heben und nutzen

                
30 Millionen Zeichen, 140.000 Blogposts, über 200 Blogs: Schon wegen seiner Größe ist das hier vorgestellte, aufbereitete Korpus für viele Bereiche der Digital Humanities, beispielsweise die Computational Literary Studies, die Digital History oder für NLP-Untersuchungen, eine wichtige Quelle für Inhaltsanalysen oder das Trainieren von Sprachmodellen. Wie in diesem Beitrag gezeigt stellt vor allem die Struktur der Weblogs eine Herausforderung dar, enthalten die WARC-Dateien, in denen die Blogs zunächst vorliegen, doch sehr viel Redundantes, das für eine Vielzahl von Inhaltsanalysen nicht nur uninteressant, sondern sogar hinderlich ist. Mit den im Zuge der Veröffentlichung der SDC4Lit-Plattform 2023
 zur Verfügung gestellten Blog-Daten in aufbereiteter Form wird eine robuste Ressource bereitgestellt, die neben den Rohdaten im WARC-Format auch das bereinigte Textkorpus in Form der inhaltlich relevanten Blogposts sowie die zugehörigen Metadaten zu jedem Post enthält. Dank der ebenfalls über SDC4Lit zur Verfügung gestellten WARC-Volltextsuche und WARC-Player wie SolrWayback können die Blogs bzw. die einzelnen Blogposts zudem – unabhängig von allen weiteren Analyseschritten – möglichst originalgetreu in ihrer ursprünglichen Repräsentationsform angesehen und erforscht werden, auch wenn die originalen Webseiten bereits nicht mehr vorhanden sind oder geändert wurden. Die Implementierung der Aufbereitung wird in Form von dokumentierten Jupyter-Notebooks bereitgestellt, dank der auch weitere, über das hier präsentierte Korpus hinausgehende (literarische) Blogs aufbereitet und damit für weitere DH-Bereiche zugänglich gemacht werden können, sodass die nunmehr bereits gehobenen Weblog-Schätze künftig nicht die einzigen bleiben.
                

            

        

            
Eine Analyse von Schilderungen traumatischer Erfahrungen in ZeugInnenaussagen vor internationalen Völkermordtribunalen

            
Gerichtstranskripte internationaler Völkermordtribunale gelten als verlässliche Quelle, um verschiede Aspekte von Völkermord und der Rolle von Überlebenden im Prozess zu beleuchten. Bisher wird die Fülle an öffentlich zugänglichen Gerichtstranskripten als historische Quelle jedoch kaum genutzt; es liegen nur wenige Studien vor, die entsprechende Transkripte direkt einbeziehen (z. B. Mullins 2009; Perrin 2016). Zudem konzentrieren sich die vorhandenen Studien meist ausschließlich auf eine recht enge Auswahl von Gerichtsprotokollen und verwenden entweder einen qualitativen oder einen quantitativen Ansatz (Brönnimann u.a. 2013; King und Meernik 2017). Um jedoch große Mengen von Textdaten aus Gerichtstranskripten zu verarbeiten und sie systematisch zu analysieren, kann eine Kombination aus qualitativen und quantitativen (einschließlich computergestützten) Methoden zu ganzheitlicheren Ergebnissen und damit zu einer vollständigeren Erfassung des Forschungsgegenstandes führen. 

            
Forschungsfrage

            
Ziel des Dissertationsprojekts ist es daher herauszufinden, wie Mixed Methods zur Analyse großer Mengen von Transkripten von Völkermord-Tribunalen beitragen können. Der Schwerpunkt liegt darauf, wie ZeugInnen als „key part of any trial“ (Extraordinary Chambers in the Courts of Cambodia 2019, 1) traumatische Erfahrungen vor Gericht beschreiben. Durch ihre Aussage stellen ZeugInnen nicht nur wichtiges Beweismaterial zur Verfügung, sie erzählen auch von ihrem persönlichen Schicksal und ihrer individuellen Überlebensgeschichte. Trotz der emotionalen Herausforderungen, die solche Schilderungen mit sich bringen können, berichten Aussagende jedoch auch von positiven Aspekten, wie Dankbarkeit gegenüber der juristischen Aufarbeitung oder einer positiven Bedeutung für den persönlichen Bewältigungsprozess (Henry 2009, 118; Strasser u.a. 2016, 161-165). 

            
Um sich der Rolle von ZeugInnen internationaler Völkermordtribunale möglichst vielseitig anzunähern, wird im Rahmen der Dissertation eine Bandbreite verschiedener Methoden des Natural Language Processing (NLP) angewendet. Der Einbezug qualitativer Methoden durch einen Mixed-Methods-Ansatz stellt weiterhin sicher, dass der Kontext der ZeugInnenenaussagen berücksichtigt wird (Creswell und Plano Clark 2007). 

            
Struktur und Inhalt

            
Als kumulatives Projekt gliedert sich die Dissertation in mehrere Einzelarbeiten, die sich mit unterschiedlichen Aspekten von ZeugInnenaussagen befassen und verschiedene methodische Ansätze verfolgen. Zunächst wurde in einem bereits erschienenen Paper als wichtige Grundlage das 
                
Genocide Transcript Corpus
 (GTC) erstellt, das Textdaten der drei größten ad-hoc Völkermordtribunale (Rote-Khmer-Tribunal, Internationaler Strafgerichtshof für das ehemalige Jugoslawien, Internationaler Strafgerichtshof für Ruanda) umfasst (Schirmer, Kruschwitz, Donabauer 2022). Ein zweites Paper (aktuell im Reviewverfahren) gibt in einem dreistufigen Mixed-Methods-Design mit NLP-Klassifikation, Sentiment-Analysen und qualitativer Inhaltsanalyse einen Überblick über verschiedene Methoden zur Analyse traumatischer Inhalte in ZeugInnenaussagen. Auf diesen beiden Papern aufbauend befasst sich der dritte Teil der Dissertation mit dem Training eines NLP-Algorithmus, der Textabschnitte mit Berichten traumatischer Erfahrungen automatisiert erkennt. Mit Hilfe eines qualitativen Ansatzes soll zudem herausgefunden werden, wie sich diese Passagen von anderen ZeugInnenaussagen unterscheiden. Im vierten Teil der Arbeit wird Topic Modeling (Blei, Ng, Jordan 2003) angewendet, um herauszufinden, welche thematischen Muster in einzelnen ZeugInnenaussagen zu finden sind. Dabei wird kritisch auf die Interpretierbarkeit von Topic Models eingegangen und es werden Möglichkeiten aufgezeigt, wie durch auf Topic Modeling aufbauende statistische Analysen neue Erkenntnisse gewonnen werden können. Die gefundenen Topics werden qualitativ überprüft. In einem abschließenden Paper werden die Ergebnisse zusammengeführt und auf verschiedene Völkermordtribunale angewandt, um Unterschiede zu berücksichtigen und durch einen breiteren Blickwinkel allgemeinere Schlussfolgerungen zu ermöglichen. 
            

            
Die skizzierten Paper zeigen die Bandbreite des Anwendungsbereichs von Mixed Methods in der Völkermordforschung auf und legen den Grundstein für zukünftige Studien dieser Art. Indem traditionelle Methoden der historischen Quellenanalyse und automatisierte, computergestützte Prozesse aus dem NLP-Bereich kombiniert werden, sollen digitale Ansätze der Geschichts- und Völkermordforschung aufgezeigt und weitere Studien in diesem Bereich ermutigt werden. 

            
Relevanz

            
Die Relevanz dieses Projekts wird anhand von drei Aspekten deutlich: Einerseits stellt die Dissertation mit ihrem Mixed-Methods-Ansatz auf methodischer Ebene einen komplett neuen Zugang in der Genozidforschung dar, der es ermöglicht, ZeugInnenaussagen systematisch und in ihrer Fülle zu analysieren. Dabei machen es NLP-Methoden erstmals möglich, Muster in Aussagen zu entdecken, die aufgrund der Fülle des Materials sonst nicht sichtbar geworden wären. Zweitens wird dieses Projekt dazu beitragen, Gerichtsprotokolle als verlässliche Informationsquelle in der Genozidforschung weiter zu etablieren, für das Feld der 
                
Digital History
 zu öffnen und damit diesen Forschungsbereich weiterzuentwickeln. Schließlich hat die Dissertation auch gesellschaftliche Relevanz, indem die Ergebnisse der durchgeführten Studien die Situation von ZeugInnen vor Gericht umfassend beleuchten und insbesondere psychologische Herausforderungen thematisieren. Die Ergebnisse werden verschiedenen NGOs zur Verfügung gestellt, wobei gemeinsam erarbeitet werden soll, wie die Erkenntnisse in die Öffentlichkeits- und Bildungsarbeit mit einbezogen werden können (Kooperation mit dem 
                
Auschwitz Institute for the Prevention of Genocide and Mass Atrocities
 und 
                
Genocide Alert e.V.
).
            

            
Die Finalisierung der Dissertation wird im Frühjahr 2024 angestrebt.

        

            
Der Vortrag gibt einen aktuellen Überblick zur Modellierung und Implementierung der 
                
Digital Heraldry Ontology
. Diese entsteht im Rahmen der Entwicklung eines Knowledge Graphen zur Erfassung, Analyse und historischen Kontextualisierung von heraldischen Darstellungen des Mittelalters und der Frühen Neuzeit unter Berücksichtigung divergierender Beschreibungen und Interpretationsperspektiven.
            

            

                
Knowledge Graphen in der historischen Forschung

                
Historische Daten unterliegen einer hohen Komplexität. Sie sind in der Regel lückenhaft, ambivalent, multiperspektivisch, vage, z.T. widersprüchlich sowie stark an eine konkrete Zeit und einen konkreten Kontext gebunden. Gleichzeitig gibt es ein wachsendes Bewusstsein dafür, Forschungsdaten nachnutzbar und interoperabel zur Verfügung zu stellen (Cremer 2021). Eine zunehmend genutzte Technologie, um mit diesen Herausforderungen umzugehen, sind Knowledge Graphen auf Grundlage von Semantic Web Technologien. Hierbei werden Daten in Form von Graphdatenbanken zur Verfügung gestellt, die durch Ontologien im Sinne einer formalisierten 
                    
shared conceptualisation
 innerhalb einer Fachcommunity strukturiert (Studer 1998) und nach den Prinzipien von Linked Open Data interoperabel und flexibel veröffentlicht werden. Durch diese Interoperabilität kann die Vernetztheit und Interdependenz von historischem Wissen berücksichtigt werden.
                

                
Dabei ist ein großer Vorteil von Knowledge Graphen, dass sie historische Daten in einer Weise abbilden, die sowohl menschen- als auch maschinenlesbar ist. Sie werden daher als 
                    
symbolische
 KI verstanden, die, im Gegensatz zu Formen der 
                    
sub-symbolischen
 KI, wie maschinellem Lernen, eine explizite und von Domänenexpert:innen modellierte Repräsentation von Wissen darstellt (Sack 2021). Dadurch lassen sich die allgemeinen Herausforderungen historischer Daten leichter adressieren.
                

                
Knowledge Graphen besitzen in vielerlei Hinsicht für die historischen Wissenschaften innovatives Potenzial, das aktuell noch nicht völlig ausgeschöpft ist: Das betrifft die epistemologischen Anforderungen an die Modellierung historischer Daten mit Ontologien (Pierazzo 2019), die Möglichkeit der Kombinierbarkeit und gemeinsamen Auswertbarkeit interoperabler verschiedener Datensätze durch Linked Data (Vogeler 2020) sowie den Einsatz von 
                    
reasoning engines
, die durch logische Inferenzen neues Wissen aus bereits vorhandenen Daten ableiten können (Hogan 2021; Ehrlinger 2016).
                

                
Die Bedeutung von Knowledge Graphen ist daher in den Digital Humanities in den letzten Jahren stark angewachsen und wird in Zukunft wohl noch weiter zunehmen. So stellen etwa große Infrastrukturprojekte, wie NFDI4Memory,
 NFDI4Culture,
 Virtual Record Treasury of Ireland
 oder Biblissima,
 diese Technologie in den Mittelpunkt. Aber auch auf spezifische Themengebiete fokussierte Forschungsprojekte nutzen Knowledge Graphen zur Strukturierung und Bereitstellung ihrer Daten sowie zur Erkenntnisgewinnung.

                

            

            

                
Die Bedeutung von Wappen für die kulturhistorische Forschung

                
Warum ist diese Technologie für die Erforschung historischer Heraldik des Mittelalters und der Frühen Neuzeit erforderlich? In diesen Epochen waren Wappen der am häufigsten gebrauchte und in Europa meist verbreitete Träger visueller Kommunikation (Hiltmann 2019; Hablot 2017). Verwendet wurden sie von fast allen sozialen Schichten und Gruppen. Wappen dienten dabei neben der Identifikation ihrer Besitzer:innen vor allem verschiedensten komplexen Kommunikationsakten, wie der Markierung von Besitz, Herrschaft, Jurisdiktion, Gruppenzugehörigkeit, Verwandtschaft, oder Ehre (Paravicini 1998). Damit erlaubt ihre Analyse wichtige Einblicke in vormoderne Gesellschaftsstrukturen und Kultur (Hofman 2021) – sie sind also als eigene geschichtswissenschaftliche (visuelle) Quelle zu betrachten.

                
Das betrifft auch den Aspekt der historischen Quellenkritik. Wappen wurden auf den unterschiedlichsten Materialien angebracht – von Handschriften über Urkunden oder Siegel und Münzen bis hin zu Wandmalereien, Teppichen und Möbeln – um nur einige Beispiele zu nennen (Biewer 2003). Dabei konnte dasselbe Wappen – abhängig von diesen konkreten materiellen Kontexten aber auch vom jeweiligen historischen Verwendungskontext – an unterschiedlichen Stellen eine unterschiedliche Bedeutung tragen. Diesen Kontexten muss daher bei der Beschreibung und geschichtswissenschaftlichen Analyse von Wappen Rechnung getragen werden.

                
Trotz ihrer historischen Bedeutung haben heraldische Quellen in den Geschichtswissenschaften nur eine überschaubare Berücksichtigung genossen. Zurückzuführen ist das nicht zuletzt auf den Umfang ihrer Überlieferung, ihre Komplexität sowie die beschriebene Ambiguität (Hiltmann 2019). Auch die Menge an überlieferten Wappen ist eine Herausforderung. Dies betrifft zum einen die Menge an Wappen auf unterschiedlichen Objekten, zum anderen die Menge verschiedener Wappenbilder, die überhaupt (z.T. auf mehreren Objekten) verwendet wurden. Hier gibt es Schätzungen von ca. einer Million verschiedener Wappenbilder die allein im Mittelalter im Umlauf waren (Pastoureau 2018: 42). Um einen leichteren Zugang zu dieser Form historischer Quellen zu ermöglichen, wird im Rahmen des Projekts 
                    
Die Performanz der Wappen
 an der Humboldt-Universität zu Berlin ein Knowledge Graph entwickelt, der einerseits Wappenbeschreibungen zur Verfügung stellt sowie andererseits Aufschluss darüber gibt, wo, wann und von wem diese Wappen verwendet wurden – ebenso, welche Bedeutung diese Wappen in den jeweiligen Verwendungskontexten vermutlich getragen haben. Damit soll der Knowledge Graph einerseits als datenhaltende Infrastruktur genutzt werden können, die diese Art historischer Quellen für die historische Forschung erstmals umfangreich und nachhaltig verfügbar macht. Vor allem aber soll der Knowledge Graph zum anderen dafür genutzt werden, konkrete geschichtswissenschaftliche Forschungsfragen beantwortbar zu machen. Innerhalb des Projekts betrifft das insbesondere Fragen zur Entwicklung und Diversifizierung heraldischer Kommunikation über die Zeit (Hiltmann 2019).
                

            

            

                
Methodologie zur Entwicklung einer Ontologie für die historische Forschung

                
Diese beiden Anforderungen – die Bereitstellung einer flexiblen und nachhaltigen Infrastruktur sowie die Berücksichtigung von Forschungsfragen – mussten bei der Entwicklung der Ontologie und der gewählten Methodologie beachtet werden. Sie orientierte sich an aktuellen Best Practices des 
                    
Knowledge Engineering
.
                

                
Diese umfassen erstens insbesondere ein durch Fragen getriebenes Design. Die grundlegenden Konzepte einer Ontologie werden hierbei auf Grundlage von sogenannten 
                    
competency questions
 entwickelt (Shimizu 2020). Hierbei handelt es sich um natürlichsprachliche Forschungsfragen, die mit einer fertigen Ontologie beantwortbar sein müssen. Competency questions geben den inhaltlichen Rahmen vor, was zu modellieren ist. Der vorgestellte Knowledge Graph soll etwa nach verschiedenen Aspekten durchsuchbar sein, die miteinander kombinierbar sein sollen. Gesucht werden soll sowohl nach einzelnen Wappenbeschreibungen als auch nach Verwendungskontexten bestimmter Wappen, ebenso danach welche Wappen auf einem bestimmten Objekt abgebildet sind, sowie welche(s) Wappen zu einer bestimmten Zeit von einer bestimmten Person verwendet wurde(n). Ein hieraus abgeleitetes Beispiel für eine 
                    
competency question
 wäre somit z.B.:
                

                
Auf welchen Wandmalereien und in welchen Buchhandschriften, die sich alle auf einen Zeitraum zwischen 1400 und 1500 datieren lassen, findet sich eine identische Sequenz an Wappenbildern, die in der gleichen Reihenfolge auftreten, und welchen Entitäten sind die einzelnen Wappen in den jeweiligen Kontexten zugeordnet?

                
Diese Vielfalt unterschiedlicher zu modellierender und ineinandergreifender Aspekte verweist zugleich auf das zweite Kernthema des modernen 
                    
Knowledge Engineering
 mit Ontologien: Wichtig ist auch eine möglichst starke Modularisierung der Architektur eines Knowledge Graphen. Dies erfolgt in der Regel über sogenannte 
                    
Ontology Design Patterns
. Hierbei handelt es sich um kleinere Teilontologien, die für sich genommen einen bestimmten Sachverhalt abbilden (etwa die Modellierung von menschlichen Personen). Dies ermöglicht es, Teilprobleme getrennt zu modellieren und dadurch eine flexiblere Nachnutzbarkeit und Anschlussfähigkeit der 
                    
Ontology Design Patterns
 in anderen Kontexten zu gewährleisten (Shimizu 2020).
                

                
Aus geschichtswissenschaftlicher Sicht sind weitere zusätzliche Anforderungen zu erfüllen, die als Grundprinzipien bei der Ontologieentwicklung gedient haben. Hierzu zählt insbesondere eine möglichst große Nähe zu den modellierten Quellen, wodurch Erkenntnisse der historischen Grundwissenschaften in Datenmodelle übersetzt werden. Als zentral wurde dabei auch die Trennung zwischen Beschreibung und Interpretation einer Quelle im Datenmodell erachtet (Beretta 2021; Theissen-Lipp 2020). Historische Quellenkritik muss also inhärenter Bestandteil eines Modellierungsprozesses sein. Aus der Sicht des 
                    
Knowledge Engineering
 bedeutet dies, dass die Umsetzung theoretischer epistemologischer Überlegungen zur Repräsentation historischer Daten in konkrete Ontologien (Beretta 2021; Eide 2019) eine Forschungsleistung des Projekts darstellt.
                

            

            

                
Semantic Web Technologien zur Beschreibung und Kontextualisierung visueller Quellen

                
Im Vortrag wird ein Überblick zum aktuellen Entwicklungsstand der für diesen Knowledge Graph erforderlichen Ontologie gegeben. Um die unterschiedlichen Aspekte heraldischer Kommunikation zu erfassen sind insgesamt fünf Teilontologien mit jeweils eigenen Funktionen erforderlich:



                    
heraldry: System zur Beschreibung von Wappen

                    
blazon: Bereitstellung von Wappen als konzeptuellen Bilder

                    
representations: Beschreibung konkreter materieller Repräsentationen dieser konzeptuellen Bilder

                    
objects: Objekte zu denen diese Repräsentationen gehören

                    
entities: Personen, Gruppen oder andere Entitäten, die durch Wappen repräsentiert werden können

                

                
Die erste Ontologie (
                    
heraldry
) dient der Beschreibung von Wappen, basierend auf der Praxis und Terminologie des Blasonierens (Hiltmann 2022; Hiltmann 2020). Entscheidend ist dabei, dass es sich bei Wappen trotz ihrer Visualität nicht um Quellen handelt, die sich ausschließlich als Bilder darstellen lassen – vielmehr sind Wappen 
                    
konzeptuelle
, 
                    
strukturelle 
Bilder (Pastoureau 1979). Konkret bedeutet dies, dass jedes Wappen eine Kombination aus bestimmten, in unterschiedlichen Wappen immer wieder verwendeten Figuren, Farben und geometrischen Mustern darstellt. Sowohl diese visuellen “Bausteine” als auch die Art wie sie auf einem Wappen angeordnet sind, werden durch eine halbwegs formale Terminologie und Fachsprache erfasst (
                    
blason
 bzw. 
                    
blazon
) (vgl. Abbildung 1). Diese heraldischen Konzepte werden in der Ontologie formalisiert und erlauben so eine sowohl von Menschen als auch von Maschinen lesbare, verstehbare und analysierbare Beschreibung von Wappen. Auf diese Weise können beliebige heraldische Kombinationen erstellt werden, die dann als Linked Data eindeutig referenziert werden können. Grundlage für die Klassen und Properties dieser Teilontologie waren daher existierende heraldische Konzepte.
                

                

                    

                    
Abbildung 1: Aufteilung heraldischer Begriffe

                

                
Eine zweite Ontologie (
                    
blazon
) speichert die unterschiedlichen Beschreibungen von Wappen, unabhängig von ihrem jeweiligen konkreten Verwendungskontext. Dadurch wird jede erdenkliche Wappenbeschreibung mit einer URI eindeutig referenzierbar.
                

                
Eine weitere Ontologie (
                    
representation
) modelliert das eindeutige Auftreten einer Wappenbeschreibung in einem konkreten materiellen Kontext auf einem bestimmten Objekt. Dieses kann z.B. eine Handschrift, eine Wandmalerei, oder ein Siegel sein. Auf diese Weise lässt sich disambiguieren in welchem konkreten Kontext ein bestimmtes Wappen verwendet worden ist.
                

                
Diese materiellen Kontexte werden in einer weiteren Ontologie (
                    
objects
) repräsentiert. Hier erfolgten ihre quellenkritische Beschreibung und historische Kontextualisierung mit objektspezifischen Metadaten. Dazu zählt beispielsweise die Datierung des jeweiligen Objekts, aber auch sein Verwendungszeitraum und -kontext, mögliche Auftraggeber:innen, oder der heutige Aufbewahrungsort. Einige dieser Objektmetadaten unterscheiden sich dabei bei verschiedenen Quellentypen – so erfordert die Beschreibung eines Siegels etwa andere Metadaten als die Beschreibung einer Wandmalerei.
                

                
Eine letzte Ontologie (
                    
entities
) erfasst schließlich die verschiedenen Entitäten, d.h. Familien, Personen, Institutionen oder auch abstrakte Konzepte, denen in einem bestimmten Kontext eine bestimmte Wappenbeschreibung zugeordnet werden kann. Hierdurch wird also abgebildet, was durch ein Wappen kommuniziert werden kann. Die Auslagerung dieser Aspekte in eine eigene Teilontologie ist dabei notwendig, um modularisierte Klassenstrukturen zu den wappentragenden Entitäten repräsentieren zu können. So lassen sich hier etwa Verwandtschaftsverhältnisse oder territoriale Bezüge abbilden, wodurch flexiblere Abfragen an den Knowledge Graphen möglich werden.
                

                

                    
Objects 
und 
                    
entities
 beschreiben somit materielle Objekte und wappentragende Entitäten in einem spezifisch heraldischen Kontext. Gleichzeitig ist jedoch auch für diese Teilontologien eine Verknüpfung zu bereits bestehenden (Norm-)Datensätzen im Sinne von Linked Open Data geplant. Über bereits etablierte top-level-Ontologien wie LIDO oder CIDOC CRM können so interoperable Verknüpfungen hergestellt und existierende Metadaten zu wappentragenden 
                    
objects
 und 
                    
entities
 nachgenutzt werden.
                

                
Mit dem beschriebenen Ansatz können heraldische Darstellungen auf unterschiedlichen Quellen und die dazugehörigen Informationen mit anderen Quellenkorpora kombiniert und gemeinsam analysiert werden.

            

            

                
Multiperspektivität von Beschreibungen und Interpretationen

                
Darüber hinaus wird jede Form einer Zuordnung event-basiert modelliert – sei es die Zuordnung einer Wappenbeschreibung zu einem konkreten Wappenbild, das auf einem historischen Objekt abgebildet ist oder die Zuweisung eines Wappenbildes in einem bestimmten historischen Kontext zu einer Person oder anderen Entität. Das bedeutet, dass auch mehrere widersprüchliche Zuweisungen innerhalb desselben Kontextes existieren können. Hintergrund ist, dass solche Zuweisungen als historiographische Interpretation verstanden werden müssen, die zunächst als gleichwertig zueinander zu betrachten sind. Eine Zuweisung eines Wappens zu einem Wappenträger ist keinesfalls immer eindeutig und auch die Beschreibung eines Wappenbildes lässt sich auf Grund materieller Fragmentarität historischer Quellen nicht immer eindeutig durchführen. Die Entität, die für diese Zuweisung verantwortlich ist, kann dabei etwa eine moderne Forscher:in sein, aber auch die Umschrift eines Siegels oder der Text in einem mittelalterlichen Manuskript.

                
Abbildung 2 zeigt einen vereinfachten Überblick zur Funktionsweise dieser event-basierten multiperspektivischen Beschreibung eines Wappenbildes in einer Handschrift. Diese konkrete Abbildung in einer Quelle wird durch eine Entität im Knowledge Graphen repräsentiert (durch die URI bei (a) in Abbildung 2). Für dieses Wappenbild existieren zwei divergierende Beschreibungen. Diese beiden unterschiedlichen Wappenbeschreibungen werden jeweils durch ein Beschreibungs-Event (b) mit der Abbildung des Wappens in der Handschrift im Knowledge Graphen verbunden. Die Beschreibungs-Events können dabei durch weitere Metadaten, wie die Entität, die die jeweilige Beschreibung dem Wappenbild in der Quelle zugewiesen hat, angereichert werden. Die eigentlichen Wappenbeschreibungen werden ebenfalls jeweils durch eine eigene Entität repräsentiert (c). Konkret wird hier die Blume im abgebildeten Wappen einmal als „cinquefoil“ und einmal als „rose“ beschrieben. 

                
Den üblichen Herausforderungen bei der Modellierung historischer Daten wie Ambiguität, Vagheit, Unvollständigkeit wird durch diese Repräsentation von Multiperspektivität in der Modellierung der Ontologie(n) Rechnung getragen.

                

                    

                    
Abbildung 2: Modellierung von Multiperspektivität bei der Beschreibung von Wappen

                

            

        

            

                
Das Spiel

                
 ist mehr als ein selbstgenügsamer Zeitvertreib für Kinder und Kind Gebliebene (Huizinga 1956). Längst, und substanziell beeinflusst durch die Genese neuerer, digitaler Spielformen, etabliert es sich als Tool, um Wissen nahezu 

                
en passant

                
 zu gewinnen: So begreifen 

                
Games with a Purpose

                
 (

                
GWAPs

                
; von Ahn und Dabbish 2004) ihre Spieler:innen als unersetzliches, online Wissen generierendes Kollektivum (

                
Crowd

                
), das sich mäßig begeisternden Aufgaben ohne materielle Entlohnung widmet. Die offensichtliche Trivialität jener 

                
Microtasks 

                
wird mit spielähnlichen Mechanismen kaschiert (Deterding et al. 2011, Morschheuser et al. 2017) und durch pädagogische Elemente angereichert (Suttie et al. 2012). Im Folgenden fokussieren wir mit dem originär durch von Ahn und Dabbish (2004) konzipierten 

                
Extra Sensory Perception (ESP) Game

                
 eine wiederholt rezipierte GWAP-Unterklasse, die 

                
Image-Labeling

                
-Prozesse anstößt. Einer der bekanntesten, für die 

                
Digital Humanities

                
 relevanten Ableger ist die seit 2010 an der Ludwig-Maximilians-Universität München entwickelte Plattform ARTigo.

                
 In ihr wird das ESP-Game zuvorderst auf kunsthistorische Bilddatenbestände appliziert.

            

            

                
Spielprinzip

                

                    
Das Spiel fußt auf dem sog. 

                    
Output-Agreement-

                    
Prinzip: Zwei anonym bleibenden Spieler:innen wird über eine 

                    
Graphical User Interface

                    
 (

                    
GUI

                    
) zeitgleich dasselbe Bild präsentiert. Ziel ist es, dieses Bild innerhalb einer festgelegten Zeitspanne mit Schlagwörtern (

                    
Tags

                    
) derart zu annotieren, dass möglichst viele Punkte durch übereinstimmende Angaben (

                    
Matches

                    
) erzielt werden. Die Schlagwörter sind frei wählbar und nicht kontrolliert wie in einem Thesaurus, können je nach Spielmodus aber durch 

                    
Tabuwörter

                    
 eingeschränkt werden. Tabuwörter stellen bspw. häufig vergebene Schlagwörter dar, denen kein zusätzlicher Informationsgewinn attestiert wird; bei Franz Marcs 

                    
Die großen blauen Pferde

                    
 (1911) etwa die Begriffe „Landschaft“ und „Pferde“. Die Spieler:innen müssen nicht simultan operieren. Stattdessen können alte Spielrunden synthetisiert und die für ein Bild dort vergebenen Schlagwörter recycelt werden.

                

                

                    
Bekanntlich erzeugen Output-Agreement-Spiele aufgrund jener Matching-Prozedur vor allem oberflächlich beschreibende 

                    
Surface Tags

                    
 (Bry und Wieser 2012, Bry et al. 2018); in ähnlicher Qualität mittlerweile ebenso von immer leistungsfähigeren 

                    
Neuronalen Netzwerken 

                    
produziert (Heinisch 2021, Milani und Fraternali 2021, Zhao et al. 2021). Bspw. durch Tabuwörter implementierte

                    
Scripting

                    
-Mechanismen, die zur Eingabe semantisch ,tiefergehender‘ Schlagwörter auffordern, begünstigen die sukzessive Diversifikation eines Tag-Bestands zwar (von Ahn und Dabbish 2004, Wieser et al. 2013). Allerdings beobachten wir zwei Defizite:

                



                    

                        
Scripting wird oft nicht in Form modularer 

                        
Add-ons

                        
 implementiert. An ihre Stelle treten vermeintlich autonome Spiele, die in sog. „Ökosystemen“ komplementär wirken sollen (Wieser et al. 2013, Bry et al. 2018). Disparate, inflexible Codebasen – wie in ARTigo – sind die Folge.

                    

                    

                        
92,26 % der Nutzer:innen von ARTigo initialisieren ein Spiel mit Tabuwörtern, ohne im Anschluss Tags zu vergeben.

                        
 Selbst aktiv Partizipierende verschlagworten jedoch durchschnittlich 4,29 Taggings weniger als in nicht-restringierten Spielrunden. Weder motiviert die GUI hinreichend noch adressiert sie potenziell am Spiel Interessierte und leitet sie klar, etwa mithilfe eines textuellen Dialogsystems, an (Scherz 2017, 214–229).

                    

                

                

                    
Im vorliegenden Beitrag schlagen wir einen Neuentwurf der ARTigo-Plattform vor, und zwar als plugin-basiertes Social-Tagging-Framework, konstituiert aus strikt funktionalen Programmbausteinen. In ihm wird Scripting mithilfe eines 

                    
proaktiven Dialogagenten

                    
 (Baudoin et al. 2005) über die GUI initiiert. Bei Vergabe eines Surface Tags (z. B. „Pferd“) empfiehlt der Agent initiativ bedeutsame Unterbegriffe („Zugpferd“, „Pony“) oder motiviert abhängig vom Spielmodus dazu, weniger deskriptiv zu annotieren (z. B. durch Fragen, etwa: „Welche Gefühle löst das Bild bei Dir aus?“). Je nach zugrunde liegendem Annotationsinteresse wird Domänenwissen gezielt provoziert, sodass bildwissenschaftlich hoch spezifische Aufgaben an die Crowd ausgelagert werden können. Dies befördert nicht nur die Akquise kunst- und kulturhistorischer Daten, sondern verortet auch das Spiel als propädeutisches Lehrmittel im Kontext der Bildwissenschaften, wie in zwei prototypischen Anwendungsszenarien deutlich werden soll.

                

            

            

                
Infrastruktur

                

                    
Konzipiert ist das Framework als dreistufige Architektur mit einer Daten-, Anwendungs- und Präsentationsschicht, die jeweils in Docker-Containern gekapselt sind, um eine Vielzahl von Installationsszenarien zu erlauben (Merkel 2014). Mit Grafana

                    
 überwachen wir die Auslastung und Interaktionsfrequenz der einzelnen Instanzen. Eine übergeordnete Konfigurationsdatei speist die Anwendungs- und die Präsentationsschicht. Der Quellcode ist frei auf GitHub unter einer 

                    
GNU General Public License

                    
 veröffentlicht.

                    
 Alle Komponenten sind 

                    
Open Source

                    
.

                

                

                    
Datenschicht

                    

                        
Primär besteht die Datenschicht aus einem PostgreSQL-Datenbankmanagementsystem.

                        
 Damit auch ressourcenintensive Suchanfragen effizient bearbeitet werden, ist OpenSearch implementiert, Amazons seit 2021 entwickelter Elasticsearch-

                        
Fork

                        
.

                    

                

                

                    
Anwendungsschicht

                    

                        
Auf die Datenschicht greift die Anwendungsschicht nur zu, wenn spielrelevante Informationen erfasst oder sie der Präsentationsschicht über eine REST-API zur Verfügung gestellt werden. Die offen zugängliche Schnittstelle ist mit Swagger UI

                        
 und ReDoc

                        
 an zwei interaktive Dokumentationsumgebungen nach OpenAPI-3.0-Spezifikation

                        
 gekoppelt, sodass externe Plattformen gleichermaßen in der Lage sind, valide Abfragen zu formulieren.

                    

                    

                        
Kern der in Python mit Django

                        
 realisierten Anwendungsschicht ist der sieben Modulgruppen umfassende Spielkonfigurationsmechanismus. Jede Modulgruppe hat genau eine Funktion mit strikt festgelegtem 

                        
Output

                        
:

                    



                        

                            
Ressourcenmodule 

                            
selektieren die zu annotierenden Bilder. Als Kriterien dienen etwa die Anzahl der bereits zugewiesenen Tags oder wann ein Bild zuletzt bespielt wurde. Der mit Memcached

                            
 integrierte Cache-Server sorgt für eine ressourcenschonende Auswahl.

                        

                        

                            
(optional) 

                            
Durch 

                            
Opponentmodule 

                            
werden Akteure generiert, die ihre Tags automatisch gemäß der bis dato für ein Bild hinterlegten Schlagwörter wählen. Ihre Potenz ist konfigurierbar.

                        

                        

                            
(optional) Tabumodule 

                            
limitieren vorsätzlich den möglichen Tagraum. Sie verhindern Annotationen ohne zusätzlichen Informationsgewinn – wie am häufigsten für ein Bild vergebene Tags. Manuell definierte Restriktionen sind möglich („Impressionismus“ darf z. B. nicht für eine Reihe bekannt impressionistischer Bilder verwendet werden).

                        

                        

                            
(optional) 

                            
Programmatisch ähneln 

                            
Input-

                            
 den zuvor eingeführten Tabumodulen. Jedoch restringieren sie nicht, sondern leiten je nach Spieltypus an, bspw. indem sie auf zu validierende Tags hinweisen.

                        

                        

                            
(optional) 

                            
Mit 

                            
Vorschlagsmodulen 

                            
werden über Scripting feingranularere Termini als die bislang eingegebenen motiviert. Dazu binden wir nachweislich variationsreiche Glossare der englisch- und deutschsprachigen Wiktionary ein (Meyer und Gurevych 2010).

                            
 Ebenso zurückgegeben werden signifikant in ähnlichen Kontexten verortete Tags, sog. 

                            
Kookkurrenzen

                            
.

                        

                        

                            
Die jeweils hinterlegten Tags evaluieren 

                            
Filtermodule 

                            
auf ihre prinzipielle Validität. Nicht zugelassen können etwa orthografisch inkorrekte, als Tabuwörter definierte oder in derselben Spielrunde bereits annotierte Tags sein.

                        

                        

                            
(optional) Bewertungsmodule 

                            
messen die Güte der validen Schlagwörter. Mit Punkten belohnt werden u. a. gematchte Tags oder solche, die auf Basis der Wiktionary explizit domänenrelevante Information tragen – bei kunsthistorischen Bildern zur Komposition, Ikonografie und Epoche. Auch durch veränderte Bewertungsmodi wird Scripting herbeigeführt (Jain und Parkes 2013).

                        

                    

                    
Um individuellere Nutzungsszenarien zu berücksichtigen, kann das Framework erweitert werden, indem der jeweiligen Modulgruppe ein geeignetes Untermodul hinzugefügt und die übergeordnete Konfigurationsdatei aktualisiert wird.

                

                

                    
Präsentationsschicht

                    

                        
Die Präsentationsschicht ist in HTML und JavaScript mit dem JavaScript-Framework Vue.js

                        
 programmiert. Alle dynamischen Webseiteninhalte werden mithilfe der JavaScript-Bibliothek axios

                        
 über die REST-API geladen. Die Anwendung ist responsiv und auch für mobile Endgeräte optimiert.

                    

                    

                        

                        

                        
Abb. 1: Dem Spiel vorgeschaltet ist eine mit Typed.js animierte Homepage, die zugleich als Tutorial fungiert.

                    

                    

                    
                    

                        

                        

                        
Abb. 2: Ein vertikaler 

                        
Stepper

                        
 assistiert den aus Fragen bestehenden Konfigurationsprozess.

                    

                    

                    

                        
Dem Spiel vorgeschaltet ist eine mit Typed.js

                        
 animierte Homepage, die zugleich als Tutorial fungiert (Abb. 1, 

                        
rechts

                        
). Zeitlich begrenzte 

                        
Quests

                        
, die bspw. zum Tagging eines Künstlers reizen, sind in einem 

                        
Navigation Drawer

                        
links gestellt (Abb. 1, 

                        
links

                        
); sie werden zumeist randomisiert aus validen Modulkombinationen generiert. Ein vertikaler 

                        
Stepper

                        
assistiert den folgenden, aus Fragen bestehenden Konfigurationsprozess (z. B. „Wie lange dauert eine Spielrunde?“ oder „Wie viele Runden möchtest Du spielen?“; Abb. 2). Er speist sich aus derselben Konfigurationsdatei wie die Anwendungsschicht. Alle Felder sind optional und mit Standardwerten vorbelegt, um in wenigen Klicks neue Spiele zu erstellen.

                    

                    

                        

                        

                        
Abb. 3: Dominiert wird die Spiel-GUI von zwei Komponenten: dem zu annotierenden Bild, 

                        
links

                        
, und der von 

                        
Instant-Messaging

                        
-Diensten inspirierten Eingabemaske, 

                        
rechts

                        
.

                    

                    
 
                    

                        

                        

                        
Abb. 4: Jede Spielsitzung endet mit einer nochmaligen Gegenüberstellung der Ergebnisse in aggregierter Form.

                    

                    

                    

                        
Dominiert wird die in Abb. 3 präsentierte Spiel-GUI von zwei Komponenten: dem zu annotierenden Bild, 

                        
links

                        
, und der von 

                        
Instant-Messaging

                        
-Diensten – etwa Metas Facebook Messenger

                        
 – inspirierten Eingabemaske, 

                        
rechts

                        
. Einen Chat(-bot) simulierend erscheinen dort sukzessive die Tags des zugeschalteten Akteurs; nicht-gematchte Eingaben sind zunächst unscharf. Damit jener trotz der synthetischen Anlage menschenähnlich – als tatsächlich Spielender – empfunden wird (Jenkins et al. 2007), verzögern wir seine Antworten minimal; derweil zeigt ein 

                        
Typing Awareness Indicator

                        
 den Eingabeprozess an (Auerbach 2014). Die GUI festigt so den 

                        
per se

                        
 dialogisch motivierten Aspekt der Verschlagwortung (Bredekamp 2010). Scripting ist u. a. durch Steuerelemente wie Kombinations- und Optionsfelder eingebettet, sodass die Anzahl der auf dem Bildschirm präsenten Elemente reduziert wird. Ein Steuerelement ist demnach ausschließlich für die Zeit der Interaktion sichtbar. Zur Fokussierung auf den Annotationsprozess sind Kopf- und Fußzeile halbtransparent gesetzt. Der über beiden Komponenten – Bild und Eingabemaske – liegende, sie einschließende Fortschrittsbalken zeigt die Restlaufzeit der momentan aktiven Spielrunde an. Nach Ablauf dieser erscheint zunächst ein dreisekündiger Countdown, währenddessen die Spielsitzung pausiert werden kann, bevor das System zum nächsten Bild weiterschaltet. Jede Spielsitzung endet mit einer nochmaligen Gegenüberstellung der Ergebnisse in aggregierter Form (Abb. 4).

                    

                    

                        
Um die Benutzerfreundlichkeit der GUI zu quantifizieren, beabsichtigen wir, Online-Fragebögen zu verschicken, die etablierte Metriken wie

                        
Perceived Ease of Use

                        
, 

                        
Perceived Usefulness

                        
 und 

                        
System Usability Scale

                        
erfassen.

                    

                

            

            

                
Use Cases

                

                    
Nicht nur die kollaborative, durch die Crowd initiierte Akquise kunst- und kulturhistorischer Daten kann mit dem hier eingeführten Framework sinnvoll befördert werden,

                    
 sondern auch das Spiel als pädagogisches Instrument, etwa im musealen Raum. Nachweislich sind in vielen Fällen Inhalte spielerisch effektiver zu vermitteln als mit grundständigen Lernmethoden (Scherz 2017, 230). Dieses Potenzial schöpft das 

                    
digitale 

                    
Spiel aus, auch weil es ein Medium verwendet, das integraler Bestandteil der Welt jüngerer Generationen ist; also derjenigen, auf die sich Bildungsbemühungen in erster Linie zu konzentrieren haben. Während das Spiel als propädeutisches Lehrmittel vermehrt in naturwissenschaftlich orientierten Domänen integriert wird,

                    
 beabsichtigt das Framework, es auch im bildwissenschaftlichen Kontext zu verorten – etwa zum Studium der Grundlagen kunsthistorischer Argumentation.

                

                
Aufgrund seiner hochgradigen Modularisierung, und der damit einhergehenden Flexibilisierung der Parameter, sehen wir zwei prototypische Anwendungsszenarien, die im Folgenden exemplarisch beschrieben werden:



                    

                        
Im Fokus des 

                        
Attributionsspiels 

                        
steht wortwörtlich die Attribution eines Bildes zu einer näher definierten Oberkategorie: einem geografischen Kontext, einer Epoche, dem oder der Kunstschaffenden selbst. Nicht zwingend entscheidend ist aber das exakte Matching. Stattdessen wird bereits eine regionale oder stilistische Annäherung an das interessierende Subjekt belohnt; etwa, wenn ein Kupferstich von Hans Baldung Grien fälschlicherweise Albrecht Dürer zugesprochen wurde. Der Schwierigkeitsgrad ließe sich erhöhen, indem das Bild als Explorationsfläche

                        
 mit einer ,Schablone‘ abgedeckt wird, die nur einen Teil des Bildes freigibt.

                        
 Durch sensorische Interaktion auf mobilen Geräten, etwa Berührungen oder Pusten, könnte der Ausschnitt bei gleichzeitigem Punktverlust modifiziert werden.

                    

                    

                        
Ziel des 

                        
Ikonografiespiels

                        
 ist die visuell motivierte Aneignung von historischen Sach- und Personenidentitäten. Jeweils vorgegeben ist eine Ikonografie, bspw. der heilige Hieronymus. Über die Eingabemaske werden die Ikonografie beschreibende Attribute verschlagwortet (z. B. „Löwe“, „Buch“, „Stundenglas“), während der textuelle Dialogagent sukzessive auf sie zutreffende Ikonografien empfiehlt. Durch die fortwährende Interaktion (Boiano et al. 2018) spüren die Partizipierenden der tatsächlich korrekten Ikonografie nach. Auf diese Weise vermittelt das Spiel zwar Fachwissen, schärft jedoch gleichermaßen implizit das kunsthistorische Sehen. Wie Scherz (2017, 226–227) nehmen wir an, dass u. a. die Dauer einer Spielrunde in ihrer Spezifik unterschiedliche Attributketten und daher Tagging-Muster begünstigt, die es weiter zu untersuchen gelte.

                    

                

                

                    
Immer treten die Spieler:innen in einen zweifachen pädagogischen Dialog: zum einen mit den jeweils angezeigten Bildern, zum anderen mit dem regelbasierten (Dialog-)System. Beide Spiele können dabei in kurzer Zeit verstanden und ihre Akzeptanz so wesentlich gesteigert werden (Kühn et al. 2009).

                

            

            

                
Zusammenfassung und Ausblick

                

                    
Mit diesem Beitrag schlagen wir einen ganzheitlichen Neuentwurf der seit 2010 existierenden Social-Tagging-Plattform ARTigo vor, die kunsthistorische Image-Labeling-Prozesse anstößt. In dem nun aus funktionalen Programmbausteinen konstituierten Framework wird Scripting – ein Mechanismus, der zur Eingabe tiefensemantischer Schlagwörter auffordert – mithilfe eines proaktiven, textuellen Dialogagenten motiviert. Wie in zwei prototypischen Anwendungsszenarien dargelegt, einem Attributions- und einem Ikonografiespiel, eignet sich das Framework für die Akquise kunst- und kulturhistorisch relevanter Daten ebenso wie als pädagogisches Instrument im Kontext der Bildwissenschaften.

                

                

                    
Wir planen, die produzierten Taggings künftig automatisch in Wikidata

                    
 zu spiegeln. Um die Integration neuer Bilddatenbestände zu vereinfachen, soll des Weiteren eine Oberfläche zum 

                    
Upload 

                    
eigener Sammlungen bereitgestellt werden. Zur präziseren Datenerhebung entwickeln wir derzeit ein adaptierbares Werkzeug, mit dem komplexe Studiendesigns ermöglicht werden: So erlaubt es die freie Parametrisierung der Spielmodule, eine große Anzahl von Studienelementen zu quantifizieren und ausgewählten Testgruppen zuzuspielen. Neben grundsätzlich breiteren Forschungsmöglichkeiten kann durch jenen Prozess auch das jeweilige Spieldesign weiter optimiert werden. In diesem Kontext evaluieren wir auch jede Modulgruppe und loten ihre Eignung für unterschiedliche Forschungsszenarien aus.

                

            

        

            
Das geisteswissenschaftliche Konsortium Text+ (
                
https://www.text-plus.org/
) hat im Oktober 2021 seine Arbeit innerhalb der Nationalen Forschungsdateninfrastrukur (NFDI) aufgenommen. Text+ widmet sich text- und sprachbasierten Daten aus den verschiedensten Disziplinen (u. a. Literaturwissenschaft, Sprachwissenschaft, Geschichtswissenschaft, Philosophie) in drei Datendomänen bzw. Task Areas: Lexikalische Ressourcen, Sammlungen und Editionen. Als Vertreter:innen der Task Area Editionen
 möchten wir mit einem Poster einen Einblick in unsere Arbeit geben, die von einem vielschichtigen Verständnis der ‚Offenheit‘ digitaler Editionen geleitet ist. 
            

            
Im Kontext digitaler Editionen wird ‚Offenheit‘ meist im Zusammenhang mit Rechtefragen, d. h. Lizenzen und speziell Open Access, diskutiert (Hannesschläger 2019; Sichani 2017; Dillen und Neyt 2016). Zentral für die Öffnung einer Edition nach außen ist darüber hinaus ihre technische Vernetzbarkeit; Editionen aggregieren einerseits Daten über Schnittstellen, und machen über diese auch ihre eigenen Daten abrufbar (Witt 2018, 255). Prinzipiell können digitale Editionen auch durch die Möglichkeit ihrer weiteren Bearbeitung, Anreicherung oder Erweiterung "offen" bleiben. Darüber hinaus gibt es in jüngster Zeit vermehrt Ansätze, ‚Offenheit‘ im Sinne von ‚Zugänglichkeit‘ aus ‚sozialer' Perspektive zu betrachten (Martinez et al. 2019; Rojas-Castro 2020). Eine Übersicht verschiedener Facetten von ‚Offenheit‘ schaffen Jeffrey Pomerantz und Robin Peek (2016), indem sie den Begriff ‚open‘ u. a. in Bezug zu den Themen Zugänglichkeit, Nutzung, Partizipation und Transparenz stellen, die – gemeinsam mit den bereits genannten Facetten von Offenheit – im Rahmen der Task Area Editionen die Zieldimensionen mit vorgeben. Zentrale Maßnahmen der Task Area Editionen zur Förderung der ‚Offenheit‘ digitaler Editionen umfassen u. a.: 

            

            
1. Empfehlungen für ‚FAIRe‘ Editionen

            
Ein Ziel von Text+ ist es, die Anwendung der FAIR-Prinzipien (Findable, Accessible, Interoperable, Reusable) (Wilkinson et al. 2016) zu fördern, weshalb die Task Area Leitlinien zur Erstellung und Publikation ‚FAIRer‘ Editionen erarbeitet. Da die Anwendung der FAIR-Prinzipien bisher im Editionskontext noch nicht tiefergehend diskutiert wurde, zugleich aber wichtige Bereiche wie Auffindbarkeit, Vernetzung, Lizenzierung und Nachnutzung betrifft, wurde eine Kooperation mit dem Rezensionsjournal RIDE des Instituts für Dokumentologie und Editorik (IDE 2014–2022) initiiert. Über einen Call for Reviews (
                
https://ride.i-d-e.de/reviewers/call-for-reviews/ride-textplus-de/
), der sich neben dem Kriterienkatalog des IDE (Sahle 2014) auch an im Rahmen von Text+ entwickelten FAIR-Kriterien (Gengnagel et al. 2022) orientiert, werden Rezensionen gesammelt, anhand derer evaluiert wird, inwieweit die FAIR-Prinzipien in ihrer aktuellen Formulierung auf digitale Editionen anwendbar sind und wie sie bisher umgesetzt werden. Die ersten Rezensionen aus der Kooperation zwischen Text+ und IDE erscheinen Anfang 2023.
            

            

            
2. Verzeichnis digitaler und gedruckter Editionen

            
Bei der sog. Registry handelt es sich um ein kuratiertes Verzeichnis von Editionen. Es soll einen strukturierten Zugriff auf die große Zahl an vorhandenen Ressourcen bieten und neben allgemeinen Zugängen nach bestimmten Kriterien (z.B. Sprachen des Edendums, Disziplinzugehörigkeit, Projektbeteiligte) auch erstmals die FAIRness digitaler Editionen berücksichtigen. Durch den holistischen Nachweis von Editionen unabhängig von ihrer Medienform, Verknüpfungen zu einer – ebenfalls im Kontext von Text+ entstehenden – Software-Registry sowie der Präsentation von Best-Practice-Beispielen für bestimmte Disziplinen, Genres oder Editionstypen (sog. model editions), geht die Editionen-Registry über bestehende Nachweissysteme (z.B. Franzini 2012–2022, Sahle 2020–2022 und früher) hinaus. Gleichzeitig beschränkt sie sich zunächst vornehmlich auf Editionen, die an Institutionen im deutschen Raum angesiedelt bzw. an denen deutsche Forschungsinstitutionen beteiligt sind, um die Auffindbarkeit und Sichtbarkeit dieser Projekte signifikant zu erhöhen und insbesondere den Zugriff auf deren Forschungsdaten zu befördern.

            

            
3. Maßnahmen zur Vernetzung von Editionen

            
Die Task Area 
                
Editionen erkundet Vernetzungspotenziale digitaler Editionen auf Basis von GND-Normdatenannotationen und evaluiert angewandte Praktiken, etwa die Vernetzung durch veröffentlichte BEACON-Dateien, entsprechende Schnittstellen und aggregierende (Fach-)Dienste (Lordick und Mache 2018).
 Diese Praxis stärkt einerseits die FAIRness von (digitalen) Editionen: Sie steigert die Auffindbarkeit durch vernetzte Recherchesysteme, ermöglicht verteilte Datenangebote durch semantische Interoperabilität und verbessert die Datenqualität durch ‚Eindeutigkeit‘. Sie wirft andererseits die Frage nach projektspezifischen Normdatenbedarfen und damit den Mitwirkungsmöglichkeiten an der GND auf: Die Task Area Editionen arbeitet deshalb mit der entstehenden „Text+ GND-Agentur“ zusammen (Kett et al. 2022).
            

            
Die drei genannten Maßnahmen stellen nur einen Ausschnitt der laufenden Arbeiten dar. Für einen möglichst inklusiven Diskurs rund um das Thema „editionsspezifische Forschungsdaten“ sind Austauschformate zur Einbindung der Community geplant – das Poster selbst (mit weiterführenden QR-Codes) gehört dazu. Grundsätzlich muss bei der Umsetzung der skizzierten Maßnahmen berücksichtigt werden, dass jede digitale Edition in einem hohen Maße individuell ist. Dies betrifft zum einen die Materialien der Edition, die Methoden der Erschließung sowie die Publikationswege und zum anderen die Rahmenbedingungen eines Editionsunternehmens (u. a. personelle, zeitliche und finanzielle Ressourcen, vorhandenes Know-How). Diese Individualität wird bei allen geplanten Maßnahmen in Text+ stets berücksichtigt, indem ‚Offenheit‘ als „Skala“ verstanden wird, „auf der Projekte, die offene Methoden anwenden wollen, den für sie jeweils angemessenen Platz finden müssen“ (Hannesschläger 2020, 143). 

        

            

                
Abstract

                

                    
Für die Korpuskonstituierung im Projekt DisKo (Diversitäts-Korpus) haben wir ein Konzept entwickelt, das sowohl auf Ansätzen aus den Digital als auch den Public Humanities aufbaut und diese zu einer Citizen-Humanities-Komponente zusammenfügt (vgl. Heinisch 2020). Dieses Konzept dient dazu, ein Erzähltextkorpus aufzubauen, in dem Gender nicht nur binär dargestellt wird. DisKo wird zur Grundlage eines Gender-Classifiers 2.0 (aufbauend auf dem Gender-Classifier 1.0 von Schumacher und Flüh 2021), den wir mithilfe von Verfahren des überwachten maschinellen Lernens so trainieren, dass diverse Gender-Zuschreibungen automatisch klassifiziert werden. Den Grundsätzen von Offenheit, Transparenz und Empowerment folgend, die für Citizen-Humanities-Projekte zentral sind (vgl. Heinisch 2020, Dunn und Hedges 2012: 19), werden Vertreter:innen unterschiedlicher Communities angesprochen und in die Korpuskonstituierung einbezogen. Dabei steht das Community-Building zwischen Offenheit und Zielgruppenspezifik.

                

            

            

                

                    
DisKo (Diversitäts-Korpus) 

                

                

                    
DisKo ist ein Kooperationsprojekt zwischen der Deutschen Nationalbibliothek (DNB), der Technischen Universität Darmstadt und der Universität Hamburg. Die DNB sammelt im gesetzlichen Auftrag seit 1913 u.a. alle in Deutschland veröffentlichten Medienwerke; seit 2006 schließt dies auch die sogenannten Netzpublikationen, also genuin digitale Werke, mit ein. Die Digitalisierungsstrategie zielt auf eine systematische und auch projekt- und anlassbezogene Digitalisierung der physischen Bestände sowie die Vernetzung zur Wissenschaft ab und schreibt seit 2020 mit dem jährlichen 

                    

                        
DH-Call

                    

                    
 ein Unterstützungsangebot für Forschende aus, die mit den Daten der DNB arbeiten möchten. Grundlage dieser Aktivität bildet die Reform des Urheberrechts-Wissensgesellschafts-Gesetzes (UrhWissG) im Jahr 2018. Die konkrete Förderung durch die DNB besteht in der Bereitstellung von Metadaten, digitalen bzw. digitalisierten Objekten und einer passenden Infrastruktur zur Bearbeitung und Analyse der teilweise sensiblen Daten. Das Projekt DisKo ist seit Frühjahr 2022 Teil dieser Förderlinie und kann darum nicht nur auf genuin digitale Medien zugreifen, sondern auch Texte retrodigitalisieren lassen, die bisher von keinem Digitalisierungsprojekt erfasst wurden. Darüber hinaus bietet die Zusammenarbeit mit der DNB die Möglichkeit, bei der Korpusbildung eine quantitative Einschätzung der Grundgesamtheit (Calvo Tello 2021: 96–97, Schöch 2017: 225) aller in einer definierten Zeitspanne in Deutschland erschienen Romane zu berücksichtigen, da jedes mit einer deutschen ISBN erscheinende Buch hier gemeldet und in doppelter Ausführung abgegeben oder als digitales Objekt eingereicht werden muss. 

                

                

                    
Im 

                    

                        
Projekt m*w 

                    

                    
werden seit 2019 Genderrollen und -stereotype erforscht. Der Gender-Classifier 1.0 erkennt und klassifiziert weibliche, männliche und neutrale Genderrollen durchschnittlich zu 78% (F1-Score) in Novellen, Romanen und Dramen (vgl. Flüh/Lemke/Schumacher 2022). Bei der Annotation des Trainingskorpus und Fallstudien mit diesem Classifier (vgl. Schumacher und Flüh 2020; Flüh und Schumacher 2021/2022; Flüh/Horstmann/Schumacher 2022) 

                    
zeigt sich, 

                    
dass Brüche mit stereotypen Genderzuschreibungen in älteren Texten selten, in zeitgenössischen aber häufiger vorkommen. Aus dem Desiderat, einen Classifier zu trainieren, der diverse, nicht nur binäre Genderrollen erkennen und klassifizieren kann, resultierte das Projekt DisKo: die Erschließung eines Trainingskorpus aus den Beständen der DNB aus zeitgenössischen Romanen mit nicht-binären Genderdarstellungen.

                

                

                    
Erfasst wird ein Zeitrahmen der letzten rund 70 Jahre; in diesem Fall beinhaltet die Grundgesamtheit also alle in Deutschland zwischen 1950 und 2022 erschienen belletristischen Werke. Übertragen auf den Gesamtbestand der DNB bedeutet das, dass prinzipiell ca. 450.000 physische Objekte und ca. 435.000 digitale Objekte mit dem Erschließungsmerkmal “Belletristik” in Frage kommen. Die Überschneidungsmenge der Bestände ist leider nicht bekannt, kann jedoch über Algorithmen des Werkclustering durch die DNB eingegrenzt werden. Angesichts des Umfangs des als Basis für das maschinelle Lernen potentiell geeigneten Datensatzes sind wir mit grundlegenden Herausforderungen der Korpuskonstituierung konfrontiert, wie sie auch Gius et al. (2019) beschreiben: Die Menge (digital) vorliegender Texte ist so groß, dass der naheliegendste Weg der Korpusbildung darin bestünde, sich dabei auf eine Auswertung der Metadaten zu beschränken. Weil es sich bei “Figurengender” um einen textimmanenten Aspekt handelt, der in den Metadaten nicht erfasst wird, ist diese Vorgehensweise hier nicht möglich. 

                    
Für den Aufbau des Korpus kommen standardisierte Methoden wie 

                    
Random Sampling

                    
 und 

                    
Stratified Sampling

                    
 (Calvo Tello 2021: 107; Schöch 2017: 226) ebenfalls nur bedingt in Frage. 

                    
Random Sampling

                    
 ist ungeeignet, weil wir Texte benötigen, in denen zuverlässig Figuren diverser Gender-Kategorien vorkommen. Beim Aufbau einer balancierten Sammlung (

                    
Stratified Sampling

                    
), in der

                    
 “für alle Kombinationen wesentlicher Merkmale eine Mindestanzahl von Datensätzen” (Schöch 2017: 226) vorkommen müsste,

                    
 ist problematisch, dass es keine endliche Liste wesentlicher Merkmale der Genderthematik gibt. 

                    
Ob eine Figur im Hinblick auf Gender stereotyp oder ungewöhnlich ist und welche Kategorien es jenseits der traditionellen Einteilung in “männlich” und “weiblich” gibt, ist nicht klar definiert. Darum nutzen wir eine dritte Methode: die Verkleinerung der Population durch ergänzende Kriterien (Calvo Tello 2021: 108–109), mit Zügen einer opportunistischen Auswahl (Schöch 2017: 226) unter Einbezug von geisteswissenschaftlichem Crowd-Sourcing (vgl. Dunn und Hedges 2012). Vor dem Hintergrund, dass große Teile der Grundgesamtheit aktuell ausschließlich physisch vorliegen und somit für digitale Analysemethoden vorerst nicht in Frage kommen, scheint dies der einzig mögliche Weg zu sein. Die drei Kriterien, die wir bei der Korpuszusammenstellung berücksichtigen, sind:

                

                

                    

                        
Kriterium der Ausbalanciertheit 

                        

                            
aus jedem Jahr wird zunächst nur ein Roman übernommen

                            
von jedem Autor/jeder Autorin wird nur ein Roman übernommen

                        

                    

                    
Kriterium der Heterogenität in Bezug auf
                        

                            
literarische Genres

                            
Autor*innengender

                        

                    

                    
Kriterium der thematischen Relevanz: Nur Romane werden übernommen, in denen Figuren vorkommen, die
                        

                            
mit stereotypen Genderrollen brechen

                            
sich nicht klar in ein binäres Gendersystem fügen

                        

                    

                

                

                    
Während Parameter der Kriterien I. und II. aus den in der DNB erfassten Metadaten abgeleitet werden können, handelt es sich bei III. um ein Kriterium, das nicht erfasst wird. Erschwerend kommt hinzu, dass die Parameter der Kategorie III. nicht klar definiert sind, sondern von Interpretationen und (unbewussten) Vorannahmen abhängen. Um sowohl im Hinblick auf die Interpretation von Figurengender als auch auf versteckte Vorannahmen eine möglichst große Heterogenität zu erreichen und auf diese Weise den Aspekt des Representation-Bias (Suresh und Guttag 2019: 4) mit einzubeziehen, haben wir für die Korpuskonstituierung eine dreiteilige Citizen-Humanities-Komponente konzipiert. 

                

            

            

                

                    
Citizen Humanities in DisKo

                

                

                    
Der Aufbau eines Diversitäts-Korpus bringt drei Herausforderungen mit sich. Erstens können die Texte in der Grundgesamtheit nicht algorithmisch erschlossen werden, da große Datenmengen nur physisch vorliegen. Zweitens ist die Auswahl von Texten, in denen Gender nicht (nur) binär dargestellt wird, bereits eine interpretatorische Leistung. Darüber hinaus muss drittens der sog. Representation-Bias mit einbezogen werden, der personengebunden funktioniert (vgl. 

                    

                        
D'Ignazio

                    

                    
 und Klein 2020: 53; 

                    
Suresh und Guttag 2019

                    
). Im Falle von DisKo ist einerseits die Frage zentral, was genau unter nicht-binären Genderdarstellungen zu verstehen ist. Reicht es aus, wenn eine literarische Figur einer (binären) Genderkategorie mit einer Reihe von Eigenschaften beschrieben wird, die traditionell eher der anderen (binären) Genderkategorie zugeschrieben werden? Oder muss eine Figur explizit mit Begriffen wie “queer” oder “gender-fluid” charakterisiert werden? Wie explizit oder implizit müssen nicht-binäre Genderdarstellungen angelegt sein, damit sie als solche erkannt werden? Zu Beginn der Korpusgestaltung steht also eine interpretatorische Leistung, deren Ziel die Auslegung des Verständnisses von nicht-binären Genderkategorisierungen ist. Hinzu kommt, dass bei dieser Thematik Aspekte von Macht und Suppression eine Rolle spielen. D’Ignazio und Klein weisen in 

                    
Data Feminism

                    
 darauf hin, dass es wichtig ist, Daten zu sammeln, die marginalisierte Gruppen sichtbar machen (D’Ignazio und Klein 2020: 119). Darüber hinaus sollte die betreffende Community beim Sammeln der Daten einbezogen werden, um einen Empowerment-Effekt zu erreichen (vgl. D’Ignazio und Klein 202: 120, Heinisch 2020: 164). Außerdem ist für den Aufbau des Diversitäts-Korpus 

                    
Embodied Knowledge

                    
 (Christie et al. 2020) von Bedeutung; gerade bei Projekten, die Aspekte des Feminismus und der queer Community umfassen, ist die körperliche soziale Erfahrung Grundbestandteil einer Verstehensleistung, die zu einem kulturellen Wissen beiträgt (vgl. Christie et al. 2020). Im Sinne eines geisteswissenschaftlichen Crowd-Sourcing (Dunn und Hedges 2012) betrachten wir es darüber hinaus als Gelingensbedingung des Projektes DisKo, möglichst viele Personen an der Korpuszusammenstellung zu beteiligen, die im Hinblick auf ihre Genderzugehörigkeit und ihren beruflichen Hintergrund divers sind. Über einen eigens entwickelten Fragebogen werden laufend Vorschläge für DisKo eingereicht. Fakultativ können gleichzeitig Daten zu Genderzugehörigkeit und beruflichem bzw. interessengeleitetem Hintergrund angegeben werden. Von Beginn an werden auf der Webseite des m*w-Projektes auf einer eigenen Seite die Liste der für DisKo eingereichten Buchtitel sowie auch die dabei angegebenen Metadaten zu den Einreichenden offen einsehbar zugänglich gemacht (vgl. Schumacher und Flüh 2022). So werden die ethischen Grundsätze von Citizen Humanities

                    
Transparenz

                    
 in Bezug auf beteiligte Interessengruppen (vgl. Heinisch 2020: 15), 

                    
Offenheit

                    
 der Ergebnisse des Crowd-Sourcing (vgl. Dunn und Hedges 2012: 19)  sowie ein informativer 

                    
Mehrwert für Beteiligte

                    
 (vgl. Heinisch 2020: 15) gewährleistet.

                

                

                    
Der Fragebogen wird in drei Phasen in unterschiedlichen Communities verbreitet, deren Auswahl auf Basis der drei Dimensionen der partizipatorischen Wissenschaft – wissenschaftlicher Impact und Output, Lernen, Involviertheit und Empowerment der Teilnehmenden und gesellschaftlicher Impact und Awareness in Bezug auf die Thematik – getroffen wurde (vgl. Heinisch 2020: 155). Der Idee Wodwards folgend, dass Communities sich um verbindende Fragestellungen herum bilden, wird jede Phase von einer Frage geleitet (Wodward 2007: 117). Ergänzend wird eine Disseminationsstrategie mit digitalen und analogen Anteilen umgesetzt, die auf Erkenntnisse aus dem Disseminationsprojekt forTEXT zurückgreift (Gius et al. 2021, Schumacher und Gius 2022, Schumacher und Horstmann 2019). Die drei Phasen sind non-exklusiv, d.h., wenn z.B. Mitglieder der primären Zielgruppe aus Phase II schon in Phase I in den Community-Diskurs eintreten, so sind sie willkommen. Seit Beginn des Projektes und über alle Phasen hinweg wird der Blog des m*w-Projektes als Herzstück der Kommunikation genutzt.

                

                

                    
Phase I: Wie wird ein literaturwissenschaftliches Korpus aufgebaut, das zur Basis automatisierter Gender-Klassifikation verwendet werden soll?

                    

                        
Kernzielgruppe dieser Phase ist die Digital-Humanities-Community, die hauptsächlich aufgrund der Methodik Interesse an dem Projekt zeigt. Als etabliertes Medium der Wissenschaftskommunikation innerhalb dieser Community, das darüber hinaus auch erhebliches Potential für geisteswissenschaftliche Wissenschaftskommunikation allgemein bietet (vgl. Geier und Gottschling 2019), wird ein Twitter-Account aufgebaut. Dabei setzen wir auf ein organisches Wachstum, bei dem Interesse an dem Projekt über die getwitterten Inhalte ausgelöst wird. Außerdem werden etablierte Informationskanäle genutzt, wie z.B. der Discord-Server DHall, die DHd-Mailingliste, der Fachinformationsdienst für Allgemeine und Vergleichende Literaturwissenschaft oder das Projektschaufenster der Webseite des DHd-Verbandes. Auch die Teilnahme an Fachkonferenzen stellt einen wichtigen Teil der ersten Phase dar.

                    

                

                

                    
Phase II: Was macht non-binäre Genderdarstellung aus? 

                    

                        
In dieser Phase geht es darum, den ersten Outreach des Projektes zu generieren, indem Mitglieder der LGBTIQ+-Community und deren sogenannte Allies, die Interesse an der Genderthematik aufweisen, angesprochen werden. Zentral ist dabei die zielgruppengenaue Ausrichtung. Auch Kenntnis literarischer Texte ist vonnöten, sodass eine Community an der Schnittstelle zwischen LGBTIQ+-Themen und Interesse für Literatur gefunden werden muss. Diese Phase ist eng mit der ersten Phase des Community-Buildings verzahnt. Beteiligte der DH-Community dienen als Multiplikator*innen, indem sie z.B. im Rahmen ihrer Lehre auf das Projekt DisKo aufmerksam machen. Seitens des Projektes werden Gastvorträge in universitären Lehrveranstaltungen durchgeführt. Außerdem werden Flyer in Bibliotheken ausgelegt, die mittels QR-Code auf die DisKo-Umfrage verweisen. Darüber hinaus wird DisKo bei der Plattform 

                        
Bürger schaffen Wissen

                        
 eingereicht.

                    

                

                

                    
Phase III: Wie bedeutsam ist non-binäre Genderdarstellung für unsere Gesellschaft?

                    

                        
Das Thema Gender und insbesondere (non-)Binarität wird aktuell in unterschiedlichen Bereichen des öffentlichen Lebens diskutiert. Die gesellschaftliche Brisanz der Gender-Thematik und die Relevanz für den alltäglichen Umgang miteinander ist aber nicht nur derzeit ein wichtiges Thema. Mit dem Projekt m*w, in dessen Rahmen DisKo aufgebaut wird, möchten wir offenlegen, dass auch in der Literaturgeschichte immer wieder Figuren eine Rolle spielen, die sich nicht in ein binäres Gendersystem fügen lassen. Wir möchten zeigen, dass die aktuelle Debatte also nicht neu ist, sondern Gender-Diversität schon lange ein gesellschaftliches Thema ist, das u.a. in Kulturprodukten wie literarischen Texten eine wichtige Position einnimmt. In dieser Phase setzen wir die in Citizen-Science-Ansätzen häufig sehr stark verankerte Idee einer 

                        
Third Mission

                        
 von Forschungsprojekten (vgl. Heinisch 2020: 152–153) um, indem wir Ergebnisse mit einer möglichst breiten Öffentlichkeit teilen und in die aktuellen Debatten einfließen lassen. Darum suchen wir in dieser Phase weitere Wege der Wissenschaftskommunikation, wie z.B. über die Videoplattform TikTok oder den Bilder-Sharing-Dienst Instagram. 

                    

                

                

                    
Erste Ergebnisse

                    

                        
Zum jetzigen Zeitpunkt (Stand Dezember 2022) befinden wir uns in Phase I des Citizen-Humanities-Projektes DisKo. Maßnahmen zur Verbreitung der Umfrage wurden auf Twitter, Mastodon, der DHd-Webseite, internen Kanälen der DNB und über Flyer umgesetzt. Außerdem wurde ein Gastvortrag im Rahmen der Ringvorlesung “Einführung in die Digital Humanities” an der Universität Hamburg gehalten. Der Rücklauf ist noch nicht sehr hoch. Insgesamt gab es 17 Teilnehmende der Umfrage. Allerdings wurden von diesen insgesamt 31 Titel angegeben, was bedeutet, dass jede*r Teilnehmende t durchschnittlich 1,8 Titel eingereicht hat. Die tatsächliche Verteilung ist recht heterogen, es wurden bis zu sechs Titel pro Person angegeben. Die freiwilligen Angaben zum eigenen Hintergrund wurden meist beantwortet, wie aus Abb. 1 ersichtlich wird.

                    

                    

                        

                        

                        
Abb. 1: Metadaten der Einreichenden der DisKo-Umfrage

                    

                    

                    
                    

                        
Die eingereichten Titel umfassen derzeit eine Zeitspanne, die von 1928–2022 reicht, also 94 Jahre abdeckt. Der Schwerpunkt liegt mit 22 Erzähltexten auf Titeln, die nach 2000 erschienen sind. Nur acht der für DisKo vorgeschlagenen Texte sind vor dem Jahr 2000 erschienen. Es handelt sich bei den Texten sowohl um ursprünglich deutschsprachige Erzähltexte als auch um Übersetzungen. Zwar birgt die Integration von Übersetzungen für das Machine-Learning-Training die Gefahr, eine unkontrollierbare Variable einzubauen, da unklar ist, inwiefern die Ergebnisse der automatischen Erkennung davon beeinflusst werden. Dafür können aber Titel der Weltliteratur aufgenommen werden, die tatsächlich gelesen werden.

                    

                    

                        
In Bezug auf die Genderdarstellungen reichen die Texte von der Erwähnung der nicht-stereotypen Genderindentität einer Figur (wie in Murakamis 

                        
Kafka am Strand

                        
) bis hin zu einer Fülle nicht-stereotyper Genderidentitäten, die zum Hauptthema des Erzähltextes werden. Letzteres ist z.B. bei Evaristos

                        
 Mädchen, Frau, etc.

                        
 der Fall, sodass wir diesen Roman für unser Machine-Learning-Training als Testtext gewählt haben. Eine linguistische Besonderheit zeigt Leckies 

                        
Maschinen-Trilogie

                        
, die durchgehend im generischen Femininum geschrieben wurde. Neben Murakami und Leckie wurden bisher acht weitere Texte ins Trainingskorpus übernommen. Von jedem der zehn Texte wurden zwei 2.000 Tokens umfassende Passagen ins Trainingskorpus integriert – eine vom Beginn und eine vom Ende des Textes (um eventuelle Transitionen berücksichtigen zu können). Erste Tests mit einem auf diesem 40.000 Tokens umfassenden Trainingskorpus trainierten Modell zeigen noch keine zufriedenstellenden Ergebnisse. Der F1-Score liegt insgesamt bei 0,35, die Erkennung der Kategorie “Divers” bei 0. Ein Blick auf die annotierten Beispiele im Trainingskorpus zeigt, dass für diese Kategorie nur 60 Vorkommnisse annotiert wurden, während die Kategorien “Frau” und “Neutral” jeweils rund 500 Vorkommnisse aufweisen, “Mann” sogar 902. Um hier zu einem ausgewogeneren Verhältnis zu kommen, könnten statt Anfangs- und Endpassagen, Ausschnitte aus den Texten ausfindig gemacht werden, in denen sich Genderzuschreibungen der Kategorie “Divers” häufen. Eine andere Möglichkeit wäre die Annotation kompletter Erzähltexte, in denen non-binäre Genderdarstellungen zum Hauptthema gemacht werden. Nach Abschluss der Pilot-Trainingsphase werden wir darum ein erneutes Training mit relevanteren Samples oder Volltexten anschließen.

                    

                

            

            

                
Fazit

                
Die Korpuskonstituierung ist für Projekte, die Verfahren des maschinellen Lernens einsetzen, ein Dreh- und Angelpunkt. Alle weiteren Verfahrensschritte und Ergebnisse wie z.B. die Performanz eines Classifiers oder Analyseergebnisse, die durch dessen Einsatz erzielt werden, werden vom genutzten Korpus massiv beeinflusst. Unser Konzept der Korpuskonstituierung greift sowohl arbeitspraktische Ansätze der Digital Humanities wie das 
                    
Random Sampling
 oder die opportunistische Auswahl als auch kritische Betrachtungen von Aspekten wie Representation-Bias und Empowerment auf. Mithilfe einer Citizen-Humanities-Komponente begegnen wir zentralen Herausforderungen beim Aufbau des Diversitäts-Korpus DisKo. Die strategische Korpuskonstituierung, die wir in unserem Vortrag präsentieren und zur Diskussion stellen wollen, ist dabei nicht nur ein optionaler Bestandteil, sondern wird zur Gelingensbedingung für eine möglichst diverse und ausgewogene Datenbasis.
                

            

        

            

                
Dieser GitMA-Workshop richtet sich an fortgeschrittene CATMA User*innen mit Vorkenntnissen in digitaler Annotation, die im Rahmen der eigenen Arbeit oder von Forschungsprojekten mit größeren Mengen von Annotationsdaten operieren (wollen). Bei GitMA handelt es sich um ein Python-Package, mit dem Annotationsdaten, die in CATMA erstellt wurden, weiter verarbeitet werden können (Vauth et al. 2021). Wie greife ich über Git auf meine CATMA-Annotationsdaten zu? Wie visualisiere ich kollaborativ erstellte Annotationsdaten, die in mehreren Collections abgelegt sind? Wie berechne ich die Übereinstimmung zwischen mehreren Annotator*innen? Diese und ähnliche Fragen werden während des Workshops beantwortet.

            

            

                
CATMA (Gius et al. 2021) ist eine webbasierte, kollaborative Textannotations- und Analyse-Plattform, die seit 2008 an der Universität Hamburg und im Rahmen des DFG-geförderten Projektes forTEXT seit 2020 an der Technischen Universität Darmstadt entwickelt wird. Hauptzielgruppe sind traditionell-analog arbeitende Geisteswissenschaftler*innen, die über eine intuitiv bedienbare GUI Texte annotieren und analysieren können. Mit dem Release von CATMA 6 im Jahr 2019 wurde für die Plattform ein auf Git basierendes Backend eingeführt. Für zahlreiche Projekte, die bereits auf sehr fortgeschrittenem Niveau CATMA nutzen, und Interessierte aus der Digital-Humanities-Community mit Erfahrung in der Nutzung von Git und Grundkenntnissen in Python, eröffnet sich dadurch eine Reihe neuer Funktionen, die es in bisherigen CATMA-Versionen nicht gab. Einige dieser Funktionen werden im Laufe dieses Workshops vorgestellt und vermittelt.

            

            
Der Workshop bietet:



                
kurze Einführung in die Nutzung von CATMA über das graphische Userinterface

                

                    
Kennenlernen der Datenstrukturen des Backends

                

                

                    
Zugriff auf das Backend mit Git

                

                
Weiterverarbeitung der Daten mit Hilfe eines zur Verfügung gestellten Python-Packages

            

            

                
Annotation in CATMA 6

                

                    
Annotation ist eine zentrale Kultur- und Forschungspraxis, die bereits seit sehr langer Zeit analog betrieben wurde (vgl. Moulin 2010), bevor sie im Rahmen der Digital Humanities ins Digitale übertragen wurde. Textauszeichnung und -anreicherung, Freitextkommentare und das taxonomiebasierte Annotieren sind Formen der Annotation, die sich zum Teil überschneiden (vgl. Jacke 2018, § 9). Alle diese Formen werden von CATMA 6 digital unterstützt. Mithilfe selbst erstellter oder auf der Plattform forTEXT.net bereitgestellter Tagsets (z.B. Flüh 2020) kann einzeln oder im Team taxonomiebasiert annotiert werden.

                

                

                    
Eine der wichtigsten Neuerungen von CATMA 6 gegenüber früheren Versionen ist die Umstellung auf eine projektzentrierte Nutzungsarchitektur. Am Beginn der Arbeit mit CATMA steht das Anlegen eines Projektes mit beliebig vielen Dokumenten, die analysiert werden sollen, und beliebig vielen Team-Mitgliedern, die daran arbeiten wollen. Einzelne und Merfachannotationen, einander überlagernde oder überlappende Annotationen oder auch widersprüchliche Annotationen sind in CATMA durch die Speicherung der Daten als Standoff-Markup möglich. Eine weitere Neuerung im Funktionsumfang ist die Möglichkeit, Textstellen und Annotationen zu kommentieren. Offene Fragen, nicht zu Ende gedachte Interpretationsansätze oder auch der Austausch mit den anderen Team-Mitgliedern können über die Kommentarfunktion in den Annotationsprozess integriert werden. Sowohl Annotationen als auch Kommentare können über die Analyse-Funktionen von CATMA durchsucht, in tabellarische Form gebracht oder visualisiert werden. Der Umfang dessen, was über die CATMA-GUI umgesetzt werden kann, ist also recht groß. Die Einführung des auf Git basierenden Backends macht das Tool für die Digital-Humanities-Community aber noch interessanter. Der undogmatische Zugang, der bisher nur zu Annotationen und Annotationstaxonomien ermöglicht wurde, erstreckt sich nun bis zu den Annotationsdaten und der Weiterverarbeitung derselben (siehe Abbildung 1). Dieser neue Teil des CATMA-Workflows wird in diesem Workshop vermittelt werden.

                

                

                    

                    
Abbildung 1: Im Workshop vermittelter Workflow zur Annotationsauswertung und -überarbeitung mit dem CATMA-Backend

                

                
            

            

                
Annotationen auswerten mit GitMA

                

                    
Technische Niedrigschwelligkeit und Nähe zu traditionell-analogen Methoden der Geisteswissenschaften (vgl. Schumacher und Gius 2022) sind nach wie vor wichtige Grundsätze, die in CATMA implementiert sind. Doch mit zunehmender Verbreitung des Tools in den digitalen Geisteswissenschaften sind neben der Möglichkeit zu hermeneutisch-vielfältiger Textanalyse 

                    
(vgl. Piez 2010) 

                    
auch die Einhaltung von Best Practices und Standards, die innerhalb der Digital-Humanities-Community entwickelt wurden, von Bedeutung. Eine Verschmelzung von CATMA und dem direkten Datenzugriff über Git zu “GitMA” ermöglicht beides. Die im Annotationsprozess erstellten Daten können zum Beispiel nach der Übereinstimmung der Annotierenden untereinander (Artstein und Poesio 2008) ausgewertet werden. Es ist möglich eine der Annotationen als ‘Silver Annotation’ festzulegen und die anderen daran zu messen. Das festgestellte Disagreement kann zur Grundlage eines Disagreement-Tagsets werden, das über das Backend auch wieder ins Frontend der CATMA-GUI zurückgespielt werden kann (siehe Abb. 1). Dasselbe gilt für die nicht übereinstimmend annotierten Passagen, welche wiederum selbst durch Annotationen dargestellt bzw. hervorgehoben werden können. So ergibt sich ein Workflow vom Frontend zum Backend und zurück, der auch die Erstellung von Goldannotationen (vgl. Wissler et al. 2014) unterstützt.

                

            

            

                
Format und Ablauf des Workshops

                

                    
Der Workshop wird als halbtägiges hands-on Tutorial angeboten.

                

                
Ablauf:

                
CATMA 6 (45 Minuten)



                    
kurze Einführung in das CATMA-Frontend

                    

                        
Struktur: Tagsets, Documents, Annotation Collections

                    

                

                

                    
Zugriff auf Annotationsdaten über Git (30 Minuten)

                



                    

                        
wie clone ich ein CATMA Project? 

                    

                    

                        
wie update ich ein CATMA Project, um neue Annotationen zu laden?

                        
 

                    

                    
Installation des Packages

                    
Laden eines Projects

                    

                        
Zugriff auf Annotation Collections, Dokumente und Tagsets

                    

                

                

                    
15 Minuten Pause

                    
 

                

                
Explorative Annotationsauswertungen (60 Minuten)



                    
Annotationsdaten visualisieren

                    
Netzwerkanalysen von Annotationsdaten

                

                
15 Minuten Pause

                
Statistische Annotationsauswertungen (45 Minuten)



                    

                        
Einführung in die Begrifflichkeiten Inter-Annotator-Agreement, Silver &amp; Gold Standard

                    

                    

                        
Festlegung der Silver Annotations

                    

                    
Umgang mit Annotationsspannen

                    

                        
Automatische Erstellung eines Disagreement Tagsets

                    

                    

                        
Darstellung von Disagreement als Annotationen

                    

                    
Manuelle Überarbeitung von automatischen Goldannotationen

                

                
Diskussion und Feedback (30 Minuten)

            

            

                
Zielgruppe

                
Nutzer*innen, die Annotationen mit CATMA in Forschungsprojekten oder Lehrsituationen managen, sowie alle, die einen schnellen Workflow zwischen Annotation bzw. Annotationsbearbeitung und Annotationsauswertung benötigen.

            

            

                
Zahl der möglichen Teilnehmer*innen

                
30

            

            

                
Technische Voraussetzungen

                

                    
Die benötigten Vorinstallationen von Git, Anaconda und GitMA (sowie dessen Abhängigkeiten) können durch die Bereitstellung eines Docker-Image vermieden werden. Die Teilnehmer*innen sollten Docker Desktop auf einem eigenen Laptop installiert haben (Touch Devices werden nicht unterstützt) und diesen zum Workshop mitbringen. Für die Durchführung des Workshops benötigen wir außerdem einen Beamer. 

                

                

                    
Zur Vorbereitung sollten Teilnehmer*innen außerdem schon einen CATMA-Account erstellt (unter 

                    

                        
https://app.catma.de/catma/

                    

                    
) und sich mit der CATMA-Nutzung bekannt gemacht haben (z.B. mithilfe von der forTEXT-Lerneinheit zu CATMA 6: 

                    

                        
Manuelle Annotation mit CATMA

                    

                    
). Wenn eigene CATMA-Annotationsdaten vorhanden sind, können diese während des Workshops analysiert werden. Für Teilnehmende, die nicht an eigenen Daten arbeiten möchten, stellen wir ein Demo-Projekt zur Verfügung, mit dem man während des Workshops arbeiten kann.

                

            

            

                
Benötigte Vorkenntnisse

                

                    
Die Teilnehmer*innen sollten über grundlegende Kenntnisse der Kommandozeile, Git und Python sowie Jupyter verfügen.

                

            

            

                
Beitragende

                

                    
Evelyn Gius, Prof. Dr.

                

                
Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Residenzschloss 1, 64283 Darmstadt

                

                    
Evelyn Gius ist Professorin für Digitale Philologie und Neuere Deutsche Literatur an der Technischen Universität Darmstadt. Sie promovierte an der Universität Hamburg mit einer Arbeit über die narrative Struktur von Konflikterzählungen. Ihre Forschungsschwerpunkte sind manuelle Annotation, Operationalisierung, Erzähltheorie, Segmentierung und Konflikte. Sie ist PI mehrerer Digital-Humanities-Projekte (EvENT, KatKit, CATMA, forTEXT) und ist Vorsitzende des Vereins Digital Humanities im deutschsprachigen Raum (DHd), Mitherausgeberin des Journal of Computational Literary Studies (JCLS) und Mitherausgeberin der Buchreihe "Digitale Literaturwissenschaft".

                

                

                    
Dominik Gerstorfer, M.A.

                

                
Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Residenzschloss 1, 64283 Darmstadt

                

                    
Dominik Gerstorfer promoviert über "Philosophische Fragen der Digital Humanities" an der Universität Stuttgart. Derzeit ist er im Projekt KatKit tätig, zuvor war er im DFG-Projekt forTEXT in Darmstadt und im Digital-Humanities-Projekt CRETA in Stuttgart beschäftigt. Dominik hat an der Universität Tübingen Philosophie, Politikwissenschaften und Soziologie (M.A.) studiert. Seine Forschungsschwerpunkte liegen in den Bereichen Wissenschaftstheorie, formale Methoden und Argumentationsanalyse. Im Rahmen von KatKit und forTEXT beschäftigt sich Dominik u.a. mit Intertextualität, Ontologien und der Entwicklung von Kategoriensystemen.

                

                
Malte Meister, B.Sc.

                
Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Residenzschloss 1, 64283 Darmstadt

                

                    
Malte Meister hat 2009 sein Informatik-Diplom (B.Sc.) in Kapstadt erworben. Im Rahmen des Abschlussprojekts für sein Diplom wurde er beauftragt, das Text-Annotations und -Analysetool CATMA, für die Universität Hamburg zu erstellen. Bis Anfang 2010 wirkte er im Team an CATMA mit, bevor er sich auf seine Karriere in der freien Wirtschaft konzentrierte. Nach mehr als zehn Jahren Berufserfahrung als Softwareentwickler und Teamleiter entschied er sich, wieder in die CATMA-Entwicklung einzusteigen. Er ist seit 2021 technischer Mitarbeiter an der TU Darmstadt und beschäftigt sich dort im Rahmen von forTEXT hauptsächlich mit dem Betrieb und der Weiterentwicklung von CATMA und den damit verbundenen Systemen.

                

                
Mareike Schumacher, M.A.

                
Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Residenzschloss 1, 64283 Darmstadt

                
Mareike Schumacher koordiniert das DFG-Projekt forTEXT (
                    
https://fortext.net
), in dem neben der Dissemination von digitalen Routinen, Ressourcen und Tools in die traditionelleren Fachwissenschaften auch die Weiterentwicklung von CATMA eine wesentliche Rolle spielt. Sie promovierte als digitale Literaturwissenschaftlerin über Orte und Räume im Roman, beschäftigt sich besonders mit den Methoden des 
                    
distant reading
 (u. a. 
                    
Named Entity Recognition
 oder Stilometrie), der Digital-Humanities-Theorie und der Verbindung von digitalen Methoden und theoriebasierter Literatur- und kulturwissenschaftlicher Forschung. 
                

            

        

            

                

                    
Einführung und Forschungsstand
                

                
Die vossianische Antonomasie (VA) ist ein rhetorisches Stilmittel aus der Familie der Antonomasien, eng verwandt mit Metonymie und Metapher. Während bei der klassischen Antonomasie ein Eigenname durch eine typische Eigenschaft ersetzt wird (wenn etwa Michael Schumacher als »der Kerpener« bezeichnet wird), funktioniert die vossianische Antonomasie genau umgekehrt. Hier wird ein typisches Merkmal einer Person durch den Eigennamen einer anderen Person evoziert.

                

                    
Wenn ein Journalist zum Beispiel Wilson Kipketer, den dänischen Mittelstreckenläufer kenianischer Herkunft, als »Greta Garbo der Leichtathletik« bezeichnet, wird eine typische Eigenschaften der Filmschauspielerin aufgerufen, in diesem Fall ihre distanzierte, zurückhaltende Art, wie dieses Zitat aus der 

                    
New York Times

                    
 

                    
zeigt: »Kipketer is as guarded [zurückhaltend] as he is fast; some reporters have labeled him the Greta Garbo of track and field.« (NYT, 8. August 1997).

                

                
Eine vossianische Antonomasie setzt sich im Normalfall aus drei Teilen zusammen: dem Target (Wilson Kipketer), der Source (Greta Garbo) und dem Modifier (Leichtathletik) (vgl. Bergien 2013). Der Modifier verschiebt eines oder mehrere Merkmale der Source in das Umfeld des Targets. In dieser Arbeit konzentrieren wir uns auf die systematische Analyse des Modifiers.

                
Die automatisierte Erkennung und Extraktion vossianischer Antonomasien hat sich in den letzten fünf Jahren rasch ausdifferenziert. Während Jäschke et al. 2017 und Fischer et al. 2019 semi-automatisierte Verfahren nutzten, um VA-Ausdrücke in großen Zeitungskorpora ausfindig zu machen, setzten Schwab et al. (2019, 2022) automatisierte Verfahren ein, die meist auf neuronalen Netzen basierten.

                

                    
Da wir mit einem großen Korpus und Word Embeddings arbeiten, ist unser Forschungsbeitrag der erste, der eine quantitative Untersuchung dieses Phänomens mit einer thematischen Gruppierung der verschiedenen Modifier verbinden kann. 

                    
Unsere Forschungsergebnisse stellen wir auch über eine interaktive Visualisierung bereit (

                    

                    
).

                

            

            

                
Datensatz

                

                    
Wir nutzen den VA-Datensatz aus Schwab et al. 2019, welcher mittels eines semi-automatisierten Verfahrens aus dem 

                    
New York Times

                    
-Korpus (Sandhaus 2008) generiert wurde. Das NYT-Korpus besteht aus mehr als 1,8 Mio. Zeitungsartikeln der NYT aus den Jahren 1987–2007. Mit Hilfe des regulären Ausdrucks 

                

                

                    
\\ b(the|an?)\\s+([\w.,’-]+\\s+){1,5}?(of|for|among)\\b

                

                

                
wurden Kandidatensätze ermittelt, d.h. alle Sätze, die Phrasen enthalten, welche mit »the«, »a« oder »an« anfangen und mit »of«, »for« oder »among« enden, wobei zwischen diesen beiden Polen ein bis fünf Wörter platziert sein können. Die Wörter zwischen Anfangs- und Endsignal stellten die potenzielle Source-Phrase dar und wurden mit einer Wikidata-Liste abgeglichen, die alle Entitätennamen aus Wikidata (inkl. Aliasse) enthielten, die die Eigenschaft ›instanceOf‹ ›human‹ aufweisen. Die Source-Kandidaten wurden also auf Menschen beschränkt, die in Wikidata verzeichnet sind (dabei handelt es sich um eine bewusste Beschränkung bei der Untersuchung des Phänomens – VA können auch mit Orten, Markennamen, Comicfiguren usw. operieren). Anschließend wurden diese Kandidaten mit einer manuell erstellten Sperrliste abgeglichen, um falsche Kandidaten auszuschließen.

                
Dieser Datensatz wurde in Schwab et al. 2022 verfeinert. Alle VA-Phrasen (Target, Source, Modifier) wurden auf Wortebene innerhalb der Sätze annotiert. Insgesamt enthält der Datensatz 5.995 Sätze, davon enthalten 3.066 einen VA-Ausdruck und 2.929 enthalten keinen, sind aber syntaktisch ähnlich aufgrund des genutzten regulären Ausdrucks.

                

                    
In Tabelle 1 sind die zehn häufigsten Modifier des Datensatzes aufgelistet. Die häufigsten Ausdrücke sind temporale Ausdrücke (»his day«, »his time«, »the 90s«), geografische Angaben (»Japan«,» China«) und Sportarten (»tennis«, »baseball«, »ballet«).
                

                
                

                
                    

                        

                            

                            
Modifier

                        

                        

                            

                            
Anzahl

                        

                    

                    

                        

                            

                            
his day

                        

                        

                            

                            
56

                        

                    

                    

                        

                            

                            
his time 

                        

                        

                            

                            
35

                        

                    

                    

                        

                            

                            
Japan

                        

                        

                            

                            
32

                        

                    

                    

                        

                            

                            
the 90s

                        

                        

                            

                            
21

                        

                    

                    

                        

                            

                            
China

                        

                        

                            

                            
17

                        

                    

                    

                        

                            
our time

                        

                        

                            

                            
17

                        

                    

                    

                        

                            

                            
tennis

                        

                        

                            

                            
16

                        

                    

                    

                        

                            
his generation

                        

                        

                            

                            
16

                        

                    

                    

                        

                            

                            
baseba

                            
ll

                        

                        

                            

                            
16

                        

                    

                    

                        

                            
her time

                        

                        

                            

                            
14

                        

                    

                    
Tabelle 1: Die zehn häufigsten Modifier im Datensatz inklusive ihrer Häufigkeit.

                

            

            

                
Methode

                
Wir nutzen kontextabhängige Word Embeddings, um die Modifier-Phrasen in hochdimensionale Vektoren zu transformieren, die die Semantik des Textes wiedergeben sollen.

                
Mit Hilfe von Word Embeddings wurden in den letzten Jahren viele Benchmarks im Bereich Natural Language Processing erstellt. Insbesondere kontextabhängige Word Embeddings, d.h. die numerische Repräsentation von Wörtern und Tokens in Abhängigkeit ihres Kontexts, haben viel Aufmerksamkeit auf sich gezogen. Der Vorteil dieser Word Embeddings im Gegensatz zu kontextunabhängigen Word Embeddings ist die Möglichkeit, Homonyme korrekt darzustellen. Wir benötigen die numerische Repräsentation der Phrasen, um anschließend ein Clustering-Verfahren durchführen zu können, welches die Modifier in Themenbereiche gruppieren soll.

                

                    
Wir greifen auf Sentence-Transformers zurück, welches aus Sentence-BERT (Reimers et al. 2019) hervorgegangen ist. Das Modell basiert auf transformerbasierten Sprachmodellen wie BERT (Devlin et al. 2019). Im Gegensatz zu BERT wird S-BERT allerdings mittels einer siamesischen Netzwerkstruktur trainiert, der Output wird durch eine Pooling Operation in einen hochdimensionalen Vektor transformiert. Dadurch kann das trainierte Modell effizient semantische Ähnlichkeiten zwischen Texten errechnen. Wir nutzen das Modell »all-mpnet-base-v2«, welches die besten Resultate in der Anwendung auf verschiedene Datensätze zeigte (siehe 

                    

                    
).

                

                
Dies wenden wir auf die einzelnen Modifier an. Das Netzwerk liefert für jeden Modifier einen 768-dimensionalen Vektor. Diese numerischen Vektoren lassen sich nun durch ein Clustering-Verfahren gruppieren.

                

                    
Wir entscheiden uns für den k-means-Algorithmus (MacQueen 1967), um die Vektoren in Cluster einzuteilen. Wir nutzen k-means aufgrund verschiedener Annahmen. Einmal gehen wir davon aus, dass es relativ wenige Ausreißer gibt, da die VA-Ausdrücke aus dem Datensatz häufig in ähnlichen Themengebieten in der 

                    
New York Times

                    
 

                    
vorkommen (vgl. Fischer et al. 2019). Außerdem können wir die Anzahl der Cluster angeben und diese während der Analyse variieren, um zu beobachten, wie sich die Gruppierungen in Abhängigkeit davon verhalten. Dies funktioniert mit dichtebasierten Clustering-Algorithmen nicht so einfach. Da k-means in der Berechnung der Cluster die quadrierte euklidische Distanz nutzt, normalisieren wir die Output-Vektoren, da die normalisierte quadratische euklidische Distanz proportional zur Kosinus-Distanz ist, welche in Reimers et al. 2019 genutzt wird, um die Ähnlichkeit zwischen zwei Vektoren zu berechnen.

                

                
Im Anschluss an das Clustering möchten wir den einzelnen Clustern Themen zuordnen, durch ein an das »Topic Modeling« angelehntes Verfahren. Stark vereinfacht basieren klassische Topic-Modeling-Modelle auf der Annahme, dass Wörter, die besonders häufig gemeinsam in Sequenzen vorkommen, ein abstraktes Thema bilden. Meist wird Topic Modeling auf längere Dokumente angewandt, bei denen von signifikanten Wort-Überschneidungen ausgegangen werden kann. 97 Prozent der Modifier-Phrasen bestehen jedoch aus einem bis vier Wörtern und weisen dadurch kaum Überschneidungen auf. Somit sind sie für klassisches Topic Modeling ungeeignet. Selbst beim sogenannten Short Text Topic Modeling wird mit Textsorten wie Tweets oder Rezensionen trainiert, welche immer noch bedeutend länger sind als unsere Phrasen.
                    
Wir nutzen stattdessen den Vorteil, dass viele der Formulierungen Nominalphrasen oder Nomen sind. Dadurch sind sie unter anderem im WordNet (Fellbaum 1998) zu finden, einer lexikalischen Datenbank, die Wortbedeutungen, Synonyme und viele andere Features bereitstellt. Das Projekt WordNet Domains (Bentivogli et al. 2004) hat zusätzlich jedem Wort bzw. jedem Synset (Gruppe ähnlicher Wörter) in WordNet semi-automatisch ein oder mehrere Domains zugeordnet, die für uns als Themengebiete genutzt werden können. Diese Domains sind hierarchisch gegliedert. Dies nutzen wir aus und weisen jeder Modifier-Phrase, soweit vorhanden, ihre Domains zu. Vorher nutzen wir noch das NLTK Toolkit (Bird et al. 2009), um alle Stoppwörter zu entfernen und die Ausdrücke dadurch auf die Nomen zu reduzieren, zum Beispiel »her time« zu »time« oder »the harmonica” zu »harmonica«. Sollte die übrigbleibende Phrase im WordNet nicht vorhanden sein, teilen wir sie in ihre einzelnen Wörter auf und verfahren wie oben beschrieben für jedes Wort der Phrase. Zum Schluss weisen wir die am häufigsten vorkommende Domain der Phrasen je Cluster dem jeweiligen Cluster als Themengebiet zu. In der Web-App kann man sich zusätzlich die zehn hochfrequentesten Domains anschauen.
                

                
Anschließend können wir die Cluster visualisieren. Da die Vektoren hochdimensional sind, nutzen wir verschiedene Dimensionsreduktionsalgorithmen, um sie auf zwei Dimensionen zu reduzieren. Wir vergleichen mehrere Algorithmen – PCA (Hauptkomponentenanalyse, Pearson 1901), t-SNE (t-distributed stochastic neighbor embedding Methode, van der Maaten 2008), UMAP (Uniform Manifold Approximation and Projection, McInnes et al. 2018), IVIS (Szubert et al. 2019) –, welche in der Web-App ausgewählt werden können. Nach einigen Durchläufen hat sich die Kombination von PCA und t-SNE als bestes Verfahren herausgestellt, welches wir kurz vorstellen. Wir wenden zuerst PCA an und reduzieren die Vektoren auf eine Länge von 50. Die Hauptkomponentenanalyse vereinfacht Daten, indem die Einträge der Vektoren durch eine geringere Zahl möglichst aussagekräftiger Linearkombinationen (die Hauptkomponenten) genähert werden. Zusätzlich nutzen wir t-SNE, um die 50-dimensionalen Vektoren auf zweidimensionale Vektoren zu reduzieren. Der Vorteil von t-SNE im Gegensatz zu PCA liegt in der Möglichkeit, nichtlineare Abhängigkeiten darzustellen. t-SNE reduziert die Vektoren außerdem so, dass Vektoren, die in der höheren Dimension eine kurze Distanz haben, auch in der reduzierten Dimension eine kurze Distanz zueinander haben. Dadurch wird die lokale wie auch globale Struktur bewahrt.

            

            

                
Erkenntnisse

                

                    

                    
Die Modifier für vossianische Antonomasien fallen im 

                    
New York Times

                    
-Korpus sehr vielgestaltig aus und sind nicht auf bestimmte Phrasen oder Wortgruppen limitiert. Abbildung 1 zeigt beispielhaft die Visualisierung mit neun Clustern, wobei die Themen von uns zunächst manuell zugeordnet wurden. Einige der Cluster lassen sich bis auf wenige Ausnahmen eindeutig bestimmten Themen zuordnen, wie Sport, Musik und Tanz, Kunst, Film und Literatur, Geografie, Politik, Wirtschaft oder temporale Ausdrücke.

                

                

                    

                        

                        
Abbildung 1: Die Abbildung zeigt die Visualisierung des Clusterings nach Dimensionreduktion mit neun Clustern. Den Clustern wurde manuell ein Themengebiet zugeordnet.

                    
Die automatisch gefundenen Themen durch WordNet Domains stimmen mit den von uns manuell zugewiesenen Themen fast vollständig überein, wie Abbildung 1 zeigt. Den von uns manuell annotierten Themen sind in Klammern die automatisch gefundenen Themen beigesellt.
                

                
Der blaue Cluster in der Mitte ist allerdings sehr divers und lässt sich nicht genau einer Kategorie zuordnen. Hier tauchen Flora und Fauna auf (»the pumpkins, »Rottweilers«), was zu dem automatisch gefundenen Themengebiet Biologie passen würde. Allerdings kommen auch »space wear«, »Buddhism« sowie »soft drinks« und »the physics world« vor. Wir haben uns für das Rubrum Naturwissenschaft entschieden, welches auf einen Großteil der Phrasen zutrifft.

                
Einer der Gründe für die Zusammensetzung dieses diversen Clusters ist der Umstand, dass k-means keine Ausreißer zulässt und daher jeder Punkt genau einem Cluster zugeordnet wird. Dadurch finden sich auch Phrasen, die eigentlich nicht in ein bestimmtes Themengebiet gehören oder für die es eigentlich zu wenige ähnliche Phrasen gibt, in einem Cluster wieder.

                
Ein anderer Grund ist die Diversität der Modifier. Viele Modifier bestehen nicht nur aus einem, sondern aus mehreren Wörtern. Diese Phrasen könnte man verschiedenen Themengebieten oder Subgenres zuordnen, z.B. »ancient Alexandria« (Temporal, Geografie), »Korean radio« (Geografie, Technologie) oder »food writing« (Speisen und Getränke, Literatur). Dies sind auch häufig Phrasen, die durch die Dimensionsreduktion nicht in der Nähe der anderen Phrasen des Clusters liegen, weil zum Beispiel »food writing« in die Nähe von anderen kulinarischen Phrasen verortet wurde, obwohl es ein Subgenre der Literatur ist. An diesem Beispiel sieht man, dass das Clustering-Verfahren das Wort richtig zugeordnet hat (»food writing« gehört zum kulturellen Cluster), aber falsch visualisiert wurde.

                
Abhängig von der Anzahl der Cluster unterteilt sich zum Beispiel die Kultur nach und nach in Subgenres wie Literatur, Musik, Tanz, Film/TV oder Kunst. Dies kann man in Abbildung 2 gut beobachten. Der linke Teil von Abbildung 2 zeigt einen Ausschnitt der Visualisierung, in der sechs Cluster gebildet wurden. Hier sind die meisten kulturbezogenen Phrasen in einem einzigen Cluster (grün) gruppiert. Im rechten Teil ist der gleiche Ausschnitt zu sehen, allerdings mit zwölf Clustern. Man kann gut erkennen, dass sich der kulturelle Cluster fast vollständig in drei neue Cluster (grau, orange, blau) aufgeteilt hat, nämlich »Kunst«, »Literatur und Film/TV« und »Musik und Tanz«. Auch hier gibt es Grenzfälle wie zum Beispiel »musicals«. Das Clustering hat die Phrase zu »Musik und Tanz« gruppiert, wohingegen in der Visualisierung das Wort in die Nähe des Film-Clusters gerückt wurde, in dem auch theaterbezogene Themen auftauchen.

                
Auch die Geografie teilt sich mit wachsender Anzahl an Clustern in zwei Hälften, wobei in der einen ein Großteil der US-amerikanischen Geografika angesiedelt sind, allerdings nicht ausschließlich.

                

                    

                        

                        
Abbildung 2: Die Abbildung zeigt einen Ausschnitt der Visualisierung mit sechs Clustern auf der linken Seite und zwölf Clustern auf der rechten Seite. Der Ausschnitt zeigt den Großteil der kulturellen Phrasen.

                    

                

                

                    

                    
Wie oben bereits angemerkt, stellen wir eine interaktive Visualisierung zur Datenexploration zur Verfügung, in der die oben beschriebenen Fälle nachvollzogen werden können. Die verschiedenen Dimensionsreduktionsverfahren können selbst ausprobiert und die Anzahl der Cluster variiert werden (1–15). Außerdem werden die zehn am häufigsten vorkommenden Domains je Cluster gezeigt, um einen Überblick über die Themen zu bekommen. Die Größe der Labels spiegelt die Anzahl der Vorkommen im Datensatz wider. Zudem kann durch eine Bereichsauswahl gezoomt werden (

                    

                    
).

                

            

            

                
Fazit und Ausblick

                
Unser Ansatz lenkt den Blick von den Eigennamen in Source und Target einer vossianischen Antonomasie auf den Modifier. Wir konnten zeigen, dass bestimmte Themenfelder besonders häufig sind, also eine besondere Neigung aufweisen, in einer vossianischen Antonomasie Verwendung zu finden. Die Themen wurden in Clustern gruppiert und zweidimensional projiziert. Durch verschiedene Verfahren kann das Modell noch verfeinert werden, z.B. durch den Einsatz anderer Cluster- oder Reduktionsverfahren. 

                
Mit Hilfe von Entity Embeddings kann man in Zukunft ähnliche Analysen der Source und des Targets durchführen, um etwa auf Zusammenhänge zwischen den einzelnen Teilen einer vossianischen Antonomasie zu fokussieren. So würde sich zum Beispiel erforschen lassen, in welchen semantischen Abhängigkeiten Source, Target und Modifier eines VA-Ausdrucks zueinander stehen und welche Entitäten signifikant häufig mit welchen Modifier-Gruppen genutzt werden.

                
Mit Hilfe der Web-App kann man die Daten und Ergebnisse interaktiv explorieren und somit weitere Erkenntnisse erlangen, welche für die automatische Erkennung, aber auch für die automatische Generierung sinnvoller vossianischer Antonomasien eine wichtige Rolle spielen können. Beide Aufgaben verfolgen wir in Zukunft.

            

        

            

                
Thematische Einordnung

                
Offene und frei zugängliche digitale Publikationen sind die Grundbedingung für einen globalen und fairen wissenschaftlichen Austausch und Erkenntniszuwachs. Diesen Grundsätzen von Open Science (vgl. Bartling und Friesike (Hg.) 2014) steht das an maximal ökonomischer Ausschöpfung orientierte Geschäftsmodell global agierender Verlagskonzerne in der Regel entgegen. So wird durch restriktive Zugangsmöglichkeiten zu aktuellen wissenschaftlichen Publikationen oder kommerziell orientierte Geschäftsmodelle eine wachsende Ungleichheit zwischen den wissenschaftlichen Playern 
                    
Forschende
, 
                    
Bibliotheken
 und 
                    
Verlage
 geschaffen (vgl. z.B. Lauer 2022). Open Access (OA) kann eine wichtige Antwort auf diese ökonomisch bedingte Schieflage im Wissenschaftssystem sein. Die verschiedenen Ausformungen, Bedingungen und Möglichkeiten des offenen digitalen Publizierens sollen im Rahmen des Workshops der AG ›Digitales Publizieren‹ (vgl. DHd 2022) auf der DHd-Konferenz 2023 thematisiert werden. Eine solche klärende Auseinandersetzung erscheint auch angesichts der teils verwirrenden Vielfalt der Open-Access-Modelle sinnvoll. Folgende Fragen dienen als Leitfaden durch den Workshop: Was ist heute bei einer digitalen OA-Publikation zu beachten? Welche rechtlichen und technischen Mindestanforderungen sollten erfüllt werden und welche Spezifika und Standards gelten für digitale Publikationen? Welche Fallstricke sind bei Open Access-Publikationen zu beachten? Was sind Hürden für (Nachwuchs-)Wissenschaftler*innen OA zu publizieren?
                

                
Besonderer Fokus wird auf die Frage gelegt, wie eine ›perfekte digitale Publikation‹ aussehen könnte und welche Strategien zu einer erfolgreichen digitalen Publikation in unterschiedlichen Szenarien führen. Im heterogenen Feld der digitalen Publikationen gibt es selbstverständlich nicht die eine Mustervorlage, an der sich alle messen müssen. Stattdessen sollte auf die Einhaltung bestimmter Qualitätsstandards – bezogen auf eine bestimmte OA-Publikation und deren Einsatzzweck – hingearbeitet werden. Die Gemeinsamkeiten aller digitalen OA-Publikationen herauszuarbeiten und zu diskutieren soll demnach ein wesentlicher Bestandteil des offenen Workshops sein. Die sachorientierte Diskussion wird durch eine kurze Einführung in das Thema durch Mitglieder der DHd-AG ›Digitales Publizieren‹ begleitet, sodass für Forschende aus allen Disziplinen der (Digital) Humanities, insbesondere auch Nachwuchswissenschaftler*innen, eine gemeinsame Diskussionsgrundlage geschaffen wird. Die Ergebnisse des 2021 überarbeiteten und neu veröffentlichten Arbeitspapiers der AG sollen dabei als Rahmen der Diskussion dienen sowie community-intern kritisch hinterfragt werden (vgl. AG Digitales Publizieren 2021).

                
Im Arbeitspapier der AG werden im Unterkapitel 3.2 Qualitätskriterien für eine Veröffentlichung vorgestellt, die auch Grundlage der thematischen Arbeit im Workshop sein werden:



                    
Die Leitlinien guter wissenschaftlicher Praxis werden beachtet.

                    
Nutzungsbedingungen der Publikation sind geklärt (z. B. durch Creative-Commons-Lizenzen; vgl. Creative Commons 2022). 

                    
Open-Access-Empfehlungen wissenschaftlicher Institutionen oder des Open-Access-Netzwerks (vgl. Open Access Network 2022a) werden beachtet. Hier wird besonders auf die Berliner Erklärung (vgl. Max Planck Gesellschaft 2003) und die Vor- und Nachteile des Grünen vs. Goldenen Weges eingegangen (vgl. Open Access Network 2022b).

                    
Peer Review als Notwendigkeit der Qualitätssicherung: Blind or non-blind peer review? (vgl. AG Digitales Publizieren 2021, Abschnitt 4).

                

                
Ziel der Diskussion ist, diese Kriterien stärker ins Bewusstsein der Teilnehmenden zu bringen und gleichzeitig deren Notwendigkeit und Sinnhaftigkeit für digitale OA-Publikationen kritisch zu diskutieren. Technische Spezifikationen digitaler OA-Publikationen sind Geisteswissenschaftler*innen häufig weniger bewusst als die technischen Eigenschaften traditioneller Publikationen, insbesondere außerhalb der digitalen Geisteswissenschaften. Dazu werden im Workshop erstens die Dateiformate diskutiert, die sich besonders für eine Online-Veröffentlichung sowie die langfristige Archivierung eignen [PDF/A-Format (ISO 19005-1:2005)]. Zweitens thematisieren wir, warum digitale OA-Publikationen wie andere digitale Objekte mit einem Persistent Identifier (DOI, URN) versehen werden sollten. Idealerweise sollten auch die Autor*innen über einen Persistent Identifier wie eine ORCID in den Werken aufgeführt werden und damit referenzierbar in Erscheinung treten. Drittens eröffnen wir die Möglichkeit, über die Notwendigkeit der Verwendung internationaler Standards wie z. B. METS/MODS, EDM oder Dublin Core für die Erschließung, Speicherung und Archivierung der digitalen Objekte zu sprechen. Viertens sollte in diesem Zusammenhang unbedingt das DINI-Zertifikat für Open-Access-Publikationsdienste, insbesondere der entwickelte Kriterienkatalog, beachtet werden (Müller et al. 2019).

                
Schließlich ist zu diskutieren, welche finanziellen Unterstützungsmöglichkeiten es für Verlage gibt. Was sind Beispiele, Erfahrungen oder Herausforderungen, mit einem Verlag (und einem institutionellen Repositorium) Open Access (CC BY und CC BY-SA) zu publizieren? Gehört die Wahl des Verlages und die Finanzierung mit zu den Kriterien einer ›guten‹ OA-Publikation? Neben diesen übergreifenden Fragen soll der Workshop auch Raum für das Weiterdenken des digitalen Publizierens bieten. Ein Angebot innerhalb des Workshops besteht daher in der Diskussion von Kriterien für hybride Publikationsformen am Beispiel von Monographien und Sammelbänden, die sowohl in digitaler als auch in gedruckter Form erscheinen. Der in Kürze erscheinende Leistungskatalog des Projekts AuROA vereint sowohl Aufgaben im Bereich digital enhancement als auch verschiedene Formen der inhaltlichen und prozessualen Qualitätskontrolle, z.B. durch Angaben zur Offenheit/Geschlossenheit der Begutachtung, zum Status der Reviewenden und zur Erfüllung bestehender Kriterien (u. a. DFG-Kodex, COPE, FAIR-Prinzipien, PRISM etc.). Durch die Einhaltung solcher Kriterien kann u.a. der Gefahr von Predatory Publishing durch Transparenz begegnet werden.

            

            

                
Beitragende (und Forschungsinteressen)

                

                    
Constanze Baum 
ist wissenschaftliche Mitarbeiterin am Institut für deutsche Literatur an der Humboldt-Universität zu Berlin. Sie baute von 2014–2017 die Zeitschrift für digitale Geisteswissenschaften (ZfdG) als vollwertiges Open Access Journal auf. Ihre Forschungsinteressen richten sich u.a. auf Fragen des digitalen Publizierens für die Literaturwissenschaft und die Rolle der Digital Humanities für ihre Fachdisziplin. 
                

                

                    
Michael Dahnke
 hat 2017/2018 als Dozent für Digitalisierungskompetenz an der Universität Würzburg gearbeitet und war 2018/2019 für das Forschungsdatenmanagement im SFB 1187 der Universität Siegen verantwortlich. Als digitaler Editionsphilologe hat er sich seit 2017 in mehreren Editionsprojekten für die technische Koordination verantwortlich gezeichnet.
                

                

                    
Patrick Dinger
 arbeitet seit 2020 an der Universitäts- und Landesbibliothek Münster/Universität Münster und ist als Referent für das digitale Service- und Sammlungsmanagement zuständig. Der studierte Historiker interessiert sich u.a. für die Digitalisierung und die digitale Präsentation von Kulturerbe, Standards, Infrastrukturentwicklung sowie digitale Publikationsformen.
                

                

                    
Yuliya Fadeeva
 ist seit 2020 im Bereich Open Access (in den Geisteswissenschaften) an der Universitätsbibliothek Duisburg-Essen tätig. Zurzeit ist sie wissenschaftliche Mitarbeiterin im Projekt AuROA. Ihre Open-Access-bezogenen Forschungsinteressen liegen im Bereich des Qualitätsbegriffs (wissenschaftlicher Arbeiten/wissenschaftlichen Arbeitens) im wissenschaftssoziologischen und -theoretischen Kontext und der Implikationen des digitalen Publizierens für die Wissenschaftspraxis.
                

                

                    
Jan Horstmann
 leitet das Service Center for Digital Humanities an der ULB der Westfälischen Wilhelms-Universität Münster. Seine Forschungsinteressen und -schwerpunkte liegen im Bereich der digitalen Methodologie mit besonderem Fokus auf die Textannotation, -analyse und Visualisierung im Bereich der computationellen Literaturwissenschaft. Infrastrukturell setzt er sich ein für die Einhaltung der FAIR- und CARE-Prinzipien einer nachhaltigen und offenen Wissenschaft.
                

                

                    
Melanie Seltmann
 arbeitet seit 2021 im Zentrum für digitale Editionen der ULB Darmstadt und ist dort für das Citizen-Science-Projekt Gruß &amp; Kuss sowie für das NFDI-Konsortium Text+, Task Area Editions zuständig. Die studierte Linguistin interessiert sich u.a. für die Bereiche Citizen Science, Wissenschaftskommunikation, Open Science, Standards sowie insbesondere für Annotationen.
                

                

                    
Timo Steyer
 leitet das Referat Informationskompetenz an der Universitätsbibliothek Braunschweig und ist Fachreferent für die Fächer Anglistik, Germanistik und Geschichte. Zu seinen Forschungsinteresse zählen neben dem digitalen Publizieren vor allem die Bereiche Datenmodellierung und Metadaten sowie aktuelle Entwicklungen in der Wissenschaftskommunikation.
                

            

            

                
Format und Zeitplan

                
Der Workshop soll als interaktives Diskussionsformat mit Gruppenarbeitseinheiten insgesamt 4 Stunden dauern, die folgendermaßen strukturiert sein werden:

                
0–0:15: Vorstellung der AG ›Digitales Publizieren‹ und der Workshopteilnehmenden

                
0:15–0:30: Vorstellung des AG Working Papers

                
0:30–0:45: Eingrenzung des Themas; Kurzvorstellung der Tische 

                
0:45–1:30: Bearbeitung von Einzelthemen/Teilthemen in Worldcafe-Tables

                
1:30–2:00: Pause

                
2:00–3:00: Bearbeitung von Einzelthemen/Teilthemen in Worldcafe-Tables

                
3:00–3:45: Übertragung der Gruppenergebnisse in die Gesamtdiskussion; Rückbinden/Überprüfung der Working Paper-Überlegungen

                
3:45–4:00: Ausblick

            

            

                
Zielpublikum

                
Der Workshop richtet sich an Forschende aus allen Disziplinen der (Digital) Humanities, insbesondere auch Nachwuchswissenschaftler*innen, die sich aufgrund des systembedingt notwendigen Renommee-Erwerbs häufig gezwungen sehen, in teuren und nicht offen zugänglichen Publikationsorganen zu publizieren, und die alternative Publikationsmöglichkeiten wie in Bibliotheksverlagen oder scholar-led Publikationsorganen häufig nicht in Betracht ziehen. Grundvoraussetzung für die Teilnahme ist lediglich das Interesse an zeitgemäßen und zukunftsweisenden Formen des digitalen Publizierens im Sinne der Open Science. Es werden keine besonderen Kenntnisse vorausgesetzt. Möchte man sich vorab näher in die Materie einarbeiten, böte es sich an, das Workingpaper der AG ›Digitales Publizieren‹ (2021) zu konsultieren. Es können bis zu 25 Personen am Workshop teilnehmen.

            

            

                
Lernziele

                
Nach diesem Workshop sind die Teilnehmenden sensibilisiert für Möglichkeiten des Open-Access-Publizieres und Möglichkeiten, selbst das richtige Publikationsorgan für ihre zukünftigen Veröffentlichungen zu finden. Sie haben einen Überblick über verschiedene Publikationsorgane und deren Umgang mit Open-Access. Zudem werden sie darin unterstützt, selbstbewusst Verlagen gegenüberzutreten und ihre Bedingungen, Open Access zu publizieren, zu vertreten. 

            

            

                
Benötigte technische Ausstattung

                
Benötigt werden ein Beamer, Moderationskoffer, offene Bestuhlung und Tische.

            

        

            

                
Einleitung und Methode

                
Digitale Wissenschaftskommunikation und die Möglichketen der sozialen Medien werfen in den letzten Jahren vermehrt wieder Fragen der klassischen Forschung zu Wissenschaftskommunikation auf (vgl. Franzen 2019, 616). Der vorliegende Beitrag schließt hier für das Konzept der 
                    
Public Humanities
 an. Damit ist auch klar, dass nicht jegliche Form der Wissenschaftskommunikation betrachtet werden kann. Im Laufe des Beitrages wird herausgearbeitet, dass es sich bei 
                    
Public Humanities
 nicht um eine traditionelle Form der Wissenschaftskommunikation handelt, wie etwa die Veröffentlichung eines (populär-)wissenschaftlichen Beitrages oder das Halten eines (populär-)wissenschaftlichen Vortrages, sondern um eine moderne, digitale, in der durch moderne Medienformen die 
                    
One-to-Many-Communication
 zu einer 
                    
Many-to-Many-Communication
 wird und der Diskurs damit interaktiv und rekursiv wird (vgl. Bucher 2019, 64). Damit einhergehend wird die Wissenschaftskommunikation von einem öffentlichen Bewusstsein für Wissenschaft zu einem bürgerlichen Engagement verschoben, die Kommunikation wird zum Dialog (vgl. Bucchi and Trench 2014, 4).
                

                
Nach einer Definition der Konzepte 
                    
Public Humanities
 sowie 
                    
Public Digital Humanities
 folgt eine Betrachtung verschiedener Arten und Dimensionen von Wissenschaftskommunikation. Hierbei fokussiere ich mich in diesem Beitrag auf den europäischen Raum. In den USA findet das Thema 
                    
Public Humanities
 schon länger Betrachtung als in Europa, ebenso wie die Forschung zu Wissenschaftskommunikation allgemein ausgeprägter ist (vgl. Schäfer et al. 2019, 79), dieser Aspekt soll hier jedoch ausgeklammert werden und für die weitere Forschung aufgehoben werden. In einem dritten Schritt soll das Konzept der 
                    
Public Humanities
 in die Wissenschaftskommunikationsmatrix nach Frick et al. (2021) eingebettet werden und damit herausgearbeitet werden, wer in den 
                    
Public Humanities
 agiert und zu welcher Form von Wissenschaftskommunikation sie gehören. An diese theoretischen Betrachtungen schließt sich ein Kapitel zum Mehrwert von 
                    
Public Humanities
 für verschiedene Zielgruppen und in Hinblick auf verschiedene Aktivitäten an. Daraufhin werden Handlungsempfehlungen für möglichst erfolgreiche 
                    
Public Humanities
 gegeben. Der Beitrag schließt mit einem Resümee.
                

            

            

                
Definitionsversuche #PublicDH

                
Zu Beginn möchte ich den Terminus 
                    
#PublicDH
 versuchen zu definieren. Wie bei so ziemlich jedem Terminus gibt es auch hier reichlich Möglichkeiten der Definition. Heinisch (2021) gibt einen umfassenden Überblick über den Unterschied zwischen 
                    
Public Humanities
, 
                    
Public Digital Humanities
 und 
                    
Citizen Humanities
. Aus ihren Ausführungen zu den 
                    
Public Humanities
 kann man zusammenfassen, dass ein wesentlicher Bestandteil ist, die Geisteswissenschaften aus ihrem universitären Elfenbeinturm herauszuheben und zum einen der Öffentlichkeit näher zu bringen, sie aber gleichermaßen auch in die Forschung einzubinden. Damit einher geht das Schaffen von neuen Formaten der Wissenschaftskommunikation sowie die Bidirektionalität selbiger. Es ist nicht mehr allein der:die Wissenschaftler:in, der:die kulturelle Artefakte untersucht, sondern sie werden in Gemeinsamkeit mit Bürger:innen reflektiert und interpretiert. Jacobson (2020) führt hierfür beispielsweise die Form des Story-Tellings an (vgl. Jacobson 2020, 165–169).
                

                
Anfangs gingen die 
                    
Public Humanities
 beinahe missionarisch vor, wenn sie die Bevölkerung belehrten und ihre eigenen Aktivitäten rechtfertigten (vgl. Gibbs 2016, 1). Heutzutage steht jedoch viel mehr der Wunsch, durch Zusammenarbeit etwas zu verändern und in der Gesellschaft zu bewirken (vgl. Miller et al. 2017). Zudem werden Disziplingrenzen überwunden und interdisziplinär etwas Größeres vermittelt und erarbeitet. Es handelt sich also um bidirektionale und disziplinübergreifende Wissenschaftskommunikation.
                

                
Im Unterschied dazu erweitern die 
                    
Public Digital Humanities 
(oder kurz 
                    
#PublicDH
) die 
                    
Public Humanities
 durch eine digitale Komponente: die vermittelte Wissenschaft wird mit Hilfe von digitalen Methoden durchgeführt. Dies betrifft sowohl die originär von Wissenschaflter:innen durchgeführte Forschung als auch dialogisch mit Bürger:innern erzeugte Forschung. Zudem kann auch die gemeinsame (Weiter-)Entwicklung dieser digitalen Methoden Teil der 
                    
#PublicDH
 sein (vgl. Heinisch 2021). Damit sind die 
                    
#PublicDH 
die Art der 
                    
Public Humanities
, die eine spezielle Form der #WissKomm für die Disziplin(en) der 
                    
Digital Humanities
 aufbereiten. An der Grenze der 
                    
#PublicDH
 liegen Wissenschaftskommunikationsformate, in denen nicht-digitale Wissenschaft durch digitale Formate für die Vermittlung aufbereitet wird. Genau genommen handelt es sich bei dieser Form der #WissKomm eher um 
                    
Digital Public Humanities
 denn um 
                    
#PublicDH
.
                

            

            

                
Arten und Dimensionen der Wissenschaftskommunikation

                
Wissenschaftskommunikation ist jedoch nicht gleich Wissenschaftskommunikation. Dafür möchte ich im Folgenden genauer betrachten, welche Formen von Wissenschaftskommunikation oder kurz #WissKomm es gibt. 

                

                    
Kommunikationsmatrix

                    
Meistens wird hier vor allem zwischen interner und externer #WissKomm unterschieden. Die interne #WissKomm, auch 
                        
Scholarly Communication
 genannt, richtet sich an die wissenschaftliche Community, häufig der eigenen Disziplin. Die externe #WissKomm adressiert dahingegen Nicht-Spezialist:innen (vgl. Könneker 2017, 454). Die Definition greift hier jedoch zu kurz. Für eine Spezifizierung verschiedener Wissenschaftskommunikationsformen sollte nicht nur der:die Adressat:in betrachtet werden, sondern auch der:die Sender:in. Folglich spannt sich hier eine Matrix auf (vgl. Figure 1).
                    

                    

                        

                        

                            
Figure 1: Matrix Wissenschaftskommunikation (Verkürzung von Frick et al. 2021)

                        

                    

                    
Das orange hinterlegte Feld (
                        
Science-to-Science
) repräsentiert die interne, die blau hinterlegten Felder (
                        
Science-to-Public
, 
                        
Public-to-Science
, 
                        
Public-to-Public
) die externe #WissKomm. Externe #WissKomm kann folglich viel mehr als Kommunikation, die nicht intern innerhalb einer Wissenschaftsdisziplin oder allgemein innerhalb der Wissenschaft erfolgt, verstanden werden. Frick et al. (2021) bestimmen externe #WissKomm genauer in drei Bereiche und setzen sie in den Kontext von traditionellen Veröffentlichungsformen und 
                        
Citizen Science
. 
                        
Science-to-Public-Communication
 ist die „normale“ externe Wissenschaftskommunikation oder auch 
                        
Science Communication
, obgleich sie nicht nur in den STEM-Fächern (Naturwissenschaften, Technik, Ingenieurswissenschaften, Mathematik) vorhanden ist, sondern auch in den SSH (Sozialwissenschaften, Geisteswissenschaften). Die Bereiche 
                        
Public-to-Science
 und 
                        
Public-to-Public
 betreffen den Bereich 
                        
Citizen Science
. In der 
                        
Public-to-Science-Communication
 liefern Bürger:innen Erkenntnisse an die Wissenschaft. Zudem gehört in diesen Bereich auch das Einbringen von Forschungsfragen durch Bürger:innen. Hierdurch wird für die Allgemeinheit relevante Forschung besonders sichtbar. Den Bereich der 
                        
Public-to-Public-Communication
 bezeichnen Frick et al. (2021) als 
                        
Citizen-Science-Communication
. Hier wird dem Rechnung getragen, dass Bürger:innen sich in 
                        
Citizen-Science
-Projekten nicht nur der Forschung beteiligen, sondern selbst als Multiplikator:innen fungieren und von ihrer Forschung und ihren Ergebnissen, seien sie positiv oder negativ, erzählen. Damit umfassen Frick et al. (2021) mit ihrem Begriff von 
                        
Citizen-Science-Communication
 nur einen Teil der Kommunikationsformen, die anderswo inhärent wären. Häufig werden auch die 
                        
Science-to-Public-Communication
 sowie die 
                        
Public-to-Science-Communication
 mit in den Bereich Citizen Science gezählt.
                    

                    
Schumacher (2021) spricht sogar von mindestens fünf kommunikativen Schnittstellen in den digitalen Geisteswissenschaften: 
                        
Humanities-to-Public
 (Wissenschafts-PR), 
                        
Humanities-to-Media
 (Wissenschaftsjournalismus), 
                        
Public-to-Humanities
 (Citizen Science), 
                        
(Digital-)Humanities-to-(Digital-)Humanities
 (Kommunikation in der eigenen Community) sowie 
                        
Digital-Humanities-to-non-Digital-Humanities
. Wobei die Zuordnung zu Frick et al. (2021) nicht eins zu eins gelingt. Schumacher (2021) lässt die 
                        
Public-to-Public-Communication
 außen vor, ergänzt dafür jedoch die 
                        
Humanities-(oder Science-)to-Media-Communication
 sowie die 
                        
Digital-Humanities-to-non-Digital-Humanities-Communication
, also die disziplinenübergreifende #WissKomm bzw. die #WissKomm zwischen Wissenschaftler:innen verschiedener Methoden. Möchte man Schumachers kommunikative Schnittstellen in die Wissenschaftskommunikationsmatrix nach Frick et al. (2021) einordnen, so wäre die 
                        
Humanities-to-Media-Communication
 eine Sonderform der 
                        
Science-to-Public-Communication
. Bucher (2019) führt aus, dass es bei der Wissenschaftskommunikation, die über Wege des Journalismus vermittelt wird, um die traditionelle Form der 
                        
Science-to-Public-Communication
 handelt, die digitalen Medien jedoch Möglichkeiten für eine direkte 
                        
Science-to-Public-Communication
 schaffen (vgl. Bucher 2019, 64). Bei der 
                        
Digital-Humanities-to-non-Digital-Humanities-Communication
 handelt es sich um eine Sonderform der 
                        
Science-to-Science-Communication
.
                    

                

                

                    
Dimensionen

                    
Alle Bereiche der #WissKomm haben ihre ganz eigenen Herausforderungen und je nach Form müssen fünf der sechs Dimensionen der Wissenschaftskommunikation (Frick et al. 2021) anders bedacht werden. Die erste Dimension (
                        
Inhalt
) ist in allen Formen der #WissKomm gleich (für einen spezifischen Kommunikationsanlass). Im nächsten Schritt entscheidet man, welche 
                        
Zielgruppe
 man ansprechen möchte (Dimension 2). Hier unterscheidet sich die folgende #WissKomm in die verschiedenen Formen. Die weiteren Dimensionen passen sich an die getroffene Entscheidung an und stehen auch untereinander in enger Beziehung. Ändert sich eine der Dimensionen, so müssen auch die anderen angepasst werden. Mit der dritten Dimension wird der verwendete 
                        
Stil
 betrachtet, in der vierten Dimension wird sich für ein adäquates 
                        
Format
 entschieden. Wichtig ist auch zu betrachten, welche 
                        
Motivation
 man bei der #WissKomm hat, also warum man etwas vermitteln möchte (Dimension 5) und schließlich sollte man sich vor dem Kommunikationsakt klar werden, welche 
                        
Rolle
 man selbst spielen möchte (Dimension 6). 
                    

                    
Auch das NaWik gibt mit ihrem NaWik-Pfeil (vgl. Könneker 2017, 468) Hilfestellung für die Planung von Wissenschaftskommunikation und rät dazu, fünf Dimension im Vornhinein zu beachten. Vier davon decken sich mit den eben beschriebenen sechs Dimensionen. Als erstes ist die Dimension 
                        
Thema
 zu beachten, die der Dimension 
                        
Inhalt
 entspricht. Daraufhin soll über den 
                        
Stil
 (Dimension 2) nachgedacht werden, der bei Frick et al. (2021) erst im dritten Schritt betrachtet wird. Als dritte Dimension wird hier nun das 
                        
Medium
 reflektiert, was dem 
                        
Format
 entspricht. Erst im vierten Schritt wird bei NaWik über die 
                        
Zielgruppe
 nachgedacht. Schließlich führt NaWik als fünfte Dimension das 
                        
Ziel
 selbst an, was ähnlich wie die 
                        
Motivation
 bei Frick et al. (2021) zu verorten ist. Der Unterschied ist also abgesehen von der Reihenfolge einiger Dimensionen, dass Frick et al. (2021) die Frage nach der Rolle, die die kommunizierende Person einnimmt, aufwirft.
                    

                

            

            

                
Einbettung des Konzepts #PublicDH in die Kommunikationsmatrix

                
Doch wie lässt sich das zuvor definierte Konzept 
                    
#PublicDH
 in die eben aufgestellte Kommunikationsmatrix einbetten? Da ich als einen zentralen Punkt die Einbindung der Öffentlichkeit definiert habe, gehören die Bereiche 
                    
Science-to-Public
 und 
                    
Public-to-Science
 zu den 
                    
#PublicDH
. Sowohl die verständliche Vermittlung der Forschung als auch der Dialog darüber sind wesentlicher Bestandteil der 
                    
#PublicDH
. Auch ist es relativ selbstverstehend, dass der Bereich 
                    
Science-to-Science
 nicht eingeschlossen ist in die 
                    
#PublicDH
. Problematischer ist die Bestimmung, ob der Bereich 
                    
Public-to-Public-Communication
 Teil der 
                    
#PublicDH
 ist oder nicht. Zwar könnte man argumentieren, dass ebenso wie bei 
                    
Citizen-Science
-Projekten auch in den 
                    
#PublicDH
 die Multiplikator:innenrolle der Bürger:innen wichtiger Bestandteil ist, jedoch plädiere ich dafür, den Bereich aus den 
                    
#PublicDH
 auszuklammern. Nach der oben aufgestellten Definition sind 
                    
#PublicDH
 die Kommunikation von Wissenschaft und Öffentlichkeit über Forschungsinhalte und -methoden der digitalen Geisteswissenschaften. Die Wissenschaft ist also den 
                    
#PublicDH
 inhärent. Folglich kann die 
                    
Public-to-Public-Communication
 nicht Teil der 
                    
#PublicDH
 (zumindest im engen Sinne) sein (vgl. Figure 2). Hierin liegt auch die Unterscheidung zur externen #WissKomm. Die Wissenschaft muss ein Agens sein und bleiben, das ist bei der #WissKomm nicht der Fall. 
                

                

                    

                    

                        
Figure 2: Matrix #WissKomm und #PublicDH (Erweiterung von Frick et al. 2021)

                    

                

            

            

                
Mehrwert #PublicDH

                
Nachdem ich definiert habe, was 
                    
#PublicDH
 sind und wie sie sich in die Wissenschaftskommunikationsmatrix einbetten lassen, betrachte ich in diesem Abschnitt, welchen Mehrwert 
                    
#PublicDH 
eigentlich bringen. Betrachtet werden dafür unterschiedliche Gruppen: zum einen der Mehrwert für die Wissenschaftler:innen, zum anderen für die Öffentlichkeit.
                

                
Hierzu sei erst einmal allgemein die Frage nach dem Mehrwert von #WissKomm gestellt. Fröhlich (1994) führen drei potentielle Mehrwerte auf, die jedoch vor allem auf interne #WissKomm zutreffen: Zum einen fördert #WissKomm die Motivation und lässt neue Ideen aufkommen, Mehrfacherfindungen werden vermieden, aber dafür Synergieeffekte erzeugt und schließlich führt die #WissKomm zu einer Form der Qualitätskontrolle und damit auch zur Selektion von Forschung (vgl. Fröhlich 1994, 7). Zudem bezeichnet Fröhlich als Mehrwert von #WissKomm den „errungene[n] Kredit auf wissenschaftliche Glaubwürdigkeit“ (Fröhlich 1994, 8) von Individuen, Gruppen und Institutionen im Bourdieuschen Sinne. 

                
Auch für die 
                    
#PublicDH
 ist die Qualitätskontrolle sicherlich ein großer Mehrwert. Hinzu kommt die Anwendbarkeit der eigenen Forschung(sergebnisse) auch in alltäglichen Situationen. Durch die Kommunikation über Forschung, Methoden und Ergebnisse wird Wissenschaft nicht nur aus dem Elfenbeinturm gehoben und erfahrbar gemacht, sondern es kommt zu einer Responsion, die die Wissenschaft wiederum bestärken und zu relevanter Forschung von Bedeutung lenken kann. Dadurch kann wiederum Motivation geschaffen und neue Ideen und Ansätze gefunden und ausprobiert werden.
                

                
So 
                    
#PublicDH
 innerhalb der sozialen Medien betrieben werden, können zudem Mehrwerte aus dem speziellen Medium erzeugt werden. "Für Wissenschaftler stehen hier die Vernetzungsmöglichkeiten im Vordergrund. Wissenschaftsjournalisten nutzen sie primär als Rechercheinstrument, Laien zur Informationsbeschaffung." (Weitze und Heckl 2016, 191) Diese Verknüpfung verschiedener Benutzungsgründe eines Mediums können in Verbindung der kommunikativen Interaktionsmöglichkeiten hervorragend für #WissKomm und auch 
                    
#PublicDH
 verwendet werden. Für den speziellen Fall von Twitter führen Geyer und Gottschling (2019) an, dass die Wissenschaftskommunikation hier „den fachinternen Dialog befördern und zugleich Erkenntnisse des Fachs der Öffentlichkeit zugänglich machen, über Methoden oder über aktuelle fachwissenschaftliche Diskussionen und Projekte informieren und auf vielfältige Weise den unmittelbaren Austausch mit zumindest Teilen der Gesellschaft erleichtern [kann].“ (Geier und Gottschling, 2019, 284) An wenig anderen Orten als in den sozialen Medien kommen die verschiedenen Akteur:innen zusammen, um ihre jeweiligen Bedürfnisse der Informationsbeschaffung und des Informationsaustauschs zu stillen. 
                

                
Zusätzlich kann als Mehrwert gesehen werden, dass die eigene Person bekannter wird und dass Wissenschaftskommunikation Spaß macht (vgl. Ziegler/Fischer 2020, 7).

                
All diese Mehrwerte beziehen sich jedoch auf die Wissenschaftler:innen selbst. Es stellt sich jedoch die Frage, ob es 
                    
#PublicDH
 auch einen Mehrwert für die Öffentlichkeit erzeugt. Der gesellschaftliche Mehrwert von Wissenschaftskommunikation findet bisher wenig Beachtung in der Forschung (vgl. Siegel/Dunkel/ Terstriep (2021). Eine der raren Untersuchungen, die auch den gesellschaftlichen Mehrwert adressiert, ist die Umfrage von Ziegler/Fischer (2020) zu Zielen von Wissenschaftskommunikation. Als gesellschaftliche Mehrwerte werden eine Steigerung der Demokratiefähigkeit sowie die Stärkung der Wissensgesellschaft genannt (vgl. Ziegler/Fischer 2020, 7). Ziegler/Fischer (2020) führen zudem an, dass ihre Untersuchungen ergeben haben, „dass Politik und Öffentlichkeit in ihren Entscheidungen stärker auf wissenschaftliche Erkenntnisse zurückgreifen“ (Ziegler/Fischer 2020, 15). Sie zeigen, dass „Forschung klar als Triebkraft gesellschaftlicher Weiterentwicklung und Innovation gesehen [wird]“ (Ziegler/Fischer 2020, 16). Des weiteren wird eine Ausbildung der 
                    
scientific literacy
 sowie die eigene Mündigkeit durch die Möglichkeit auf wissenschaftliches Wissen zuzugreifen als Mehrwert herausgearbeitet. Nichtsdestotrotz ist hier noch ein weites Feld für weitere Untersuchungen.
                

            

            

                
Konsequenzen und Handlungsempfehlungen

                
Was für Konsequenzen können nun aus den Mehrwerten der 
                    
#PublicDH
 gezogen werden und welche Handlungsempfehlungen für digitale Geisteswissenschaftler:innen ergeben sich daraus? Zum einen sollte erkannt werden, welches Potential in den 
                    
#PublicDH
 liegt, und überlegt werden, wie dieses Potential für die eigene Forschung genutzt werden kann.
                

                
Der Dialog mit anderen Wissenschaftler:innen, aber auch mit der Öffentlichkeit kann das Weiterkommen in der eigenen Arbeit bestärken. Durch Diskussionen und Fragen können sich über die relevanten Säulen ihrer Forschung bewusster werden und den Kern ihrer Forschung genau durchdenken. Insofern kann die Forschung von 
                    
#PublicDH
 nur profitieren.
                

                
Wichtig ist jedoch, die verschiedenen Dimensionen (s. Kapitel 2.2) zu beachten und entsprechend anzuwenden, damit #WissKomm gelingen kann. Es ist keine Schande, sich für einzelne (oder auch alle) Dimensionen Hilfe zu holen und diesbezüglich weiterzubilden. Unterstützung kann hier beispielsweise der Referenzrahmen Wissenschaftskommunikation (Frick und Seltmann, in Vorbereitung) bieten. Es lohnt auch über den wissenschaftlichen Rahmen hinauszublicken und sich in den Bereichen Marketing und Journalismus Hilfestellungen zu holen. Zwar kann es auch mit gut ausgearbeiteten Dimensionen dazu kommen, dass der Dialog nicht funktioniert, aber dies sollte nur selten der Fall sein, insofern ein echtes Interesse des Austauschs besteht. Wichtig ist zudem zu beachten, dass #WissKomm im Allgemeinen und 
                    
#PublicDH
 im Besonderen Zeit benötigen. Allerdings ist diese Zeit sinnvoll investiert.
                

            

            

                
Resümee

                
Der Titel des Beitrags stellt die Frage, ob 
                    
#PublicDH
 mehr ist als „nur“ #WissKomm. Hierfür habe ich zuerst definiert, was 
                    
#PublicDH
 ist (Kapitel 2). Daraufhin habe ich die Kommunikationsmatrix nach Frick et al. (2021) beschrieben und mit den kommunikativen Schnittstellen von Schumacher (2021) verglichen (Kapitel 3). Einen wesentlichen Beitrag zur Beantwortung der aufgestellten Frage liefert die Einbettung des Konzepts 
                    
#PublicDH
 in die Fricksche Kommunikationsmatrix (Kapitel 4). Hierdurch wurde klar, wie der Zusammenhang zwischen #WissKomm und 
                    
#PublicDH
 ist und dass beide nicht identisch sind. Das Konzept der 
                    
#PublicDH
 ist präziser als nur #WissKomm für digitale Geisteswissenschaften, auch als externe #WissKomm für digitale Geisteswissenschaften. Denn es beinhaltet als wesentlichen Bestandteil den Diskurs zwischen Wissenschaft und Öffentlichkeit. Damit sind beide Seiten Agens und Patiens im Diskurs gleichermaßen. Nach dieser Feststellung habe ich den Mehrwert von 
                    
#PublicDH
 beschrieben (Kapitel 5) und schließlich Handlungsempfehlungen daraus abgeleitet (Kapitel 6). Mit diesem Beitrag ist die Forschung zu den 
                    
#PublicDH 
natürlich keinesfalls abgeschlossen. Zu betrachten ist einerseits, inwiefern die 
                    
#PublicDH
 überhaupt anders funktionieren als Wissenschaftskommunikation anderer Disziplinen und was Abgrenzungskriterien sind. Des weiteren wäre interessant zu untersuchen, inwiefern in den 
                    
#PublicDH
 spezifisches (methodologisches) Wissen der DH Anwendung findet. Viele weitere Fragestellungen können sich anschließen.
                

            

        

            

            
1. Einleitung

            
Die Open-Access-Transformation des wissenschaftlichen Publikationssystems hat nicht nur eine Rekonzeptualisierung des Zugangs zu Erkenntnissen sowie eine Neuausrichtung der Finanzierung wissenschaftlichen Publizierens eingeleitet; mit ihr gehen auch neue institutionelle Organisationsformen und technische Gestaltungsspielräume einher (u.a. Wissenschaftsrat 2021). Neben die Openness als Zugang treten daher weitere Szenarien der Öffnung. Gerade dort, wo Open-Access-Publikationsmedien ohne die Beteiligung etablierter Verlage gegründet werden, können die Rollen, die Logiken und die Konventionen des wissenschaftlichen Publizierens derzeit neu verhandelt und womöglich sogar neu erfunden werden. 

            
Im Folgenden berichten wir aus dem Work in Progress der Gründung eines verlagsunabhängigen Open-Access-Journals in den Digital Humanities, des 
                
Journal of Computational Literary Studies
 (JCLS).
 Der erste Call for Papers von JCLS wurde im Herbst 2021 veröffentlicht. Mit seinem ersten Rolling Issue im Herbst 2022 wird JCLS den Publikationsbetrieb aufnehmen. Anlässlich dieser Gründung möchten wir das Journal zum einen vorstellen (Abschnitt 2); zum anderen möchten wir – ausgehend von den Erfahrungen der Gründungsphase des Journals – drei Felder umreißen, auf denen sich aus unserer Sicht aktuell Entwicklungs- und Öffnungspotenziale im Publikationssystem bieten. Dabei handelt es sich um das Feld der Community (Abschnitt 3.1), das Feld des Reviews (Abschnitt 3.2) und das Feld des Workflows (Abschnitt 3.3). Damit wollen wir auch zu einem breiten Austausch über die konkrete Ausgestaltung von unabhängigen Open-Access-Journals anregen. 
            

            

            
2. Über JCLS 

            
JCLS ist dauerhaft als internationales Golden-Open-Access-Journal ohne Gebühren für Schreibende oder Lesende angelegt. Es bietet eine Publikationsplattform für Arbeiten zur Entwicklung, Anwendung und Kritik von computergestützten Ansätzen in den Literaturwissenschaften. Die Gründung der Zeitschrift erfolgte zu einem Zeitpunkt, an dem die Computational Literary Studies (CLS) im Rahmen der zunehmenden Ausdifferenzierung der Digital Humanities eine Sichtbarkeit erlangt haben. Die Zeitschrift will Forschung fördern, die das Spektrum computergestützter Methoden zur Analyse literarischer Texte und ihrer (kulturellen, sozialen, historischen, performativen) Kontexte erweitert. Sie bietet ein Forum, um den Aufbau literarischer Korpora, die Identifizierung von Besonderheiten literarischer Texte, die Domänenanpassung von Methoden, die Operationalisierung von Konzepten, die Annotation von Texten, die Evaluation von Messverfahren, die Interpretierbarkeit von Ergebnissen und deren Reproduzierbarkeit zu behandeln. JCLS will schließlich auch die Debattierbarkeit der Kernkonzepte der CLS, Computationalität und Literarizität, adressieren.

            
Institutionell ist JCLS als verlagsunabhängiges Journal ausgerichtet, das in Kooperation von drei Professuren (in der Rolle der Herausgeber:innen) mit der Universitäts- und Landesbibliothek Darmstadt (ULB) als Infrastrukturpartner betrieben wird. Die ULB sorgt für die nachhaltige Verfügbarkeit der publizierten Artikel (in PDF, XML, HTML und LaTeX) und stellt über eine Kooperation mit der gemeinnützigen Open Library of Humanities
 (OLH) das auf Python basierende Redaktionsmanagement- und Publikationssystem Janeway
 zur Verfügung. Dieses wird als Open-Source-Software von der OLH entwickelt (Eve und Byers 2018). Auch die OLH agiert als Infrastrukturpartner, nicht als Verlag.
            

            

            
3. Entwicklungspotenziale

            

            
3.1 Community

            
Die digitale Transformation von Öffentlichkeiten ermöglicht auch ein breiteres Verständnis der Akteursrolle von Publikationsmedien. Während traditionelle Publikationsorgane sich vornehmlich als Publikationsdienstleister verstehen, begreifen sich Publikationsmedien derzeit zunehmend als Community-Hubs, die auch jenseits der Publikation von wissenschaftlichen Inhalten mit ihrer Community interagieren und etwa eventbasiert Räume für die Community schaffen. Openness erweist sich hier als Praxis der Öffnung von diversifizierten Kommunikationsräumen. Mit der Gründung von JCLS wurde entsprechend (neben einem Twitter-Account) zugleich die 
                
Annual Conference of Computational Literary Studies
 ins Leben gerufen, die erstmals im Juni 2022 ausgerichtet wurde.

            

            

            
3.2 Review-Verfahren

            
Es gibt zunehmend – auch in den Digital Humanities (vgl. Burghardt et al. 2022) – Forderungen nach einer konsequenten Umstellung der wissenschaftlichen Qualitätssicherung auf Open Peer Review (vgl. Ross-Hellauer 2017). Dennoch sprechen insbesondere internationale Anerkennungs- und Gratifikationsmechanismen derzeit noch für ein Double Blind Peer Review, das als maßgeblich für die Akzeptanz von Publikationsorganen gilt. JCLS hat sich daher, einem Majoritätsvotum des Editorial Boards folgend, für ein Double Blind Peer Review-Verfahren entschieden. Zugleich integrieren wir im Rahmen der Conference-Paper-Issues eine Phase des offenen, kollaborativen Reviews in den Qualitätssicherungsprozess, die Ideen des Open Peer Review aufgreift. 

            

            
3.3 Redaktionsworkflow

            
Die technische Realisierung von Redaktionsworkflows basiert bis heute meist auf einer Logik des Up- und Downloads von Dateien, was u.a. Probleme mit der Versionierung, Hürden für die kollaborative Textbearbeitung und vermeidbaren Mehraufwand mit sich bringt. Mit JCLS haben wir erste Schritte in Richtung eines online-basierten Workflows eingeleitet, bei dem die Texte in einem webbasierten LaTeX-Editor mit Git-Anbindung auf der Grundlage eines gezielt entwickelten und zur Nachnutzung zur Verfügung gestellten LaTeX-Templates
 kollaborativ verfasst werden können. Auch den Reviewer:innen wird die Möglichkeit gegeben, anonym Änderungsvorschläge direkt in diesem Online-Dokument vorzunehmen. Projektiert wird derzeit eine Weiterentwicklung dieses Workflows, durch die eine direkte, API-basierte Kommunikation zwischen Janeway und dem LaTeX-Editor möglich werden soll.
            

        

            
Das 
                
Online-Compendium der deutsch-griechischen Verflechtungen
 (ComDeG) ist ein laufendes Forschungs- und Publikationsprojekt im Open Access, initiiert und konzeptualisiert durch das Centrum Modernes Griechenland (CeMoG) unter Federführung der Professur Neogräzistik (FU Berlin).
 Das ComDeG umfasst zum einen, in Kooperation mit dem Institut für griechisch-deutsche Beziehungen (EMES) der Nationalen und Kapodistrias-Universität Athen, wissenschaftliche Essays und Fallanalysen (Mikrogeschichten, Makrovorgänge, Metanarrative und Präsentationen)
 sowie enzyklopädische Artikel und tabellarische Biogramme zu Akteuren der deutsch-griechischen Verflechtungen,
 die die deutsch-griechische Geschichte seit dem ausgehenden 18. Jahrhundert als europäisches Aktionsfeld transnationaler Interaktionen, Interpretationen, Übersetzungen und Transfers ausloten und erschließen. Zum anderen beinhaltet das dynamisch verknüpfte Informationsangebot die CeMoG-Wissensbasis mit angereicherten Indexeinträgen zu Personen und Institutionen, Wirkungsorten, Kontaktzonen und Vermittlungspraktiken
 sowie bibliographische Sammlungen mit u.a. Forschungsliteratur zu den deutsch-griechischen Verflechtungen, zu deutsch-griechischen und griechisch-deutschen Übersetzungen,
 die ebenfalls mit allen Inhaltsbereichen des ComDeG verknüpft wurden.
            

            
Am kollaborativen Aufbau der Inhalte dieses multiperspektivischen Online-Sammelwerks ist ein breitgefächertes Netzwerk von Forscher:innen, primär aus Deutschland und Griechenland, beteiligt. Die Inhaltserstellung für das Compendium erfolgt auf der Grundlage einschlägiger Workshops, wozu Forscher:innen eingeladen werden, ihre Fachexpertise einzubringen, Fallgeschichten mit weiteren Expert:innen zu diskutieren und die wissenschaftlichen Erträge zur Veröffentlichung in das ComDeG aufzubereiten.
 Weitere Beiträge stammen von Forschenden an deutsch- bzw. griechischsprachigen Universitäten oder Forschungsinstitutionen mit einschlägiger Fachkompetenz wie etwa Instituten der Germanistik, der Neogräzistik und der Südosteuropa-Geschichte.
 Darüber hinaus bietet sich das ComDeG als geeignetes Repositorium für die (Teil-)Veröffentlichung von Forschungsergebnissen (etwa in der Form von Bibliographien, biographischen Profilen, enzyklopädischen Lemmata oder Mikropublikationen), die eine ähnliche thematische und methodische Perspektivierung vornehmen.

            

            
Ein internationaler wissenschaftlicher Beirat steuert gemeinsam mit den beiden Herausgebern des Compendiums, Prof. Dr. Miltos Pechlivanos (Freie Universität Berlin) und Prof. Dr. Alexandros-Andreas Kyrtsis (Nationale und Kapodistrias-Universität Athen) die inhaltliche Entfaltung und prüft die Qualität aller wissenschaftlichen Beiträge.
 Das ComDeG-Redaktionsteam ist hauptverantwortlich für die redaktionelle Aufbereitung und Vernetzung aller Inhalte, kuratiert und ergänzt die Datensammlungen (Bibliographie und CeMoG-Wissensbasis) und koordiniert die Übersetzung aller Compendium-Inhalte, die sowohl in deutscher als auch in griechischer Sprache veröffentlicht werden.
 Neue Inhalte werden laufend hinzugefügt.
            

            

                
Ein freizugängliches Informationsangebot und Recherchetool

                
Das ComDeG versteht sich als eine Brücke der Informationsvermittlung, der Zusammenarbeit und der Vernetzung, die darauf abzielt, eine gemeinsame deutsch-griechische Geschichtskultur zu ermöglichen. Seine Inhalte richten sich an eine möglichst breite deutsche und griechische Öffentlichkeit, die daran interessiert ist, ihr Wissen über die politischen, gesellschaftlichen und wirtschaftlichen bis hin zu wissenschaftlichen und kulturellen Verflechtungen vom ausgehenden 18. Jahrhundert bis in die jüngste Vergangenheit zu erweitern. Seit September 2020 steht der deutsch-griechischen Fachcommunity und der interessierten Öffentlichkeit auf comdeg.eu ein qualitätsgesichertes Informationsnetzwerk mit wissenschaftlichen Beiträgen zu deutsch-griechischen Verflechtungen und (bibliographischen und prosopographischen) Forschungsdaten zu deren historischen Akteuren im Open Access zur Verfügung. Die Register der CeMoG-Wissensbasis sowie die erweiterten Suchfunktionen in den bibliographischen Sammlungen unterstützen das Auffinden von passenden Inhalten im gesamten ComDeG
 und ermöglichen eine personenbezogene bzw. thematisch eingegrenzte Recherche in den bereitgestellten Publikationen.
                

                
Das ComDeG verknüpft disparates Wissen, stellt überblickende Zusammenstellungen bereit und macht fokussierte Fallstudien für neue Fragestellungen anschlussfähig. Es bildet nicht nur ein breitgefächertes Forschungsnetzwerk, das Wissenschaftler:innen aus unterschiedlichen Disziplinen in ein Gemeinschaftsprojekt zusammenbringt, sondern generiert zugleich eine öffentlichkeitswirksame Sichtbarkeit für seine Forschungserträge. Es trägt wesentlich dazu bei, das Forschungsfeld der deutsch-griechischen Verflechtungen in seiner Größe und Vielfalt zu vermessen und seinen Untersuchungsstand zu dokumentieren. Avisierte Publikums- und Nutzer:innengruppen sind nicht nur Forscher:innen, denen das ComDeG als heuristisches Forschungsinstrument und domänenspezifische Publikationsplattform dient, sondern auch Lehrende und Studierende, Jounalist:innen, Kulturarbeiter:innen und sonstige Interessierte (Soethaert 2020).

            

            

                
Offenheit als konzeptionelle Anforderung an die Entwicklung des ComDeG

                
Der öffentliche Angebotscharakter einer geisteswissenschaftlichen Online-Publikation im Open Access, wie das ComDeG, wird meistens unter vier Gesichtspunkten thematisiert bzw. konkretisiert (vgl. AG Digitales Publizieren 2021, §§ 58-68 und §§ 79-81; Kleineberg und Kaden 2017; Open Knowledge Foundation 2015; Wissenschaftsrat 2022, 38-47): Zugänglichkeit und Nachnutzbarkeit (die Online-Veröffentlichung der Compendium-Inhalte unter der Open Access-Lizenz CC BY-NC-ND 4.0),
 Auffindbarkeit (die Zitierbarkeit aller Inhaltsbereiche, der Einsatz von URIs und normierten Deskriptoren für alle interne Links, die Berücksichtigung von Suchmaschinenoptimierungs-Faktoren für wissenschaftliche Inhalte, vgl. Putnings 2017; Schilhan 2020; Schilhan, Kaier und Lackner 2021) und Gebrauchstauglichkeit (etwa durch die Bereitstellung von erweiterten Such- und Filteroptionen in allen Registern sowie die Einrichtung einer Zeitleiste als Facettensuche für die Compendium-Inhalte, vgl. Russell-Rose und Tate 2013; Tunkelang 2009).
                

                
Dem öffentlichen Start des ComDeG ging ein mehrjähriger Design- und Entwicklungsprozess voraus, in dem Offenheit als konzeptionelle Anforderung auch alle Aspekte in der informationstechnischen Konzeption des ComDeG als lebhaftes Publikationsprojekt anbelangte, sei es im Bereich der inkrementellen Inhaltserstellung und -publikation, in der Verwaltung, Anwendung und Ergänzung der Verschlagwortungsvokabulare (in der Regel mit GND-ID), oder in der Integration von fortwährend mit Zotero verwalteten bibliographischen Daten.
 Angesichts der angestrebten netzartigen Fortschreibung des Publikationsprojekts wurde schnell deutlich, dass das ComDeG sich nicht auf die Implementierung eines digitalen Äquivalents zu einer herkömmlichen Printpublikation mit vorstrukturierten Inhaltsanordnungen und festgeschriebenen Bezugnahmen einschränken ließe; vielmehr sollten in der digitalen Mediatisierung der lebhafte Charakter des ComDeG sowie die Bildung und die fortschreitende Ansammlung seiner inneren Verbindungen Rechnung getragen werden.
                

                
Der konzeptionelle und funktionelle Designprozess des ComDeG sah sich in diesem Kontext mit einer praktischen Herausforderung konfrontiert: Wie kann die Offenheit vor dem Hintergrund eines stetigen Inhaltszuwachs als konzeptionelle Anforderung längerfristig und ressourcenschonend ausgetragen werden, ohne dass neu hinzukommende Inhalte die redaktionelle Überarbeitung bestehender Informationsarrangements verursachen würden? Anders gesagt: Wie kann man im Verlauf dieses ‚Work-in-Progress‘ die Möglichkeit bereithalten, den einzelnen Beiträgen graduell mehr Kontext hinzuzufügen, ohne jeden einzelnen Beitrag neu editieren bzw. immer wieder weiter verknüpfen zu müssen? Wie können Verbindungen zu fokussierten Beiträgen angelegt werden, die (noch) nicht existieren? Und, nicht zuletzt, wie ermöglicht das ComDeG, dass lose Forschungsfäden in den dokumentierten Informationen an Komplexität und Kontext gewinnen können, indem der interessengeleitete Spürsinn anderer Forscher:innen auf anderweitige Informationscluster gelenkt wird (vgl. Krämer 1998, 79; 2007, 18-19)?

                
Mit dem Open Encyclopedia System (OES), das 2016-2020 parallel zum ComDeG am Center für Digitale Systeme der Freien Universität Berlin als standardisierte Plattform zur Erstellung, Publikation und Pflege von lemmabasierten Sammelwerken konzipiert und erstentwickelt wurde,
 bot sich die besondere Chance an, die Leitprinzipien der OES-Systemarchitektur (Modularität, Offenheit, Datenintegrität und Schnittstellen; vgl. Apostolopoulos, Schimmel und Egilmez 2017, 382) auch im Hinblick auf ein wissens- und erkenntnisorientiertes Datendesign für das ComDeG zu prüfen und produktiv zu machen. Denn durch die Beteiligung des Centrum Modernes Griechenland am Projektkonsortium für die Entwicklung des OES ergab sich die Gelegenheit, einige Module (wie etwa die Bibliographie und die Verschlagwortung) neu zu überdenken und dabei die Offenheit, anders als primär auf Open Access, Open Content, Open Licenses und Open Data ausgerichtet, auch als tragfähiges Design- und Verknüpfungsprinzip aufzugreifen.
                

            

            

                
Funktionale Leistungseigenschaften der CeMoG-Wissensbasis

                
Die Funktionalisierung und Implementierung der Offenheit macht das besondere informationstechnische Merkmal des ComDeG aus und versetzt die Plattform in die Lage, ihr Potenzial nicht nur als Medium einer domänenspezifischen Wissenskommunikation sondern vor allem auch als heuristisches Forschungsinstrument für neue kontextbezogene Wissensgenerierung zu entfalten. Denn, im Vergleich zu anderen, bereits veröffentlichten OES-Anwendungen, in denen die jeweiligen Artikel 
                    
indexiert
 werden,
 zeichnet sich das ComDeG durch die Verknüpfung 
                    
aller
 Segmente der Plattform untereinander (Bibliographie, Wissensbasis, Compendium) aus. Die CeMoG-Wissensbasis bildet das strukturelle Rückgrat des ComDeG: ihre Einträge stellen nicht nur ein offenes aber kuratiertes Verschlagwortungsvokabular zu Personen und Institutionen für die bibliographischen Sammlungen sowie für die Essays und Artikel des Compendiums bereit, sondern werden durch neu hinzukommende Inhalte stetig um 
                    
neue
 Einträge ergänzt. Darüber hinaus verstehen die Einträge der CeMoG-Wissensbasis sich als Datenverknüpfungsobjekte, die diverse Verweise auf ComDeG-Inhalte an einer Stelle darstellen und Lesepfade zwischen ihren verschiedenen Kontexten etablieren. Sie sind mit anderen Worten eigenständige Datenobjekte, die mit Attributen angereichert werden und über Relationen in Beziehung zu anderen Datenobjekten (wie etwa Artikel, Essays, Biogramme, bibliographische Datensätze) stehen.
                

                
Die Vorteile der Einrichtung solcher stabil adressierbaren und dennoch offen in ihren Verknüpfungen OES-Objekte machen sich leicht bemerkbar. Kommt ein neuer Essay oder Artikel hinzu, bildet deren Verschlagwortung 
                    
mit bestehenden Einträgen
 eine implikative und kontextuelle Beziehung zu 
                    
anderen
, bereits damit verknüpften ComDeG-Inhalten, ohne dass an der Stelle umgekehrt ein expliziter Verweis angelegt werden muss. Das Datenverknüpfungsobjekt dokumentiert und bündelt in einer übersichtlichen Darstellung alle angelegten 
                    
und
 neu hinzukommenden Verbindungen. Es erfüllt die wichtige Voraussetzung, dass Informationen zusammenkommen bzw. miteinander kombiniert werden können, um neue Erkenntnisse zu generieren. Kommt durch die Aufnahme eines neuen Essays oder Artikels auch ein 
                    
neuer Eintrag
 zu einer Person oder Institution zu Stande, wird dieser ebenfalls mit biographischen Kurzinformationen (und womöglich mit einer GND-Referenzierung) sowie mit relevanten Verweisen auf (bestehende oder neue) Einträge in den bibliographischen Sammlungen ausgestattet.
                

                
Die Einträge der CeMoG-Wissensbasis stellen folglich durch die Verschlagwortung in den jeweiligen Segmenten immer schon vernetzte Datenobjekte dar, die explizit gemachte Beziehungen abbilden und/oder lose Forschungsfäden aus den jeweiligen ComDeG-Segmenten festhalten. Gerade weil sie aus mehreren Kontexten heraus erreichbar sind und die verfügbaren Verweise jeweils an einer Stelle akkumulieren, können sie andererseits auch auf diverse Zusammenhänge hinweisen. Das ist ihre Kernleistung. Infolgedessen (und anders als in gängigen Indexierungsverfahren) verfügen diese eigenständigen OES-Objekte über die Kapazität, auf etwaige Informationslücken im Compendium hinzuweisen, z.B. wenn in den einzelnen Darstellungen zu der betroffenen Person oder Institution (noch) kein Verweis auf einen entsprechenden Artikel bzw. weiterführenden Essay vorliegt.

                

                    

                        

                        
Abb. 1 Der Eintrag zu Alexander Steinmetz in der CeMoG-Wissensbasis

                    

                    
                

                
Die zuverlässigen und beständigen Identifizierungen der CeMoG-Wissensbasis sichern dem ComDeG seinen nachhaltigen Ausbau aus redaktionstechnischer Sicht zu, ermöglichen die sukzessive Netzwerkbildung affiner Inhalte und eröffnen nicht zuletzt im Frontend nicht-lineare Navigationswege bzw. Lesepfade durch die Segmente des ComDeG. Die Einträge der CeMoG-Wissensbasis signalisieren nicht zuletzt über ihre noch ausstehenden Nachweise, dass wir nicht über ein (alles umfassendes) 
                    
Online-Compendium der deutsch-griechischen Verflechtungen
 verfügen, sondern immer daran arbeiten. Es handelt sich um ein kollaboratives Unterfangen, das ein solches Compendium als Möglichkeit verhandelt, als Perspektive weiterentwickelt und als Projekt fortschreibt, ohne den synthetisierenden Punkt jeweils erreichen oder alle Erwartungen vollumfänglich erfüllen zu können.
                

                
Das narrative Vorgehen des ComDeG orientiert sich an einem Modus fragmentarischer Geschichtsschreibung, der nach einem Montageprinzip Momente und Konstellationen der deutsch-griechischen Verflechtungen aufschließt, bestimmte Navigationspunkte (wie etwa Personen, Institutionen, Medien, Objekte, Orte, Kontaktzonen und Vermittlungspraktiken) dichteren Beschreibungen unterzieht, ohne mit den Narrativisierungen von Mikrogeschichten, Makrovorgängen und Metanarrativen ein einheitliches oder erschöpfendes Bild anzustreben (vgl. Pechlivanos 1995; Büttner und Kim 2022). Die Essays des ComDeG erzählen Geschichten (
                    
stories
) über Geschichte (
                    
history
); sie stellen als Textkorpus eine Ansammlung von einzelnen Geschichten oder Zusammenführungen dar, die keinesfalls einen abgeschlossenen Sinn ergeben.
                

                
Den originellen Beitrag, den das Centrum Modernes Griechenland mit der Integration der CeMoG-Wissensbasis in das ComDeG für die Indexierung von dynamischen Artikelbeständen mit dem Open Encyclopedia System geleistet hat, liegt einerseits in der „strukturellen Ausdifferenzierung des Publikationsobjektes in unterschiedlich verarbeitbare und aktualisierbare Teile“ (vgl. Kaden 2016, 19), andererseits in der Ausarbeitung und Implementierung eines tragfähigen Konzepts für die fortwährende Informationsorganisation und die praktische Ausgestaltung jener Leistungseigenschaften solcher Online-Sammelwerken, die sie als Werkzeug für die Forschung nutzbar machen. Das ComDeG ist weniger ein Compendium (im Sinne eines Handbuchs) als ein Vektor, ein vorwärts gerichtetes Publikationsprojekt, das Änderungen und Ergänzungen offen gegenübersteht, nicht abgeschlossen ist und nie einen fertigen, in sich abgeschlossenen Zustand erreichen wird.

            

        

            

                

                    
Fragestellung und theoretischer Rahmen
                

                
Die aus der Medientheorie stammende Vorstellung von den überlieferten Dokumenten, den Artefakten als "Wissensdingen",
 "denen zeitgenössisch zugeschrieben wurde, ein inhärentes Wissen über ihre eigene Natur zu enthalten sowie dieses erschließbar und vermittelbar machen zu können" (Müller 2020, 17), und des durch ihre Kontextualisierung sichtbar gemachten "Wissensraums" (Rheinberger 1992) soll auf Nachlässe und ihre Aufarbeitung in Form von Digitalen Editionen übertragen werden:
 Das "artefaktische" (Sahle 2017, 239) Dokument bildet das "Wissensding", es ist Träger des Wissens, des Textes im Nachlass. Es wurde von der Autorin erschaffen oder von ihr in ihre Arbeit einbezogen. Mit jedem Objekt verband sie ein Gedanke, eine Idee. Es repräsentiert ein "Stück" ihres Denkens und Arbeitens. Der Nachlass bildet 
                    
als Kontext des Denk- und Arbeitsprozesses
 den "Wissensraum", der das (Text)Werk als Ganzes verkörpert. 
                

                
Um das "Autorinnen-Werk" zu erfassen und die Dynamik des Schaffensprozesses
 aufdecken zu können, entwickelt die Dissertation eine Ontologie, die die Nachlassbestandteile als typologisch, konzeptionell, inhaltlich und strukturell relationierte Elemente beschreibt (Zangerl &amp; Pollin 2020, 125; Spadini &amp; Tomasi 2021, 1f.). Auf Modellebene wird damit ermöglicht, die Arbeitspraktiken, die Prozesse des Denkens und Schreibens, den "Laborcharakter"
 des Werks nachvollziehbar zu machen. Dabei liegt die Frage zugrunde, inwieweit Ansätze und Methoden der Wissensmodellierung (Davis et al. 1993; Flanders &amp; Jannidis 2015) dazu geeignet sind, den Nachlass als kontextualisierten "Wissensraum" zu öffnen, in dem sich Arbeitspraxis und Werk der Autorin begreifen lassen.
                

            

            

                
Positionierung in den DH: SDE zwischen Text und Daten

                
Obwohl sie mit ihrer stetigen Veränderbarkeit und Aktualität das Potential haben, als "
                    
Protokoll
 des Forschungs
                    
prozesses
" (Sahle 2010, 27) (kursiv übernommen) zu gelten, bleibt der editorische Blick auch Digitaler Editionen (SDE) der statischen, dokument-basierten Sichtweise verhaften (Van Zundert 2016, 83-106). Zur Erschließung von Nachlässen existieren für jeden Dokumenttyp eigene meist der bibliothekarisch-archivarischen Domäne entstammende Regeln,
 was wenig Raum lässt für Ansätze der Intertextualität (Broich &amp; Schulte-Middelich 1985; Spadini &amp; Tomasi 2021), der Idee, alles sei ein einziger "Text, der sich selbst permanent zitiert" (Neuhaus 2014, 236). Ein übergreifendes, semantisch differenzierendes Modell, in das alle Dokumente eingebettet wären, und damit eine Möglichkeit, ein adäquates Bild der Arbeitspraktiken zu schaffen, die zu ihrer Entstehung innerhalb des Werkkontexts geführt haben, fehlt.
                

                
Die "Transmedialisierung" verlangt nach Sahle 2017 von der SDE, ihren Schwerpunkt von der medialen Präsentation des Materials auf das "als Daten gefasste akkumulierte Wissen und das ihnen zugrundeliegende Modell als Explikation der editorischen Methode"
 zu verlagern. Diese Datafizierung (Hyvönen 2020) versucht, eine Antwort auf die relevanter werdende Frage nach der Langzeitverfügbarkeit und Nachhaltigkeit (Fritze 2019) von SDE zu geben: Angesichts der "Begrenzungen von TEI" (Sahle 2017, 247) und der Entwicklung immer neuer, lokaler, eigener Lösungen steigt die Dringlichkeit, die Daten über den Projektzeitraum hinaus "lebendig" und für menschliche und maschinelle Anwendungen nutzbar und interoperabel zu halten und damit einem Informationsverlust entgegenzuwirken (Daquino &amp; Tomasi 2015, 1f.). 
                

                
Dennoch verbleiben in der praktischen Umsetzung die projektspezifisch angepassten Modelle und (TEI/XML-)Daten häufig innerhalb der (geschlossenen) Projektdatenbank, eine übergreifend interpretierbare, graph-basierte Semantik fehlt, das Konzept des Knowledge Graphs (Rehbein 2017, 165) findet kaum Verwendung (Spadini &amp; Tomasi 2021, 1). Diese Problematik zeigt sich paradigmatisch für den Bereich der Nachlasserschließung:
 Derzeit befinden sich mehrere Ansätze in der Entwicklung,
 es fehlt jedoch an Anwendungsbeispielen,
 während das Nebeneinander vieler neu entwickelter Modelle einen zusätzlichen Aufwand der Konsolidierung erfordert. 
                

            

            

                

                    
Praktischer Ansatz
                

                
Ziel des Projekts ist es, durch die Entwicklung einer domänenspezifischen Ontologie den Nachlass Niklas Luhmanns (1927-1998)
 in einem graph-basierten Datenmodell abzubilden (Allemang &amp; Hendler 2011). Dazu werden existierende Modellierungsansätze aus den Bereichen Museum, Archiv und Bibliothek evaluiert, um die in den Nachlassdokumenten ermittelten Entitäten und Relationen adäquat zu beschreiben. Das im Projekt bereits genutzten FRBRer-Modell (Madison et al. 2009) wird auf seine Passfähigkeit überprüft und ergänzt bzw. durch Alternativen ersetzt. Im Fokus steht FRBRoo/LRM als eine an CIDOC-CRM
 angepasste FRBR-Version,
 daneben werden gängige Metadatenschemata wie DCTerms, SKOS, PRISM oder die SPAR Ontologies (Lüschow 2020, 82; Tomasi 2012) sowie spezifisch für bestimmte Fragestellungen entwickelte Ontologien,
 insbesondere außerhalb des Bibliothekskontextes,
 untersucht.
                

                
Das Vorhaben befindet sich noch in der Startphase. Ein erstes Mapping zu FRBRoo/LRM zeigt sich als prinzipiell machbar, die andauernde Entwicklung des Werkmodells im Projekt erfordert jedoch eine kontinuierliche Überprüfung des gewählten Ansatzes.

            

            

                

                    
Kontextualisierung
                

                

                    
Der Erkenntnisgewinn des Vorhabens liegt im Bereich der Datenmodelle und Ontologien für die Nachlasserschließung: Die Arbeit am Nachlass Luhmanns steht stellvertretend für wissens- und werktheoretisch basierte Untersuchungen, insbesondere bedingt durch den Zettelkasten als "Werk" und Arbeitsinstrument, dessen Spuren sich durch die weiteren Arbeiten Luhmanns ziehen und den Prozess der Wissensanreicherung nachvollziehbar machen. Der Nachlass in seinem Gesamtkontext ist damit gut geeignet, die trotz bekannter Einschränkungen (s.o.) weithin genutzten Modellierungsansätze aus den bibliothekarisch-archivarischen bzw. editionswissenschaftlichen Bereichen auf die Fragestellung nach Werkcharakter und Arbeitsprozess hin zu überprüfen. Gleichzeitig ist im Verlauf der Modellierung zu erwarten, das Verständnis von Werk und Arbeitsprozess vertiefen und Erkenntnisse auch auf wissenstheoretischer Basis gewinnen zu können.

                

                
Mit dem resultierenden, konzeptionell und technisch implementierten Knowledge Graph lässt sich das Verhältnis der Dimensionen von "Werk" auf Basis des Nachlasses abbilden. Dieser proof of concept zeigt eine Möglichkeit, der SDE eine Ebene in Form formal explizierter Information
 (Vogeler 2021, 79) zu geben, die mithilfe informationswissenschaftlicher Methoden Fragen editionswissenschaftlichen, werk-konstitutiven Charakters neu zu betrachten hilft, perspektivisch mit Auswirkungen auch auf die editorische Praxis.
                

            

        

            

                
Die Reichskrone des Heiligen Römischen Reiches ist eines der wichtigsten Symbole europäischer Geschichte. Heute ist sie Teil der Sammlungen des Kunsthistorischen Museums (KHM) in Wien. Im Zuge des vom KHM initiierten CROWN-Projekts

                
 wird eine umfassende Analyse der Reichskrone durchgeführt. Ziel dieser Analyse ist es nicht nur den konservatorischen Status des Objektes zu bestimmen, sondern auch die Diskussion um Entstehungszeit und Entstehungsort voranzubringen. Dazu werden alle Bestandteile der Krone, die Platten, das Stirnkreuz, der Bügel, die Edelsteine und Zierelemente etc. aus unterschiedlicher Perspektive - naturwissenschaftlich, konservatorisch, (kunst)historisch - analysiert. Dieses interdisziplinäre Vorhaben läuft noch bis 2024.

            

            

                
Im Rahmen des CROWN-Projekts wird ein einzelnes Objekt - die Reichskrone - im Vergleich zu einigen wenigen ausgewählten Vergleichsobjekten eingehend und möglichst umfassend beschrieben und analysiert. Die Forschungsdaten, die sich aus der Anwendung naturwissenschaftlicher Analysetechniken zur Untersuchung der Herstellungstechniken und der verwendeten Materialien ergeben, werden mit The Museum System (TMS)

                
 erfasst. TMS ist eine weit verbreitete, aber proprietäre Softwarelösung, die für Museen entwickelt wurde. Sie bietet eine relationale Datenbank, die für die Inventarisierung, Dokumentation und Verwaltung von Sammlungen verwendet wird. Die Verwendung von TMS ist nicht nur auf vorhandene Ressourcen zurückzuführen, sondern hat pragmatische Gründe: Die Etablierung eines weiteren Systems zur Erfassung kann im jetzigen Projektkontext nicht erfolgen. Bei den im CROWN-Projekt erfassten Daten handelt es sich nicht um Inventardaten, also deskriptive Daten, die Objekte in einer Sammlung beschreiben und wofür TMS primär entwickelt wurde, sondern um hochspezifische Forschungsdaten. Ziel dieses Beitrags ist es, einen Workflow zur Überführung der in TMS erfassten Daten zu beschreiben, an dessen Ende eine hochstrukturierte, den FAIR-Kriterien entsprechende und Linked-Open-Data-fähige (LOD) Ressource steht.

            

            

                
LOD beschreibt die freie und offene Zurverfügungstellung von strukturierten Daten über das Web. Oft wird der Technologiestack des Semantic Web verwendet. Fundamentale Grundlage ist dabei das Resource Description Framework (RDF)

                
, das ein standardisiertes und graphenbasiertes Modell zur Darstellung und zum Austausch von Daten und deren Semantik.

            

            

                
Die verschiedenen Forschungsfragen können nur im Rahmen einer interdisziplinären Untersuchung der gemeinsamen Auswertung und Interpretation des Datenmaterials, zu dem auch historische Bild- und Schriftquellen gehören, beantwortet werden. Diese wiederum, d.h. die Verbindung von wissenschaftlichen Messungen und Quellen, geht weit über die übliche Beschreibung von Objekten in Sammlungen hinaus. Da den Autor*innen für dieses Vorhaben kein geeigneter Standard bekannt ist, wird ein Semantic-Web-Ansatz verfolgt. Dieser beinhaltet die Entwicklung einer domänenspezifischen Ontologie und deren Anbindung an die Top-Level-Ontologien CIDOC-CRM

                
 und Basic Formal Ontology (BFO)

                
. Im Ontology Engineering Prozess wird auf etablierte Ontology Design Patterns

                
 und Modellierungswerkzeuge zurückgegriffen. Die Domäne umfasst die formale Abbildung der im CROWN-Projekt erfassten Daten, also die Zusammenführung technologischer Untersuchungen und naturwissenschaftlicher Analysen mit Ergebnissen der historischen bzw. kunsthistorischen Forschung. Folgendes Beispiel soll die Domäne veranschaulichen:

            

            

                
Eine “Hochfassung für Perle mit Einsteckstiften” ist eine Komponente an einer Platte der Reichskrone. Dabei handelt es sich um eine 30 x 30 x 20 mm große Fassung aus Gold. Fünf Punkte auf dieser Fassung werden im Zuge des CROWN-Projektes mit drei Analyseverfahren untersucht: 3D Mikroskopie, Röntgenfluoreszenzanalyse (XRF) und Multispectral Imaging (MSI). Jede Analyse umfasst eine Beschreibung des Messvorgangs, Messdaten in tabellarischer Form als CSV oder XLSX, sowie Ergebnisse in Form von Diagrammen als bspw. BMP, sowie eine (kon-)textuelle Interpretation bzw. Schlussfolgerung der Messung durch die Fachwissenschaftler*in. Zusätzlich gibt es ggf. eine historische Quelle, etwa eine überlieferte Beauftragung eines Goldschmieds, in der weitere Informationen enthalten sind, oder eine kunsthistorische Quelle in Form eines Gemäldes in dem die Fassungen zu einem bestimmten Zeitpunkt abgebildet sind. Ziel der domänenspezifischen Ontologie ist es, genau diese Informationen formal zusammenzuführen und als Forschungsressource nachnutzbar zu machen. 

            

            

                
Der dabei entwickelte Best-Practice-Workflow kann von anderen Forschungsvorhaben in Museen, die TMS nutzen, nachgenutzt werden. Dieser beinhaltet das Ontology Engineering, also die Umsetzung der Domäne als Ontologie und den Export und die Transformation von TMS nach LOD mittels Python. Python erlaubt weiters ein Semantic Enrichment, also die Normalisierung von Entitäten mittels Reconciliation durch Wikidata, sowie andere kontrollierte Vokabularien (z. B. Getty

                
 oder GND

                
)

            

            

                

                    
Am Ende dieser Überlegungen steht nicht eine Software, sondern ein beschriebener Workflow, in dem unterschiedliche Komponenten modular austauschbar sind. Das heißt konkret: statt TMS könnte auch ein anderes System zur Verwaltung und Erfassung von Inventardaten stehen, oder auch eine andere Programmiersprache verwendet werden als Python, mit der man in der Lage ist, programmatisch über einen Export oder eine Schnittstelle Daten nach RDF zu überführen. Der Fokus liegt auf der Interoperabilität verschiedener Werkzeuge und der Gestaltung dieser Schnittstellen. Die Autor*innen sind sich bewusst, dass ein Modell, das dieses Vorhaben auf generischer Ebene löst (Top Level Ontology), von zentralem Interesse ist. Das gegenständliche Projekt ist als eine prototypische Vorarbeit zu einem solchen Vorhaben zu verstehen. 
                

            

        

            

                
Einleitung und Problemstellung

                
Zu allen Zeiten haben sich Menschen ein Bild von der Welt gemacht und festgehalten, wie sie diese verstanden und interpretiert haben. Seit ihren skizzenhaften Anfängen ist bis heute eine Vielzahl von schematischen Bildern entstanden. “Praktiken visueller Welterzeugung” (Reudenbach 2011, Vorbemerkung) in Form von Zeichnungen lassen sich bereits in der Antike beobachten und haben sich bis heute als Mittel zur Konstruktion von Ordnungsvorstellungen bewährt. Anschaulichkeit als grundlegende Kategorie für das Verständnis von der Welt manifestiert sich auch im Diagramm. Das wissenschaftliche Interesse an der Diagrammatologie ist in den letzten Jahrzehnten stark gestiegen. In den digitalen geistes- und sozialwissenschaftlichen Fächern wurde die Darstellung abstrakter Daten und Zusammenhänge in graphisch-visuell erfassbarer Form immer stärker zu einer wichtigen Quelle bei der Generierung von Wissen (Lancaster, Schaal 2016, 5). Es wirkt der sogenannte 
                    
„visual turn“
, der sich abwendet von einer rein sprachlichen Wissensvermittlung und den Fokus stattdessen auf bildhafte Narrative legt: Die textuelle Ebene wird ergänzt durch die visuelle Dimension. Bisher ist die Editionswissenschaft eher unreflektiert mit der Frage umgegangen, wie man Diagramme als bildhafte Darstellungen kritisch wiedergeben kann, da es bis dato keine editorische Theorie der Diagramme gibt. Die Editorik versteht sich ursprünglich als Philologie mit dem Interesse an Sprache und Text, weniger mit deren Veräußerung in Bildern. 
                

            

            

                
Quellen

                
Das Thema der Promotion ist grundsätzlich 
                    
transdisziplinär
 angelegt. Zwar handelt es sich bei den Fallstudien um früh- bzw. hochmittelalterliche Texte, allerdings spielen die philologische und die historische Dimension nur eine Nebenrolle. Stärker wird der Blick auf kulturwissenschaftliche und medientheoretische oder gar kunsthistorische Fragen zu richten sein. Letztlich geht es um editorische Fragen, die verschiedene Disziplinen betreffen. Zwei Quellen sollen für eine Analyse unter editionswissenschaftlichen Gesichtspunkten untersucht werden. Die in zahlreichen Handschriften durch das Mittelalter überlieferte Kosmologie 
                    
De natura rerum
 des Isidor von Sevilla (560-636) und das bekannteste Werk des Petrus von Poitier (1125/1130-1205) 
                    
Compendium historiae in genealogia christi
. Sie markieren einerseits in dem die Antike tradierenden Frühmittelalter und andererseits in dem hier für Innovation stehenden Hochmittelalter das Entstehen der Diagrammatik im engeren, heutigen Sinne.
                

                
Isidor von Sevilla behandelt naturkundliche Themen. Er nutzt seine Diagramme als Erklärung von mathematisch-physikalischen Konzepten, die inhaltlich logisch und schlüssig, jedoch zu komplex sind, als dass man sie textuell in Form eines Narrativs beschreiben könnte. Die diagrammatischen Darstellungen sollen diese Prozesse bildhaft darstellen und so ihr Verständnis legitimieren. Das „Compendium“ von Petrus von Poitiers ist wegen seiner Rezeption für die in den folgenden Jahrhunderten entstandenen graphischen Visualisierungen von Geschichte von großer Bedeutung. Er nutzt mehrere Diagrammformen, um die biblische Erzählung mit Erläuterungen zu versehen, bzw. durch bildhafte Darstellungen verständlicher zu machen. Seine Diagramme machen etwas, das verbal beschrieben wird als visuelle Struktur sichtbar und zeigen damit, dass die textliche Beschreibung und das damit gemeinte jeweils unterschiedlich interpretiert werden kann. Er wählt das Diagramm als eine Form der Wissensvermittlung, die über Sprache hinausgeht.

            

            

                
Forschungsfragen und Methode

                
Mit Blick auf die einleitend formulierte Problemstellung können zwei zentrale 
                    
Forschungsfragen
 aufgezeigt werden:
                

                

                    
Editorische Herausforderung: Was ist aus der klassischen Textkritik auf die „Editorik der Diagramme“ übertragbar? Ziel der Dissertation ist keine Edition beider Werke, sondern die Entwicklung einer „Diagrammkritik“ und ein dafür aufgestelltes Regelwerk, welches explizit auf die beiden vorgestellten Quellen angewendet werden soll.

                    

                        
Re-medialisierung und Re-Codierung von Diagrammen: Gegenstand des praktischen Teils ist die Entwicklung von Formen einer kritischen Wiedergabe diagrammatischer Darstellungen. Das Konzept der Repräsentation als Skala geht mit der Frage einher, wie man ein Diagramm für heute „sprechend“ und verständlich machen kann. Beginnend bei einem quellennahen Abbild über eine fortschreitende Abstraktion, Normierung und Idealisierung zu immer mehr "Nutzer*innennähe." 

                    

                

                
Mit SVG als Verfahren der digitalen Editorik ist die 
                    
Methode
 zu benennen, die im Praxisteil der Promotion Anwendung finden soll. Für die digitale Bildrepräsentation soll unter Hinzunahme von 
                    
SVG
 als XML-basierter Technologie die unter 2. formulierte Frage diskutiert werden, inwieweit eine editorisch naheliegende oder eine auf Ästhetik abzielende Mimetik durch eine systematisierende Wiedergabe ergänzt werden kann. Die Realisierung unterschiedlicher Abstraktionsstufen gibt einerseits Aufschluss über Entstehungskontexte des Diagramms, über Schreiberspezifika oder offenbart stemmatologische Nachbarschaften und Abfolgen. Sie ermöglicht andererseits die Produktion abstrahierender und idealisierender Abbilder und konfrontiert die Quelle mit einer gegenwartsbezogenen Perspektive: Was wäre eine zeitgemäße Form der Wiedergabe? 
                

            

            

                
Bezug zu Themen aus den Digital Humanities

                
Das Thema dieser Arbeit bietet 
                    
Anknüpfungspunkte
 zu weiteren, durchaus diskussionswürdigen Themen in den DH: Wie können Kulturartefakte codiert werden? Wie werden sie re-medialisiert? Wie können wir Relationen mentaler Denkstrukturen und medialen Ausdrucksformen systematischer aufdecken? Wie beeinflussen Technologien und Medien, die uns zur Verfügung stehen, wie wir unsere Welt sehen und mit ihr umgehen?
                

            

        

            

                

                    
Das Projekt „Korrespondenzen der Frühromantik”
                

                
Die Jenaer (und Berliner) Frühromantik gilt als die herausragende intellektuelle Revolution junger deutscher Autor*innen und Gelehrter an der Epochenschwelle um 1800. Die Gruppe agierte öffentlichkeitswirksam und nachhaltig, dispers und zugleich netzwerkbildend; sie reflektierte und praktizierte „Geselligkeit“ beispielsweise auch mittels der Kommunikationsform „Brief“. Die (auch quantitative) Auswertung dieser epistolaren Kommunikationsprozesse zwischen den Frühromantiker*innen einschließlich einer Untersuchung des dabei erfolgenden Wissenstransfers ist eines der großen Desiderate der Romantikforschung, dem das DFG–Projekt „Korrespondenzen der Frühromantik. Edition – Annotation – Netzwerkforschung“ begegnen möchte. Ein grundlegender Schritt auf dieses Ziel hin ist die Erstellung kontrollierter Vokabulare in Gestalt normierter Meta- und Registerdaten, die in einen Knowledge Graphen auf Grundlage einer domänenspezifischen Ontologie eingebunden werden sollen. Einen Aspekt dieser Modellierungsprozesse präsentiert dieser Beitrag. 

                
Die Datengrundlage bilden die Briefe der wichtigsten Protagonist*innen der Frühromantik (wie z. B. Friedrich, Dorothea, August Wilhelm und Caroline Schlegel, Novalis u.a.) untereinander und mit ihren weiteren Korrespondenzpartner*innen zwischen 1790 und 1802, also von den diversen ‘Vorgeschichten’ bis zum Zerfall des Jenaer Kreises (Schanze 2018, 18). Die Daten werden systematisch und vollständig erfasst, digital in Open Access publiziert und literaturwissenschaftlich wie netzwerktheoretisch ausgewertet. Darunter fällt auch die semantische Annotation von Aussagen in diesen Brieftexten, bei der dort erfasste Registerentitäten, wie Personen, Werke, Körperschaften oder Periodika, über eine zweiteilige Prädikatstruktur aus Illokution (dem eigentlichen Verb) und Proposition (einer Aussage, die dieses Verb näher spezifiziert) miteinander verknüpft werden. Zusammen mit den Register- und Metadaten der Briefe werden diese Annotationen in einen Knowledge Graphen überführt, in dem die Daten weiter angereichert werden. Auf dieser Basis erfolgen schließlich Auswertungen mittels quantitativer netzwerkanalytischer sowie qualitativer Ansätze. Die digitale Bereitstellung und Erschließung philologisch zuverlässiger Briefdigitalisate, die Annotation der Briefe sowie die parallele und abschließende graphen- und netzwerktheoretische Auswertung stellen demnach die drei Kernbereiche des Projekts dar.

                
Der Vortrag stellt die zwei Arbeitsphasen vor, in denen die kontrollierten Vokabulare der Illokutionen und Propositionen im Zusammenspiel von digitaler Edition und Annotation entwickelt werden: Zunächst wird ein festes Begriffsset aus den Aussagen der Briefe destilliert und definiert, das danach in die Strukturen eines kontrollierten Vokabulars für die Verwendung als Linked Open Data überführt wird. Anschließend wird die Einbindung dieser Vokabulare in das Datenmodell des Knowledge Graphen präsentiert. 

            

            

                

                    
Entwicklung von Vokabularen zur semantischen Annotation von Aussagen
                

                
Der ursprüngliche Plan zur Erstellung des Begriffssets orientierte sich an Tripelstrukturen wie aus dem Semantic Web bekannt (Subjekt-Prädikat-Objekt). Insbesondere das Prädikat wurde, wie bereits angesprochen, im Laufe des Projekts jedoch differenziert und in zwei Kategorien – Illokution und Proposition – aufgesplittet. Diese können zu Analyse- und Publikationszwecken (z.B. als kontrolliertes RDF-Vokabular) wieder zusammengeführt werden, werden jedoch zur Gewährleistung größtmöglicher Flexibilität in der datenhaltenden Schicht zunächst in dieser ausführlicheren Variante vorgehalten. Ein den Semantic-Web-Prinzipien ähnlicher Ansatz findet sich auch in der 
                    
quantitative narrative analysis
 (QNA) (Franzosi 2010): Nach der sozialwissenschaftlichen Methode bilden semantische Tripel die grundlegenden Einheiten einer Erzählung (Sudhahar u.a. 2015, 2). Von uns wird diese Vorgehensweise erstmals für die Annotation von Briefen genutzt. Durch die zweiteiligen Prädikate werden Meta- und Registerdaten der Briefe (Subjekt und Objekt) semantisch verknüpft. Subjekte können in diesem spezifischen Fall nur Personen sein, Objekte Personen, Körperschaften, Werke und Periodika.
                

                
Ziel dieser Auszeichnungen ist es, die Prozesse intellektuellen Austauschs und die Spuren kollaborativen Schaffens in den Brieftexten für eine formale quantitative Analyse zu öffnen. Auf Basis der kontrollierten Vokabulare der Illokutionen und Propositionen (sowie der schon in Gestalt der Meta- und Registerdaten vorgegebenen Registereinträge) werden die konkreten Akte, aus denen sich Strukturen von Kommunikation und Wissenstransfer ergeben, formal expliziert, womit sich weiterführende datengestützte Auswertungsperspektiven im Sinne der Forschungsfrage ergeben.

                
Die Vokabulare entstehen teils
                    
bottom up
, teils 
                    
top down
: mehrere Bearbeiter*innen annotieren parallel ein kleines Sample von Briefen. Aus dem im Team diskutierten Abgleich der gewählten Formulierungen ergibt sich eine Liste von zusammengesetzten Prädikaten im weiteren Sinne, die sich aus einem die jeweilige Kommunikationsfunktion bezeichnenden und eine kommunikative Handlung vollziehenden illokutionären Verb (also etwa „behaupten“, „erbitten“, „grüßen“, „positiv bewerten“) und fakultativ einer „welthaltigen“ Proposition, der Aussage, die hier auf den Punkt zu bringen ist, zusammensetzt, also etwa: „Publikation“; „Buchsendung“; „Arbeitsplan“. Die Proposition spezifiziert die abstrakte illokutionäre Aussage und beantwortet Fragen wie: „
                    
Was
 wird positiv bewertet?“ oder „Wozu wird jemand (das Objekt) aufgefordert?“ Bei der Erstellung dieser Liste(n) stellt der historische Bedeutungswandel der Sprache ein grundlegendes Problem dar. Wir versuchen diesem Problem zu begegnen, indem wir Anachronismen und begriffliche Überschneidungen mit romantischen Konzepten zu vermeiden versuchen. In einzelnen Fällen wird das nicht möglich sein. Hier können wir lediglich darauf verweisen, dass unser modernes Verständnis von „Kritik” nicht identisch ist mit Friedrich Schlegels Begriff. Das Korpus der Propositionen soll 200 bis 300 nicht überschreiten, die Zahl der illokutionären Verben wird etwa 80 bis 90 erreichen. Mit diesen Termini schließen wir an sprachwissenschaftliche Standards, nämlich die Sprechakttheorie, an, die zwischen illokutionärem Akt und Proposition unterscheidet. Dabei sind nicht alle Aussagen eines Briefes zu annotieren. Die Auswahl ergibt sich aus folgenden Fragen: Sind identifizierte/identifizierbare Akteur*innen, Werke, Periodika, Körperschaften beteiligt? Ist die Aussage mit möglichst generischen Formulierungen zu erfassen? Kann die Aussage Fragen zu unseren Forschungsschwerpunkten beantworten? Wollte man auf diese Einschränkungen verzichten und eine komplette maschinenlesbare Paraphrase von 6.000 teils umfangreichen und thematisch oft äußerst heterogenen Briefen leisten, würde der dafür notwendige Zeitaufwand den eines finanzierbaren Forschungsprojekt überschreiten. Die Aussagenkette würde den Brief quantitativ um ein Mehrfaches übertreffen.
                

                
Bei der Erstellung der Begriffssets ist oberstes Gebot die intersubjektive Prüfbarkeit. Während Subjekt und Objekt positive Gegebenheiten sind, da Kopfdaten des Briefs oder Nennungen in den Registern, müssen Proposition und Illokution erst kontextuell abgeleitet werden. Eine Herausforderung, der man bei der Erstellung der Vokabulare begegnet, ist die Vereinheitlichung des zusammengesetzten Prädikats bei komplexen Aussagen, die auf zwei oder mehr Annotationsketten verteilt werden müssen. Dabei müssen die einzelnen Elemente – Illokution und Proposition – generisch und möglichst abstrakt gehalten werden, um das Vokabular zu begrenzen und eine quantitative Auswertung und potentielle Interoperabilität zu ermöglichen.

                
Auch die Entscheidung, was als Illokution gelten kann und was nicht, ist nicht immer einfach. Möglichst sollten keine Dopplungen von Begriffen im Propositions- und Illokutionsregister auftauchen wie beispielsweise 
                    
senden 
als Illokution und in seiner substantivierten Form 
                    
Sendung 
als Proposition. Andererseits können komplexere Aussagen wie „Buchsendung erbitten”, also das Phänomen sekundärer Prädikation, nur um den Preis gewisser Unschärfen ausgedrückt werden. So muss zuweilen auf Nuancen verzichtet werden, um die Konsistenz der Annotationspraxis zu gewährleisten. Hier dient das kontrollierte Vokabular auch der Organisation von Informationen (ANSI/NISO, 10).
                

                
Die intersubjektive Prüfbarkeit soll durch den Anschluss an sprachwissenschaftliche Standards garantiert werden. So werden Illokutionen und Propositionen jeweils Oberbegriffsklassen zugewiesen. Sie sind die Objektivation des 
                    
top down
-Elements bei der Annotationspraxis, die eben nicht allein auf einer abstrahierenden Paraphrase einzelner Briefpassagen beruht. Im Falle der Illokutionen ist das die Zuordnung zu Illokutionstypen nach Searle (Searle 1976, 1–23). Er teilt Verben in die Gruppen Assertive, Direktive, Kommissive, Expressive und Deklarative ein. Die Oberbegriffe zu den Propositionen sollen abgeglichen werden mit Erkenntnissen der Romantikforschung, der Briefforschung und dem Wissen über die Zeit um 1800 einschließlich ihrer Alltagskultur. Dies ist mehr noch als bei den Illokutionen ein 
                    
top down
-Element, das auf vorgängigen Erkenntnisinteressen und dem Stand der Forschung beruht. Mit diesen in einem Trial-and-Error-Verfahren zu erarbeitenden Begriffsklassen verfolgen wir mehrere Absichten: Wir dokumentieren den Anschluss an vorausgehende wissenschaftliche Überlegungen (wir vermeiden dabei Objektsprache), wir erhöhen den Grad an Disambiguierung in unserem Korpus (oder stellen uns den zwischen den Klassen sicherlich unvermeidlichen Ambiguitäten, wenn etwa Unterbegriffe mehrfach zugeordnet werden können), wir geben Nutzer*innen unserer Editionsplattform als zusätzliche Suchoption Registerelemente an die Hand, erhöhen also die Usability unserer Textsammlung, und fügen schließlich für maschinelle Auswertungen eine zentrale, wenngleich händisch erhobene Datenquelle hinzu. Dieses Angebot soll u.a. die literaturwissenschaftliche Forschung befruchten, aber auch ein Beitrag sein zur Erforschung der Leistung der Kommunikationsform „Brief“ in ihrer Textualität. Nicht zuletzt wird im Projekt selbst ein Wechselspiel von quantitativem und qualitativem Arbeiten erprobt, aus dem neue Erkenntnisse und Arbeitsweisen für die digitalen Geisteswissenschaften hervorgehen können.
                

            

            

                

                    
Implementierung in den Knowledge Graphen
                

                
Für die Implementierung in den Knowledge Graphen dient folglich zunächst ein festes Set aus Begriffen als Grundlage, das die Briefaussagen, die im Zusammenhang mit Kommunikationsprozessen und dem daraus folgenden Wissenstransfer stehen, als die Verknüpfung von Registerentitäten durch Illokution und Proposition mit zugehörigen Oberklassen abbildet. Dieses Begriffsset wird bei der Modellierung einer Ontologie der „Korrespondenzen der Frühromantik” berücksichtigt und anschließend in einer RDF-Serialisierung in ein kontrolliertes Vokabular im Sinne des Semantic Webs überführt.

                
Eine Ontologie wird als eine formale und explizite Spezifikation einer gemeinsamen Konzeptualisierung von Wissen definiert (Gruber 1993, 199). Ein kontrolliertes Vokabular als eine „Zusammenstellung von Bezeichnern (URIs) mit klar definierter Bedeutung“ (Hitzler u.a. 2008, 48) stellt zunächst Informationen dar, identifiziert diese Informationen darüber hinaus aber auch nochmals eindeutig in einer maschinenlesbaren Form und bildet somit die Voraussetzung für die Beschreibung von semantischen Beziehungen innerhalb einer Ontologie. Hier werden diese Begriffe dann in komplexe, ggf. hierarchische Beziehungen gesetzt.

                
Herausfordernd ist in vorliegendem Fall die Transformation der oben beschriebenen Aussagen in Konzepte: Die Aussagen in den Briefe spiegeln deren natürliche Sprache; sie werden in den Vokabularen aus Illokutions- und Propositionsklassen formalisiert und in einen semantischen Zusammenhang gestellt. Diese Arbeit wird schließlich im Knowledge Graphen fortgesetzt, indem diese Aussagen als Konzepte innerhalb einer Ontologie abgebildet werden. Der Begriff ‘Konzept’ wird hier als eine aus der Wahrnehmung abstrahierte Vorstellung, also als eine mentale, wortähnliche Repräsentation von Dingen verstanden und somit eben nicht als ein spezifischer, in einem Brief beschriebener Sachverhalt (Margolis 2022). Von vornherein werden nicht lediglich die Aussagen der Briefe rekonstruiert bzw. formalisiert. Vielmehr wird das übergreifende und somit für weitere semantische Verarbeitung relevante Konzept hinter dieser Aussage bereits in den generischen Elementen des Begriffssets sichtbar. So ist beispielsweise nicht relevant, dass Schlegel in seinem Brief Schleiermacher um seine Kritik an einer bestimmten von ihm übersetzten Textpassage bittet. Relevant sind vielmehr die Begriffe „Kritik“ und „erbitten“ bzw. „Bitte“ (also: Proposition und illokutionäres Verb) im Verhältnis zu Subjekt und Objekt, da durch diese Konzepte sowohl die Selbstreferentialität der Kommunikation (Illokutionen) als auch die relevanten Themen der Kommunikationsprozesse (insbesondere der Wissenstransfer) angesprochen werden. Wie diese Aussagen in das kontrollierte Vokabular übernommen werden – z.B. ob man „Kritik erbitten“ als ein Konzept wertet oder nicht – hat Auswirkungen auf das weitere Datenmodell, also die Ontologie, und ihre Fähigkeit, generische Aussagen der frühromantischen Wissensdomäne abzubilden.

                
Um dieser Problematik für das Datenmodell adäquat zu begegnen, wurden zunächst verschiedene Ansätze geprüft, die sich mit der Modellierung geisteswissenschaftlicher Daten, insbesondere Briefdaten, beschäftigen.
                    

                    
 Das Datenmodell der „Korrespondenzen der Frühromantik“ referenziert auf das Cidoc Conceptual Reference Model (CRM) als Upper Ontology. Auch wenn das Cidoc CRM eigentlich als eine Ontologie für die Museumsdomäne entwickelt wurde, folgt die Orientierung daran der Definition einer Ontologie als einer 
                    
gemeinsamen
 Konzeptualisierung von Wissen, da sich zahlreiche geisteswissenschaftliche Projekte an dem Modell orientieren. Zudem kommt die logische Ausrichtung des Cidoc CRMs auf Ereignisse (
                    
event driven architecture
) den Forschungsfragen des Projekts zugute, da so bspw. die Korrespondenzen als Prozesse modelliert werden und somit Flexibilität innerhalb des Modells garantiert wird. 
                    

                    
So wird jede Klasse 

                    
der

                    
 frühromantischen Domänenontologie als Superklasse einer Klasse des Cidoc CRMs übergeordnet oder es werden, falls möglich, die Klassen des Cidoc CRMs direkt nachgenutzt.
 Ebenso wird bei den Properties vorgegangen. Darüber hinaus wurden Aspekte der Auszeichnungslogik von 
                    
correspdesc
 miteinbezogen, da die bereits publizierten Daten der August-Wilhelm-Schlegel-Edition (Strobel 2014–2020) dieser folgen.
                

                
Für die Modellierung der Aussagen bedeutet dies nun Folgendes: Innerhalb des Konzeptes Brief (Klasse 
                    
Letter
 mit Superklasse 
                    
E33 Linguistic Object 
des Cidoc CRMs) können Personen (Klasse 
                    
E21

                    
Person
), Zeitschriften (
                    
Periodical
 mit Superklasse 
                    
E33
), Werke (
                    
Work
 mit Superklasse 
                    
E33
), Institutionen (
                    
Institution
 mit Superklasse 
                    
E74 Group
), Schlagwörter (
                    
Theme
 mit Superklasse 
                    
E55 Type
) und Aussagen (
                    
Statement
 mit Superklasse 
                    
E13 Attribute Assingment
) miteinbezogen sein. Verbunden sind diese Klassen mit 
                    
Letter 
jeweils mit der Property 
                    
P129 is about
 des Cidoc CRMs. Das kontrollierte Vokabular wird in den Klassen 
                    
Illocution
 und 
                    
Proposition
 abgebildet, beide mit Superklasse 
                    
E55 Type
. 
                    
Illocution
 ist zudem mit der Gruppe der Illoktutionstypen durch die Klasse 
                    
Illocutiongroup
 verbunden, die ebenfalls die Superklasse 
                    
E55 Type 
hat. Innerhalb der Klasse 
                    
Statement
 als Domain werden die Aussagen nun modelliert, indem das Subjekt durch 
                    
has_Subject
 (Subproperty von 
                    
P140 assigned attribute to
) mit 
                    
E21 Person
 als Range verbunden wird, das Objekt als Range mit den Klassen 
                    
E21 Person
, 
                    
Periodical
, 
                    
Work
 und/oder 
                    
Institution
 mit der Property 
                    
has_Object
 (Subproptery von 
                    
P141

                    
assigned
) sowie das Prädikat mit 
                    
has_Predicate
 (Subproperty von 
                    
P177 assigned property of type
) mit den Klassen 
                    
Illocution
 und 
                    
Proposition 
(Abb. 1). Die jeweiligen Einheiten einer Aussage werden somit als gesonderte Klassen betrachtet, die erst innerhalb der Aussage selbst wieder zusammengeführt werden, wobei die Properties die Subjekt-Illokution-Proposition-Objekt-Struktur festlegen. Perspektivisch bedeutet dies, dass bspw. die Proposition „Kritik“ und die Illokution „erbitten“ erst für die Netzwerkanalyse aus zuvor getrennt vorgehaltenen Einheiten der Graphdatenbank zusammengesetzt werden. Hierdurch wird eine flexible Struktur garantiert: Die Komplexität der Konstruktion von Aussagen wird durch das Aufgliedern in ihre einzelnen Bestandteile reduziert, sodass eine Nachnutzung der Daten – unter der Berücksichtigung der Auswahl der Begriffe, die von den angesprochenen Forschungsfragen bestimmt war – auch in anderen Kontexten möglich ist. Es gilt dementsprechend aus den spezifischen und eine konkrete „Sache” betreffenden Aussagen des Quellmaterials generische und allgemeine Konzepte zu entwickeln und diese als Linked Open Data darzustellen, welche dann einerseits für den im Projekt zu entwickelnden Knowledge Graphen bzw. die Ontologie genutzt, andererseits aber auch für andere Forschungsinteressen, welche die Zeit um 1800 oder das Kommunikationsmedium „Brief“ betreffen, 
                    
nach
genutzt werden können. Diese Entwicklung generischer und offener Forschungsdaten aus konkretem Quellenmaterial stellt eine Aufgabe dar, denen viele Forschungsprojekte aus dem Bereich der Digital Humanities begegnen, die jedoch nicht minder signifikant ist: Nur so werden Forschungsdaten erzeugt, die auch über den Projektkontext hinaus genutzt werden und folglich die Forschungslandschaft ergänzen können. Zudem können durch den Linked Open Data-Ansatz die projektinternen Forschungsdaten an andere Datenkorpora angeschlossen werden, was einerseits zu einer größeren Sichtbarkeit, andererseits zu vielfältigen Anschlussperspektiven führt, beispielsweise für die Historische Netzwerkforschung, die Geschichtswissenschaften (v.a. zur Kultur-, Sozial- und Ideengeschichte, aber auch zu Alltags- und Emotionsgeschichte), die Genderforschung oder die Judaistik.
                

            

            

                

                    
Ausblick
                

                
Durch den Aufbau solcher kontrollierter Vokabulare, die Entwicklung des Datenmodells und die Integration in einen Knowledge Graphen eröffnet sich die Möglichkeit tiefergehender Analysen mit Methoden der historisch-geisteswissenschaftlichen Netzwerkforschung, innerhalb derer die Strukturen des Netzwerks der Jenaer (und Berliner) Frühromantik sowie seine Genese und Entwicklung als relationale Phänomene rekonstruiert werden. So wird ein neues Bild der einleitend erwähnten Akteur*innen gezeichnet, werden zentrale Personen, homo- und heterogene Strukturen, Überschreitungen sozialer Barrieren und die Kreuzung sozialer Kreise ausgemacht sowie Dynamiken dieser Strukturen aufgeschlüsselt. Da sich die Netzwerkforschung in Bezug auf die umfassende, auch inhaltliche Erschließung von Korrespondenzen noch in den Anfängen befindet, ergibt sich in der Kombination mit einer umfassenden, hochstrukturierten Datenbasis die Möglichkeit, neue Forschungsansätze in der Synthese literaturwissenschaftlicher und netzwerktheoretischer Arbeitsweisen zu entwickeln und für den Anwendungsfall der Jenaer Frühromantik eingehend zu prüfen. 

            

            

                
Abbildung

                

                    

                        

                        
Abb. 1: Ausschnitt aus Datenmodell der „Korrespondezen der Frühromantik"

                    

                

            

        

            

                
Kurzbeschreibung des Projekts

                
Das DFG-geförderte Projekt „Erziehung über Grenzen denken - Wilhelm Reins pädagogischer Korrespondenznachlass“
                    

                    
 zielt auf eine transnationale und vergleichende Analyse vielfältiger historischer pädagogischer Kontakte und länderübergreifender pädagogisch-reformerischer Diskurse und leistet damit Grundlagenforschung mit bildungstheoretischem Schwerpunkt. Quellengrundlage ist der umfangreiche, langjährige und vielfältige internationale pädagogische Korrespondenznachlass Wilhelm Reins. Der in seiner Wirkungszeit international einflussreiche Erziehungswissenschaftler, Lehrerbildner und pädagogische Netzwerker Rein (1847–1929) hatte an der Universität Jena das erste Ordinariat für Pädagogik in Deutschland inne. Das Forschungsvorhaben wird verbunden mit der archivarischen Bearbeitung der Originalquellen, der Aufbereitung des detailliert vorliegenden Metadatenkorpus sowie der Bereitstellung des gesamten pädagogischen Briefnachlasses in edierter Form als ein frei verfügbares nachnutzbares Instrument für weiterführende
                    
 Forschungen. 
                

            

            

                
Ausgangslage

                
Das Projekt
                    

                    
 baut auf Arbeitsergebnisse aus einer Vorbereitungsphase (2016-2017) und einem Anschubprojekt (1/2018-6/2019)
                    

                    
 auf, in deren Verlauf das Quellenkorpus durch die Forschenden im Projekt erfasst, kategorisiert, strukturiert, systematisiert und digitalisiert wurde. Während der Vorbereitungsphase wurde der pädagogische Korrespondenznachlass aus dem Gesamtbestand des schriftlichen Nachlasses Reins herausgelöst und in (A) Briefe an Rein, (B) Briefe von Rein und (C) Kondolenzschreiben zum Ableben Reins unterteilt. Diese Dokumentmenge bildet das Quellenkorpus, das im Rahmen des vorgestellten Projekts bearbeitet und ausgewertet wird. 
                

                
Im Anschubprojekt wurde die pädagogische Korrespondenz alphabetisch nach Korrespondenzpartner*innen sowie chronologisch strukturiert, erfasst und gemäß den 
                    
DFG-Praxisregeln „Digitalisierung“ 
(Deutsche Forschungsgemeinschaft 2016) digitalisiert. Den Korrespondent*innen wurden projekteigene Grundsignaturen, den Briefen Dokumentsignaturen zugewiesen. Die grundlegenden Metadaten der Dokumente (1) Signatur, (2) Verfasser*in/Empfänger*in, (3) Ausstellungsdatum, (4) Ausstellungsort, (5) Umfang, (6) Dokumenttyp, (7) Sprache, (8) Prüfvermerke und (9) Dateinamen der zugehörigen Images wurden im Bibliographieprogramm Zotero
                    

                    
 erfasst. Als Grundlage zur Auszeichnung und Auswertung der Quellen wurden eine Indexsystematik und ein fachsystematischer Index zur kontrollierten Indexierung entworfen. Im Juli 2019 wurde mit der Recherche bio-bibliographischer Angaben zu den Verfasser*innen begonnen, welche in der aktuellen Projektphase fortgeführt wird.
                    

                    

                

                
In der Anschubphase wurden, basierend auf der Sichtung der Briefe und den erzeugten Metadaten, erste inhaltliche Erkenntnisse gewonnen, welche auf ein vielfältiges Erkenntnispotenzial des Bestandes verweisen und die zur Herleitung der u.g. Forschungsschwerpunkte geführt haben. So kann man z. B. aus Erkenntnissen zu Umfang, Dauer und Reichweite der Korrespondenz und zur Diversität der beteiligten Korrespondent*innen auf eine bemerkenswert heterogene Struktur des Korrespondenzgefüges schließen. Diese heterogene Struktur wird durch den Korrespondenznachlass wesentlich dokumentiert, so dass dieses Quellenkorpus vergleichende wie transzendierende Analysen (s. u.) von Diskursen und Dynamiken zwischen vielfältigen Akteur*innen über vielfältige Begrenzungen hinweg ermöglicht.

            

            

                
Quellenkorpus als Datengrundlage

                
Der pädagogische Korrespondenznachlass Reins umfasst insgesamt 6.301 Korrespondenzdokumente (Briefe, Postkarten, Telegramme) von mehr als 3.500 Korrespondent*innen aus 42 Ländern. Diese Originalquellen – überwiegend in deutscher Kurrentschrift – bilden in einem Zeitraum von sechs Jahrzehnten (1869-1929) interne Perspektiven vielfältiger Akteur*innen ab, die in kontroverse und einflussreiche bildungspolitische Diskurse mit oftmals internationaler Reichweite eingebunden waren. Zu den Korrespondent*innen zählten Personen unterschiedlichster Hintergründe und Weltanschauungen, darunter Vertreter*innen einer pädagogisch-akademischen Elite (z. B. Nicolas Murray Butler, Adolf Damaschke, Friedrich Wilhelm Förster, Helene Lange, Paul Geheeb, Paul Natorp, Friedrich Paulsen, Eduard Spranger) und einflussreiche Personen aus Politik und Gesellschaft (z. B. Gertrud Bäumer, Houston Stewart Chamberlain, Else Fisch, Marie Fischer-Lette, Friedrich Naumann), aber auch zu einem wesentlichen Teil Personen, die aufgrund vielfältiger Faktoren (z. B. Geschlecht, sozialer, fachlicher oder akademischer Status, Religion oder geographische Herkunft) in pädagogischen Diskursen und bildungsgeschichtlich unterrepräsentiert sind. Rein zählte in mehreren Bereichen (insbesondere Lehrerbildung, Volksschulbildung, höhere Frauenbildung) als progressiver pädagogischer Reformer. Sein Expert*innennetzwerk war weitverzweigt und im zeitgenössischen Vergleich bemerkenswert heterogen und inklusiv. Der Korrespondenznachlass dokumentiert dieses in wesentlichen Auszügen und konserviert sowohl Stimmen von Akteur*innen, die ihr Wirken und ihre Expertise in der (fach)öffentlichen Wahrnehmung breiter sichtbar machen konnten, als auch von solchen Personen, die maßgeblich in die Verbreitung und Rezeption pädagogischer Theorie und Praxis eingebunden waren, aber keine nennenswerte Sichtbarkeit erlangen konnten. Das macht dieses bisher kaum erforschte Quellenkorpus aus bildungsgeschichtlicher Sicht bemerkenswert. Das Wirken unterschiedlicher Akteur*innengruppen, ihre Kontakte und Synergien, pädagogische Entwicklungen und Phänomene können vergleichend und vielfältige Abgrenzungen transzendierend (z. B. Kultur, Status, geographische und geopolitische Grenzen, Ideologien, Geschlecht) exemplarisch erforscht werden. Bei der Auswertung muss die besondere Art der Quellendokumente berücksichtigt werden. Briefe
 stellen besondere Anforderungen an die Auswertung, bieten aber auch Forschungspotenzial, das andere historische Quellentypen nicht aufweisen (vgl. Baillot 2020, S. 390, Budde 2020, Henzel 2020, S. 226 ff.). Sie transportieren als Lebenszeugnisse (Nutt-Kofoth 2016) und Ereignisse (Stadler, Illetschko und Seifert 2016), die in größere kommunikative Zusammenhänge eingebunden sind, mehr Informationsschichten als z. B. Fachpublikationen. Als solche sind Briefe sensible Dokumente, die ursprünglich nicht für eine (breitere) Öffentlichkeit bestimmt waren. Sie repräsentieren einen im zeitgenössischen Kontext auch durch das Postgeheimnis gesetzlich geschützten (vgl. Standhartinger 2020, S. 272) Kommunikationsraum, in dem Verfasser*innen persönliche Meinungen, fachliche Positionen oder auch private Belange tendenziell unverstellter darlegen können. 
                

                

                    
Zielperspektiven des Projektes

                    
In der 
                        
Grundlagenforschung bzw. Quellenauswertung
 folgt das Projekt der Annahme, dass sich pädagogische Reform als ein verbindendes wie mehrdeutiges Motiv auf die Verbreitung, Rezeption, Gestaltung und Entwicklung pädagogischer Theorie und Praxis ausgewirkt hat. Schwerpunktmäßig soll untersucht werden, (1) ob pädagogische Reform nachweislich ein wesentliches Motiv des zeitgenössischen pädagogischen Austausches war, (2) in welchen thematischen Kontexten pädagogische Reform ggf. diskutiert worden ist, (3) ob sich verschiedene bzw. welche unterschiedlichen Konnotationen von pädagogischer Reform sich identifizieren lassen, (4) ob bzw. in welchem Maße das Sprechen über pädagogische Reform einen fachlichen Austausch über Grenzen hinweg begünstigt hat und (5) ob bzw. inwiefern unterschiedliche Konnotationen oder Verwendungen des Motivs pädagogische Reform zu Störungen in der fachlichen Kommunikation geführt haben. Ergänzend dazu sollen bio-bibliographische Daten und Erkenntnisse zu Kontakt- und Netzwerkstrukturen zwischen den beteiligten Korrespondent*innen gewonnen werden. Ein besonderes Interesse gilt bildungshistorisch unterrepräsentierten Akteur*innen, ihrem Einfluss und Wirken und den Themen und Positionen, die durch sie bearbeitet und vertreten worden sind.
                    

                    
Als 
                        
strukturelles Ziel
 wird im Laufe des Projekts ein übersichtliches recherchier- und zitierfähiges, digitales Korpus als nachnutzbare Grundlage für die inhaltliche Auswertung und weiterführende Forschungen erstellt. Dazu werden die Quellentexte in digitale Volltexte in TEI/XML transkribiert, indexiert und inhaltlich ausgewertet. Aufgrund des Entstehungszeitraumes der Briefe sind zwar keine Persönlichkeitsrechte der Korrespondenzpartner*innen sowie der in den Briefen Erwähnung findenden dritten Personen zu beachten, hingegen gelten noch Urheberrechte für einen Teil der Briefe. Daher werden die Volltexte gemeinsam mit den digitalen Faksimiles der Korrespondenzdokumente über ein abgestimmtes Rechtemanagement sukzessive im Projektverlauf über die Editionsumgebung der BBF (EditionenBildungsgeschichte
                        

                        
) soweit möglich unter freier Lizenz bzw. gemäß § 60f UrhG verfügbar gemacht. Die gewonnenen bio-bibliographischen, geographischen sowie Kontakt- und Netzwerkdaten werden nach Abschluss der Auswertung als wertvolle Ergänzung der edierten Texte veröffentlicht. Zudem ist die Anreicherung der GND um bisher unbekannte Personen bzw. um hinzugewonnene biographische Informationen zu bereits erfassten Personen ein weiteres Projektziel. Von zentraler Bedeutung im Projekt ist darüber hinaus die dauerhafte Sicherung sowohl der unikalen Originaldokumente durch entsprechende konservatorische Maßnahmen als auch aller digitalen Projektdaten. 
                    

                

                

                    
Daten, Methoden und Werkzeuge

                    
Im Projekt werden vier Datenarten unterschieden, deren Erzeugung aufeinander aufbaut: Er-schließungsdaten (A), digitale Faksimiles (B), digitale Volltexte (TEI/XML) (C) und semantische Daten (D). Am Anfang der Forschungsdatenerzeugung steht die archivarische Nacherschließung (A) und Image-Publikation (B). Die Volltexterzeugung (C) erfolgt im TEI/XML DTA-Basisformat, das schließlich die Grundlage eines strukturierten, zitierfähigen Korpus bildet (Ausgangsdaten). Durch die darauf aufbauende semantische Anreicherung, Auswertung und automatisierte Analyse werden Forschungsdaten (Arbeitsdaten) erzeugt (D).

                    
Neben der dauerhaften Sicherung dieser bildungs- und kulturhistorisch wertvollen Originalquellen verfolgt das Infrastrukturteilprojekt die Nachnutzung der im Anschubprojekt erzeugten Forschungsdaten. Der für das Projekt entwickelte Workflow vereinbart archivarische Erschließungskonventionen mit den Bedarfen eines bildungshistorischen Forschungsprojektes. Die im Anschubprojekt erzeugten Metadaten werden zunächst mithilfe des Datenbereinigungstools Open Refine
                        

                        
 normalisiert, mit den digitalen Repräsentanten verknüpft sowie Orts-, Personen- und Körperschaftsangaben mit GND-Nummern angereichert. Nach dem Import in die Archivdatenbank erfolgen eine Qualitätsprüfung und die Vergabe von eindeutigen, dauerhaften Archivsignaturen. Die XML-Exporte aus der Archivdatenbank bilden die Basis für die Erzeugung von TEI/XML-Vorlagen für jeden Brief, die so bereits alle relevanten Metadaten mitführen und damit als Grundlage für die Transkription der Briefe dienen. Die Transkription erfolgt gemäß einer leicht angepassten Version des DTA-Basisformats (Haaf, Geyken und Wiegand 2014), sodass die Ergebnisse direkt in die Editionsinfrastruktur der BBF eingebunden werden können. Parallel dazu erfolgt die Veröffentlichung der Faksimiles über ScriptaPaedagogica
                        

                        
, dem digitalen Textarchiv der BBF. Das System sichert die dauerhafte Verfügbarmachung der Bilder durch die Vergabe von URN, und der integrierte IIIF-Server erlaubt den Export der Digitalisate und der zugehörigen Metadaten in einschlägigen Formaten (z. B. .jpg, .tiff, .mets). Zudem werden für die Digitalisate standardmäßig DFG-Viewer-kompatible METS/MODS-Pakete erstellt, die in Zusammenhang mit den hochauflösenden Masterscans im TIFF-Format eine Voraussetzung für den digitalen Langzeiterhalt bieten. Für die weitere wissenschaftliche Auswertung bilden die Transkription, die Metadaten sowie die Auszeichnung und Kodierung der Volltexte d
                        
ie Grundlage. Diese werden im freien, portablen und offenen Format TEI/XML gespeichert und sind somit ebenfalls langzeitarchivierungsfähig. Zudem ist so eine plattformunabhängige Nachnutzbarkeit der Daten möglich. Im Projekt werden die angereicherten TEI/XML-Dateien in strukturierte Textdateien überführt, welche wiederum als Grundlage für die weitere Auswertung in der Analysesoftware MAXQDA Analytics Pro
                        

                        
 dienen. 
                    

                    
Bei der Erzeugung und Auswertung derjenigen Forschungsdaten, die der inhaltlichen Analyse dienen, werden (teil)automatisierte Analyseverfahren in ein klassisches hermeneutisches Vorgehen integriert. Die Volltexte werden auf Grundlage von deduktiv wie induktiv generierten Indizes ausgezeichnet. Angewendet werden ein fachsystematischer Index, ein Kontext- und ein Beziehungsindex. Die induktive Weiterentwicklung der Indizes stellt dabei bereits eine eigene Forschungsleistung dar, wobei die Indizes die Grundlage für die Auszeichnung expliziter wie impliziter Nennungen bilden. Als zentrales Werkzeug dient MAXQDA Analytics Pro, das durch die Anwendung und Generierung von sehr umfangreichen „lebenden“ Indizes eher unkonventionell zur Unterstützung einer hermeneutischen Textanalyse durch qualitative und quantitative Verfahren genutzt wird. MAXQDA bietet in dieser Version Funktionen (z. B. Keyword-in-Context- und Mixed-Method-Anwendungen), die eine Anreicherung eines qualitativ-quantitativ-hermeneutischen Vorgehens durch teilautomatisierte Verfahren wie Kollokations- bzw. Kookkurrenzanalysen ermöglichen. Über die initialen Forschungsfragen hinaus soll das Textkorpus perspektivisch über Topic-Modeling-Verfahren genauer beschrieben werden.

                    
Die inhaltliche Analyse folgt einem vergleichend-transzendierenden Ansatz. Das besondere Potenzial des Quellenkorpus wird genutzt, um Dynamiken der Verbreitung und Rezeption pädagogischer Theorie und Praxis zwischen vielfältigen Akteur*innen über vielfältige Grenzen hinweg nachzuzeichnen. So wird ein Beitrag geleistet, Bildungsgeschichte(n) kritisch zu hinterfragen und aktuelle Bedingungen im Bildungswesen besser zu verstehen. Als besonders interessant erscheinen Unterschiede zwischen typischen und untypischen, wie zwischen repräsentierten und unterrepräsentierten Akteur*innen; eine systematische Unterscheidung, die im Projekt als zentrales Forschungswerkzeug entwickelt wurde. Im Ansatz werden vergleichende Perspektiven mit Perspektiven zusammengeführt, bei denen es um die Überwindung oder Transzendierung vielfältiger Abgrenzungen geht. Für letztere Perspektiven wird in dem hier beschriebenen Forschungsprojekt der Terminus „transzendierende Ansätze“ neu gesetzt, um Zugänge zu benennen, die Phänomene Grenzen überwindend und abgegrenzte Bereiche durchdringend beschreiben. Transzendierende Ansätze (die über transnationale Ansätze hinausgehen
) werden genutzt, um Transfer- und Zirkulationsprozesse und -zusammenhänge zu erforschen und darzustellen. Vergleichende Perspektiven werden hinzugezogen, um spezifische Ausprägungen zu untersuchen. Transzendierend wie vergleichend werden pädagogische Akteur*innen(gruppen), ihr Wirken, ihr Einfluss sowie Kontakte und Netzwerke zwischen ihnen analysiert. Mit Blick auf inhaltliche, theoretische und paradigmatische Grenzen wird z. B. mit Fokus auf unterschiedliche Milieus oder Berufsgruppen u. a. untersucht, wie Motive, Positionen und Diskurse voneinander abgegrenzt und wie um Deutungshoheiten gerungen wurde, aber auch wie Motive, Positionen und Diskurse, die zunächst als widersprüchlich erscheinen (z. B. traditionelle bzw. konservative versus reformorientierte Positionen) in Theorien und Praktiken vermischt oder miteinander in Einklang gebracht wurden (vgl. Geiss/Reh 2020). 
                    

                

            

            

                
Herausforderungen

                
Wie oben beschrieben, werden die Daten je nach Verwendungszweck in verschiedenen Formaten benötigt. Also muss sichergestellt werden, dass erstens alle Daten ohne Verlust oder Mehraufwand sowohl für die Ansprüche eines umfänglich durchsuchbaren Volltextkorpus, als auch für die Analyse mit MAXQDA aufgearbeitet werden und zweitens die erzeugten Datenarten idealerweise aufeinander aufbauen und sich gegenseitig ergänzen. Es mussten u. a. Lösungen gefunden werden, Auszeichnungsdaten z. B. zu Personen, Orten und Werken, die während der Transkription in TEI-XML vorgenommen werden, so in der Textdatei auszugeben, dass diese mit den Volltexten nach MAXQDA importiert und dort als bereits ausgezeichnet übernommen werden.

                
Bei der Bearbeitung und Auswertung des Textkorpus wird mit unterschiedlichen Repräsentationsformen der Dokumente gearbeitet. Dabei müssen die Charakteristika der unterschiedlichen Formate – sowohl der physischen Originaldokumente wie aller erzeugten Repräsentanten – stets mitgedacht und Hinzufügungen wie Annotationen mitgeführt werden. Gleichzeitig bedeutet die forschungsgestützte Auszeichnung und Migration von Texten in andere Datenformate nicht nur eine Anreicherung der Originaltexte, sondern auch das Hervorbringen von neuen Daten bzw. Quellen. Das geht mit der Herausforderung einher, durch die Auswahl von geeigneten Tools die gewonnenen Informationen nachnutzbar und dauerhaft verfügbar zu machen.

            

            

                
Fazit

                
Das Projekt baut auf Ergebnisse aus zwei vorhergehenden Projektphasen auf. Es operiert mit unterschiedlichen Datenformaten und verbindet zentrale Aufgaben und Verpflichtungen von Forschenden mit den Aufgaben einer Forschungsbibliothek mit angegliedertem Archiv (vgl. Müller 2019; Reh et al. 2020). Daraus ergeben sich komplexe Herausforderungen: (1) in der Kommunikation zwischen den unterschiedlichen Beteiligten, (2) in den Verfahrensweisen und (3) in der technischen Umsetzung. Diese gehen neben der erwähnten Verwendung unterschiedlicher Datenformate auf verschiedene (Konnotationen von) Begrifflichkeiten und Zielperspektiven zurück. Als ein Projekt, das wesentlich mit digitalen Methoden arbeitet, verorten wir es als ein einschlägiges DH-Projekt für die Bildungsgeschichte. Die Projektergebnisse können in gleichem Maße als End- und Ausgangspunkt von Forschung verstanden werden, da aus Forschendenperspektive tatsächlich neue Quellen erzeugt werden, die durch neue Daten angereichert sind. Aufgrund des erzeugten Mehrwertes werden weiterführende Forschungen eher die erzeugten Abbilder und Datenvarianten der Originalquellen als die ursprünglichen physischen Dokumente auswerten. Alle Daten werden dabei gemäß den FAIR-Prinzipien dauerhaft auffindbar, zugänglich, interoperabel und nachnutzbar bereitgestellt (vgl. Wilkinson et al. 2016).

                
Bei der Überlegung, welche Datenformate im Projekt genutzt werden und auf welche Weise sie in andere Formate überführt werden, muss zudem deren mehrdimensionale Wirksamkeit – Dokumentation, Erforschung 
                    
und
 Erzeugung bildungsgeschichtlicher Daten bzw. Quellen – stets mitgedacht werden. Diese Anforderung geht über den Anspruch hinaus, dass Daten so erzeugt werden müssen, dass sie sowohl Forschung wie auch editorische Bearbeitungen ermöglichen. Tatsächlich zeigt das hier vorgestellte Projekt exemplarisch, welche Bedeutung auch in den Sozial- und Geisteswissenschaften einer engen Kooperation zwischen Akteur*innen in Forschung und Infrastruktur zunehmend zukommt (vgl. Cremer et al. 2021). Um anschlussfähige und langfristig nachnutzbare Forschungsergebnisse und Forschungsinfrastrukturen zu gewinnen, müssen ab der Planung über die Durchführung bis zur Nachbereitung wissenschaftlicher Projekte Synergien zwischen Forschenden und Expert*innen aus Bereichen der Infrastrukturen genutzt werden. So können nicht nur punktuell die Infrastrukturen möglichst genau auf die Bedarfe von Forschenden angepasst werden, sondern auch erprobte Verfahrensweisen und Strukturen bei der dauerhaften Sicherung von analogen und digitalen Quellen genutzt werden. Durch die Verständigung über Vorgänge, Methoden, Werkzeuge, Begrifflichkeiten und Konzepte und deren gemeinsame (Weiter-)Entwicklung leisten solche Kooperationen auch wertvolle Übersetzungsarbeit zwischen unterschiedlichen Akteur*innengruppen. Erfahrungen und Ergebnisse aus einer solchen gegenseitigen Verständigung kann zukünftigen Vorhaben zugutekommen, da Akteur*innen sowohl aus der Forschung wie in Infrastruktureinrichtungen für die Bedarfe, Denkungsarten und Vorgehensweisen der jeweils anderen Seite sensibilisiert werden. So werden auch Forschende geübt und befähigt, bei der Wahl von Forschungsmethoden und der Arbeit mit Forschungsdaten Potenziale, Bedarfe und Grenzen von Forschungsinfrastrukturen mitzudenken und für aktuelle wie weiterführende Projekte nutzbar zu machen.
                

            

        

            

                
Einleitung

                
Die Nationale Forschungsdateninfrastruktur (NFDI)
 wurde im Jahr 2020 mit dem Ziel ins Leben gerufen, die Datenbestände aus Wissenschaft und Forschung für das deutsche Wissenschaftssystem systematisch zu erschließen, zu vernetzen und nutzbar zu machen. NFDI4Culture startete im Oktober 2020 als Konsortium der ersten Förderrunde und widmet sich Forschungsdaten zu materiellen und immateriellen Kulturgütern (Altenhöner et al. 2020). Die Interessengemeinschaft von NFDI4Culture reicht von Architektur-, Kunst- und Musik- bis hin zu Theater-, Tanz-, Film- und Medienwissenschaft und besteht aus über 70 beteiligten Organisationen
. Dieses multidisziplinäre Konsortium produziert und verarbeitet eine große Menge heterogener Daten und unterhält Repositorien und Services, die für das deutsche Wissenschaftssystem und auch darüber hinaus in Kunst und Kultur von großer Bedeutung sind. Zum Datenspektrum von NFDI4Culture gehören 2D-Digitalisate von Gemälden, Fotografien und Zeichnungen ebenso wie digitale 3D-Modelle kulturhistorisch bedeutender Gebäude, Denkmäler oder audio-visuelle Daten von Musik-, Film und Bühnenaufführungen (Bicher et al., 2022). In den Datendomänen des Konsortiums werden bisher jedoch nur vereinzelt einheitliche offene Standards und Datenmodelle genutzt. Forschungsdaten liegen oftmals in sogenannten Datensilos vor, die weder von außen gefunden noch wiederverwendet werden können, da auch die Zugangsmöglichkeiten uneinheitlich und kompliziert sind. Zudem sind die Lizenzen zur Nutzung der Ressourcen oft nicht sofort ersichtlich oder vollständig geklärt, was deren Nachnutzung zusätzlich erschwert. Weiterhin stellen insbesondere auch die mit digitalen Kulturgütern zusammenhängenden, häufig komplexen datenrechtlichen und datenethischen Aspekte eine Herausforderung dar. Ziel von NFDI4Culture ist es daher, eine bedarfsgerechte Infrastruktur für die Forschungsdaten der Interessengemeinschaft zu schaffen, die den F.A.I.R. Prinzipien folgt und somit das Auffinden, den Zugang, die Nutzbarkeit und Interoperabilität der Ressourcen für alle sicherstellt (Wilkinson et al., 2016).
                

                
Die Implementierung der F.A.I.R. Prinzipien erfolgt im Konsortium über die Bereitstellung und Nutzung von domänenspezifischen Ontologien, die Entwicklung von Knowledge Graphs und die Verknüpfung einer Vielzahl von strukturierten Datenbanken und Knowledge Graphs untereinander. Ein einheitlicher und intuitiver Zugriff auf die dezentral vorliegenden Forschungsdatenressourcen des Konsortiums wird über eine zentrale Plattform, das “Culture Information Portal”
                    

                    

                    
 sichergestellt.
                

                
In diesem Beitrag wird der aktuelle Stand und die weiteren Planungen der technisch übergreifenden Task Area 5 von NFDI4Culture zur Knowledge Graph-basierten Integration von Forschungsdaten materieller und immaterieller Kulturgütern vorgestellt. Dies beinhaltet eine Diskussion aktueller technischer und domänenspezifischer Herausforderungen, die Vorstellung der NFDICO Ontologie und des NFDI4Culture Knowledge Graphen sowie eine Darstellung zur Implementation des Culture Information Portals.

            

            

                
Herausforderungen der Datenintegration in NFDI4Culture

                
Die von NFDI4Culture in den Blick genommene Forschungslandschaft ist durch eine starke Diversität gekennzeichnet. Sie umfasst nicht nur eine Vielzahl von Forschungsdisziplinen, sondern auch unterschiedlichste Organisationen, darunter Universitätsinstitute, Kunst- und Musikhochschulen, Akademien, Galerien, Bibliotheken, Archive, Museen und einzelne Forscher*innen. Dementsprechend sind die Forschungsprozesse- und ressourcen, die auffindbar, interoperabel und wiederverwendbar gemacht werden müssen, ebenfalls heterogen und liegen nicht nur in divergierenden Standards und Formaten vor, sondern auch in unterschiedlich aufbereiteten Zuständen. Kollektionen sind nicht immer vollständig digitalisiert und erschlossen, weshalb oft nur wenige beschreibende Metadaten zur Verfügung stehen. Andere Datensätze liegen bereits vollumfänglich als Linked Open Data vor und können problemlos mit dem NFDI4Culture Knowledge Graph verknüpft werden. Die Implementierung der F.A.I.R. Prinzipien ist ein Hauptziel des Konsortiums. Daher werden dedizierte Ontologien zur Verfügung gestellt und Maßnahmen getroffen, um alle Akteur*innen dabei zu unterstützen, eigene Daten und Ressourcen langfristig und nachhaltig selbst in Linked Open Data zu transformieren. 

                
Ein wissenschaftsgeleitetes Forschungsdatenmanagement erfordert die aktive Teilnahme aller. Das Konsortium sieht daher umfangreiche Beteiligungsmöglichkeiten für die Nutzenden aller involvierten Fachdisziplinen, aber auch für Kunst- und Kulturschaffende unterschiedlichster Tätigkeitsbereiche und Vertreter*innen der Zivilgesellschaft vor. Es zielt darauf ab, das breite Spektrum der verschiedenen Akteur*innen im Bereich des Kulturerbes zu repräsentieren. Unter anderem ist vorgesehen, dass die Fachgemeinschaft selbst den Knowledge Graph mit eigenen Ressourcen erweitert und dessen Inhalte pflegt. Bedingung dafür ist eine technische Infrastruktur, die es ermöglicht, Ressourcen intuitiv und ohne tiefe technische Kenntnisse zu kuratieren, hinzuzufügen, zu verknüpfen und zu durchsuchen. Diese Infrastruktur wird eine semantische expressive Repräsentation der Daten ermöglichen, um eine vollumfängliche Umsetzung der F.A.I.R Prinzipien zu gewährleisten.

            

            

                
NFDICO und der NFDI4Culture Knowledge Graph 

                
Ein Ziel von Task Area 5 in NFDI4Culture ist die Bereitstellung von Ontologien, um die Forschungsdaten in NFDI4Culture standardisiert und formal zu repräsentieren und miteinander verknüpfen zu können. In einem “bottom-up” Ansatz nach der sogenannten “Waterfalls” Methode (Keet, 2020) und im kontinuierlichen Austausch mit den Domänenexpert*innen des Konsortiums wurde dazu die NFDICO Ontologie entwickelt. Sie verknüpft Datensätze, Forschungsprojekte, Services, Repositorien, Institutionen und Forschungsdisziplinen und dient als Grundlage für den NFDI4Culture Knowledge Graph. Als Modellierungsgrundlage dienten die durch die NFDI4Culture Community übermittelten Beschreibungen ihrer Forschungsressourcen. Die Klasse nfdico:Contribution
 (Abb. 1) repräsentiert die Beiträge der Fachgemeinschaften und ordnet die Arten der Beiträge (z.B. Datenportal, Datensatz, Kollektion, Software, Infrastruktur, Service) unter.
                

                

                    

                        

                        
Abbildung 1: Modellierung der Klasse nfdico:Contribution

                    
Instanzen können beispielsweise durch Medientypen, Lizenzangaben, zugehörige Personen und Institutionen und Projekte beschrieben werden. Ein Modellierungsbeispiel für die Klasse nfdico:Service
 als Unterklasse von nfdico:Contribution ist in Abb. 2 dargestellt. Die Klasse wird beispielsweise durch die akademische Disziplin, Medientypen, verwendete Normen und Ontologien spezifiziert. 
                

                

                    

                        

                        
Abbildung 2: Modellierung der Klasse nfdico:Service

                    
NFDICO besteht in der Version 1.1 aus 36 Klassen und 60 Objektattributen. Die spezifisch für den Anwendungsfall in NFDI4Culture definierten Klassen (Prefix nfdico) wurden den “best-practices” in der Ontologieentwicklung folgend zur Sicherstellung hoher semantischer Expressivität und Interoperabilität mit 24 bereits existierenden Ontologien verknüpft, darunter frapo
, fabio
, void
 und schema
 (Keet, 2020). Die Ontologie ist seit Juni 2022 öffentlich verfügbar und vollständig dokumentiert
. NFDICO folgt einem generischen Modellierungsansatz und repräsentiert die Beiträge der Fachgemeinschaft nicht ausschließlich für NFDI4Culture Domänen, sondern ermöglicht die Nachnutzung durch weitere NFDI Konsortien anderer Fachrichtungen. So dient NFDICO beispielsweise als Grundlage der Basisontologie des NFDI Konsortiums NFDI-MatWerk
. Gemäß der Waterfalls Methodologie der Ontologieentwicklung wird NFDICO in Absprache mit Domänenexpert*innen iterativ und modular erweitert.
                

                
Wie bereits o.g., dient NFDICO als Grundlage für den NFDI4Culture Knowledge Graph. Eine erste Version des Graphen ist publiziert und kann über einen öffentlichen SPARQL Endpunkt
 abgefragt werden. Der inhaltliche Aufbau des Knowledge Graphen erfolgte auf Basis der Forschungsressourcen und beschreibenden Metadaten, die von der NFDI4Culture Community übermittelt wurden. Alle Beiträge wurden normalisiert und via RDFLib
 in den Knowledge Graph integriert. Außerdem wurden (soweit möglich) Verknüpfungen der Entitäten aus dem Knowledge Graph zu Wikidata
 und der Gemeinsamen Normdatei (GND)
 hergestellt. Das heißt, die Daten im NFDI4Culture Knowledge Graph sind von Anfang an anschlussfähig und können mittels föderierter SPARQL Abfragen erweitert werden. So können für alle in NFDI4Culture beteiligten Organisationen über die jeweiligen Wikidata-Verknüpfungen zusätzliche Informationen, wie zum Beispiel der Typ der Organisation (z.B. Bibliotheken
 ) in die Ergebnisanzeige mit einbezogen werden, obwohl diese Informationen im NFDI4Culture Knowledge Graph nicht explizit enthalten sind.
                

                
Der NFDI4Culture Knowledge Graph enthält aktuell 1796 Entitäten, darunter 173 Organisationen, 156 Forschungsressourcen, die mit ca. 10.000 RDF-Tripeln beschrieben werden. Der Knowledge Graph wird kontinuierlich erweitert und schrittweise für Beiträge durch die Community geöffnet, sodass alle Akteur*innen in Zukunft selbst eigene Ressourcen verknüpfen und öffentlich zugänglich machen können. Die Interaktion der Community mit dem Knowledge Graph erfolgt über eine einheitliche und intuitive Schnittstelle, das im Folgenden beschriebene Culture Information Portal.

            

            

                
Culture Information Portal und Integration des Knowledge Graph 

                
Mit dem Culture Information Portal verfolgt NFDI4Culture das Ziel, einen einheitlichen, intuitiven und zentralen Einstiegspunkt auf die dezentral gespeicherten Forschungsdaten der Community und alle weiteren Dienste des Konsortiums (wie z.B. ein übergreifendes Helpdesk, nachnutzbare Guidelines zu allen Bereichen des Forschungsdatenmanagements oder das konsortiumsweite Identitäts- und Zugriffsmanagement für die gemeinsamen Kommunikations- und Kollaborationswerkzeuge) bereitzustellen. Forschungsdaten sollen durch die Community selbst hinzugefügt, kuratiert und auffindbar gemacht werden. Weiterhin informiert das Culture Information Portal über aktuelle Veranstaltungen und Neuigkeiten des Konsortiums, sowie über beteiligte Akteure und Projektfortschritte. Alle Informationen im Portal sollen außerdem im Sinne der F.A.I.R. Prinzipien als Linked Open Data vorliegen und eine gute Anschlussfähigkeit an europäische Informationsinfrastrukturen wie die European Open Science Cloud gewährleistet sein. Das Portal wurde aus diesem Grund als webbasiertes Forschungsinformationssystem (Current Research Information System / CRIS) auf Basis des Open Source Content Management Systems TYPO3
 realisiert. Das Datenmodell des Portals orientiert sich am CERIF-Standard der euroCRIS
, wodurch gleichzeitig eine sehr gute Kompatibilität zu NFDICO gegeben ist. Für die CRIS Implementierung wurde die TYPO3-Extension “Academy” nachgenutzt und weiterentwickelt
.
                

                
Zur Integration des Culture Knowledge Graphen wurden mit dem Ziel einer föderierten Informationsinfrastruktur zunächst existierende Systeme auf die oben genannten Kriterien überprüft, darunter Wikibase
 und WissKI
. Wikibase ist eine freie Software zur kollaborativen Kuratierung von Datenbanken mit der Möglichkeit, Daten im Sinne von LOD zu strukturieren, zu verknüpfen und abzufragen. Wikibase bietet allerdings nicht die Möglichkeit, externe, bereits existierende Ontologien zu integrieren, was die semantische Expressivität der Daten stark begrenzt. Außerdem ist es mit Wikibase nicht möglich, eigene Eingabemasken zu entwickeln, was die intuitive Interaktion mit dem Knowledge Graph einschränkt. WissKI ist eine webbasierte Software zum Sammeln, Strukturieren und Teilen von forschungsbezogenen Daten. Feste Grundlage von WissKI bildet dabei die Optimierung der Software auf CIDOC-CRM, was sich für den hier beschriebenen Anwendungsfall als zu einschränkend gezeigt hat. Die Modellierung in CIDOC-CRM folgt einem Ereignis-basierten Paradigma. Dadurch gerät die Modellierung einfacher Fakten oft sehr komplex und behindert den typischen Anwendungsfall in NFDI4Culture. Durch CIDOC werden einfache SPARQL-Abfragen zu Personen und Organisationen oft hochkomplex und daher ineffizient. Aufgrund der großen Menge an Daten, die in NFDI4Culture erwartet werden, ist eine Abfrage-Effizienz allerdings relevant. Der Zugang zu NFDI4Culture Daten soll für alle Nutzer*innen so einfach wie möglich gestaltet werden, eine CIDOC-CRM-basierte Modellierung schafft aufgrund ihrer Komplexität zusätzliche Barrieren. Dennoch ist CIDOC ein wichtiger und gängiger Bestandteil der GLAM Community. Daher wird ein Mapping des NFDI4Culture Datenmodells nach CIDOC durchgeführt, um einen CIDOC-basierten Export zu gewährleisten.
                

                
Als beste Lösung zur Gewährleistung der semantischen Expressivität der erfassten Ressourcen und Metadaten mittels NFDICO und weiterer Ontologien einerseits bei gleichzeitiger Umsetzbarkeit der benötigten Kurationsmechanismen andererseits stellte sich die direkte Implementierung der benötigten Funktionalitäten in TYPO3 heraus. Die TYPO3 Extension “LOD”
 bietet hierbei einen unmittelbar über der relationalen Datenbank des CMS realisierten “Semantic Layer” mit einem konfigurierbaren IRI-Generator sowie IRI-Resolver für alle Datensätze sowie einem RDF-Serializer für alle Datenbankinhalte. Alle im Portal bzw. Culture CRIS erfassten Ressourcen werden dabei über eine standardisierte LOD API unter Verwendung des Hydra Core Vocabulary
 in verschiedenen RDF Serialisierungen (z.B. RDFa, Turtle, JSON-LD u.a.) veröffentlicht
. Hierdurch können die Daten für den Culture Knowledge Graph bereits jetzt im Kreis der Mitarbeitenden des Konsortiums dezentral kuratiert und kontinuierlich erweitert werden. Die über die LOD API des CMS publizierten Daten werden mittels Ingest-Routinen in den eigentlichen Knowledge Graph integriert. Zum Einsatz kommt oxigraph
 als nativer RDF-Store mit einem pythonbasierten Wrapper
 für ein leichtgewichtiges Deployment des öffentlichen SPARQL-Endpoints, der über ein grafisches Interface wiederum direkt in das Culture Information Portal integriert ist.
                

            

            

                
Weiteres Vorgehen und Zusammenfassung 

                
NFDI4Culture öffnet Forschungsdatensilos und schafft einheitliche und intuitive Zugriffsmöglichkeit auf Forschungsdaten. Dieses Vorhaben wird in der Task Area 5 durch die Bereitstellung dedizierter Ontologien, die Implementation und Verknüpfung von Knowledge Graphen und die Umsetzung des Culture Information Portals erreicht. Akteur*innen der Community können mit ihrer Beteiligung am Vorhaben ihre Daten also auffindbar, interoperabel und wiederverwendbar machen, die Zitierfähigkeit eigener Ressourcen gewährleisten und die multidisziplinäre Verknüpfungen der Daten für ihre Forschung ausnutzen. Alle hier präsentierten Beiträge sind öffentlich verfügbar und nutzbar. Im weiteren Vorgehen wird die Ontologie bedarfsorientiert erweitert und das Culture Information Portal schrittweise für die Community geöffnet werden, um mit den Inhalten im Knowledge Graph zu interagieren und den Knowledge Graph mit Forschungsdaten anzureichern und diese zu kuratieren. Die technische Infrastruktur wird stetig verbessert, um unter anderem teil-automatisierte Qualitätskontrollen der Daten zu umzusetzen (z.B. durch SHACL
) und die Integration größerer Datenmengen über den SPARQL Endpunkt zu ermöglichen.
                

                
Vergleichbare und verwandte Initiativen werden mit der “Linked Data Platform Finland”
 und dem “Dutch Digital Heritage Network”
 umgesetzt. Auch NFDI4Culture ist eine nationale Initiative, gleichsam ist allen Beteiligten die Bedeutung einer globalen Vernetzung bewusst, denn auch deutsche Kulturdaten und deutsches kulturelles Erbe sind international und von internationalem Interesse. In NFDI4Culture wurden bereits Maßnahmen implementiert, um auch Organisationen und Forschende außerhalb des Konsortiums zu involvieren. Beispielsweise dienen dazu das Steering Board
 und die “Linked Open Data Working Group”. Technisch ist eine Vernetzung mit anderen Plattformen und die Nutzung international anerkannter Standards ebenso elementar. Der NFDI4Culture Knowledge Graph enthält bereits Verknüpfungen zu Wikidata, die mit Hilfe der Nutzer*innen stetig erweitert werden. Zudem besteht auf der NFDI4Culture Ontologie-Ebene bereits ein Mapping mit dem “Common European Research Information Format (CERIF)”. Diese Verknüpfungen werden in Zukunft weiter ausgebaut werden.
                

                
Unter Umsetzung der F.A.I.R. Prinzipien und mit der Beteiligung der gesamten Community wird in NFDI4Culture eine Forschungsinfrastruktur geschaffen, die langfristig und nachhaltig Forschungsdaten auffindbar, zugreifbar und nutzbar für alle macht. 

            

        

            

                
Offene Archive in virtuellen Räumen

                
Literaturarchive – wie literarische Gedächtniseinrichtungen im Allgemeinen – stehen heute mehr denn je vor der Herausforderung, ihre gesellschaftlichen Aufgaben der partizipativen Vermittlung von und der diskursiven Verständigung über Literatur, Kultur und Geschichte und also ihr Selbstverständnis als gegenwartsorientierte Akteure des kulturellen Gedächtnisses vor dem Hintergrund des medialen Wandels neu zu bestimmen (Wettmann 2018), z.B. im Kontext aktueller Konzeptualisierungen von „Virtual Heritage“ (Champion 2021). Auf die erste Phase der digitalen Öffnung von Literaturarchiven – die Transformation zu offenen Datenräumen – folgt derzeit eine zweite, in der sich die Gedächtniseinrichtungen selbst in den virtuellen Raum übersetzen, um weitere Möglichkeiten der Zugänglichkeit zu eröffnen (Bekele und Champion 2019). 

                
Forciert durch die Pandemie bedeutet dies zunehmend auch, translokale Formen der Präsenz im Archiv zu erproben und damit das Archiv als Ort (Cunningham 2017) neu zu denken. Gepaart mit den Digitalisierungs- und Bereitstellungsbestrebungen von offenen Kulturdaten und -objekten im digitalen Raum, bieten sich so neue Experimentierräume für digitale Interaktionen (vgl. museum4punkt0 2022). Dabei sind die Akteure auf Technologien angewiesen, die Forschungs- und Vermittlungsziele offenlegen und derart kommunizieren, dass über eine museale Präsentation hinaus eine Interaktion mit Kulturobjekten und -praktiken im digitalen Raum möglich wird. Das kann durch virtuelle Lern- oder Spielumgebungen geschehen, die interaktive Erzählräume schaffen. Datenvisualisierung und Datendesign erfolgen beispielsweise durch virtuelle und räumliche Darstellungen (vgl. Glinka u.a. 2020) sowie zunehmend durch simulierte Umgebungen und virtuelle Welten, etwa 3D-Rundgänge (vgl. z.B. Österreichisches Staatsarchiv o.J.). 

                
Das Kooperationsprojekt „FontaneVR“ der Theodor Fontane Gesellschaft e.V. und des Theodor-Fontane-Archivs der Universität Potsdam hat auf diese Überlegungen aufbauend im Jahr 2022 einen Prototyp einer digitalen 3D-Ausstellungs- und Interaktionsumgebung konzipiert und entwickelt. Dafür ist eine VR-basierte Begegnungsstätte gestaltet worden, die der Villa Quandt, dem Sitz des Theodor-Fontane-Archivs, nachempfunden ist, diese aber auch erweitert und hybridisiert. Für diesen virtuellen Ort werden begleitend digitale Vermittlungsformate (Literatur- und Bildungsevents) sowie interaktive Ausstellungsformate entwickelt, die sich an ein diverses Publikum richten und, erreichbar über Webbrowser, individuell oder kollektiv besucht werden können.

            

            

                
„FontaneVR“: Umsetzung und Konzept

                
Das Projekt „FontaneVR“
 verfolgt zwei Schwerpunkte: Zum einen den 
                    
Bau eines 3D-Modells
 der Villa Quandt, samt realistischer Nachbildung der Umgebung, photogrammetrischer Erfassung der Architektur und maßstabsgetreuer, realistischer, grafischer 3D-Darstellung der Räumlichkeiten und Ausstellungsobjekte. Zum anderen die Konzeption verschiedener 
                    
Vermittlungsformate
, die im Rahmen des 3D-Modells virtuelle Begehung, Exploration und Begegnung erlauben.
                

                

                    
3D-Modell

                    
Der detailgetreuen Modellierung der Villa Quandt (siehe Abb.1), ihrer Innenräume und ausgewählter Objekte (siehe Abb. 2) liegt ein objekt- und materialangemessener 3D-Digitalisierungsprozess nach dem Prinzip der Photogrammetrie (siehe Abb. 3.) sowie eine webbasierte 3D-Präsentation über Mozilla Hubs zugrunde. Während ein fotorealistisches Modell des Archiv-Gebäudes die Grundlage bildet, werden einzelne Räume zugleich gezielt ›fiktionalisiert‹, d.h. mit Möglichkeiten etwa zur musealen Präsentation und Kontextualisierung von Archivobjekten ausgestattet, die in der realen Villa Quandt nicht gegeben sind (Abb. 3). Damit ist das 3D-Modell nicht nur eine Abbildung des Archivortes, sondern eine Erweiterung, die VR bewusst zum Aufbau zusätzlicher archivarischer Funktionen nutzt.

                    

                    

                        

                        
Abb. 1: prototypische Außenansicht des 3D-Modells der Villa Quandt

                    

                    

                        

                        
Abb. 2: Innenraum der Villa Quandt (3D-Referenzmodell)

                    

                    

                        

                        
Abb. 3: Verschiedene photogrammetrische Erfassungen von Außen- und Innenräumen der Villa Quandt

                    

                    

                        

                        
Abb. 4: ›Fiktionalisierte‹ Innenräume für museale Inszenierungen mit 3D-Ausstellungsobjekten

                    

                

                

                    
Vermittlungsformate

                    
Als konkrete Vermittlungsformate sind eine Reihe von Implementierungen und eventbasierte Nutzungen der virtuellen Begegnungsstätte geplant, die zuvor kuratorisch konzipiert und entwickelt wurden.

                    

                        
Ästhetische Erfahrung

                        
Die 
                            
literarische Ausstellung
: In den virtuellen Räumen werden verschiedene Ausstellungen konzipiert, die es ermöglichen, die Materialität des Fontane-Nachlasses (z.B. Handschriften-Digitalisate) zu erfahren, die Aktualität Fontanes zu entdecken (z.B. durch Tafeln zu neuen Forschungspositionen) oder die Geschichte der Fontane Gesellschaft und des Archivs kennenzulernen. Die Ausstellungen können dabei zeitungebunden durch interessierte Nutzer*innen besucht werden; darüber hinaus werden von professionellen Guides durchgeführte Führungen durch die Ausstellung angeboten.
                        

                    

                    

                        
Information und Unterhaltung

                        
Die 
                            
literarische Veranstaltung
: In Form von Vorträgen, Lesungen, Vorträgen, Werkstattgespräche oder Diskussionen werden diskursive Formen der Vermittlung von Literatur und Geschichte angeboten, zu denen Interessierte sich ortsungebunden in der virtuellen Begegnungsstätte versammeln können.
                        

                    

                    

                        
Gamification

                        
Das 
                            
literarische Spiel
: Insbesondere – aber keineswegs nur – für die jüngere Generation (etwa Schülerinnen und Schüler) wird die virtuelle Begegnungsstätte zur Spielstätte transformiert, in der sich Such- und Erkundungsspiele rund um das literarische Werk und den handschriftlichen Nachlass Fontanes durchführen lassen.
                        

                    

                

            

        

            

                
Einführung: Motivation

                
Text+ ist ein Konsortium der nationalen Forschungsdateninfrastruktur (NFDI). Seine Partner vereinigen die Expertise der bestehenden Infrastrukturverbünde CLARIN-D (Hinrichs und Trippel 2017) und DARIAH-DE (Neuroth u. a. 2016) und integrieren weitere Partner aus den Bereichen Datenzentren, Bibliotheken, Universitäten, Akademien und Rechenzentren. Durch diesen Zusammenschluss entsteht ein einzigartiger Schatz textueller Korpora, die Text+ durch sein Angebot nicht nur unter Beachtung der FAIR- (Wilkinson u. a. 2016) und CARE-Prinzipien (Carroll u. a. 2021) der Community zur Verfügung stellt, sondern Dienste und Beratung anbietet, um auf vielfältige Weise mit ihnen zu arbeiten. Dieser Beitrag wirft einen beispielhaften Blick auf vorhandene Korpora in Text+ (in Text+ vor allem innerhalb der Datendomäne Sammlungen), veranschaulicht die Ebenen des gemeinsamen Zugriffs und zeigt Möglichkeiten zur weiteren Integration von Ressourcen auf.

                
Text+ kann bei der Integration von Ressourcen auf vielfältige Vorarbeiten zurückgreifen. Die Vorarbeiten beziehen sich dabei etwa auf verschiedene Infrastrukturen, darunter CLARIN-D als nationalem Zweig der europäischen Infrastruktur CLARIN (Hinrichs und Krauwer 2014) und DARIAH-DE als nationalem Zweig der europäischen Infrastruktur DARIAH (Kálmán u. a. 2019; bzw. überblicksartig Gray 2021). Allerdings umfasst Text+ auch weitere Partner, die zuvor nicht an diesen geisteswissenschaftlichen Infrastrukturverbünden beteiligt waren, die aber unabhängig davon insbesondere große Inventare an Referenzdaten erstellt haben und weitere Erfahrungen in das Konsortium einbringen. 

                
 

            

            

                
Beispielhafte vorhandene Korpora

                
Auch wenn das NFDI-Konsortium Text+ erst am 1. Oktober 2020 formal begonnen hat, gibt es zahlreiche Vorarbeiten, die durch die Partner bereits zur Verfügung stehen und von vielen Forschenden (nach-)genutzt werden. Einige dieser Referenzdatensätze, die in Text+ zu den Sammlungen gehören, werden hier kurz vorgestellt. 

                

                    
DeReKo

                    
Das deutsche Referenzkorpus (DeReKo, siehe Kupietz u. a. 2018; 2010; Lüngen 2017; Kupietz und Keibel 2009) ist eine Sammlung elektronischer deutschsprachiger Korpora. Mit über 53 Milliarden Wörter ist diese linguistische Sammlung die weltweit größte ihrer Art. DeReKo enthält belletristische und wissenschaftliche Texte, Periodika und viele weitere Textarten der Gegenwart und früheren Vergangenheit. Über die Werkzeuge COSMAS II und KorAP ist es niedrigschwellig zugänglich und für diverse linguistische Fragestellungen nutzbar. 

                

                

                    
Zeitungskorpora der DNB

                    
Die Deutsche Nationalbibliothek (DNB) sammelt gemäß ihres gesetzlichen Auftrags Zeitungen und Zeitschriften, die in Deutschland publiziert werden. Der umfangreiche Bestand beläuft sich aktuell auf ca. 311.000 gedruckte Titel und ca. 3,2 Millionen Ausgaben E-Paper. Insgesamt umfasst die digitale Sammlung der DNB aktuell ca. 12 Millionen Objekte. 

                

                

                    
Das Zeitungsportal der DDB. 

                    
Über das neu geschaffene Zeitungsportal der Deutschen Digitalen Bibliothek (DDB) ist eine große Sammlung historischer Zeitungen aus den Jahren 1671–1950 zugänglich. 

                

                

                    
ELTeC-Korpus

                    
Das Korpus der European Literary Text Collection (ELTeC, Schöch u. a. 2021) im TextGrid Repository (TextGrid Repository 2020) ist eine Textsammlung von 2500 linguistisch annotierten Romanen in mindestens zehn Sprachen. Sie ermöglicht es, Methoden und Verfahren der Textanalyse über mehrere Nationalliteraturen hinweg zu vergleichen. 

                

                

                    
Baumbanken

                    
Baumbanken haben in der Sprachwissenschaft eine lange Tradition. Im Gegensatz zu anderen Arten von Korpora und Sammlungen enthalten sie geparste Texte, die syntaktische Annotationen enthalten. Dadurch werden sie für die Forschung zur Grammatik, Typologie, Sprachtechnologie, etc. eingesetzt (siehe z. B. Kübler, McDonald und Nivre 2009; Hinrichs, Filippova und Wunsch 2005). Ein Beispiel für eine Baumbank ist die Tübinger Baumbank des Deutschen/Zeitungskorpus (TüBa-D/Z, siehe auch Telljohann, Hinrichs und Kübler 2004), die auf Artikeln der Zeitung ‘die tageszeitung’ (taz) basiert und in der letzten Ausgabe (Version 11) auf 3816 Artikeln und über 100.000 Sätzen mit fast 2 Millionen Token beruht (siehe 
                        
https://uni-tuebingen.de/de/134290
). Mit spezialisierten Suchprogrammen wie Tündra (Martens 2013) kann nach Wörtern und syntaktischen Strukturen gesucht werden. Tündra ist dabei nicht auf das Deutsche beschränkt. Die Spannweite reicht dabei von Werken von Thomas von Aquin (Martens und Passarotti 2014) bis zu den vielen Baumbanken der Universal Dependency Treebanks Initiative (
                        
https://universaldependencies.org/
), die für viele Sprachen Baumbanken zur Verfügung stellt. 
                    

                

                

                    
Beispiele weiterer Korpora in Text+ 

                    
Daneben gibt es in Text+ noch zahlreiche weitere wichtige Korpora. Dazu gehört beispielsweise das Deutsche Textarchiv (DTA) an der Berlin-Brandenburgischen Akademie der Wissenschaften mit Texten ab ca. 1600 bis 1900. Gesprochensprachliche Korpora sind z. B. am Bayerischen Archiv für Sprachsignale an der LMU München, mit der Datenbank Gesprochenes Deutsch (DGD) am IDS Mannheim oder auch in der Digitalen Bibliothek an der SUB Göttingen vorhanden. Diese Aufzählung ist bei weitem nicht vollständig, zeigt aber auf, dass die obige, beispielhafte Auswahl von Ressourcen nur einen kleinen Teil des Portfolios abdeckt und daher Lösungen innerhalb von Text+ zwar mit Hilfe einiger Beispiele implementiert werden, aber die Lösungen immer die Erweiterungsmöglichkeit auch auf weitere Daten und Partner erlauben muss. Alle Ressourcen, Einrichtungen und Angebote können als Module von Text+ gesehen werden, die durch die Infrastruktur vernetzt werden, so dass auf sie gemeinsam zugegriffen werden kann. 

                

            

            

                
Ebenen des gemeinsamen Zugriffs

                
Ressourcen in Text+ liegen an unterschiedlichen Institutionen verteilt vor, teilweise sind sie abgeschlossen, teilweise werden sie noch aktiv weiterentwickelt. Da viele bestehende und im Aufbau befindliche Ressourcen Rechte Dritter berühren, z. B. über Verlagsrechte an Texten, bedeutet ein FAIRer Zugang nicht, dass der Zugang offen und frei sein kann. Vielmehr stehen viele Ressourcen unter expliziten Lizenzen, die die Nutzungsart einschränken. So kann beispielsweise auf viele digitale textuelle Daten der DNB nur innerhalb der Bibliothek und über deren Infrastruktur zugegriffen werden, um rechtliche Bestimmungen zu wahren. Für andere Sammlungen gibt es Zugangsbeschränkungen, die über ein Login den Zugang auf autorisierte Nutzende einschränkt, um Lizenzbestimmungen und -verträge einhalten zu können. Eine Grundlage für die konsortiumsweite und communityintegrierende Arbeit an und mit den Korpora ist daher ein gemeinsamer, niederschwelliger Zugriff. In Text+ wie in verschiedenen Vorarbeiten wird dazu eine gemeinsame AAI-Lösung (Authentication and Authorization Infrastructure) verwendet, über die Nutzende die verschiedenen Ressourcen und Werkzeuge verwenden können. 

                
Da der Zugriff auf die Daten oft eingeschränkt ist, gibt es häufig keinen Vollzugriff auf die (digitalen) Daten. 
                    
Über ein föderiertes Anmeldeverfahren, das auf Shibboleth und den Diensten des Deutschen Forschungsnetzes (DFN) und deren europäischer Zusammenarbeit im Rahmen von GÉANT aufsetzt, sind viele Dienste institutionsunabhängig zugänglich, z. B. Dienste, die über die AcademicCloud unseres Partners GWDG angebunden sind. Dienste wie die Federated Content Search oder die DARIAH Collection Registry bieten bereits jetzt übergreifende Suchmöglichkeiten nicht nur in Korpora. Daten werden über verschiedene Archive bereitgestellt, darunter das DARIAH-DE Repository an der SUB Göttingen sowie die zahlreichen zertifizierten CLARIN-Zentren.

                

                
In vielen Fällen besteht ein freier Zugriff auf Informationen in Katalognachweissystemen. Diese enthalten in der Regel zumindest Informationen über die Existenz der Korpora (bzw. anderer Forschungsdaten) und andere grundlegende, beschreibende Metadaten. Im Bereich der Metadaten besteht eine Herausforderung in den unterschiedlichen verwendeten Metadatenformaten, die nicht zuletzt bezüglich ihrer integrierten semantischen Interoperabilität divergieren. Hier reicht die Spannbreite von Standardformaten (z. B. dem TEI-Header, Dublin Core, Marc21, ISO 24622-X CMDI) bis zu relativ frei definierten Beschreibungen von Ressourcen. Mit mehreren Hunderttausend Datensätzen der Text+ Partner insbesondere im Bereich der Sammlungen stellt die Vernetzung der Daten eine erhebliche Herausforderung dar. In Arbeitsgruppen von Text+, die über die verschiedenen Daten- und Aufgabendomänen hinweg gebildet wurden, werden für den Zugang Lösungen entwickelt. Die Arbeitsgruppe zur Text+ Registry arbeitet dabei an den einzusetzenden Verfahren und Schnittstellen, um ein Inventar der verschiedenartigen Ressourcen anbieten zu können. Dabei zeichnet sich ab, dass für eine solche große Datenmenge eine Listendarstellung nicht ausreichend ist und zumindest grundlegende gemeinsame Informationseinheiten in den verschiedenen Metadaten enthalten sein müssen, um eine sinnvolle Überblicksdarstellung des Inventars zu ermöglichen. Die unterschiedlichen Anforderungen an Metadaten durch Bestandsdaten, Datentypen und neuere Entwicklungen werden dabei gewahrt werden. Eine weitere Arbeitsgruppe beschäftigt sich mit Fragen des Linked Data, wobei hier zwischen Linked Data für beschreibende Metadaten und Objektdaten unterschieden wird. Durch die Verknüpfung von Metadaten mit z. B. Normdaten und dem Angebot im Rahmen von Linked Data-Initiativen wird untersucht, welche Mehrwerte für Forschungsfragen der Text+ Community geschaffen werden können. Die Darstellung von Objektdaten wie Korpora in Linked Data-Formaten sowie die Schaffung rechtskonformer Angebotsmöglichkeiten sind hier noch am Beginn der Entwicklungen. 

                
Ein zusätzlicher, gemeinsamer Zugang zu den Daten über die Metadaten hinaus wird in Text+ mit sogenannten abgeleiteten Textformaten (Schöch u. a. 2020) weiterentwickelt. Abgeleitete Textformate sind dabei Informationen über die Texte, die nicht die Lizenzbedingungen verletzen, aber trotzdem jene Informationen enthalten, die für konkrete Forschungsfragen erforderlich sind. Ein Ziel besteht dabei darin, z. B. die bei Partnern von Text+ befindlichen vollständigen Sammlungen rechtskonform nutzbar zu machen, etwa die Zeitungen, die an der DNB gesammelt werden oder Korpora des IDS, die nicht im Volltext außerhalb der Lizenzbestimmungen bereitgestellt werden dürfen. Eine zusätzliche Zugangsmöglichkeit zu den zugrundeliegenden Texten unter Berücksichtigung der Rechte Dritter entwickelt Text+ auf Grundlage einer föderierten Suche über den Inhalt von Ressourcen, die Federated Content Search (FCS, Vorarbeiten siehe Stehouwer u. a. 2012). Eine Herausforderung bei der Inhaltssuche sind die unterschiedlichen Annotationsebenen, welche die teils aufwändig annotierten Korpora enthalten, seien es linguistische aber auch andere Annotationen, die z. B. in anderen TEI-Repräsentationen von Texten enthalten sind. Hier muss – ebenso wie für andere Suchen – eine gemeinsame Anfragesprache definiert werden. Da diese gemeinsame Anfragesprache zwangsläufig nur eine Teilmenge der Möglichkeiten spezialisierter Werkzeuge bieten kann, können Anfragen, die z. B. durch hochspezialisierte Forschungsfragen motiviert sind, häufig besser mit den spezialisierten Werkzeugen bearbeitet werden. Ein konkretes Beispiel sind Anfragen zu bestimmten syntaktischen Strukturen, die in Baumbanken enthalten sind, die aber auf nicht syntaktisch annotierten Daten nicht zu einem Ergebnis führen können und daher in einer föderierten Inhaltssuche nur eingeschränkt zur Verfügung stehen. 

            

            

                
Integration weiterer Ressourcen

                
Ein Anspruch, den eine nationale Forschungsdateninfrastruktur adressieren muss, ist der, dass eine Vollständigkeit kaum gewährleistet werden kann und daher eine Offenheit gegenüber neuen Daten und Datengebenden bestehen muss. Insbesondere vor dem Hintergrund zunehmender Anforderungen an das Forschungsdatenmanagement im Rahmen des Forschungsprozesses – etwa der Anforderung, Daten, die als Grundlage von Forschungsergebnissen dienen, für mindestens 10 Jahre aufzubewahren (siehe DFG-Leitlinien zur Sicherung guter wissenschaftlicher Praxis
) – können Forschende darauf angewiesen sein, verlässliche Partner zu finden, die sie beim Datenmanagement unterstützen. Text+ adressiert diesen Bedarf auf unterschiedliche Weise: (1) durch Kooperationsprojekte zur Förderung neuer Angebote zur Integration in die Text+ Infrastruktur; (2) durch offene Schnittstellen, durch die Datenzentren ihre Angebote auch über die Infrastruktur von Text+ bereitstellen können und (3) durch Angebote von Partnern von Text+, Daten zu hosten.
                

                
Kooperationsprojekte sind Projekte, die im Rahmen von Text+ das Portfolio an Daten und Diensten für die wissenschaftliche Gemeinschaft erweitern. Jedes Jahr erfolgt in Text+ eine Ausschreibung, durch die zusätzliche Arbeiten gefördert werden können, die eine Integration im Rahmen der NFDI-Angebote von Text+ ermöglichen. Ausgestattet mit substantiellen Mitteln können dadurch Bestandsdaten und -dienste die Infrastruktur erweitern.

                

                
Neben den Kooperationsprojekten basiert die Offenheit von Text+ auch auf den Schnittstellen. Als ortsverteilte Infrastruktur mit einer verteilten Datenhaltung, unterschiedlichen Schwerpunkten der Beteiligten, diversen Dateiformaten und zugrundeliegenden Technologien der Archivsysteme bei den verschiedenen Partnern sind Schnittstellen zur Zusammenarbeit der Partner in gemeinsamen Angeboten unverzichtbar. Ein Beispiel dafür sind die bereits beschriebenen unterschiedlichen Korpora, die durch die föderierte Inhaltssuche über eine gemeinsame Schnittstelle zugänglich sind. Diese Schnittstellen werden in Text+ offen spezifiziert und sind damit auch für Datenzentren außerhalb des Konsortiums nutzbar. Dadurch können auch Datenzentren außerhalb von Text+ ihre Angebote über die Infrastruktur verfügbar machen und können, so sie ihre Angebote nachhaltig bereitstellen möchten, verlässlich integriert werden. Über diesen Mechanismus können sich z. B. auch Anbietende von bisher nicht einbezogenen Communities, etwas aus dem Bereich der sog. „kleinen Fächer“, über die Infrastruktur vernetzen. 

                
Daneben übernehmen die unterschiedlichen Daten- und Kompetenzzentren von Text+ aber auch selbst Daten Dritter. Wenn Daten und Dienste zur Spezialisierung und zu den Möglichkeiten eines Partners passen, können sie dort gehostet werden. Die Prozesse – angefangen von Dateiformaten bis zu den dafür notwendigen Lizenzen und Übereinkommen – basieren auf den jeweiligen lokalen Kontexten der Partner. Als Fallback steht aber auch eine Bitstream-Archivierung von einem Partner in Text+ zur Verfügung, hier werden Daten archiviert, aber nicht notwendigerweise in weitere Infrastrukturdienste integriert, z. B. weil sie nicht die formalen Voraussetzungen erfüllen oder Daten nicht zu den bestehenden Daten- und Kompetenzzentren passen. Auch dies erlaubt es, die Community zu erweitern. 

                
Die Erweiterungsmöglichkeiten von Text+, gerade im Bereich der Sammlungen, sind dabei sehr vielfältig und erlauben individuelle Absprachen mit Forschenden. Dabei ist es entscheidend, dass die Forschenden durch die Infrastruktur ihre Interessen wahren und die Verantwortung für und Rechte an Ressourcen nicht verlieren. 

            

            

                
Ausblick auf weitere Entwicklungen in Text+

                
Das Bestreben der Vernetzung der an Text+ beteiligten Institutionen, ihrer Daten und Dienste wird auch in weiterer Hinsicht in diversen Arbeitsgruppen von Text+ vorangetrieben. Ein Bestandteil für diese Vernetzung und Integration ist eine übergreifende Registry, über die ein Zugang zu Ressourcen aus allen Text+-Datendomänen trotz deren Unterschieden, etwa ihrer disziplinären Genese, Institution, Sprache, etc. möglich werden soll. Diese Registry, aber auch soweit möglich die Daten an sich, sollen über eine Erweiterung der Federated Content Search durchsuchbar gemacht werden. Schließlich gibt es zur Verbesserung der Vernetzung der Daten eine Arbeitsgruppe zu Linked Data (LOD) in Text+. Um den Zugriff auf all diese Dienste zu gewährleisten und zu verbessern, wird die Text+-Homepage (
                    
https://www.text-plus.org
) sukzessive zu einem Text+-Portal als universalen Einstiegspunkt in die Text+-Infrastruktur transformiert. 
                

            

        

            

                
Einführung

                
Das im Januar 2022 gestartete Projekt „Sprachanfragen“ (https://www.ids-mannheim.de/gra/projekte2/sprachanfragen/) verfolgt das Ziel, Sprachanfragedaten – also Daten, die im Rahmen von verschiedenen Sprachberatungsszenarien entstehen, wie beispielsweise (1) – zu erfassen, aufzubereiten und ein wissenschaftsöffentliches Monitorkorpus aus ihnen zu erstellen. Dazukommend wird eine Rechercheschnittstelle entwickelt, mit der die Sprachanfragen systematisch wissenschaftlich analysierbar gemacht werden. 

                
(1) „[Frage:] Heißt es "Dramaform" oder "Dramenform" […]?

                
[Antwort:] In allgemeinsprachlichen Wörterbüchern ist diese Zusammensetzung nicht erfasst. Im allgemeinen Schreibgebrauch wird - wie eine Internetrecherche ergab - die Form mit Fugen-en vorgezogen.“ 

                
Sprachanfragen bieten einen authentischen Einblick in Probleme und Themen, die Sprecher:innen außerhalb der linguistisch-fachwissenschaftlichen Gemeinschaft beschäftigen. Wie Breindl (2016, 86f.) ausführt, bietet eine systematische Auswertung der Sprachberatungspraxis eine wertvolle Grundlage für die Erforschung einer großen Bandbreite verschiedener Fragestellungen. So können diese Daten u. a. dazu benutzt werden, um (i) Zweifelsfälle zu analysieren, wodurch Normierungslücken aufgedeckt werden können, und um (ii) Sprachwandelphänomene nachzuvollziehen. Ebenfalls können Sprachanfragen herangezogen werden, um (iii) Strategien zu erforschen, wie fachspezifische Inhalte von Nicht-Fachpersonen erfragt werden. Dadurch können bspw. die Zugangswege zu grammatischen und orthographischen Inhalten in einem webbasierten Informationssystem optimiert werden. Eine mögliche Optimierung wäre, Sprachanfragen automatisch in Form eines Chatbots zu beantworten.

                
Das Poster gibt einen Überblick über das Projekt, zeigt erste Ergebnisse und bietet einen Ausblick auf Überlegungen zur Konzeption eines Chatbots zur automatisierten Beantwortung von Sprachanfragen.

            

            

                
Datengrundlage

                
Das Monitorkorpus wird zum einen aus ~50.000 Sprachanfragen, die an den Sprachberatungsservice des WAHRIG-Verlags per E-Mail geschickt wurden, aufgebaut. Diese decken einen Zeitraum von 1999 bis 2018 ab. Die zugehörigen Antworten werden ebenfalls in das Korpus aufgenommen. Zum anderen wird das Korpus kontinuierlich mit Sprachanfragen erweitert, die im Leibniz-Institut für Deutsche Sprache eingehen. Um mehr Daten für das Trainieren eines Chatbots zu generieren, werden darüber hinaus Sprachanfragen aus Online-Quellen, wie z.B. gutefrage.net, extrahiert.

                
In einem ersten Schritt werden die Daten aufwendig vorverarbeitet. Dabei werden sie anonymisiert, um den Datenschutz zu gewährleisten und das Korpus wissenschaftsöffentlich zur Verfügung stellen zu können. Für die Anonymisierung ist die Nutzung eines NamedEntity-Erkenners, wie in anderen Arbeiten geschehen (vgl. u.a. Bleicken et al., 2016; Kleinberg et al., 2017), nicht optimal, da u. a. Namen ebenfalls Teil der Fragestellung sein können (vgl. (2)). Somit müssen automatisierte Lösungswege gefunden werden, um primär tatsächlich personenbezogene Daten zu ersetzen und die anschließende manuelle Nachkorrektur maßgeblich zu erleichtern. 

                
(2) "[...] Der Genitiv des Wortes "Paulus" […] sollte wie lauten: "Pauli" oder "Paulus'"? [...]"

                
Darüber hinaus werden die Sprachanfragen nach orthographischen und terminologischen Kriterien strukturiert, indem sie mit grammatischen Termini (z. B. „Dativ“, „Fugen-s“, „Getrenntschreibung“) annotiert werden. Basis dafür ist die terminologische Ressource der Abteilung Grammatik des Leibniz-Instituts für Deutsche Sprache, die sogenannte Wissenschaftliche Terminologie (WT, https://grammis.ids-mannheim.de/terminologie). Diese beinhaltet ~6.000 Termini aus der Domäne Deutsche Grammatik (vgl. u.a. Suchowolec et al., 2019). Berücksichtigt werden Uni- (z.B. „Substantivierung“), Bi- (z.B. „indirekte Rede“) und Trigramme (z.B. „negationsinduzierend additive Konnektoren“). Mit Hilfe eines Pattern Matchings werden vorkommende Termini in den lemmatisierten Sprachanfragen automatisiert detektiert. Über exakte Treffer hinaus werden bei der Annotation von Unigrammen auch Teiltreffer am Anfang oder am Ende eines Lemmas aufgenommen, bspw. „
                    
Genitiv
bezug", „
                    
Dativ
form“, „Muss-
                    
Komma
“. Somit werden auch Ausdrücke erfasst, die einen Terminus als Erst- oder Zweitglied beinhalten, als Ganzes jedoch nicht als Termini in der WT auftreten.
                

                
Um zu evaluieren, wie gut die Automatisierung der beiden Vorverarbeitungsschritte funktioniert, wird ein Subkorpus aus 1.000 zufällig extrahierten Sprachanfragen erstellt. Dieses wird manuell anonymisiert sowie terminologisch annotiert und als Goldstandard bei der Auswertung der automatischen Methoden herangezogen.

            

            

                
Ausblick: automatisierte Beantwortung von Sprachanfragen

                
Eine weiterführende, zukünftige Zielsetzung ist zudem, bei ausreichender Größe des Monitorkorpus, einen Chatbot zur automatischen Beantwortung von Sprachanfragen zu entwickeln. Dafür werden die Sprachanfragen nach den zugeordneten Termini gruppiert und ein Modell je Gruppe trainiert. Als Baseline wird ebenfalls ein regelbasierter Chatbot implementiert. Denkbar wäre auch eine Kombination aus regelbasiertem und trainiertem Chatbot. Das Ziel ist es, mit einem solchen System eine nicht-kommerzielle und offene (im Sinne von Veröffentlichung des Quellcodes) Alternative zu anderen Online-Grammatik- und Rechtschreibhilfetools (z. B. Deepkomma, Duden-Mentor, LanguageTool oder StudiKompass) zu schaffen, die durch nahtlose Anknüpfung an die umfassenden sprachwissenschaftlichen Ressourcen des hauseigenen wissenschaftlichen Informationssystem zur deutschen Grammatik grammis (https://grammis.ids-mannheim.de/) umfangreiche Materialien zum weiterführenden Selbststudium auf verschiedenen Komplexitätsstufen bietet.

                
Im Fokus der automatischen Beantwortung soll also nicht nur die Korrektur, sondern es sollen auch die sprachwissenschaftlichen Hintergründe einer Frage stehen. Zum Beispiel sollen im Fall der folgenden authentischen Sprachanfrage: „Was ist korrekt: „Haushalthilfe“ oder „Haushaltshilfe“, „Haushaltpflege“ oder „Haushaltspflege“?“ über die Angabe der korrekten Variante hinaus das zugrundeliegende Phänomen (Fugenelemente) benannt und entsprechende Artikel aus grammis verlinkt werden.

            

        

            

                

                    
Kontext und Bedarf
                

                
In diesem halbtägigen Workshop stellen wir ein auf spaCy basierendes Pipeline-System für das Natural Language Processing (NLP) narrativer Texte vor und erproben mit den Teilnehmer*innen dessen praktische Anwendung, besonders im Hinblick auf Untersuchungsgegenstände der digitalen Literaturanalyse.

                
Die Analyse von literarischen Texten ist eine besondere Herausforderung für die automatische Sprachverarbeitung, da sie oft komplexe Interaktionen linguistischer Strukturen auf der syntaktischen, semantischen und pragmatischen Ebene betrifft. Für die Interpretation solcher Texte ist es zum Beispiel wichtig, neben traditionellen NLP-Verarbeitungsschritten wie Eigennamenerkennung, Sentiment-Analyse etc., auch komplexere Analysen durchzuführen, um z. B. die Sprechinstanzen im Text zu identifizieren, Bezüge zur realen Welt zu erkennen oder zeitliche Strukturen im Text zu analysieren. Auf der praktischen Ebene bedeutet dies, dass automatische Analysen in der digitalen Literaturwissenschaft in der Regel die (oft komplexe) Kombination mehrerer basaler Sprachverarbeitungswerkzeuge auf Token-, Teilsatz-, Satz- und Passagen-/Diskursebene erfordert. Dies ist in der Praxis nicht immer trivial, z. B. weil Ein- und Ausgabeformate verschiedener Werkzeuge nicht kompatibel sind.

                

                    
Bibliotheken wie spaCy

                    
 

                    
stellen sehr umfassende Sammlungen von Sprachverarbeitungswerkzeugen zur Verfügung, können potenzielle Nutzer*innen durch ihre Fülle und Heterogenität aber auch überfordern. Das vorgestellte Pipeline-System 

                    
MONAPipe

                    
 

                    
(

                    
Dönicke u. a. 2022

                    
) soll hier Abhilfe schaffen, indem es Werkzeuge für linguistische und literaturwissenschaftliche Analysen komfortabel bündelt und flexibel erweiterbar ist. Der Fokus liegt dabei auf narrativen Texten und auf typischen Anwendungsszenarien der digitalen Literaturanalyse.

                

                

                    
Der Workshop vermittelt (i) die Grundlagen von spaCy und dessen Kernkomponenten (Tokenisierung, Lemmatisierung, Erkennung von Satz- und Teilsatzgrenzen, Dependency Parsing), (ii) demonstriert, wie MONAPipe an die eigenen Zwecke durch Custom-Komponenten angepasst werden kann, und versetzt

                    
  

                    
(iii) die Teilnehmer*innen mit hands-on Praxisbeispielen in die Lage, die in MONAPipe integrierten Komponenten zur Erschließung der linguistischen und narrativen Struktur eines Textes im Rahmen eigener Projekte kompetent auszuwählen, anzuwenden, zu erweitern und die Ergebnisse zu beurteilen. Unter anderem behandeln wir die Erkennung von Named Entities (sowie das Linking zu Normdaten; vgl. 

                    
Barth u. a. 2022

                    
), von Zeitformen (

                    
Dönicke 2020

                    
), Eventtypen (

                    
Vauth u. a. 2021

                    
) und Redeformen (direkte, indirekte, erlebte Rede; vgl. 

                    
Brunner u. a. 2020

                    
), Animatheit (

                    
Tuggener u. Klenner 2014

                    
) sowie Sentiment- (

                    
Remus u. a. 2010

                    
) und Emotionsanalyse (

                    
Mohammad u. Turney 2013

                    
). Schließlich erproben wir mit den Teilnehmer*innen, wie die Wechselwirkung einzelner Komponenten von MONAPipe Muster in Erzähltexten aufdecken kann, die zur Modellierung komplexer linguistischer und narrativer Phänomene geeignet sind 

                    
(

                    
z. B. Generalisierungen (

                    
Gödeke u. a. 2022

                    
)

                    
 

                    
oder narrative

                    
 

                    
Kommentare

                    
 

                    
(Weimer u. a. 2022)

                    
).

                

            

            

                
Technische Voraussetzungen

                

                    
Wir stellen 

                    
Jupyter-Notebooks

                    
 

                    
bereit, in denen MONAPipe und alle benötigten Dependencies vorinstalliert sind. Die Teilnehmer*innen benötigen Kenntnisse in Python; Erfahrung im Umgang mit Jupyter-Notebooks und der Unix-Kommandozeile ist hilfreich.

                

            

            

                
Zielpublikum

                
Der Workshop ist als Tutorial geplant und richtet sich an Literaturwissenschaftler*innen, Linguist*innen, DH-Forschende, und andere Personen, die an Textanalyse interessiert sind. Die Teilnehmer*innen bekommen die Möglichkeit, die Funktionalitäten von MONAPipe auszuprobieren und in vorbereiteten Texten eine Reihe von Phänomenen automatisch zu identifizieren. Die Teilnehmerzahl ist auf 30 beschränkt.

                
Lernziele und Methodik

                
Der Workshop verfolgt mehrere Ziele: (1) Er soll die Teilnehmer*innen mit spaCy und dessen Kernkomponenten vertraut machen und Ihnen praktische Erfahrung in der Nutzung von MONAPipe für typische Textanalysekomponenten auf Token-, Satz-/Teilsatz- und Passagenebene vermitteln. (2) Darüber hinaus erproben die Teilnehmer*innen die Einbindung neuer Komponenten, um damit wie sie MONAPipe für eigene Zwecke anpassen können. Aufbauend auf diesen Grundlagen lernen die Teilnehmer*innen an einem konkreten Beispiel, (3) wie sie MONAPipe konkret für Forschungsprojekte insbesondere in der digitales Literaturanalyse nutzen können. Dies umfasst die Auswahl geeigneter Komponenten für die Forschungsfrage sowie die Reflektion der Ergebnisse. Am Ende des Workshops haben die Teilnehmer*innen zum einen (i) ein besseres theoretisches Verständnis für die verschiedenen Sprachanalyseschritte, können komplexe Analysen durch Kombination mehrerer basaler Werkzeuge durchführen und die Qualität der automatischen Analyse beurteilen; Zum anderen (ii) haben die Teilnehmer*innen praktische Erfahrung im Umgang mit spaCy und verschieden Sprachverarbeitungswerkzeugen erworben  und Problemlösungsstrategien für den Umgang mit NLP-Werkzeugen gelernt.

                
Methodisch kombiniert der Workshop Theorie und Praxis, wobei der Praxisanteil überwiegt. Um das Gelernte zu festigen und zu vertiefen, bekommen die Teilnehmer*innen zunächst kurze Arbeitsaufträge (zu den Sprachverarbeitungskomponenten) und später komplexere Aufgaben (zur Analyse narrativer Texte), deren Lösungen im Anschluss diskutiert werden. Der Praxisteil im zweiten Teil des Workshops bietet außerdem die Möglichkeit, MONAPipe für ein eigenes Forschungsproblem anzuwenden und dazu Feedback von den Organisator*innen des Workshops zu bekommen.

                
Auf technischer Ebene arbeiten wir mit der interaktiven Programmierumgebung Jupyter-Notebook und stellen vorbereitete und ausführlich dokumentierte Notebooks zur Verfügung, um einen möglichst reibungslosen Ablauf zu ermöglichen und den Teilnehmer*innen zu helfen, sich auf die Workshopinhalte zu konzentrieren. 

                
Organisation und Ablauf

                
Wir planen einen vierstündigen Workshop bestehend aus zwei Blöcken. Der erste Block (1:45 h) beinhaltet aus einem einführenden Vortrag sowie einem Zeitslot zur Einrichtung der Jupyter-Notebooks , wobei die Organisator*innen nach Bedarf Hilfestellung bei der Einrichtung leisten. Anschließend erfolgt eine 45-minütige Session mit vorbereiteten Notebooks, bei der zunächst kürzere textuelle Phänomene auf Token-Ebene (wie Named Entities), Phänomene auf Teilsatz-Ebene (z. B. Zeitformen) sowie Phänomene, die längere Textpassagen umfassen (z. B. Redeformen), behandelt werden.

                
Im zweiten Block des Workshops (1:45 h) erstellen die Teilnehmer*innen eine eigene Komponente in spaCy. Anschließend erhalten die Teilnehmer*innen die Möglichkeit durch Lektüre narrative Strukturen in exemplarischen Textpassagen qualitativ zu bestimmen. Anhand der zur Verfügung stehenden spaCy-Komponenten soll evaluiert werden, welche Features sich zur Identifikation komplexer narrativer Strukturen eignen. Alternativ können die Teilnehmer*innen an eigenen Texten und Fragenstellungen arbeiten und hierfür Unterstützung durch die Workshoporganisator*innen erhalten.

                

                    
Alle Teilnehmer*innen erhalten einen Gastaccount bei der GWDG (Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen), um die Jupyter-Notebooks auf den GWDG-Jupyter-HPC-Servern nutzen zu können. Eine vorherige lokale Installation von Python oder zugehörigen Paketen ist nicht notwendig, dies wird im Vorfeld von der GWDG erledigt. Die HPC (High Performance Computing) Umgebung bietet die Möglichkeit, auch rechenaufwendige Pipelines zu testen. Im Rahmen der text- und sprachbasierten Forschungsdateninfrastruktur 

                    
Text+

                    
 

                    
stellt die SUB Göttingen Schnittstellen bereit, mit denen literarische Texte aus der Digitalen Bibliothek in TextGrid direkt verwendet werden können.

                

                

                    

                        
Phase

                        
Inhalt(e)

                        
Zeit in Minuten

                    

                    

                        
1. Einführung (Vortrag)

                        
Grundkonzepte der maschinellen Sprachverarbeitung, narrativen Konzepten und der Programmbibliothek spaCy

                        
20

                    

                    

                        
2. Einrichtung Jupyter-Notebooks

                        
Technische Einrichtung und Kurzüberblick zur Funktionsweise von Jupyter-Notebooks

                        
20

                    

                    

                        
3. Textuelle Phänomene (Vortrag + hands-on)

                        
Vorbereitete Jupyter-Notebooks mit Aufgaben zu textuellen Phänomenen mit unterschiedlichen Spans:
                            

                                
Token-Ebene (Named Entities, Zeitmarker)

                                
Teilsatzebene (Zeitformen)

                                
Passagen (Redeformen)

                            

                        

                        
1:05

                    

                    

                        
Pause

                        

                        
25

                    

                    

                        
4. Einbindung einer Custom-Komponente (hands-on)

                        
Teilnehmer*innen integrieren eine eigene spaCy-Custom-Komponente (z. B. Fremdworterkennung)

                        
45

                    

                    

                        
5. Narrative Strukturen (hands-on + Diskussion)

                        
Arbeitsaufgabe: narrative Strukturen in Texten erkennen

                        
45

                    

                    

                        
6. Abschluss

                        

                        
5

                    

                    
Tabelle 1

                

            

            

                

                    
Nach dem Workshop

                

                

                    
Wir tragen der Nachhaltigkeit der Forschung bei und stellen MONAPipe in einem Git-Repository zur Verfügung. Jupyter-Notebooks, die im Workshop benutzt werden, werden in einem separaten Git-Repository zur Verfügung gestellt.

                

            

            

                

                    
Forschungsinteressen der Beitragenden

                

                

                    
Hanna Varachkina

                    
, M. A., ist wissenschaftliche Mitarbeiterin und Doktorandin am Seminar für Deutsche Philologie der Universität Göttingen. Ihre Forschungsinteressen liegen in computergestützter Textanalyse: Modellierung und Erkennung von Textstrukturen und Diskurs-Phänomenen.

                

                

                    
Florian Barth

                    
, M. A., ist wissenschaftlicher Mitarbeiter und Doktorand am Göttingen Centre for Digital Humanities und Mitarbeiter der Abteilung Forschung und Entwicklung der SUB Göttingen. Seine Forschungsinteressen liegen im Bereich der computationellen Textanalyse mit besonderem Fokus auf narrativen und fiktionstheoretischen Phänomenen sowie in der konkreten Anwendung dieser Forschung im Bereich der Infrastrukturen für die Digital Humanities.

                

                

                    
Tillmann Dönicke

                    
, M. Sc., ist wissenschaftlicher Mitarbeiter und Doktorand am Göttingen Centre for Digital Humanities der Universität Göttingen. Seine Forschungsinteressen liegen in der strukturellen Textanalyse, insbesondere im Zusammenhang mit Narration und Fiktion, sowie der automatischen Erkennung narrativer Phänomene.

                

                

                    
Johannes Biermann, 

                    
M. A., ist wissenschaftlicher Mitarbeiter bei der Gesellschaft für wissenschaftliche Datenverarbeitung (GWDG) im Bereich High Performance Computing (HPC). Die GWDG erfüllt u.a. die Funktion eines Rechen- und IT-Kompetenzzentrums für die Universität Göttingen. Im Zuge des Verbund für Nationales Hochleistungsrechnen (NHR-Verbund) ist er Berater für Anwendungen aus dem Bereich Digital Humanities. Sein Forschungsinteresse ist es, DH Fragestellungen auf High-Performance-Computing-Cluster zu adaptieren und dort zu rechnen.

                

                

                    

                    
Caroline Sporleder, 

                    
ist Professorin für Digital Humanities am Institut für Informatik der Universität Göttingen und Leiterin des Göttingen Centre for Digital Humanities. Ihre Forschungsinteressen liegen im Bereich der computationellen Semantik und Diskursanalyse, besonders für Anwendungen der Geistes- und Kulturwissenschaften.

                

            

        

            

                
The ERC Project “From Digital to Distant Diplomatics” (DiDip, https://didip.eu) attempts to build an innovative and sustainable (virtual) research infrastructure and environment (VRE) to facilitate large-scale analyses of historical documents. It will extend the Monasterium.net infrastructure, which is provided in an aging software (Bürgermeister et al. 2018). However, Monasterium.net is still the largest repository of digital representations of medieval and early modern charters. For the future use of this corpus, it is crucial to make data, in particular gold standard annotations, and methods as open as possible. We plan to combine traditional approaches to analyzing such charters with state-of-the-art computational methods and artificial intelligence.  The data produced and the methods used will be available under open licenses (code repository 

                

                    
https://github.com/Didip-eu

                

                
 ).

            

            
The project addresses an unsolved problem in the domain of diplomatics, i.e., the historical auxiliary science, studying medieval and early modern single sheet legal documents: With pure human intellectual capacity, the empirical part of this research had to focus on local, regional, or chancery level in the face of overwhelming quantities of charters (Hlavacek 2006). While earlier approaches to applying digital methods to the field focussed on digital representation of individual descriptions (Ambrosio et al. 2014, Vogeler 2009, Bradley et al 2019), the large-scale “distant reading” approach has been scarce. This changed only recently: In the field of computer vision, Handwritten Text Recognition (HTR) has provided the first results in changing this (Hodel 2017). In addition, Computer Vision (CV) can provide quantified stylistic attributes of all graphical features of a charter. Leifert et al. 2020 and Christlein 2018 extracted graphical elements from charters (e.g., decorations, notarial signs). There are indications that CV can infer the date of a historical document (Cloppet 2017, Seuret 2021) and can classify the handwriting style from a paleographical perspective. 

            
We conclude that a typical CV pipeline for analyzing a charter should consist of: layout analysis, HTR or word segmentation, and finally an analysis of non-text attributes such as style, material, seals etc. Most of these tasks utilize publicly available datasets that are focussed on manuscript books (Simistira 2016) that cannot encapsulate the diversity that is observed in large charter collections. 

            
The most precious resource in such a pipeline is the diplomatist's time spent on annotating data. We drastically economize annotation effort by reformulating layout analysis as an object detection problem instead of the typical image segmentation approach. Indicatively, this allowed us to annotate the layout of 1175 charter images in a fraction of the time that would be needed normally. Figure 1 shows an example of this kind of annotation. Preliminary experiments demonstrate that this approach works well, e.g., it can detect seals with an accuracy above 95% when using a YOLOv5 (Jocher 2022) based model. With a 50% Intersection over Union (IoU) threshold this result is, of course, mainly usable for classification tasks, while segmentation will have to make use of approaches (Leipert et al. 2021). For tasks like HTR, writer identification, and layout analysis, we consider binarization as a useful step. We made experiments indicating that purely synthetic data could be used for the binarization step (Nicolaou 2022), yet comprehensive performance analysis on charters specifically would need manual annotation of the ground-truth pixel-by-pixel. Although the target CV pipeline will be under construction for a while, a few stand-alone methods required for such a pipeline have already been successfully developed and tested.

            

                

                
Figure 1: Image annotation example using FRAT (https://github.com/anguelos/frat)

            

            
            

                
A “distant reading” approach has been taken by Telihuan et al. 2012 and 2014, Perreaux 2021, Leclerq et al. 2021, who study statistical features of larger text corpora extracted from charters. We plan to generalize these approaches with the application of Natural Language Processing (NLP) as a custom, multi-level pipeline. It will resolve information retrieval from both HTR and human-produced data, i.e., address tasks like named entity recognition or relationship extraction, text reuse, and in particular formulaicity detection, but also reflect on the possibilities of named entity linking and text summarisation. We are currently working on laying the basis for this. The wide diversity of dialects and less-resourced languages (early vernacular) is one of the most significant analytic difficulties experienced in medieval and early modern charters. Additionally, the existing OCR of charter texts has insufficient quality for further NLP processing. We show the adoption of a multilingual generic system to tackle both problems by BERT (Devlin et al 2019) models. We create a domain-specific  model currently under the pseudonym of “RatisBERT” for OCR post correction, that will be made available through the project’s GitHub repositories. It will be based on human controlled data from Monasterium.net itself, charter corpora like ALIM, CDLM, DEEDS, Glessgen 2016, CAO, and enriched by non-charter specific gold standard corpora like the reference corpora for medieval and early modern German (REM, REF, REN including Rhenish) or the PalaFraFro V2-2. The system takes into consideration a variety of errors originating from HTR and various historical periods and linguistic regions, and provides an effective and automated post-correction approach. We use XLM-RoBERTa for language and variant detection. Through the second layer, the pipeline identifies named entities in the formulaic language of charters, thus forming a solid subset for the abstract generating task, which creates a condensed version of a document in English and other modern languages while preserving its essential information in the standardised format of the charter abstract.

            

            
The project is planning to integrate these solutions built on the collection of Monasterium.net with generic Digital Humanities (DH) tools through RESTful application programming interfaces (API) and provides access to its own methods through their own, thriving to set up the domain-specific diplomatics VRE as part of the growing DH API infrastructure.

        

            

                
Das Forschungscluster Open Heritage und seine Zielsetzungen

                
Das Museum für Naturkunde Berlin (MfN) verwahrt mehr als 30 Millionen Objekte aus Zoologie, Paläontologie, Geologie und Mineralogie aus allen Teilen der Erde. Dass die gesamte Sammlung und das mit ihr verbundene Wissen der Öffentlichkeit zugänglich sein soll, ist Gründungsgedanke und mit dem Zukunftsplan
 konkretisierter Auftrag dieses Forschungsmuseums der Leibniz-Gemeinschaft. Neben baulichen Sanierungsmaßnahmen bildet die digitale Erschließung der Sammlung einen Teil dieses Auftrags.
                

                
Die Dokumentation der Objekte am MfN fand bislang überwiegend nach biologischen Kriterien statt. Seit einigen Jahren aber erfahren Informationen zu Herkunft und Erwerbsumständen eine verstärkte Aufmerksamkeit, nicht zuletzt angesichts der zunehmenden gesellschaftlichen Debatte um die Verantwortung der Museen, Unrechtskontexte aufzuarbeiten, Menschen aus den Herkunftsgesellschaften einen Zugang zu ihrem kulturellen Erbe zu ermöglichen und nicht zuletzt die Rückgabe unrechtmäßig erworbener Kultur- und auch Naturgüter neben dem Sammeln, Bewahren, Forschen, Ausstellen und Vermitteln als Teil ihrer Aufgabenbereiche zu verstehen.
 Vor diesem Hintergrund wurde das Forschungscluster „Open Heritage – Naturkunde in globalen Kontexten. Sammlung erforschen, Zukunft gestalten“
 am MfN ins Leben gerufen. Kernaufgabe ist es Strategien und Werkzeuge zu entwickeln, um museale Sammlungen zukünftig als nachhaltige und global zugängliche Wissensressourcen bereitzustellen. Dabei stehen Fragen der Öffnung, Mehrstimmigkeit, Ermöglichung von Teilhabe sowie der Zukunft von musealen und archivalischen Räumen im Fokus, ebenso die kritische Reflexion vergangener, gegenwärtiger und zukünftiger Forschungs-, Sammlungs- und Dokumentationspraktiken der Naturkunde und -geschichte. Das Cluster vereint inter- und transdisziplinär ausgerichtete Projekte aus verschiedenen Forschungsbereichen des Museums, die sich mit kultur- und sozialwissenschaftlichen Fragestellungen der Erforschung, Erschließung und Reflexion der Sammlung im globalen Kontext nähern. Die digitale Erschließung und Bereitstellung von Sammlungsinformation können einen niedrigschwelligen Zugang, Transparenz, neue Ordnungssysteme und Analyseformen, faire und inklusive Kooperationen über Disziplinen und Institutionen und gesellschaftliches Engagement hinweg bedeuten. Dabei müssen Forschungs-, Entscheidungs- und Übersetzungsprozesse zwischen Informations- und Datenwissenschaften, Katalogisierungs- und Ordnungssystemen, Verschlagwortungen, Thesauri sowie Repräsentationsformaten sowohl in digitalen Datenbanken als auch in der Kuration im analogen und digitalen Raum kritisch reflektiert und transparent gemacht werden (vgl. Odumosu 2020).
 Vor allem im Bereich der Provenienzforschung liegt eine weitere Herausforderung darin, Kontexte zu rekonstruieren und dabei Information aus verteiltem Sammlungs- und Schriftgut miteinander zu vernetzen. Digital auslesbare Archivkataloge oder Findbücher, Transkriptionssoftware oder digitale objektbasierte Sammlungsdatenbanken, die Metadaten zu Provenienzen mitberücksichtigen oder in ihren Fokus rücken (Hopp 2018, 40, Sousa/Moser 2020, 86), bilden zwar gegenüber der analogen Recherche vereinfachte Bedingungen und beschleunigen Provenienzrecherchen. Dennoch mangelt es an Verknüpfungen zwischen den einzelnen archivierten Dokumenten und den Sammlungsbeständen, an interinstitutionellen Standards oder an nachhaltiger Dokumentation von Sammlungs-, Forschungs- und Archivierungspraktiken. Dies stellt Forschende vor Schwierigkeiten in der Nachvollziehbarkeit und erfordert einen immensen zeitlichen Recherche- und Rekonstruktionsaufwand. Denn Sammlungsobjekte und die dazugehörigen Informationen zu ihrer Beschaffungs-, Nutzungs- und Verlagerungsgeschichte liegen in der Regel nicht nur in einer Sammlung – wie der des MfN – räumlich verstreut vor, sondern weisen Verteilungsnetzwerke sowie Verbindungen mit anderen Akteur*innen, Institutionen oder Orten auf, die auch immer von Leerstellen durchzogen sind (vgl. Kuster et al. 2019, 106).
                

                
Obgleich die einzelnen Forschungsprojekte des Clusters Mikroperspektiven des Makrosystems Museum auf der Ebene verschiedener Sammlungsbestände, Sammler*innen oder historischer Sammlungskontexte beleuchten, vereint sie der methodologische Zugriff über die interdisziplinäre, quellenbasierte Rekonstruktion der Sammlungs- und Objektgeschichten. Mit einem Schwerpunkt auf einen Sammler, spezifische Sammlungsbestände oder einen bestimmten Expeditionskontext wird der Versuch unternommen, zugehörige Objektbestände und ihre Provenienzen zu erschließen, die heterogenen Materialien aus Sammlungsgegenständen, dokumentierendem Schrift- oder Bildgut miteinander zu verknüpfen und digital auffindbar zu machen. Zwei dieser Projekte und ihre unterschiedlichen Ansätze werden im Folgenden vorgestellt. Dabei stehen hinter den Projekten des Clusters letztendlich auch verschiedene Vorstellungen, Ziele und Definitionen dahinter, was die Begriffe „Open Heritage” in ihrer Umsetzung bedeuten könnten. Die Definition von „Open Heritage” wird bei den folgenden Fallbeispielen im Sinne der Zugänglichkeit und Sichtbarkeit der Quellen aufgegriffen. Mithilfe der Forschungstools soll so Teilhabe an der Nutzung und Auswertung der Quellen ermöglicht werden (vgl. Sousa/Moser 2020, 96).

                

            

            

                
Semantische Annotation der historischen Jahresberichte des Museums

                
Eine äußerst ergiebige Quelle zu Personalentwicklungen, Sammlungspraktiken, Bestandszu- und -abgängen sowie räumlichen Vernetzungen und Wegen von Personen oder Objekten bilden statistische Publikationen oder Jahresberichte sammelnder Institutionen.

                
Das Projekt „Forschungsfokus Provenienz: Digitale Edition der Jahresberichte des Museums für Naturkunde 1887-1915 und 1928-1938“ beschäftigt sich mit der digitalen Erschließung dieser Quellenbestände, insbesondere in Hinblick auf die Zeit der kolonialen Expansion des Deutschen Reiches.
 Die mineralogisch-petrographische, die geologisch-paläontologische und die zoologische Sammlung, die in dieser Zeit gemeinsam mit der Generalverwaltung das MfN konstituierten, waren institutionell ein Teil der Friedrich-Wilhelms-Universität, der heutigen Humboldt-Universität zu Berlin (HU). Diese publizierte in den Jahren zwischen 1887 und 1915 sowie 1928 und 1938 jährlich eine inzwischen von der HU-Bibliothek digitalisierte Chronik,
 in der jede ihr zugehörige organisatorische Einheit aufgefordert war, einen Bericht über die Aktivitäten und Ereignisse des Vorjahres einzureichen. In den Berichten der vier Abteilungen des MfN lassen sich Informationen zur Organisationsstruktur des Museums, zu personellen Veränderungen, Veröffentlichungen und Lehrveranstaltungen, Raumnutzung, Museumsinfrastruktur und -ordnungen, Nutzung und Zuwachs der einzelnen Museumssammlungen sowie ihrer wissenschaftlichen Auswertung und Bearbeitung finden. Zentral sind außerdem Listen der Zu- und Abgänge von Sammlungsobjekten durch Tausch, Kauf und v. a. sogenannte Schenkungen durch Forscher oder Kolonialbeamte. Da insbesondere diese Daten eine einzigartige Quelle für die Sammlungsgeschichte des MfN darstellen, wurden speziell diese Aspekte in dem Projekt digital erschlossen.
                

                
Das Ziel dieser Digitalisierung ist es, ein sowohl für Menschen als auch Maschinen langfristig abfragbares Repositorium dieser Informationen zu erstellen. Bei der Modellierung der Daten waren vor allem drei Anforderungen relevant: Erstens mussten Textsequenzen und ihre jeweiligen Kontexte repräsentiert werden, die über die Jahre von unterschiedlichen Autoren und damit aus unterschiedlichen Perspektiven verfasst wurden. Deshalb wurde als Methode eine manuelle semantische Annotation der Textteile mit 
                    
INCEpTION
, einem für diesen Zweck an der TU Darmstadt entwickelten Tool, ausgewählt.
 Zweitens sollte die Möglichkeit bestehen, einerseits die semantischen Entitäten durch zusätzliche Quellen, sowohl aus digital noch unerschlossenem Archivmaterial als auch aus Repositorien wie Wikidata, inhaltlich zu bereichern, andererseits die strukturierten Daten im Sinne eines Linked Open Data Ansatzes in fremde Datensätze barrierefrei zu inkludieren – die Modellierung in einem Wissensgraphen lag damit nahe. Schließlich mussten die in den Jahresberichten angesprochenen Themenbereiche adäquat abgebildet werden können. Die von dem International Council for Documentation (CIDOC) des International Council of Museums als ISO-Standard entwickelte Ontologie CIDOC CRM bot sich als ISO-Standard mit ereigniszentriertem Dokumentationsansatz dafür an.
 Mithilfe dieses Ansatzes konnten aus den Jahresberichten über 12.000 individuelle Transaktionen von Objekten an das Museum rekonstruiert werden. Nahezu 80% davon konnten über 2.300 Sammler*innen sowie über 1.200 einzigartige Ursprungsorte der Objekte zugeordnet werden.
                

            

            

                
Die digitale, textbasierte Rekonstruktion der Berliner Kunstkammer

                
Ein weiteres Projekt des Clusters, das sich mit der textbasierten Wiedergewinnung von Sammlungs- und Objektinformation befasst, ist das DFG-Projekt „Das Fenster zur Natur und Kunst. Eine historisch-kritische Aufarbeitung der Brandenburgisch-Preußischen Kunstkammer“.
 Die Naturalien der Berliner Kunstkammer – eine Sammlung, die zwischen 1600 und 1875 an verschiedenen Orten auf der Spreeinsel existierte und deren Bestände sich fortwährend neu formierten – gingen 1810 in den Besitz der neu gegründeten Friedrich-Wilhelms-Universität (HU) über, zu der das Museum für Naturkunde bis 2009 gehörte. Damit bilden Objekte der enzyklopädisch angelegten Kunstkammer der preußischen Kurfürsten und Könige einen Grundstock des MfN. Neben einer Buchpublikation (vgl. Becker et al. 2023) entstand als Ergebnis des Projekts eine virtuelle Forschungsumgebung,
 in der die wichtigsten Archivalien zur Berliner Kunstkammer im Zeitraum von 1603 bis 1812 (ca. 25 Quellen) und die darin überlieferten Objekte recherchiert werden können. Ziel war es, neben der Erforschung einzelner Objektwege die Bestände ausgehend von Archivalien zu rekonstruieren. Dabei entstanden knapp 2000 digitale Objekteinträge. Einige Objekte haben sich zwar heute noch erhalten, so in der Sammlung des MfN, der HU oder der Staatlichen Museen zu Berlin. Viele von ihnen sind jedoch nur noch in historischen Quellen wie Inventaren, Museumsführern oder Reisebeschreibungen nachweisbar. Diese Tatsache wird durch die textbasierte Rekonstruktion berücksichtigt, denn so kann vor allem die Herkunft der Information zu Objekten nachvollziehbar gemacht werden. Alle aus den Quellen gewonnenen Objektinformationen werden beim Objekteintrag gebündelt, so etwa ihre Bezeichnung, das Material, aus dem sie bestehen, Motive, die sie zeigen, Personen, die mit ihnen in Verbindung stehen wie Hersteller oder Erfinder, die Art ihrer Präsentation, ihr Standort in den Sammlungsräumen oder auch Angaben zu ihrer Herkunft. Indem versucht wurde, die Objekte durch die verschiedenen Quellen hindurch immer wieder zu identifizieren, können divergente Angaben aus den Quellen nun verglichen werden. So lässt sich genau verfolgen, wie sich beispielsweise die Bezeichnung eines Objekts im Laufe der Zeit verändert, sein Standort oder sein Ort in der Systematik der Sammlung wechselt. Dieser Ansatz bietet die Möglichkeit, Schlussfolgerungen zum Bedeutungswandel eines Objekts oder Veränderungen in der Sammlungspraxis und -logik zu ziehen.
                

                
Bei der digitalen Erschließung kam die open source Software 
                    
WissKI
 als Grundgerüst sowie ein – wie auch bei der HU-Chronik – auf dem CIDOC CRM basierendes Datenmodell zum Einsatz (vgl. Wagner 2020, Wagner 2023). WissKI ist auf die standardbasierte Dokumentation heterogener Materialien ausgerichtet, erlaubt es, individuelle Sachverhalte zu modellieren und miteinander in Beziehung zu setzen, und bildet eine ideale Grundlage für Linked Open Data und damit die Basis für die digitale Vernetzung und Bereitstellung von Information aus dem Bereich kulturellen Erbes nach den FAIR-Prinzipien (Wilkinson et al. 2016). Das entwickelte Datenmodell, bei dem ausgehend von Schriftgut bzw. dessen Transkription Objekte und Sammlungen referenziert sowie ihnen zugewiesene Eigenschaften und Kontexte im Wortlaut – und zusätzlich mit Normdaten angereichert – dokumentiert werden, wird nun mit Blick auf seine Tragfähigkeit auf weitere Provenienzforschungsprojekte des Clusters ausgeweitet.
                

            

            

                
Herausforderungen und Potenziale für die Provenienzforschung

                
Die hier vorgestellten Erschließungsprojekte zeigen Herausforderungen und Potenziale der Provenienzforschung auf. Beide Projekte speichern die gewonnene Information in auf die jeweiligen Sachverhalte angepasste Wissensgraphen, die unterschiedliche Einstiegspunkte in die erfassten Daten ermöglichen und auf eine Anbindung an externe Informationsressourcen ausgelegt sind. Die Erschließungsprojekte stellen daher Daten zu Quellen zur Verfügung, die in bereits existente Forschungsdateninfrastrukturen von NFDI4Culture oder Text+ eingebettet werden und so zu einer Wissens- und Datenvernetzung beitragen können (Fuhrmeister/Hopp 2019, 220). Die Annotation der Jahresberichte des MfN bietet die Möglichkeit, die in der Schriftquelle genannten semantischen Objekte, beispielsweise Ereignisse, Personen, Bestände oder Regionen, als untereinander mittelbar und unmittelbar verschränkte Netzwerke zu erfassen und eingängig visuell aufzubereiten. Hiermit lassen sich zum Beispiel geografische Translokationen gesammelter Güter aufzeigen.  Diese Annotationsform geht somit über die Indexsuche von Personen- oder Ortsnamen und deren Verknüpfung mit GND-Einträgen hinaus, wie bei Transkriptionsprojekten wie der Transkribus
-basierten Plattform des Projekts „Die Rezesse der niederdeutschen Städtetage“.

                

                
Auch die virtuelle Forschungsumgebung zur Berliner Kunstkammer bietet verschiedene Rechercheeinstiege. So wurde für Objekte und Quellen jeweils eine eigene Suche mit spezifischen Suchfacetten eingerichtet, wie dies in ähnlicher Weise durch die Objektkategorien und Filterfunktion der Datenbanken von PAESE (vgl. Andratschke/Müller 2021) und des BASA-Museums
 umgesetzt wurde. Daneben existieren Zugriffe über Personen, Motive, Objektarten oder auch Herkunftsorte, die wiederum mit Schriftgut und Objekten vernetzt und damit kontextualisiert sind. Auf diese Weise wird die Sammlungskonstitution des MfN über die einzelnen Objektgeschichten in Vernetzung zu anderen Objektkonvoluten, Schrift- und Bildquellen nachvollziehbar gemacht.
                

                
Die vorgestellten digitalen Erschließungsmethoden bieten Ansätze für erforderliche Strategien der digitalen Provenienzforschung für eine „mögliche[...] ‘Sichtbarmachung’ von räumlichen und zeitlichen Abläufen des Kulturguttransfers” (Hopp 2018, 42)
 – auch für naturkundliche Sammlungen, die in diesem Diskurs bislang unterrepräsentiert sind.
 Die Rekonstruktion und Sichtbarmachung der historischen Kontexte der Sammlungsbestände des MfN liegen in der Verantwortung des Museums, insbesondere wenn diese aus Gewaltkontexten des Imperialismus oder Kolonialismus stammen, unter machtasymmetrischen Erwerbsbedingungen oder im Zuge problematischer Sammlungs- und Forschungspraktiken in das Museum gelangt sind. Die Transparenz und Sichtbarmachung kolonialer Verflechtungen innerhalb der Bestandskonstitution naturkundlicher Sammlungen setzt daher die Aufarbeitung der Provenienzen der Sammlungsbestände und ihre möglichst niedrigschwellige Bereitstellung durch Digitialisierungsmaßnahmen voraus. Dabei erfordert der koloniale Kontext der Sammlungspraktiken und die Sensibilität der betreffenden Sammlungsgegenstände auch eine Reflexion und sensible Herangehensweisen in der Forschungs- und Digitalisierungspraxis, die Forschende vor spezifische Epistemologien sowie Logistiken stellt, ethische und politische Standards benötigt, aber auch einen kritischen sowie politischen Reflexionsprozess anregen kann.

                

            

        

            

                
Motivation und Korpus

                
In einer Szene des Films „The Wolf of Wall Street” hält der Börsenmakler Jordan Belfort, verkörpert durch Leonardo Di Caprio, eine Ansprache vor seinen Mitarbeitern, woraufhin eine Parade nackter Musiker durch das Großraumbüro zieht, gefolgt von einer Gruppe leicht bekleideter Frauen und einer orgiastischen Feier in den Büroräumen. Auch wenn sich der Film auf eine besonders schillernde Persönlichkeit konzentriert, so veranschaulicht er doch, wie verrückt es an der Börse bisweilen zugehen kann. Tatsächlich spielen dort wie auch in anderen Lebensbereichen nicht nur „harte“ Informationen, sondern eben auch vermeintliche „weiche“ Aspekte wie Stimmungen und Gefühle eine wichtige Rolle, die sich in kollektiven Irrationalitäten, Hypes und Ängsten niederschlagen (Akerlof and Shiller 2009; Shiller 2015). Der Ökonom John Maynard Keynes prägte 1936 dafür den Ausdruck der „animal spirits“ (Keynes 1936). Ein Ansatz, die Bedeutung der Stimmung für die Börse greifbar zu machen, besteht darin, das in der Finanzpresse zum Ausdruck kommende Media Sentiment mittels Sentiment Analyse (Liu 2015) zu messen und dessen Einfluss auf die Kursentwicklung zu bestimmen (Tetlock 2007; García 2013; Hanna, Turner und Walker 2020).

                

                
Das so gemessene Media Sentiment wird dabei als Spiegel der Stimmung der Börsianer, des Investor Sentiment, interpretiert. Das vorliegende Projekt verfolgt das Ziel, Sentiment Daten für die Berliner Börse für den Zeitraum zwischen 1872 und 1930 zu generieren, die dann für verschiedene inhaltliche Fragestellungen genutzt werden können.
 Das Ziel des Beitrags besteht darin, die mit der Erhebung dieser Daten verbundenen Herausforderungen aufzuzeigen und unseren Ansatz der Zero-Shot-Klassifikation vorzustellen.
                

                
Datengrundlage ist ein Textkorpus, das aus täglichen Marktberichten der Berliner Börsen-Zeitung (BBZ), der wichtigsten Finanzzeitung dieser Epoche, besteht.
 Diese Berichte, die im Median etwa 540 Wörter umfassen, enthalten eine kompakte verbale Beschreibung des täglichen Geschehens an der Berliner Börse: In welcher Stimmung befand sich die Börse? Was beeinflusste die allgemeine Stimmung? Welche Aspekte wiesen Besonderheiten auf? Wie entwickelten sich bestimmte Teilmärkte, etwa Eisenbahnaktien?
                

                
Die Berichte wurden von uns in einem mehrstufigen Prozess aus den ganzseitigen Scans der BBZ extrahiert (Liebl und Burghardt 2020), die die Staatsbibliothek Berlin zur Verfügung stellt.
 Dazu wurde mittels einer eigenen OCR-Pipeline und Layout-Detection zunächst die relevante Ausgabe (Morgen vs. Abend), dann die relevante Seite und zuletzt der jeweils relevante Seitenabschnitt identifiziert, was sich aufgrund des wandelnden Layouts der Zeitung und insbesondere der Berichte als durchaus komplexes Problem erwies. Insgesamt wurden so knapp 18 000 Berichte extrahiert, was einem Textvolumen von etwa 9,87 Millionen Wörtern entspricht. Für den Großteil des Untersuchungszeitraums zählt das Korpus knapp 300 Berichte pro Jahr (der Börsenhandel fand von Montag bis Samstag statt). Für die Zeit des Ersten Weltkriegs und die Jahre zwischen 1922 bis 1924 ergeben sich aufgrund eines wenig standardisierten und häufig wechselnden Berichtsformats sowie häufiger Börsenfeiertage bislang nur 150 bis 200 Texte pro Jahr. Einige weitere Datenlücken in den 1870er Jahren, die aus unvollständigen Überlieferungen der BBZ resultieren, wurden mit dem Pendant aus der 
                    
Vossischen Zeitung
 gefüllt, die ebenfalls einen täglichen Börsenbericht veröffentlichte. Dabei wurde durch verschiedene Tests sichergestellt, dass beide Zeitungen eine weitestgehend übereinstimmende Bewertung der Gesamtstimmung an der Berliner Börse angeben.

                

            

            

                
Methode

                
Die Börsenberichte der BBZ weisen im Hinblick auf die Bestimmung des in ihnen enthaltenen Sentiments eine vierfache Herausforderung auf. Zunächst wird die Analyse durch die spezielle Domäne verkompliziert. Die Tonalität eines ökonomischen Texts bzw. eines Finanztexts hängt in hohem Maße vom Kontext bzw. der „Richtung“ der Aussage ab (Loughran und McDonald 2011; Malo et al. 2014; Xing et al. 2020). So induziert bspw. das Wort „Verlust“ nur dann eine negative Tonalität, wenn dieser „steigt“, „sich einstellt“, „verharrt“, etc. Geht er jedoch zurück, resultiert dagegen eine Aussage mit positiver Tonalität. Im Börsen-Kontext besonders problematisch: Für die eine Marktseite mögen fallende Kurse etwas Negatives sein. Für die andere, die darauf gesetzt hat, dass die Kurse zurückgehen, ist der gleiche Vorgang hingegen etwas Positives (Hausse- vs. Baissepartei). Hinzu kommt, dass das Börsenjargon durch ein ganz eigenes Vokabular gekennzeichnet ist („Contremine“, „debarrassieren“, „Reprise“, usw.). Neben solch hochspezifischen Ausdrücken finden sich weiterhin viele Beispiele für die Verwendung eines Standardlexikons um eine spezifische Marktstimmung – teilweise geradezu metaphorisch – zu beschreiben: In der Fachsprache der Börse sind die Geschäfte etwa „flau“, „matt“, „fest“ oder „lustlos“ (Krupke 1904; Kautsch 1912). Einerseits standardisiert derlei Jargon zwar den Sprachgebrauch erheblich; andererseits erschwert es den Einsatz von off-the-shelf Lösungen was beispielsweise die automatische Sentiment Analyse solcher Sprachbelege angeht, da in Standardressourcen die Börsen-spezifische Bedeutung diese Begriffe bzw. die Begriffe selbst nicht berücksichtigt werden. Dies wird zuletzt noch dadurch verstärkt, dass wir es teils mit einem veralteten Sprachgebrauch zu tun haben, der sich neben terminologischen Besonderheiten durch Schachtelsätze, Verklausulierungen, vielfache Verneinungen, Konjunktive, Querverweise und andere Besonderheiten auszeichnet. Historizität und Sprachwandel erschweren jedoch nicht nur die Sentiment Analyse, sondern auch andere NLP-Ansätze wie etwa Named Entity Recognition (siehe etwa Hellrich et al. 2019, Ehrmann et al. 2021), da im Falle historischer Sprache viele Standard-NLP-Ressourcen nicht ohne Weiteres nachnutzbar sind.

                
Vor dem Hintergrund dieser vielfältigen Herausforderungen, die sich durch die spezifische Domäne und die historische Sprachstufe ergeben, waren erste Experimente mit den in Tabelle 1 aufgeführten Sentiment-Lexika nicht erfolgreich. Dies untermauert bestehende Erkenntnisse aus der einschlägigen Fachliteratur bzgl. einer geringeren Performance Wörterbuch-basierter Ansätze gegenüber Machine-Learning-Verfahren (siehe etwa Mishev et al. 2020 und van Atteveldt et al.2021).

                

                
Als Alternative wurden deshalb aktuelle Ansätze aus dem Bereich neuronaler Sprachmodelle erprobt. Durch das Vortrainieren großer Transformer-basierter Sprachmodelle, wie etwa BERT, wurden in den letzten Jahren immer wieder Durchbrüche in verschiedenen Anwendungsbereichen des Natural Language Processing erzielt (Devlin et al. 2019). Das übliche Vorgehen ist es, ein solch vortrainiertes Sprachmodell auf Task-spezifischen Daten nachzutrainieren und es damit an die eigene Anwendungsdomäne anzupassen (Finetuning). Man spricht hier von 
                    
transfer learning
 (Zhuang et al. 2020). Unter dem Namen Zero-Shot-Klassifikation (früher: Data-less Classification, Chang et al. 2008) existieren seit Kurzem Verfahren, bei denen für die Zielaufgabe gar keine aufwendig erstellten Trainingsdaten mehr für das Finetuning zur Verfügung stehen müssen, aber dennoch ein domänenspezifischer Klassifikator generiert werden kann (vgl. Yin et al. 2019, Veeranna et al. 2016, Brown et al. 2020). Der große Vorteil des Zero-Shot-Verfahrens ist es, dass hierfür keinerlei (also zero) Trainingsdaten vorliegen müssen – deren Generierung je nach Projekt mit hohen Kosten verbunden sein kann –, sondern Klassifikationsergebnisse im Sinne eines Transfer-Lernens erzielt werden. Einer der erfolgreichsten Ansätze für die Zero-Shot-basierte Textklassifikation stützt sich dabei auf sog. 
                    
Entailment
-Modelle, die darauf trainiert sind, einen logischen Widerspruch oder Implikationen zweier Sätze zu erkennen (Yin et al. 2019). Dazu werden die Zielkategorien für den Klassifikator in natürlichsprachliche Sätze umformuliert und mit dem zu klassifizierenden Text verglichen.
                

                

                    

                        
Name

                        
Sprache

                        
URL

                    

                    

                        

                            
BPW dictionary

                        

                        
Deutsch (optimiert für Finanzsprache)

                        
https://www.uni-giessen.de/fbz/fb02/forschung/research-networks/bsfa/textual_analysis/index_html

                    

                    

                        
SentiWS

                        
Deutsch (allgemein)

                        
http://www.ulliwaltinger.de/sentiment/

                    

                    

                        
German Emotion Analysis

                        
Deutsch (allgemein)

                        
https://www.romanklinger.de/emotion/

                    

                    

                        
Affective Norms for German Sentiment Terms (ANGST)

                        
Deutsch (allgemein)

                        
https://link.springer.com/article/10.3758/s13428-013-0426-y

                    

                    

                        
Lexikon von 
                            
Chen/Skiena

                        

                        
Multilingual

                        
https://aclanthology.org/attachments/P14-2063.Datasets.zip

                    

                    
Tabelle 1: Übersicht zu genutzten Sentiment-Lexika für die deutsche Sprache.

                

                
                
In unserem konkreten Anwendungsfall erstellen wir einen Klassifikator für die Sentiment-Kategorien „positiv“, „neutral“ und „negativ“, welche entsprechend in den folgenden Sätzen verbalisiert werden: „Die Stimmung an der Börse ist {positiv|neutral|negativ}“. Der Klassifikator entscheidet durch automatische Analyse aller sprachlichen 
                    
Entailments
 sodann, welche dieser drei Hypothesen sich am wahrscheinlichsten aus dem Eingabetext schlussfolgern lässt. Verwendet wurde hierzu ein BERT-Modell
 , das auf dem deutschsprachigen Teil des XNLI (
                    
cross natural language inference
) Datensatzes (Conneau et al. 2018) getestet wurde.
                

                
Vor der Klassifizierung wurden zunächst alle Berichte in einzelne Sätze segmentiert. Nach ersten Experimenten fiel auf, dass einige Sätze in den Marktberichten stark deskriptiven Charakter haben, angezeigt bspw. durch eine längere Auflistung von Unternehmen in direkter Abfolge. Solche Sätze enthalten aber keine relevanten Informationen aus Perspektive des Media Sentiment und wurden deshalb schon im Vorfeld heuristisch herausgefiltert. Dabei wurde Domänenwissen in generalisierbare Heuristiken übertragen, die dann automatisiert auf das gesamte Korpus angewendet wurden. Beispielsweise prüft die Heuristik, ob längere Aufzählungen oder gehäuftes Auftreten von numerischen Werten in einem Satz vorkommen. Letzteren kommt eine besondere Bedeutung zu, da sie meist in Aussagen wie „Der Kurs ist um 2 % gefallen“ oder „Der Kurs stieg bis 218“ auftreten. Zwar können diese Sätze, wie das erste Beispiel zeigt, durchaus eine Form von Tonalität enthalten. Allerdings drücken sie aus unserer Sicht keine Stimmungen oder Gefühle, sondern eine finanzwirtschaftliche Information aus, wie sie auch an anderen Stellen der Zeitung, etwa der Kurstabelle, zu finden ist (siehe mehr dazu im Abschnitt „Herausforderungen“). Das Herausfiltern der Sätze mit numerischen Werten soll sicherstellen, dass unser Sentiment Index nicht durch solche Informationsaussagen verzerrt wird. Weiterhin wurden sehr kurze (kürzer als 30 Zeichen) und sehr lange Sätze (länger als 800 Zeichen) sowie Überschriften entfernt. Alle verbleibenden Sätze wurden dann nach dem vorgestellten Zero-Shot-Ansatz klassifiziert und einem Sentiment Score zugewiesen (positiv: 1, negativ: -1, neutral: 0), der letztlich über alle Sätze eines Berichts zu einem Gesamt-Sentiment-Score gemittelt wurde.

                
Um die Qualität der Zero-Shot-Klassifizierung anekdotisch zu evaluieren, wurden 150 Sätze als Gold Standard durch einen Domänenexperten annotiert. Auf dieser Basis ergab sich eine korrekte Klassifikation des automatischen Ansatzes in 74% der Fälle. Von den 26% falsch erkannten Sätzen entfällt der Großteil auf solche, die fälschlicherweise als positiv bewertet wurden, obwohl sie eigentlich als neutral einzustufen sind. Tabelle 2 fasst verschiedene Metriken zur Evaluation des Zero-Shot-Ansatzes sowie die Verteilung der Klassen zusammen.

                

                    

                        
 

                        
Negativ

                        
Neutral

                        
Positiv

                    

                    

                        
Precision

                        
83.67%

                        
36.84%

                        
82.35%

                    

                    

                        
Recall

                        
70.69%

                        
46.67%

                        
83.17%

                    

                    

                        
F1-score

                        
76.64%

                        
41.18%

                        
82.76%

                    

                    

                        
Distribution

                        
30.69%

                        
15.87%

                        
53.44%

                    

                    
Tabelle 2: Evaluationsmetriken und Klassenverteilung des Zero-Shot Klassifikators

                

                
                
In einer ersten Voranalyse des Korpus (siehe Abb. 1) ergeben sich die nachfolgenden Zeitreihen, die jeweils die diachronen Sentiment-Werte für ein Gleitfenster von 30 und 365 Tagen darstellt. Eine Darstellung der Sentimentausschläge auf Tagesbasis wurde verworfen, da diese starken Fluktuationen unterworfen ist und damit nur schwer interpretierbar ist.

                

                    

                    
Abbildung 1: Zeitreihe Sentiment über den gesamten Zeitraum des Korpus (Moving Average).

                

                
                
In der geglätteten Darstellung auf Monats- (blau) und Jahresbasis (orange) zeichnet sich dagegen eine Entwicklung mit Phasen unterschiedlicher Stimmungslagen ab, die zumindest auf den ersten Blick plausibel erscheint.

                

                
Beispielsweise ist die Spekulationsblase der frühen 1870er Jahre mit einer sehr positiven Stimmung verbunden, die sich nach dem Börsenkrach von 1873 rasch eintrübt. Das gleiche Schema ergibt sich auch für andere Phasen rasch steigender Kurse, wie etwa in den Jahren 1888–90 und 1926/27. Auch der massive Einsturz der Kurse während des Ersten Weltkriegs ist mit einem starken, wenn auch zeitlich verzögerten Rückgang des Sentiment-Index verbunden. Gerade der Anstieg des Sentiment-Index zu Beginn des Ersten Weltkriegs wirft jedoch Fragen auf. Zum einen stellt sich an dieser Stelle die technische Frage, wie belastbar die Ergebnisse angesichts einer Fehlerrate von 26% sind; diesen Punkt führen wir weiter unten aus. Zum anderen zeigt dieser Aspekt inhaltliche Fragestellungen auf: In welchem Verhältnis stehen Stimmung und Kursentwicklung zueinander? Sind beispielsweise steigende (fallende) Kurse immer mit einer positiven (negativen) Stimmung verbunden? Welche Trendwenden lassen sich in der Börsenstimmung identifizieren? Und welche historischen Ereignisse und Entwicklungen lassen sich als Ursachen für veränderte Stimmungslagen festmachen?

            

            

                
Herausforderungen

                
Unsere aktuellen Experimente zeigen deutlich, dass – eingedenk der eingangs beschriebenen Korpus-Probleme – die Anwendung eines Zero-Shot-Verfahrens auf dem Korpus von Marktberichten bereits vielversprechende Ergebnisse liefert. Gleichzeitig ist die derzeitige Fehlerrate von 26% zu hoch, um belastbare Aussagen treffen zu können. Zudem ist die Klassifizierung der Aussagen in nur eine Kategorie mit drei Ausprägungen angesichts der in den Börsenberichten enthaltenen Aussagen nicht unproblematisch, da sie nur ein sehr grobkörniges und, was schwerer wiegt, potentiell verzerrtes Bild liefert. Konkret scheint uns die Berücksichtigung zweier weiterer Kategorien geboten, die von den uns bekannten Studien zu Finanzmarktsentiment allerdings ausgeblendet werden. Grundsätzlich beziehen sich die einzelnen Aussagen eines BBZ-Berichts auf drei verschiedene Ebenen. Aussagen wie „Das Aussehen der Börse war heute überaus unfreundlich“ beziehen sich auf die gesamte Börse, andere wie „Schantung-Aktien setzten nach niedrigerem Beginn eine Besserung durch“ dagegen auf einzelne Wertpapiere. Dazwischen rangieren Aussagen wie „Elektrische Werte blieben behauptet“, die sich auf einen bestimmten Teilbereich des Börsenhandels beziehen. Damit ein Sentiment Score die Gesamtstimmungslage an der Börse korrekt widerspiegelt, muss eine Gewichtung dieser Aussagentypen erfolgen, da ansonsten die Gefahr einer Verzerrung der ermittelten Tonalität besteht.

                
Die Aussagen unterscheiden sich zudem in einer weiteren Dimension. Beispiele wie „Auf dem Rentenmarkte hat sich die Stimmung für Italiener auch heute nicht gebessert“ beschreiben die Börsenstimmung selbst, während Aussagen wie „Die rheinisch-westfälischen Bahnen büßten durchschnittlich nur 3 pCt. ein“ zwar auch Tonalität enthalten, allerdings eher eine (positive, neutrale oder negative) finanzwirtschaftliche Information wiedergeben. Während beispielsweise Takala et al. (2014) solchen Aussagen ein positives Sentiment bescheinigen, würden wir sie als Informationsaussagen mit positiver Tonalität definieren, da es aus unserer Sicht zwischen Sentiment im Sinne der Stimmung an der Börse und Sentiment als Ausdruck einer Informationsaussage zu differenzieren gilt. Beide Unterscheidungen, Aussagebene und Aussagegegenstand, scheinen uns für die Bestimmung eines repräsentativen Sentiment Scores als sehr wichtig. Dementsprechend erfordern die Börsenberichte der BBZ eigentlich eine Aspekt-basierte Sentimentanalyse, bei der neben der Entitätskategorie (Börse, Teilmarkt, Einzeltitel) auch die Aussageart (Informations- vs. Stimmungsaussage) berücksichtigt wird.

            

            

                
Fazit

                
Die Erkenntnis, dass der Zero-Shot-Ansatz bei einfachen Klassifzierungsaufgaben im Falle einer sehr spezifischen und komplexen Domäne bereits eine hohe Datenqualität liefert, stiftet Zuversicht, da in diesem Fall bereits geringe Mengen von Annotationsdaten ausreichen, um die Datenqualität zu evaluieren und gegebenenfalls durch entsprechendes Nachtrainieren zu erhöhen. Nun stellt sich die weitergehende Frage, inwieweit dieser Ansatz auch für komplexere Aufgaben wie Aspect-Based-Sentiment geeignet ist. Erste Analysen sowie die Arbeit von Shu et al. (2022) stimmen hier optimistisch. Dies wäre insofern eine relevante Erkenntnis, als die meisten praxisbezogenen Aufgaben eher komplexerer Natur sind. Komplexere Aufgaben lassen sich im Bereich des maschinellen Lernens häufig durch eine größere Menge an annotierten Trainingsdaten lösen. Ein Vorteil des Zero-Shot Ansatzes läge hier darin, dass das Verfahren auf den Zieldaten zunächst direkt evaluiert werden kann und nur im Bedarfsfall weitere Daten für das Nachtrainieren manuell erstellt werden müssen. Insgesamt scheint dieser neuartige Ansatz aus dem Bereich des Transfer Learning also sehr vielversprechend, auch für sehr heterogene Textkorpora, wie sie in den Digital Humanities häufig vorliegen.

            

        

            

                
Einführung

                
Werden kulturelle Daten nachhaltig gespeichert, sind sie die Basis nicht nur der heutigen, sondern auch zukünftiger Wissenschaftsgenerationen. In der Gegenwart geschieht dies typischerweise digital und auch – in Hinblick auf kulturelle Teilhabe und kollaborative Partizipation – offen (Schöch 2017).

                
Diesem Grundsatz der Offenheit haben sich die beiden Konsortien der Nationalen Forschungsdateninfrastruktur NFDI4Culture und Text+ – wie auch die gesamte NFDI – verschrieben. Vor dem Hintergrund des gemeinsamen Wissenschaftsbereiches und ihrem Fokus auf Sprach-, Text- und Kulturdaten wird diese Offenheit auch in der intensiven Zusammenarbeit der beiden Konsortien gelebt. Die Zusammenarbeit erfolgt hierbei in einem communitygestützten dynamischen Prozess, in den auch verwandte weitere Konsortien und Konsortialinitiativen eingebettet werden. So können Bedarfe innerhalb der gesamten NFDI mit einer Stimme artikuliert werden. Umso wichtiger ist es, einen Überblick über beteiligte Akteursgruppen zu erhalten und in die (Fach-)Öffentlichkeit kommunizierbar zu machen. Dazu bedürfen die vorhandenen Datenpunkte nicht nur der Vernetzung, sondern auch der Visualisierung. Dieser zweiteiligen ganz praxisorientierten Fragestellung – (1) Wie sammele ich meine Akteursdaten und (2) wie werte ich diese graphisch passend aus? – widmet sich das hier vorgeschlagene Posterprojekt. 

            

            

                
Hintergrund: NFDI und das Memorandum of Understanding

                
Basierend auf einem Bund-Länder-Beschluss 2018 (Bundesanzeiger 2018) hat die Nationale Forschungsdateninfrastruktur (NFDI) zum Ziel, Datenbestände entlang der FAIR-Prinzipien (Wilkinson et al. 2016) zu erschließen und langfristig zu sichern. Sie wird dabei “in einem aus der Wissenschaft getriebenen Prozess als vernetzte Struktur eigeninitiativ agierender Konsortien aufgebaut” (DFG 2020). Zwei dieser bislang 19 geförderten Konsortien sind NFDI4Culture und Text+. Mit den beiden NFDI-Initiativen NFDI4Memory und NFDI4Objects haben sie sich 2019 in einem Memorandum of Understanding (Brünger-Weilandt 2020) zusammengeschlossen, um die Bedarfe der Geistes- und Kulturwissenschaften gemeinsam zu bearbeiten und organisatorische und technische Lösungen für deren Fragestellungen zu finden. Diese Interdisziplinarität bietet die Chance, übergreifende Angebote zu entwickeln und die Vision von Open Humanities voranzutreiben – für die Wissenschaft, GLAM-Einrichtungen und die breite Öffentlichkeit.

            

            

                
Zusammenarbeit von NFDI4Culture und Text+

                
NFDI4Culture und Text+ vertreten äußerst vielfältige und disziplinär diverse (Fach-) Communities. Gleichzeitig gibt es mit Blick auf die an den beiden Konsortien beteiligten Institutionen und Einzelpersonen interessante Schnittmengen.

                
Eine genauere Untersuchung von Verbindungen, Rollen und gemeinsamen aber auch unterschiedlichen Handlungsebenen der an den beiden geistes- und kulturwissenschaftlichen NFDI-Konsortien beteiligten Fachcommunities, Institutionen und Einzelpersonen steht bislang noch aus. Gleichzeitig existiert mit der strukturierten Erfassung der NFDI im Rahmen eines Wikidata-Projektes (vgl. 
                    

                        
https://www.wikidata.org/wiki/Wikidata:WikiProject_NFDI

                    
) in Zusammenarbeit zwischen NFDI-Direktorat und -Konsortien eine erste Datenbasis für die Analyse. Hinzu kommen verfügbare Daten aus den beiden Internetportalen von NFDI4Culture und Text+, die zusätzliche Strukturfacetten liefern.
                

                
Die Daten von NFDI4Culture können über eine API und einen SPARQL-Endpoint, den Culture Knowledge Graph, abgefragt werden. Die Informationen zu Personen, Institutionen, Projekten, Nachrichten, Veranstaltungen, Forschungsprodukten und Services liegen als Linked Data in den Formaten Turtle, JSON-LD und RDF/XML vor. Text+-seitig liegen entsprechende Daten in tabellarischer Form vor, die dann mit den Daten von NFDI4Culture verschnitten werden. Ein konkretes Beispiel für unsere Datenauswertung ist die Identifikation von Akteuren in der NFDI, die (a) gleichzeitig Mitglieder in Text+ und NFDI4Culture sind oder (b) in Sektionen/AGs/Task Forces der NFDI gleichzeitig präsent sind. Zusätzlich sollen auch Zugangspunkte für die Communities visualisiert werden sowie deren Möglichkeiten zur Beteiligung.

                
Das gemeinsame Poster stellt die grafische Auswertung des gemeinsamen Akteurs-Netzwerkes der beiden Konsortien NFDI4Culture und Text+ vor. Hierbei werden die diagrammatischen Möglichkeiten der sozialen Netzwerkanalyse durchgespielt (vgl. Drucker 2014), die über die tabellierten und ‘graphierten’ Daten der Akteursmatrix gelegt werden. Dabei werden – wo möglich und praktikabel – Daten aus den jeweiligen konsortialen Wissensgraphen und den Wikidata-Identifikatoren (http://www.wikidata.org/entity/Q98276929, http://www.wikidata.org/entity/Q98271443) verwendet. Der Arbeitsprozess ist aufgrund der dynamischen Entwicklungen in beiden Konsortien explorativ und dient auch dem Versuch zu fassen, was mit datengetriebenen diagrammatischen Methoden ‘neu’ erkannt werden kann (vgl. Gold 2012).

                
Berücksichtigt werden auch gemeinsame Veranstaltungen, gemeinsam genutzte Dienste und/oder Repositorien, gemeinsame Arbeit in NFDI-Sektionen etc.

            

        

            
Die vDHd2021-Tagung, die als Ersatz für die coronabedingte Verschiebung der DHd-Jahrestagung 2021 stattfand, stand unter dem Motto “Experimente” und wurde von einem Publikationsexperiment begleitet, das von der Community initiiert wurde. Das Herausgeber*innengremium bildete sich in der Folge im Rahmen der vDHd-Planungen. Unter dem Titel „Fabrikation von Erkenntnis: Experimente in den Digital Humanities” wurde schließlich eine digitale Publikation herausgegeben, die das experimentelle Potenzial der Digital Humanities in unterschiedlichen Beitragstypen ergründet (Pawlicka-Deger 2020, Lane 2016, Earhart 2015, Knorr-Cetina 1991).

            
Zwar ist der Band losgelöst von den Beiträgen der vDHd2021, dennoch greift er den Anspruch der DHd-Jahrestagungen auf, zur Sichtbarkeit aktueller DH-Aktivitäten im deutschsprachigen Raum beizutragen, wobei er selbst einen experimentellen Ansatz als “living publication” verfolgt. Das Poster thematisiert die experimentellen Aspekte der Publikation und die damit verbundenen Erfahrungen.

            
Am Beginn stand ein Call for Publications (Burghardt et al. 2021), der die Erkundung des experimentellen Potenzials der DH in den Vordergrund stellte. Willkommen waren thematische und formale Experimente, die in einem rollenden Verfahren als Ko-Publikation und Sonderband der Zeitschrift für digitale Geisteswissenschaften und Melusina Press erscheinen sollten.

            
Drei Typen von Einreichungen waren möglich: 

            

                
Fachartikel zu experimentellen Methoden und Verfahren der DH

                
Daten-Experimente / Publikation von Datensätzen

                
Code-Experimente / Publikation von ausführbaren Notebooks

            

            
Die Kategorie der Fachartikel stellt die etablierteste Publikationsart innerhalb des Sonderbands dar. Das Experiment bei dieser Beitragsart bestand nicht im Bereich der eigentlichen Publikation, sondern im Reviewverfahren. Die Autor*innen konnten zwischen einem traditionellen Double-Blind und einem Open-Review-Verfahren wählen. So bot sich der Band an, Erfahrungen mit einem offenen Begutachtungsprozess zu sammeln und wertvolle Impulse für die Diskussionen innerhalb des DHd-Verbands bezüglich des zukünftigen Reviewverfahrens bei den Jahrestagungen zu liefern. Die überwiegende Mehrzahl der Autor*innen entschlossen sich für das offene Verfahren, das ein öffentliches Kommentieren eines Preprints vorsah. Auch der Community stand die Möglichkeit offen, die Artikel zu kommentieren und sich damit in den Veröffentlichungsprozess einzubringen. Zusätzlich verfassten die Gutachter*innen eine abschließende Bewertung mit Hinweisen zu Stärken und Verbesserungsmöglichkeiten. Wenn sich alle Seiten einverstanden erklärten, wurden auch diese Gutachtenformulare mit dem Artikel publiziert. Nach der Überarbeitung der eingereichten Fassung wurde der fertige Artikel publiziert, wobei Preprint und Anmerkungen auch weiterhin publiziert bleiben.

            
Die Kategorie der Datenexperimente stellt innerhalb des Sonderbandes eine neuere Publikationsart dar. Sie sind an das Beispiel sogenannter Data Papers (Schöpfel et al. 2019, 3) angelehnt und bestehen aus einem Artikel, der einen oder mehrere Datensätze beschreibt, vorrangig in Text und Bild, und der Publikation des zugehörigen Datensatzes. Im CfP wurde explizit nach eher unkonventionellen Datensätzen (corpora obscura) gefragt, wichtig war aber insbesondere, dass die Datensätze nach den FAIR-Prinzipien frei verfügbar sind beziehungsweise gemacht wurden.

            
Der Schwerpunkt der Data Papers liegt auf der Beschreibung der Datensätze bzw. der Potenziale für die Forschung, nicht so sehr auf den damit erzielten Forschungsergebnissen (Schöpfel et al. 2019, 3). Data Papers zählen zu den neueren, in den Geisteswissenschaften noch nicht weit verbreiteten Publikationsarten (Schöpfel et al. 2019, 9) und gehören ursprünglich nicht zur Bandbreite der ZfdG. Daher mussten die Herausgeber*innen eigene Richtlinien und Empfehlungen für die Autor*innen zur Umsetzung des Formats entwickeln und an diese auch die Reviewempfehlungen anpassen. Für die Entwicklung dieses Kriterienkatalogs orientierte sich das Herausgeber*innenteam an bereits bestehenden sogenannten Data Journals (wie z. B. das JOHD oder RDJ). Eine Herausforderung war das Finden geeigneter Reviewer*innen (bei den Data Papers single-blind Verfahren), da sowohl fachliches als auch technisches Verständnis für den jeweiligen Datensatz notwendig war.

            
Ein weiteres Experiment dieses Sammelbands ist die Veröffentlichung von Code Experimenten in Form von Executable Publications (ein ähnliches Format bietet etwa das JDH). Es handelt sich dabei um interaktive Jupyter Notebooks (Python und R-basiert), die zusammen mit den verwendeten Daten und einer technischen Dokumentation als Git-Repositorium eingereicht wurden. Alle Notebooks haben neben den Code-Abschnitten eine klar strukturierte und verständliche textuelle Komponente, durch die die häufig anzutreffende Rollenverteilung von Text als Mittel der Interpretation und Daten/Code als Ort empirischer Stringenz aufgebrochen wird. Dank der Code Experimente werden Methodik und Material zu evaluierbaren Gegenständen und die Publikation damit zum Medium sich ergänzender Mittel des Argumentierens. Bei dieser Publikationsform war, neben dem Review Prozess, vor allem der Aufbau einer entsprechenden Infrastruktur bei Melusina Press eine Herausforderung.

            
Das Poster wird nicht nur kurz auf alle Einreichungstypen eingehen und die Begutachtungsverfahren darstellen, sondern den Band auch statistisch evaluieren und besonders auf die gewonnenen Erfahrungen innerhalb des Herausgeber*innenteams eingehen (vgl. dazu auch Walkowsi 2022). Dabei stehen, wie bereits angedeutet, besonders Lessons Learned in Bezug auf die Koordinierung von zwei Publikationsorten, technische Herausforderungen, den allgemeinen Kommunikationsaufwand und die Annahme des offenen Begutachtungsverfahren im Mittelpunkt.

        

            
Seit der Veröffentlichung des ersten Konzepts eines „Semantic Web“ als Erweiterung des World Wide Web (Berners-Lee und Lassila Hendler 2001) haben sich GeisteswissenschaftlerInnen mit den Möglichkeiten und Grenzen der maschinenlesbaren Modellierung ihrer Daten im Rahmen dieses Entwurfs beschäftigt. Das Datenmodell des Resource Description Framework (RDF) und die Serialisierung in Turtle oder N-Triples hat sich zum Standard in der Modellierung von maschinenlesbaren semantischen Aussagen entwickelt. Obwohl sich eine Reihe von Erwartungen aus der Entstehungszeit des Semantic Web nicht erfüllt haben (umfassende Erweiterung des WWW mit semantischen Daten, Stabilität der Uniform Resource Identifier etc.), bildet das RDF-Datenmodell heute die Grundlage verschiedener Wissensbasen (DBpedia, Wikidata) und weiterer Wissensgraphen (knowledge graphs), die zurzeit in verschiedenen Zusammenhängen entstehen. Aus diesem Grund sind das Semantic Web und die Verlinkung von offen zugänglichen Daten (Linked Open Data) für die digitalen Geisteswissenschaften weiterhin und sogar verstärkt von Interesse (Beretta 2021, Beretta &amp; Alamercery 2020, Hiltmann &amp; Riechert 2020, Meroño-Peñuela 2017, Meroño-Peñuela et al. 2014, Pollin 2017, Wettlaufer 2018, Wettlaufer et al. 2015). 

            
Der ganztägige Workshop bietet eine Einführung in die Thematik „Semantic Web und Linked Open Data“ mit einem Schwerpunkt auf den Geschichtswissenschaften. Das Angebot richtet sich an Teilnehmende ohne Vorkenntnisse im Bereich Semantic Web/Linked Open Data und eignet sich für Forschende aller Fachbereiche. Didaktisch teilt sich der Workshop in vier Teile, wobei die praktische Übung etwa zwei Drittel der Zeit beansprucht.

            
Zu Beginn des Workshops werden die Grundlagen des Semantic Web, des Resource Description Frameworks sowie damit verbundener www-Standards im Rahmen einer einführenden Darstellung behandelt. Besondere Aufmerksamkeit kommt dabei den Themen Wikidata
 und SPARQL
 zu, die in den anschließenden Übungen eine wesentliche Rolle spielen. Folgende Themenblöcke sind für diesen ersten, einführenden Teil vorgesehen:
            

            
Die Idee des Semantic Web: kurze Vorstellung der Grundidee, entwickelt aus dem Grundproblem der maschinellen Verarbeitung natürlicher Sprache. Das Resource Description Framework (RDF) als Grundlage für formalisierte Aussagen. Die Bedeutung stabiler URIs für die Idee des Semantic Web. Die Turtle Serialisierung von RDF als Grundlage für die Abfragesprache SPARQL. Namespaces und ihre Bedeutung, auch im Semantic Web. RDF-Schema und Ontologien zur Formulierung komplexerer Aussagen. Linked Open Data, Knowledge Graphs und die LOD Cloud. Wikidata (und DBpedia) als zentrale Knoten der LOD Cloud. Kurzer Exkurs zu alternativen Ansätzen zur Verlinkung von Normdaten: Beacon-Dateien. Übersicht zu Ressourcen im Semantic Web und in der LOD Cloud für die Geschichtswissenschaften.

            

                

                
Abbildung 1. The Semantic Web Technology Stack. 
                    
http://bnode.org/media/2009/07/08/semantic_web_technology_stack.png

                

            

            
Im zweiten Teil des Workshops sollen die Teilnehmenden in die Benutzung der Linked Open Data Plattform Wikidata eingeführt und Grundlagen für die Verwendung der Abfragesprache SPARQL gelegt werden.

            
Wikidata ist nicht nur der momentan größte frei verfügbare Wissensgraph, sondern bietet Daten unter freien Lizenzen und erlaubt genau wie die Wikipedia die freie Mitarbeit beim Aufbau der Wissensbasis. In der Übung lernen die Teilnehmenden somit eine der relevantesten Datenquellen für das Semantic Web kennen (Jacobsen et al. 2018). Außerdem ermöglicht Wikidata mit dem QueryService
 einen niederschwelligen Einstieg in SPARQL, für den keine lokalen Installationen oder technischen Kenntnisse notwendig sind. Für die Übungen wird lediglich ein internetfähiges Gerät (vorzugsweise Laptop) sowie ein Wikidata-Account benötigt. Auch die graphische Benutzeroberfläche der Wikidata eignet sich gut für die Vermittlung der theoretischen Konzepte des Semantic Web, ohne dabei Kenntnisse der Informatik voraussetzen zu müssen.
            

            
Der erste Übungsblock erläutert zunächst die Datenstrukturen der Wikidata. Anhand eines Beispieleintrags (Item) werden die theoretischen Konzepte aus dem ersten Teil des Workshops in ihrer Anwendung innerhalb der Wikidata gezeigt. Der Beispieleintrag repräsentiert ein Item, das über Properties mit weiteren Items verbunden werden kann. Durch eine solche Verknüpfung entsteht ein Statement. Dieses kann wiederum durch sogenannte Qualifier näher beschrieben werden. Qualifier sind für die Nutzung der Wikidata in der Geschichtswissenschaft besonders relevant. Sie ermöglichen unter anderem die Modellierung der Herkunft einer Information. Als „Referenz“ können so Internetressourcen verlinkt werden, aus denen die Information entnommen wurde. Qualifier wie „Startzeitpunkt“ und „Endzeitpunkt“ erlauben die Spezifikation eines Zeitraumes, innerhalb dessen eine Information gültig ist. Neben dem Aufbau der Wikidata-Items behandelt dieser Teil des Workshops auch eine Einführung in das Benennungssystem der Wikidata und die verschiedenen RDF-Namespaces, die dort verwendet werden. Schließlich kann ein Item der Wikidata mit mehreren Labels ausgezeichnet werden, die eine Benennung und Beschreibung in unterschiedlichen Sprachen ermöglichen.

            
Eingeübt wird der Umgang mit Wikidata anschließend anhand von Datensätzen aus dem Forschungsprojekt Germania Sacra
, welches sich mit der Erforschung kirchlicher Institutionen und Personen des Mittelalters und der Frühen Neuzeit beschäftigt. Für den Workshop werden aufbereitete Forschungsdaten zur Verfügung gestellt, welche die Teilnehmenden selbstständig in die Wikidata einpflegen können. So ergänzen sie bestehende Datensätze zu Bischöfen des Alten Reiches um weitere Informationen wie ihren Begräbnisort. Die händische Eingabe der Daten vertieft das Verständnis für die Datenstrukturen, ist mit größeren Datenmengen aber nicht praktikabel. Als Ausblick auf den Einsatz von Wikidata im Forschungsalltag werden daher mit den Tools „Quickstatements“
 und „OpenRefine“
 Möglichkeiten zur seriellen Eingabe von größeren Datenmengen vorgestellt. 
            

            
Der nächste Block der Übung behandelt die Grundlagen der Abfragesprache SPARQL. Ziel ist es, dass die Teilnehmenden ein Verständnis dafür entwickeln, wie geisteswissenschaftliche Fragestellungen als Abfrage formuliert und mit SPARQL auf Wikidata umgesetzt werden können. Dafür muss zunächst recherchiert werden, wie die in der Fragestellung enthaltenen geisteswissenschaftlichen Konzepte in der Wikidata modelliert sind. Anschließend wird eine passende Abfrage formuliert. Diese folgt mit SELECT, WHERE und gegebenenfalls OPTIONAL immer derselben Grundstruktur, die um weitere, komplexere Befehle ergänzt werden kann (siehe Abbildung 2). Diese Grundbausteine von SPARQL werden zunächst mit einfachen Abfragen wie „Finden Sie alle Datensätze zu Bischöfen mit einer WIAG-Kennung“ geübt. Vertiefend behandelt die Übung dann das Verketten von Abfragemustern und das Abfragen von Labels aus der Wikidata. Diese Grundlagen der Abfragesprache SPARQL werden durch Rückbezüge zum theoretischen Teil des Workshops auch mit den formalen Grundlagen des Semantic Web verknüpft. Während der Übung wechseln sich Demonstrationen neuer Konzepte und die Bearbeitung von aufeinander aufbauenden Übungsaufgaben ab. Während der Übungen sind die Teilnehmenden dazu eingeladen, sich mit anderen auszutauschen, Ergebnisse zu vergleichen und Fragen zu stellen. So erarbeiten sie sich Schritt für Schritt die Abfrage der im ersten Teil der Übung eingepflegten Datensätze.

            

                

                
Abbildung 2. SPARQL Abfrage im Wikidata Query-Service. Abrufbar unter https://w.wiki/5FuN

            

            
Das technische Framework, in das die Wikidata eingebettet ist, bietet den Nutzenden frei verfügbare Tools, mit denen abgefragte Daten graphisch dargestellt werden können. Dazu zählen ein interaktiver Graph, ein Zeitstrahl, eine Karte, eine Bildergalerie und viele andere Visualisierungsmöglichkeiten. Diese Tools werden zum Abschluss des praktischen Teiles vorgestellt und ausprobiert. Als Ergebnis dieses Hands on Übungsteils entsteht eine Visualisierung der Daten, die die Teilnehmer zu Beginn des Workshops in die Wikidata eingepflegt und mit den erworbenen SPARQL-Kenntnissen abgefragt haben.

            
Im vierten und letzten Teil des Workshops soll den Teilnehmenden ein Einblick in den Datenbestand der Wikidata zu geschichtlichen Themen und in die plattformspezifische Modellierung dieser Daten vermittelt werden. Daran anknüpfend soll ein Blick auf die Potenziale von wikibase-basierten Wissensgraphen für die Geschichtswissenschaften geworfen werden. Die Vor- und Nachteile einer Datensammlung, die in einem kollaborativen Prozess entsteht, werden diskutiert. Dabei gilt ein kritischer Blick den Fragen der Datenqualität, der Datenmodellierung und der Vollständigkeit der Daten.

            

                

                
Abbildung 3. Darstellung des Linked Open Data Webs der Wikimedia (Quelle: https://meta.wikimedia.org/wiki/LinkedOpenData/Strategy2021/Joint_Vision) 

            

            
Auch Alternativen zur Wikidata werden in diesem Teil des Workshops thematisiert. Wikibase
, das technische Framework, das der Wikidata zugrundeliegt, kann auch als eigenständige, von Wikidata unabhängige Instanz verwendet werden. Diese Lösung hat das Potenzial, die Vorteile des Systems zur Verlinkung von Daten zu nutzen und gleichzeitig eine unabhängige und durch die Forschenden selbst kuratierte Datensammlung aufzubauen. Hierfür gibt es in den Digital Humanities einige Anwendungsbeispiele, die kurz vorgestellt werden.
            

            
Als Grundlage für eine anschließende praxisorientierte Betrachtung eigenständiger Wikibase-Instanzen dienen kleinere Pilotprojekte, die unter anderem im Rahmen von Lehrveranstaltungen an der Universität Göttingen realisiert wurden. Es wurde nicht nur die Wikidata, sondern auch die vom Forschungszentrum Gotha der Universität Erfurt betriebene Wikibase-Instanz FactGrid
 mit Daten angereichert. Im Fokus standen dabei die Bischöfe des Alten Reiches, die ihre Servitienzahlungen an die päpstliche Kurie mit Hilfe des Florentiner Bankhauses der Familie Medici abwickelten. Mit den im Workshop erworbenen Kenntnissen können die Teilnehmenden diese Daten abfragen und sich einen Einblick in die konkrete Nutzung von Linked Open Data in den Geschichtswissenschaften – und auch für ihre eigene Forschungsfragen – verschaffen. 
            

            
Zur Nachbereitung des Workshops werden den Teilnehmenden die Übungsaufgaben inklusive einer Musterlösung zur Verfügung gestellt. Sie erhalten außerdem in Form eines „Cheat Sheet“ eine Übersicht über alle in der Übung verwendeten Befehle.

            

                
Zielgruppe:
 HistorikerInnen und GeisteswissenschaftlerInnen ohne Vorkenntnisse in SWT und LOD.
            

            

                
Didaktisches Konzept:
 Einführende Vermittlung von Grundlagenwissen, Interaktive Übungen, Gruppenarbeit/Hands on Beispiele. 
            

            

                
Erwartete Teilnehmerzahl:
 5-25
            

            

                
Technische Austattung:
 Seminarraum mit HD Beamer
            

            
Vortragende: 

            
Bärbel Kröger, M.A.

            
Akademie der Wissenschaften zu Göttingen

            
Geiststraße 10

            
37073 Göttingen

            

                

                    
bkroege@gwdg.de

                

            

            
Bärbel Kröger arbeitet im Akademievorhaben Germania Sacra und forscht zum Einsatz von Linked Open Data im Bereich der mittelalterlichen Kirchengeschichte. Sie leitet ebenfalls das Linked Data Projekt WIAG (Wissensaggregator Mittelalter und Frühe Neuzeit). 

            
Johanna Störiko, M. Sc.

            
Georg-August-Universität Göttingen

            
Institut für Digital Humanities

            
Nikolausberger Weg 23

            
37073 Göttingen

            

                

                    
johanna.stoeriko@uni-goettingen.de

                

            

            
Johanna Störiko (geb. Danielzik) untersuchte in ihrer Masterarbeit historische Werbeanzeigen in Kulturzeitschriften der Jahrhundertwende mit digitalen Methoden. Sie interessiert sich für den Einsatz von Technologien des Semantic Web und Linked Open Data in den digitalen Geschichtswissenschaften.

            
Dr. Jörg Wettlaufer 

            
Koordination Digitalisierung und Datenkuration | Digitale Akademie

            
Akademie der Wissenschaften zu Göttingen

            
Theaterstraße 7

            
37073 Göttingen

            
Germany

            

                

                    
jwettla@gwdg.de

                

            

            
Jörg Wettlaufer leitet die Digitale Akademie der Wissenschaften zu Göttingen und forscht zu Themen der Digitalen Geschichtswissenschaft, insbesondere dem Einsatz Semantic Web Technologien in den Digitalen Geisteswissenschaften sowie zur Rechts- und Sozialgeschichte. 

            
            

                

                    
9:00

                    
Vorstellungsrunde und Einführung in die Veranstaltung (30 min.)

                

                

                    
9:30

                    
Teil 1: Einführung Grundlagen des Semantic Web und LOD (60 min.)

                

                

                    
10:30

                    
Kaffeepause

                

                

                    
11:00

                    
Teil 2: Übung mit Wikidata anhand von Beispielen (90 min.)

                

                

                    
12:30

                    
Mittagspause

                

                

                    
13:30

                    
Teil 3: Übung SPARQL auf Wikidata (90 min.)

                

                

                    
15:00

                    
Pause

                

                

                    
15:30

                    
Teil 4: Beispiele für den Einsatz von Wikidata und LOD in den Geschichtswissenschaften (90 min.)

                

                

                    
17:00

                    
Ende

                

                
Geplanter Ablauf des Workshops:

            

        

            

                
Hintergrund

                

                    
Die Omnipräsenz von Geschichten in der menschlichen Kultur - wie auch zahlreiche akademische Reflexionen - machen deutlich: Narrative Strukturierung von Inhalten gehört zu den wesentlichsten Gestalten und Gestaltungsstrategien für die Vermittlung neuer, relevanter oder unterhaltsamer Informationen sowohl in der heutigen Kultur als auch in der breiteren Geschichte (Bolin, 2010; Dykes, 2019). Zahlreiche Hypothesen liefern hierfür mögliche Gründe, (u. a. ein spezieller Modus der narrativen Informationsverarbeitung in der menschlichen Kognition), aber die zentrale Folgerung für modernes Informations- und Kommunikationsdesign ist verhältnismäßig simpel: Gutes Storytelling kann die Aufnahme und das Verständnis von komplexer Information entscheidend verbessern und auch in digitalen Medien zu einer vertieften Auseinandersetzung mit diversen Inhalten führen. Dies hat auch im Feld der Visualisierung dazu geführt, dass datengetriebenes, visuelles Storytelling in den letzten Jahren zu einem allgegenwärtigen Thema in der Erforschung und Entwicklung von Visualisierungen geworden (Segel &amp; Heer, 2010; Riche et al., 2018). Auch in der visuellen Vermittlung von digitalen kulturellen Sammlungen (Windhager et al., 2018) oder von Biografiedaten kommen narrative Methoden immer öfters zum Einsatz (Kusnick et al., 2021).

                

            

            

                
Das InTaVia-Projekt

                

                    
Das InTaVia-Projekt (“In/Tangible European Heritage - Visual Analysis, Curation and Communication, 

                    

                        
https://intavia.eu

                    

                    
) zieht größere Bestände von materiellem und immateriellem Kulturerbe in eine transnationale Datenbasis zusammen (Windhager, Mayr, Schlögl, &amp; Kaiser, 2022, in Druck). In den letzten Jahrzehnten wurde sowohl die Digitalisierung von materiellen Objektsammlungen vorangetrieben (

                    
Khan, Shafi, &amp; Ahangar, 2018

                    
), wie auch die Digitalisierung von biografischem Wissen über Kulturschaffende (ter Braake et al., 2015; 2017). Diese Entwicklungen bieten eine gute Basis für eine digitale Analyse und Kommunikation des Lebens und Werks von Kulturschaffenden (

                    
Khulusi et al., 2016; 

                    
Schlögl, Windhager, Mayr, &amp; Kaiser, 2019; Windhager et al., 2018), aber fehlende Verknüpfungen, Harmonisierungen und ein Mangel an Werkzeugen erschweren die entsprechende Arbeit - besonders für Praktiker*innen im Bereich der Kulturvermittlung. Mit Blick auf etablierte Nationalbiografien und korrespondierende kulturelle Objektdaten arbeitet das InTaVia-Konsortium an der Entwicklung von Lösungen und harmonisiert zu diesem Zweck nationale Datenbestände (inkl. der Biografieprojekte von Finnland, Niederlande, Österreich und Slowenien). Darauf aufbauend entwickelt es ein prototypisches Informationsportal für die visuelle Analyse und Kommunikation dieser integrierten Kulturdaten. So werden synoptische Ansichten auf historischen Daten zu Leben und Werken aus verschiedenen Perspektiven der Datenvisualisierung (geografisch, relational, kategorial, chronologisch) möglich, sowie die narrative Gestaltung und Vermittlung dieser Information mittels Methoden des individuellen und kollektiven Storytellings.

                

            

            

                
Zielsetzung Workshop

                

                    
Als “Early-Access Workshop” zielt die Veranstaltung auf die Erprobung und Diskussion von prototypischen Methoden des visuellen Storytellings mit Kulturdaten. Teilnehmende werden zur Nutzung und Erprobung der InTaVia-Plattform eingeladen, um dort mit exemplarischen Daten die Möglichkeiten der narrativen visuellen Gestaltung auszuloten und zu diskutieren. Er richtet sich sowohl an Forscher*innen wie auch Praktiker*innen im Bereich des kulturellen Erbes, der Kunst- und Kulturgeschichte, sowie angrenzender Geisteswissenschaften. Eine einführende Diskussion von State-of-the-Art-Methoden aus DH-Perspektive wird dabei verbunden mit einer kurzen Vorstellung der InTaVia-Plattform und ihrer Technologien - mit spezifischen Fokus auf Module der Datenkuratierung und des visuellen Storytellings. Teilnehmende Expert*innen können so Einblicke in aktuelle Entwicklungen der narrativen Visualisierung gewinnen, während die Veranstalter*innen des Workshops mögliche Anregungen und Wünsche für die partizipative Weiterentwicklung der Plattform dokumentieren werden.

                

                

                    
Bei Interesse wird in diesem Kontext über die Veranstaltung hinaus auch die Entwicklung von gemeinsamen 

                    
Fallstudien 

                    
angeregt. Für Teilnehmer*innen wird in diesem Fall ein persistenter Zugang zur InTaVia-Plattform geschaffen, über den die Auswahl oder der Import von eigenen Daten mit Bezug zu individuellen Forschungs- und Vermittlungsthemen möglich ist. So wird ein extensiver Austausch zu den Möglichkeiten und Grenzen der Plattform möglich. Expert*innen können die Plattform nutzen, um neue Designs und Vermittlungsmethoden für ihre eigenen Daten und Themen zu gewinnen und um diese im Rahmen von gemeinsamen Fallstudien für eigene kommunikative Zwecke zu nutzen. Feedback zu den Möglichkeiten und Grenzen der Plattform wird wiederum dem Konsortium wertvolle Einblicke in die entscheidenden Bedürfnisse von Praktiker*innen liefern.

                

            

            

                
Ablauf Workshop 

                

                    
1) Projektvorstellung: 

                    
Die InTaVia-Plattform verknüpft Datensammlungen verschiedenen Typs (i.e. kulturelle Objektsammlungen und biografische Textsammlungen) zu einer integrierten Graphdatenbank (Abbildung 1). In einer kurzen Vorstellung werden die wichtigsten Forschungsfragen des Projekts gemeinsam mit seinen technologischen Zielen und Modulen vorgestellt. Dies inkludiert Information über das Modul zur manuellen Kuratierung dieser Daten (Data Curation Lab) und das Modul zum visuellen Storytelling (Visual Storytelling Suite). 

                

                

                    

                    
Abbildung 1: Architektur der InTaVia Plattform

                

                
                

                    
2) Hands-On-Vorstellung des Storytelling-Moduls:

                    
Eine kurze Vorstellung der integrierten Graphdatenbank (IKG - InTaVia Knowledge Graph) wird zu einem Verständnis des zugrundeliegenden Datenmodells führen. Auf diese Weise werden Teilnehmende mit wichtigen Aspekten der Lebens- und Werkdaten vertraut, deren narrative Vermittlung die InTaVia-Plattform unterstützt. Dies ist von besonderer Relevanz für die Möglichkeit der manuellen Aufbereitung und Zusammenführung von Kulturdaten (sowohl Biografie- wie auch kulturelle Objektdaten), welche in einem eigenen Datenkuratierungs-Modul angesiedelt ist. Anhand einer Auswahl von Arbeitsdaten für den Workshop werden hierbei die Möglichkeiten aufgezeigt, die sich aus einer etwaigen Nutzung der Plattform für eigene Fallstudien ergeben. 

                

                

                    
Kulturelle Objektdaten und Biografiedaten haben eine Vielzahl von Facetten und Dimensionen die für Historiker*innen und Kulturwissenschaftler*innen von Interesse sein können. Zu diesen Dimensionen zählen die geografische Position von biografischen oder künstlerischen Ereignissen, diverse Kategorien von Ereignissen oder kulturellen Entitäten (Objekte oder Personen), Relationen zwischen Personen und/oder Objekten, sowie chronologische Abfolgen und Zusammenhänge. Diese Aspekte können auf verschiedenen Ebenen der Aggregation visualisiert werden - und in der Folge mit Medien, Texten und interaktiven Elementen angereichert und narrativ vermittelt werden (vgl. Abbildung 2). Der Workshop wird zu diesem Zweck das Visualisierungsmodul der InTaVia-Plattform kurz einführen, um den Schwerpunkt auf die praktische Gestaltung von Geschichten mit exemplarischen Objekt- und Akteursdaten zu legen.

                

                

                    

                    

                    
Abbildung 2: Ausschnitte aus einam Storyboard mit geographischer, temporaler und Häufigkeitsvisualisierung (v. links nach rechts)

                

                

                
                

                    
3) Feedback: 

                    
Während der explorativen Arbeit mit den Modulen der Plattform werden Fragen und Hinweise der Teilnehmer*innen notiert um im Rahmen der weiteren Arbeit am Forschungsprojekt in die nutzer*innen-zentrierte Entwicklung der Plattform einfließen zu können. Dazu werden sowohl die anonymisierten Notizen zu Aktivitäten des ‘lauten Denkens’ von Teilnehmer*innen dienen, wie auch die Rückmeldungen eines kompakten strukturierten Feedbackbogens.

                

            

            

                
Format:
                    

                

                

                    
Der Workshop ist als Halbtagesveranstaltung konzipiert mit Fokus auf die Erkundung, Erprobung und Diskussion von Methoden der narrativen Kulturdatenvisualisierung. Seine intendierte Zielgruppe reicht von interessierten Praktiker*innen aller kulturellen Institutionen bis hin zu Historiker*innen und Expert*innen der digitalen Geisteswissenschaften mit Interesse an Storytelling und Wissensvermittlung. Für die Teilnahme gibt es keine Voraussetzungen mit Blick auf inhaltliches oder technisches Vorwissen. Für die praktische Arbeit an den Daten genügt die Mitnahme eines Laptops. Die Gruppengröße ist auf 30 Teilnehmer*innen beschränkt. Für die technische Raumausstattung wird ein Beamer, ein Medienkoffer, sowie Whiteboards oder Pinnwände beantragt. 

                

                

                    

                    

                    
Fördernachweis
: Das Projekt InTaVia (
                    

                        
https://intavia.eu

                    
) wird von der Europäischen Kommission im Rahmen des H2020 Research and Innovation Programme, Grant Agreement No. 101004825 gefördert.
                

            

        

            

                
Hintergrund

                

                    
In den letzten Jahrzehnten wurde die Digitalisierung der materiellen Objektsammlungen von zahlreichen Kulturerbe-Institutionen vorangetrieben (

                    
Khan, Shafi, &amp; Ahangar, 2018; Münster et al., 2019

                    
). Gleichzeitig wurde immaterielles Kulturerbe – wie biografisches Wissen über KünstlerInnen – digital erfasst und in biografischen Datenbanken verfügbar gemacht (Hyvönen, 2018; ter Braake et al., 2015; 2017). Diese Entwicklungen bieten eine gute Basis für eine digitale Analyse und Kommunikation des Lebens und Werks von Kulturschaffenden (

                    
Ruecker, Radzikowska, &amp; Sinclair, 2016; Khulusi et al., 2016; 

                    
Schlögl, Windhager, Mayr, &amp; Kaiser, 2019; Windhager et al., 2018), jedoch verhindern mangelnde Verknüpfungen von lokalen Datensammlungen sowie fehlende Werkzeuge oft eine optimale Nutzung durch interessierte Forscher*innen und Praktiker*innen.

                

            

            

                
Die InTaVia-Plattform

                

                    
Das InTaVia-Projekt (

                    

                        
https://intavia.eu

                    

                    
) arbeitet an der Reduktion solcher Barrieren und führt erstmalig materielles und immaterielles Kulturerbe mehrerer Länder in eine integrierte Datenbasis zusammen (Windhager, Mayr, Schlögl, &amp; Kaiser, 2022, in Druck). Das Konsortium harmonisiert zu diesem Zweck nationale Kulturdatenbestände (inkl. Finnland, Niederlande, Österreich und Slowenien) und entwickelt ein prototypisches Informationsportal für die visuelle Analyse und Kommunikation dieser integrierten Kulturdaten. So wird eine synoptische Visualisierung und Betrachtung von historischen Daten zu Leben und Werken aus verschiedenen Perspektiven (geografisch, relational, kategorial, chronologisch) und auf verschiedenen Ebenen der Aggregation (von close bis distant reading) möglich.

                

            

            

                
Zielsetzung Workshop

                

                    
Der Workshop ist als “Early-Access Workshop” für die InTaVia-Plattform konzipiert und zielt auf die Erprobung und Diskussion von prototypischen Visualisierungsmethoden, sowie auf den Austausch mit interessierten Forscher*innen in Feldern des digitalen kulturellen Erbes, der digitalen (Kunst-)Geschichte und angrenzender Geisteswissenschaften. Zu diesem Zweck wird eine Diskussion der Thematik aus DH-Perspektive verbunden mit einer Vorstellung der InTaVia-Plattform und ihrer Technologien - mit spezifischen Fokus auf Module der Datenkuratierung und auf Methoden der visuellen Analyse. So können teilnehmende Expert*innen Einblicke in synoptische Methoden der Datenvisualisierung und -kuratierung gewinnen, während die Veranstalter*innen des Workshops mögliche Anregungen und Wünsche für die partizipative Weiterentwicklung der Plattform dokumentieren werden. 

                

                

                    
Bei Interesse soll der Workshop auch der Initiierung von gemeinsamen 

                    
Fallstudien 

                    
dienen. Für teilnehmende Expert*innen wird in diesem Fall im Nachfeld des Workshops ein Zugang zur prototypischen InTaVia-Plattform geschaffen, über den die Auswahl oder der Import von eigenen Daten mit Bezug zu individuellen Forschungsthemen möglich ist. In der Folge ist ein ausführlicherer Austausch zu den sich entwickelnden Möglichkeiten und Grenzen der Plattform angestrebt: Inhaltliche Expert*innen können die Tools der Plattform nutzen, um neue Einsichten in ihre jeweiligen Daten und Themen zu gewinnen und um Visualisierungen im Rahmen von gemeinsamen Fallstudien für eigene analytische oder kommunikative Zwecke zu nutzen. Feedback zu den sich entwickelnden Möglichkeiten und Grenzen der Plattform kann wiederum den Entwickler*innen der Plattform wertvolle Einblicke in die entscheidenden Bedürfnisse von Praktiker*innen liefern.

                

            

            

                
Ablauf Workshop: 

                

                    
1) Projektvorstellung: 

                    
Die InTaVia-Plattform verknüpft Datensammlungen verschiedenen Typs (i.e. kulturelle Objektsammlungen und biografische Textsammlungen) zu einer integrierten Graphdatenbank (Abbildung 1). In einer kurzen Vorstellung werden die wichtigsten Forschungsfragen des Projekts gemeinsam mit seinen technologischen Zielen und Modulen vorgestellt. Dies inkludiert Information über das Datenmodell IDM (InTaVia Data Model), das Modul zur manuellen Kuratierung dieser Daten (Data Curation Lab) und das Modul zur visuellen Analyse von ausgewählten Daten und Themen (Visual Analytics Studio). 

                

                

                    

                    
Abbildung 1: Architektur der InTaVia Plattform

                

                
                

                    
2) Hands-On-Vorstellung von Datenmodell und Kuratierungsmodul:

                    
 Mit Blick auf die transnationale Graphdatenbank (IKG - InTaVia Knowledge Graph) wird eine kursorische Vorstellung der vier nationalen Biografiedatenprojekte zur Darstellung des integrierten Datenmodells führen. Auf diese Weise werden Teilnehmende mit den Facetten der Lebens- und Werkdaten vertraut, deren Analyse und Aufbereitung die InTaVia-Plattform unterstützt. Dies ist von besonderer Relevanz für die potentielle manuelle Aufbereitung und Zusammenführung von Daten (sowohl Biografie- wie auch kulturelle Objektdaten), welche in einem eigenen Datenkuratierungs-Modul angesiedelt ist. Anhand einer Auswahl von Arbeitsdaten für den Workshop werden hierbei die Möglichkeiten aufgezeigt, die sich aus einer etwaigen Nutzung der Plattform für eigene Fallstudiendaten ergeben.

                

                

                    

                    
3) Hands-On-Vorstellung von Visualisierungswerkzeugen: 

                    
Kulturelle Objektdaten und Biografiedaten haben eine Vielzahl von Facetten und Dimensionen die für Historiker*innen und Kulturwissenschaftler*innen von Interesse sein können. Zu diesen Dimensionen zählen die geografische Position von biografischen oder künstlerischen Ereignissen, diverse Kategorien von Ereignissen oder kulturellen Entitäten (Objekte oder Personen), Relationen zwischen Personen und/oder Objekten sowie chronologische Abfolgen und Zusammenhänge. Diese Aspekte können auf verschiedenen Ebenen der Aggregation - von historischen Individuen bis hin zu diversen Gruppierungen - für verschiedene Fragestellungen von Relevanz sein. Der Workshop wird zu diesem Zweck die Arbeit mit dem multiperspektivischen Visualisierungsmodul der InTaVia-Plattform ins Zentrum stellen und mit den Teilnehmer*innen skalierbare Blicke (inkl. close &amp; distant reading) auf exemplarische Objekt- und Akteursdaten entwickeln (vgl. Abbildung 2).

                

                

                    

                    
Abbildung 2: Überblick über relevante Visualisierungstechniken in InTaVia

                

                
                

                    
4) Feedback:

                    
 Während der explorativen Arbeit mit den Modulen der Plattform werden Fragen und Hinweise der Teilnehmer*innen notiert, die im Rahmen der weiteren Arbeit am Forschungsprojekt in die nutzer*innen-zentrierte Entwicklung der Plattform einfließen werden. Dazu werden sowohl die anonymisierten Notizen zu Aktivitäten des ‘lauten Denkens’ von Teilnehmer*innen dienen, wie auch die Rückmeldungen im Rahmen einer kurzen abschließenden Feedbackrunde.

                

            

            

                
Zielgruppe und Voraussetzungen

                

                    
Der Workshop ist als Halbtagesveranstaltung geplant mit Fokus auf abwechslungsreiche Inputs zur Praxis, Erprobung und Evaluierung von aktuellen State-of-the-Art-Methoden der kulturellen Sammlungs- und Biografiedatenanalyse. Seine intendierte Zielgruppe reicht von interessierten Historiker*innen und Praktiker*innen bis hin zu Expert*innen der digitalen Geisteswissenschaften mit einem Schwerpunkt der Datenmodellierung, Kuratierung oder Visualisierung. Für die Teilnahme gibt es keine Voraussetzungen mit Blick auf inhaltliches oder technisches Vorwissen. Für die praktische Arbeit an den Daten genügt die Mitnahme eines Laptops. Die Gruppengröße ist auf 30 Teilnehmer*innen beschränkt. Mit Blick auf die technische Raumausstattung wird ein Beamer, ein Medienkoffer, sowie Whiteboards oder Pinnwände beantragt. 

                

                

                    
Fördernachweis
: Das Projekt InTaVia (
                    

                        
https://intavia.eu

                    
) wird von der Europäischen Kommission im Rahmen des H2020 Research and Innovation Programme, Grant Agreement No. 101004825 gefördert.
                

            

        

            

                

                    
Modellierung und Simulation in den Digital Humanities

                

                

                    
Die Modellierung von Forschungsgegenständen sowie die Operationalisierung von Forschungsfragen sind zentrale Bestandteile geisteswissenschaftlicher Forschung (Beynon et al. 2006, Flanders and Jannidis 2015, Thaller 2017, Piotrowski, 2019). Mit Einzug der Digitalisierung können Forschungsgegenstände in strukturierte Forschungsdaten transformiert und durch diese formalisierte Modellierung mit informationstheoretischen Methoden untersucht werden. Die Kompetenzen, die für eine adäquate konzeptionelle und formale Modellierung sowie für eine quantitative Analyse geisteswissenschaftlicher Forschungsinteressen erforderlich sind, bilden den “fächerübergreifende[n] Kern der Digital Humanities” (Thaller 2017, S. 16).

                

                

                    
Die Fähigkeit zur kritischen Auseinandersetzung mit den Vorannahmen und Auswirkungen der Reduktion, die für die formale Beschreibung von Forschungsgegenständen notwendigerweise einhergeht, ist mehreren geisteswissenschaftlichen Disziplinen gemein und von zentraler Bedeutung (Thaller 2017, Ciula et al, 2018, Piotrowski 2019). 

                    
 

                

                

                    
In den Geschichtswissenschaften findet sich diese Auseinandersetzung einerseits in Diskussionen über die Anwendung formaler Methoden zur Beschreibung und Analyse historischer Phänomene,

                    
 andererseits aber auch in Reflektionen über Möglichkeiten der Vermittlung dieser Methodik in der Lehre. Ein Beispiel hierfür bilden analoge und digitale Simulationen, die in den Geschichtswissenschaften seit Langem als didaktische Methode eingesetzt werden, um die Modellierung von historischen Konflikten zu erlernen (Sabin 2011, Sabin, 2012, Sabin 2016).

                

            

            

                

                    
Konfliktsimulationen als didaktisches Instrument im Unterricht

                

                

                    
Konfliktsimulationen sind ein äußerst leistungsfähiges didaktisches Instrument für den akademischen und nicht-akademischen Unterricht; auf diese Weise eingesetzt können sie zu den 
                        
serious

                        
games
 gezählt werden (Jones 1995; Michael and Chen 2006). Sie stellen ein Element partizipatorischen Unterrichts dar und eröffnen beispielsweise im Fach Geschichte den Teilnehmenden einen Zugang zu den für die Vorbereitung der Simulation notwendigen Quellenmaterialien, der sich stark von einer eher rezeptiven Wissensaufnahme unterscheidet: für die Teilnahme an einer Konfliktsimulation, in deren Mittelpunkt immer die Verarbeitung von Information und das Entscheiden auf der Basis dieser prozessierten Information steht, ist eine auf die Ziele der Simulation hin ausgerichtete Aufbereitung des jeweils relevanten Quellenmaterials notwendig. Hierdurch bieten sich den Teilnehmenden Perspektiven, die ohne eine derartige Aufbereitung kaum deutlich würden. 
                    

                    
 

                

                

                    
Ein weiteres wichtiges Einsatzgebiet ist die Auseinandersetzung mit den Problemen, die bei der Prozessierung von Information, der darauf basierenden Entscheidungsfindung und anschließenden Kommunikationsprozessen entstehen. Hier können Konfliktsimulationen eindrücklicher aufzeigen, als dies in anderen Unterrichtsformen möglich wäre, wie durch dysfunktionale Entscheidungsprozesse Handlungsmöglichkeiten von Akteuren eingeschränkt werden.

                    
 

                

                

                    
Neben der Teilnahme führt schließlich die Erstellung einer Konfliktsimulation zu einer deutlich vertieften Auseinandersetzung mit dem gegebenen Phänomen (Sabin 2012, Sabin 2016). Bei einem erfolgreichen Simulationsdesign handelt es sich immer um eine stark abstrahierende Modellierung einer sehr komplexen Realität; der Versuch einer solchen Modellierung beinhaltet daher immer auch eine Auseinandersetzung mit der Frage, welche Faktoren Eingang in die Simulation finden sollen, und welche unberücksichtigt bleiben können.

                    
 

                

                
Über die Frage nach Perspektivbildung und Modellierungsleistung der Teilnehmenden hinaus führt die bei der Durchführung von Simulationen schließlich immer wieder zu beobachtende Immersion zu einer sinnvollen Auflockerung des Unterrichts.

            

            

                

                    
Konfliktsimulationen und das Problem der Zugänglichkeit im Unterricht

                

                

                    
“Sheldon: I am here to sit with you and keep you company. - Bernadette: Oh, that's nice. - Sheldon: Yeah, by playing the most complicated board game ever invented: Campaign for North Africa. I bought it off eBay. It smells a little like chili, but all the pieces are there.”

                

                

                    
Ungeachtet ihrer Vorteile ist der Einsatz von Konfliktsimulationen im Unterricht mit einem zentralen Problem verbunden: der Zugänglichkeit für die Teilnehmenden. Jede Simulation stellt eine verregelte Reduktion der Realität dar – die vollständige Abbildung eines realweltlichen Phänomens in einer Simulation ist unmöglich –, bei der komplexe Geschehen in der Regel auf wenige Faktoren reduziert werden. Das Ineinandergreifen dieser Faktoren wird dann durch ein Regelwerk abgebildet, das der Simulation zugrunde liegt. Als Konsequenz ist eine erfolgreiche Teilnahme an einer Simulation nur auf der Grundlage belastbarer Regelkenntnis möglich. Konkret gesprochen bedeutet der Einsatz beispielsweise des Schachspiels, dass alle Teilnehmenden dessen Regelwerk beherrschen müssen.

                    
 

                

                

                    
Dieser Umstand hat drei wichtige Konsequenzen. Zum ersten wirken Simulationen umso abschreckender, je komplexer die Regelwerke sind. Aus diesem Grund ist der weitaus größte Teil der kommerziell erhältlichen Konfliktsimulationen für eine Unterrichtssituation im Normalfall denkbar ungeeignet (Sabin 2012); das im voranstehenden Zitat genannte 
                        
The Campaign 

                        
for

                        
 North 

                        
Africa

                    

                    
 nimmt zwar bis heute eine Ausnahmestellung ein, die allermeisten kommerziell erhältlichen Konfliktsimulationen sind aber ohne eine längere Einweisung nicht durchführbar. Eine zweite wichtige Konsequenz liegt in der Auswirkung, die das Regelwerk auf die Teilnehmenden an der Simulation hat. In den meisten Fällen interagieren diese aus dem Bemühen heraus, keine Fehler zu machen, mehr mit dem Regelwerk als mit der eigentlichen Simulationssituation. Dabei besteht die Gefahr, dass die Teilnehmer sich stärker auf das „korrekte Bedienen“ des Regelwerks konzentrieren als auf den eigentlichen Inhalt der Simulation. Zum dritten führen die sich aus dem Einsatz komplexer Regelwerke ergebenden Notwendigkeiten häufig dazu, dass Konfliktsimulationen 
                        
- 
wenn überhaupt
                        
 -
 nur in sehr einfacher Form im Unterricht Einsatz finden (Wintjes und Pielström 2018).
                    

                    
 

                

                

                    
Das grundsätzliche Dilemma der Unvereinbarkeit von leichter Zugänglichkeit und komplexer Simulation lässt sich auch durch eine digitale Simulation nicht lösen, da auch hier eine Zunahme an Komplexität der Simulation immer mit einer Zunahme an Komplexität der Bedienung verbunden sein wird; in dieser Hinsicht verhalten sich digitale Simulationen nicht anders als analoge Simulationen.

                    
 

                

            

            

                

                    
facilitator-based simulations (FBS) im Unterricht

                

                

                    
Einen Ausweg aus dem oben beschriebenen Dilemma bieten sogenannte 
                        
facilitator-based-simulations
 (FBS), bei denen es sich um die älteste Form edukativer Konfliktsimulationen handelt (Wintjes und Pielström 2019, Wintjes 2022). Bei diesen ist eine Regelkenntnis der Teilnehmer nicht nötig; diese interagieren nicht direkt mit dem Regelwerk, vielmehr geben sie Entscheidungen, Aufträge oder Handlungsanweisungen an das Leitungsgremium der Simulation, die facilitator, heraus, die diese dann gemäß dem der Simulation zugrunde liegenden Regelwerk umsetzen und über die Ergebnisse den Teilnehmenden wiederum Bericht erstatten (Jones 1995). Das Regelwerk ist somit von den Teilnehmenden abgeschirmt, die sich ganz auf ihre Aufgaben der Informationsprozessierung und des Entscheidens konzentrieren können. Durch den Wegfall der Notwendigkeit, das Regelwerk zu beherrschen, erweisen sich FBS als ausgesprochen zugänglich auch für diejenigen, die keinerlei Vorerfahrungen mit Konfliktsimulationen aufweisen können.  
                    

                    
 

                

                

                    
Für den Einsatz im Unterricht stellen FBS eine nahezu ideale Lösung dar, ermöglichen sie es doch mit einem Minimum an Aufwand – der leitende facilitator muss das Regelwerk beherrschen, im Idealfall wird er je nach Umfang der Simulation durch weitere facilitator unterstützt – eine Simulation durchzuführen, bei der sich die Teilnehmenden ganz auf ihre Rolle in der Simulation konzentrieren können.

                    
 

                

                

                    
Diese stehen dabei vor der Herausforderung, sich nicht nur auf analytische Art und Weise mit den einzelnen Gegenständen der Simulation auseinanderzusetzen, sondern ihr Zusammenwirken innerhalb eines komplexen, multifaktorischen Systems nachzuvollziehen. Das hierbei entstehende ganzheitliche Verständnis für beispielsweise historische Phänomene kann dann die Voraussetzung für eine adäquate multiperspektivische Modellierung komplexer Phänomene bilden; im Idealfall schließt sich daher an die Teilnahme an einer Konfliktsimulation in einem zweiten Schritt die Erstellung einer solchen an.

                

            

            

                

                    
agent-based simulations (ABS) im Unterricht

                

                

                    
Insbesondere in Veranstaltungen mit fortgeschrittenen Studierenden, die bereits grundsätzlich mit dem Instrument der Konfliktsimulation vertraut sind, bietet es sich an, die eigenständige Modellierung und kollaborative Implementierung von Konfliktsimulationen zu ausgesuchten Teilproblemen zu einer zentralen Aufgabe von Kleingruppen zu machen, um anschließend die – zunächst oft unbewusst getroffenen – Modellierungsentscheidungen der einzelnen Gruppen zu präsentieren und diskutieren.

                    
 

                

                

                    
Für die Implementierung formaler Modelle in imperative Programmiersprachen eignet sich dabei besonders das Konzept der objekt-orientierten Programmierung (OOP). Bei diesem Ansatz werden Forschungsdaten als Objekte mit Attributen repräsentiert, die über Objekten spezifisch zugeordnete Funktionen miteinander interagieren. Generell handelt es sich bei den für eine derartige Umsetzung notwendigen Fähigkeiten zur Abstraktion und Modularisierung um zentrale Kompetenzen für die imperative Programmierung (Jannidis 2017, S. 88). 

                    
 

                

                

                    
Als Beispiel sei hier ein Seminar aus dem Sommersemester 2022 an der Julius-Maximilians-Universität Würzburg kurz vorgestellt: In diesem Seminar wurden Studierende des Masterstudienganges Digital Humanities an Thema der mathematischen Modellierung von Abnutzung in militärischen Konflikten herangeführt. Die Teilnehmenden wurden zunächst, nach einer kurzen Einführung in den historischen Kontext, vor die Aufgabe gestellt, in zwei kleinen Gruppen mit Hilfe von Würfeln und Markern Regeln zu entwickeln, mit deren Hilfe das beschriebene Phänomen simuliert werden kann. Gleichzeitig sollten die Studierenden experimentell untersuchen, welche Erkenntnisse über das Phänomen sich aus ihren Modell ableiten lassen. In einem zweiten Schritt wurde der Kurs mit dem wichtigsten deterministisch-mathematischen Modell konfrontiert, das bis heute für die Modellierung dieses Problems genutzt wird (Lanchester 1916). Zu diesem Zeitpunkt war den Teilnehmenden der Vergleich verschiedener Modellierungskonzepte für dasselbe Problem bereits möglich. In den folgenden Seminarstunden wurde das Konzept des 
                        
agent-based

                        
modeling
 (vgl. Gavin 2014 und Romanovska et al. 2021) eingeführt und ein einfaches Python-Framework
                    

                    
 für eine 
                        
agent-based

                        
simulation
 (ABS) von Abnutzung vorgestellt. Dieses Framework dient
                        
e
 initial zur Umsetzung vergleichender Experimente, aber auch als Grundlage für die Implementierung neuer Aspekte und der Setzung eigener Schwerpunkte durch die Studierenden. 
                    

                    
 

                

                
Dieses Unterrichtskonzept erweist sich als ein geeigneter Rahmen für die generelle Diskussion über die Eignung verschiedener Modellierungsansätze im Hinblick auf unterschiedliche Forschungsprobleme, eine Einführung in die Erstellung und die Anwendung einer ABS und darüber hinaus ein technisch niederschwelliges Framework aus der Forschungspraxis für das objektorientierte Programmieren.

            

            

                
Ausblick

                

                    
“Come, Watson, Come! The Game is afoot!”

                

                

                    
Bei Konfliktsimulationen – und insbesondere bei FBS – handelt es sich um leistungsfähige Werkzeuge, deren didaktischer Wert in drei Bereichen zu finden ist: zum ersten bieten sie den Teilnehmenden einen partizipatorischen Zugang zu dem jeweils behandelten Gegenstand und damit eine alternative Lernerfahrung. Zum zweiten erfahren die Teilnehmenden direkt die mit der Prozessierung komplexer Information sowie der darauf basierenden Entscheidung verbundenen Schwierigkeiten und gewinnen so Einsichten in ihre eigenen Entscheidungsprozesse. Die Auseinandersetzung mit dem Problem der Erstellung einer Simulation stellt schließlich einen niederschwelligen Einstieg in das Problem der Modellierung von Forschungsgegenständen sowie der Operationalisierung von Forschungsfragen dar; im Rahmen der Erstellung einer Simulation können die Teilnehmenden anhand konkreter Beispiele den Aufbau möglicher Forschungsvorhaben diskutieren und kritisch reflektieren.

                    
 

                

                

                    
Über den konkreten Nutzen im Rahmen des historischen bzw. Gesellschaftswissenschaftlichen Unterrichts hinaus kann die Auseinandersetzung mit Konfliktsimulationen gerade aufgrund des niederschwelligen Zugangs einen wichtigen Beitrag zur allgemeinen Auseinandersetzung mit Modellierungsfragen, mit dem Erwerb weiterer Kompetenzen (
                        
Digital 

                        
Literacy
) und damit einer erfolgreichen Partizipation in einer vernetzten, hoch technologisierten Gesellschaft leisten. Der Einsatz von Konfliktsimulationen, ihre Fähigkeiten und Begrenzungen verdienen daher auch in den Digital Humanities mehr Beachtung, als sie momentan erfahren.
                    

                    
 

                

            

        



            

                

                    
Hintergrund und Beschreibung des Themas
                

                

                    
‘Scheitern’ und ‘Misserfolg’ sind von Haus aus negativ konnotierte Ausdrücke, die oft den Eindruck eines Scherbenhaufens oder zumindest von vergeudeter Zeit und verschwendetem Geld hervorrufen. Die wenigsten Vorhaben in den Digital Humanities (DH) enden vollständig ohne Ergebnisse.Dennoch sind Projekte, die eines von mehreren Deliverables nicht oder nicht vollständig erbringen konnten, in denen Tools nicht die gedachten Ergebnisse lieferten oder eine versprochene Entwicklung nicht vollständig erbracht werden konnte, durchaus häufiger anzutreffen. Während sich die meisten Akteur*innen der Digital Humanities jetzt – etwas verstohlen – an solche Fälle erinnern werden, spiegelt sich dieser Umstand kaum in (projektbezogenen) Konferenzbeiträgen, Veröffentlichungen oder Projektberichten wider.
                

                

                    
So vielfältig wie die Ursachen für das Scheitern sind auch die Ursachen für die verhaltene – in den meisten Fällen eher abwesende – Kommunikation. Dass über das Scheitern und seine vielfältigen Ursachen aber so wenig gesprochen wird, ist ein nicht zu unterschätzendes Problem. Negative Ergebnisse – um gleich die Erkenntnis, dass ein bestimmter Weg nicht zum gewünschten Ziel geführt hat, von der vollständigen Ergebnislosigkeit abzugrenzen – sind für die Forschung relevant: sie führen zu alternativen Ansätzen und schließen unfruchtbare Wege aus der Suche aus.
                

                

                    
Erst die offene Kommunikation über ‘Sackgassen’ verhindert, daß Andere die gleichen Pfade erneut beschreiten. Überspitzt ausgedrückt: Das Verstecken von sogenannten Misserfolgen beraubt die Forschung eines Ergebnisses und führt unweigerlich dazu, dass Fehler wiederholt werden – und damit weitere Projekte scheitern können. Nicht das einzelne negative Ergebnis kostet Geld, sondern die fehlende Fehlerkultur. Deshalb bedarf es unter anderem einer offenen Kommunikation über positive wie negative Projektergebnisse und nicht zuletzt auch eines guten Projektmanagements, das mögliche Fehlermodi rechtzeitig erkennt und diesen gegensteuert sowie aktuelle Diskussionen und relevante Veröffentlichungen auswertet und in das Projekt einbringt. Negative Ergebnisse müssen mehr Akzeptanz erfahren, um den Nimbus des (vielleicht gar persönlichen) Scheiterns (mit entsprechenden, befürchteten wie tatsächlichen, Konsequenzen für die eigene Zukunft) zu verlieren.
                

            

            

                
Motivation und Leitfragen bzw. Themencluster

                
Verständlicherweise ist in den Digital Humanities das Phänomen des Scheiterns nicht unbekannt. Problematisiert wurde das Scheitern von Projekten im DHd-Kontext zuletzt in einigen projektspezifischen sowie projektübergreifenden Kontexten (z. B. Frank 2022, Gengnagel 2021-2022, RaDiHum20 2022, Schuhmacher 2022, Zarei et al. 2022). Auch darüber hinaus gibt es neben dem wegweisenden Artikel von Dombrowski (2014) weitere interessante Beispiele des Umgangs mit diesem Diskurs, wie z. B. Drucker 2014, Graham 2019. Diese Beispiele nehmen jedoch nicht weg, dass der Diskurs über das Scheitern noch spärlich und angstbehaftet ist (siehe FuReSH 1-2 2022) und relativ wenig Eingang in die Publikationskultur findet, wobei Ausnahmen hier die Regel zu bestätigen scheinen, wie die Rezension von Dombrowski (2019) von Failing Gloriously and Other Essays (Graham 2019) zu denken gibt: “Graham acknowledges upfront the privilege that underpins his ability to talk so openly about failure. He’s a white man with tenure, which counts for a lot [...].”

                
Es stellt sich die Frage, welche unterschiedlichen Fehlerkulturen aus den Ursprungsdomänen in den DH zusammenkommen und ob z. B. stärker technisch geprägte Felder (und damit auch Publikationsoutlets wie z. B. die 
                    
Proceedings der Computational Humanities Tagung
) von vornherein und aus wissenschaftlichen Notwendigkeiten eine andere Fehlerkultur haben. Während in diesen Bereichen das Scheitern bzw. der Umstand, unterschiedliche Lösungswege ausprobiert zu haben, von technischer Kompetenz zeugen und ihnen somit auch für Early Career Researcher ein Potential innezuwohnen scheint, die eigene Technik- und Methodenkompetenz unter Beweis zu stellen, scheint es in den sogenannten Buchwissenschaften selbst für etablierte Wissenschaftler*innen weniger Raum zu geben, um über Scheitern zu sprechen. Dabei gibt es mittlerweile sogar Open-Access-Journals, wie JOTE (Journal of Trial and Error)
, die auch für Digital-Humanities-Forschung die Möglichkeit bieten, für alle Beteiligten nutzbringend ihre 
                    
Errores
 zu publizieren.
                

                
Vor diesem Hintergrund will das Panel Bezüge zwischen Kulturen des Scheiterns und Open-Bewegungen in den Digital Humanities Praktiken sondieren und relevante Positionen kartieren. Das geplante Panel will nicht nur einfach die eben skizzierten Probleme beschreiben, sondern den Austausch über diese Themen speziell im Kontext der Digital Humanities durch einen strukturierten thematischen Impuls forcieren, bei dem insbesondere folgende Themenbereiche bzw. Leitfragen adressiert werden: 

                

                    
Was bedeutet es eigentlich, dass ein Projekt gescheitert ist? 

                    
Warum ist eine gute Fehlerkultur für Open Humanities wichtig bzw. ist die Offenlegung des Forschungsprozesses eine Gefahr für die Karriere?

                    
Ist in den technischeren Feldern der DH das Beschreiben von verschiedenen Lösungswegen inklusive Irrwegen bereits stärker etabliert als in den sogenannten Buchwissenschaften? 

                    
Welche Rolle spielen wissenschaftssoziologische Faktoren, wie die Forschungshierarchie und Leistungsdruck, für die Möglichkeiten, um über Scheitern zu sprechen?

                    
Welche Verantwortung haben Forschungsförderer beim Etablieren einer besseren Fehlerkultur? Was können wir aus anderen Disziplinen lernen?

                    
Inwieweit ist Scheitern ein inhärenter Bestandteil des wissenschaftlichen Prozesses und inwieweit brauchen wir deshalb eine andere Fehlerkultur?

                    
Welche Chancen und Herausforderungen bestehen bezüglich der Verbesserung bestehender Kulturen und Strukturen?

                

                
Primäres Ziel des Panels ist neben der Herausarbeitung momentaner Schmerzpunkte die Sichtbarmachung der Vielschichtigkeit des Diskurses und die Entwicklung von multiperspektivischen Handlungsoptionen.

            

            

                
Ablauf und Organisation des Panels

                
Um eine lebendige Diskussion anzuregen, verzichtet das Panel auf die sonst üblichen Kurzreferate der Teilnehmer*innen. Nach einer kurzen Einführung in die Thematik des Panels durch die Moderator*innen werden die Panelist*innen in pointierten Statements ihren Bezug zum Panelthema vorstellen (15 Minuten). Dabei stellt jede Teilnehmer*in eine spezifische Perspektive vor. Dann soll ein multiperspektivischer Austausch über bestehende Kulturen des Scheiterns sowie die Frage nach Veränderungsbedarfen und möglichen Lösungsansätzen anhand der oben skizzierten und im Vorfeld des Panels (u. a. anhand der im Vorfeld eingehenden Stellungnahmen bzw. Problematisierungen, siehe unten) ggf. weiter zu konkretisierenden Leitfragen, die den Teilnehmer*innen des Panels im Vorfeld zur Verfügung gestellt werden, im Mittelpunkt stehen (45 Minuten). An diesem Punkt wird bereits frühzeitig der Diskurs in Richtung Publikum geöffnet und dieses interaktiv für direkte Erwiderungen in die Diskussion einbezogen. Die letzten 30 Minuten sind explizit für die Diskussion zwischen dem Panel und dem Publikum vorgesehen.

                
Während des gesamten Panels wird die Moderation auf eine sachliche und gewaltfreie Kommunikation achten. Das Panel wird unterstützt durch die Möglichkeit der Beteiligung via Twitter oder anderer Social-Media-Kanäle, vor, während und nach der Diskussion. Insbesondere soll auch im Vorfeld bzw. während des Panels eine niedrigschwellige Beteiligung ermöglicht werden, z. B. durch das Einbringen anonymer kürzerer Stellungnahmen und Problematisierungen (ggf. mittels digitaler Feedbacklösungen), wobei sich die Organisator*innen und Moderator*innen die Freiheit der Auswahl erlauben. Auf diese Weise kann schon im Vorfeld des Panels die Diskussion gebündelt und um weitere Perspektiven erweitert werden. Es wird angestrebt, die Ergebnisse des Panels der Fachöffentlichkeit zur Verfügung zu stellen (als Blogbeitrag, White Paper, Thesenpapier, Fachartikel o. ä.).

            

            

                
Teilnehmende (Vorstellung der Panelist*innen und Moderation)

                
Teilnehmende:

                

                    
Tessa Gengnagel
 (internationale Perspektive) ist als Postdoc am CCeH (Universität zu Köln) in der Geschäftsführung tätig. Für das Panel wird sie zu Failure als Topos einer konstituierenden Kraft in DH-Narrativen beitragen (siehe Gengnagel 2021-2022) und dazu als Folie eine wissenschaftssoziologische Perspektive kontrastieren, die intersektionale Kriterien berücksichtigt.
                

                

                    
Sarah Lang 
(Perspektive AG Empowerment) ist Computational Humanist am Grazer Zentrum für Informationsmodellierung. Sie lotet im Rahmen des Panels wissenschaftstheoretische Aspekte des Umgangs mit Fehlern vor dem Hintergrund breiterer Fragen, wie z. B. unsichtbaren Machtstrukturen, Prekariat und Diskriminierung, aus.
                

                

                    
Stefan Karcher
 (Perspektive Fördergeber) ist Referent bei der Deutschen Forschungsgemeinschaft (DFG) und dort unter anderem für den Bereich Digital Humanities zuständig. Im Panel wird er die Frage von für die Forschung zuträglichen Kulturen des Scheiterns aus der Perspektive eines Drittmittelgebers diskutieren.
                

                

                    
Torsten Schrade
 (Perspektive Research Software Engineering und wissenschaftliche Infrastruktur) leitet die Digitale Akademie der Akademie der Wissenschaften und der Literatur | Mainz und ist Spokesperson von NFDI4Culture. Zu den Leitsprüchen seiner Forschungs- und Entwicklungsaktivitäten zählen "Weniger schlecht programmieren" (Passig/Jander) und "Fail better" (Beckett).
                

                
Moderation: 

                

                    
Ulrike Wuttke
 (Fachhochschule Potsdam, Fachbereich Informationswissenschaften) lehrt und forscht am Fachbereich Informationswissenschaften der Fachhochschule Potsdam. Zu ihren Schwerpunkten gehören Open Science Advocacy und Training.
                

                

                    
Dario Kampkaspar
 (Technische Universität Darmstadt, Universitäts- und Landesbibliothek) leitet das Zentrum für digitale Editionen und ist seit über 10 Jahren in digitalen Projekten sowohl öffentlicher wie privater Fördergeber in Deutschland und Österreich inhaltlich wie organisierend tätig.
                

            

        

            

                

                    
Auf unserem Poster zeigen wir, wie wir von einem digitalen Nachlass mit 3,3 Millionen Dateien zu einem Data-Set mit etwa 30.000 Dateien gelangt sind, mit dem wir sinnvoll zum Born-digital-Nachlass des Literaturwissenschaftlers und Medientheoretikers Friedrich Kittler (1943–2011), der im Deutschen Literaturarchiv Marbach (DLA) aufbewahrt wird, forschen können und dürfen. Mit unserem Fallbeispiel wollen wir zeigen, wie umfangreich ein Born-digital-Nachlass im DLA sein kann, wie man mit diesem technisch, konservatorisch und rechtlich umgeht und welches Potential darin liegt. Anhand unseres kuratierten Arbeitskorpus können wir sodann beispielsweise an der statistischen Auswertung und Visualisierung der (technischen

                    
) Metadaten arbeiten und damit zur Erschließung des Nachlasses beitragen.

                

            

            

                
Digitale Vor- und Nachlässe im DLA

                

                    
Mit Kittlers Nachlass kam 2011 der bisher umfangreichste und komplexeste digitale Nachlass ins DLA. Immer noch gehört dieser mit 762 Datenträgern (648 Disketten, 100 optische Medien, 1 USB Speicher, 12 Festplatten) und einer Größe von 1,6 TB zum quantitativ größten digitalen Bestand im DLA. Berücksichtigt man Kittler nicht, umfassen die gesamten digitalen Vor- und Nachlässe von etwa 70 Bestandsbildner*innen im DLA (also im Archiv, nicht in der Bibliothek) derzeit mit etwa 1600 Datenträgern insgesamt knapp 5,2 TB.

                

                

                    
In den Jahren 2012 bis 2013 galt es, diesen neuen Bestand analog zu sichten, zu sortieren und zu verzeichnen. Dieser Workflow wurde bereits dokumentiert und publiziert (Enge/Kramski 2017). Die PCs, Laptops und physischen Datenträger zu sortieren, reicht (anders als beispielsweise das Sortieren von Papiermanuskripten) allerdings nicht aus, um den Nachlass zu katalogisieren. Im DLA haben wir für digitale Vor- und Nachlässe eigens sogenannte „Digital Curator[s]“, die man auch Data Librarian oder „Datenarchäolog[innen]“ nennen könnte (Bülow/Kramski 2011: 159), die seit Mitte der 2000er Jahre immer wichtiger werden (Jaillant 2022: 8). Diese bereiten ‚rohe‘ digitale Nachlässe für verschiedene Anliegen auf. Bei digitalen Archivobjekten „muss die Lese- und Interpretationsfähigkeit [nämlich] zuerst aufwendig hergestellt werden.“ (Bülow/Kramski 2011: 159) Zudem ist eine manuelle Sichtung der Dateien bei einem kleineren digitalen Nachlass zwar vorstellbar, für Kittlers Nachlass allerdings wegen des Umfangs und der Art der Dateien eher unpraktisch. 

                

            

            

                
Kittlers digitaler Nachlass: Von 3,3 Millionen zu ca. 30.000 Dateien

                

                    
Bei erst kürzlich verstorbenen Autor*innen, wie Kittler, muss immer mitbedacht werden, dass selten der ganze Nachlass benutzbar ist und als Ganzes gar nicht oder zu einem späteren Zeitpunkt erforscht werden kann – bei Born-digital-Nachlässen spricht man so auch von „dark archives“, denn selten kann alles (online) leicht zugänglich gemacht werden, obwohl es bereits digital vorliegt. „Nobody would reasonably claim that all born-digital data should be unlocked and openly accessible. Yet, it is important to recognize that ‚dark‘ archives contain vast amounts of data essential to scholars […].“ (Jaillant 2022: 7). 

                

                

                    
Der aktuelle Kittler-Bestand umfasst ca. 3,3 Millionen Dateien. Der erste Schritt besteht in dem ‚Identifizieren und Aussortieren‘ von hunderttausenden nicht-unikaler

                    
 Dateien, also Dateien, die nicht von Kittler selbst erstellt wurden. Dies erfolgte automatisiert vor allem über einen Abgleich mit der National Software Reference Library (NSRL) des NIST

                    
. Für solche Vorgänge sind auch KI-gestützte Methoden denkbar; allerdings werden solche derzeit eher für „low-level tasks“ herangezogen, etwa bei der Identifizierung von sensiblen persönlichen Informationen (Jaillant 2022: 14). Damit gelangten wir zu etwa 2,25 Millionen Dateien. 

                

                
In einem nächsten Schritt wurde die Menge auf die Dateien begrenzt, die von Seiten der Nachlassverwaltung begutachtet und mit einem Status versehen wurden (freigegeben, vorläufig gesperrt, gesperrt). Unser Arbeitskorpus ist also als Momentaufnahme zu sehen, da er nur auf den bereits bewerteten Dateien basiert. Ohne die bislang unbekannten bzw. noch nicht bewerteten Dateien kommen wir auf 219.989 Dateien. 

                
Aus dieser Menge wurden zuletzt nun die für die Forschung vollständig freigegebenen Dateien (Metadaten und Inhalt) herausgefiltert, die zudem von Kittler erstellt wurden. Wir landeten bei etwa 30.000 Dateien, also etwa 0,88% von den 3,3 Millionen Dateien.

                

                    
Bei den meisten Dateien in unserem Arbeitskorpus handelt es sich um Textdateien in unterschiedlichsten Formaten. Kittler hat „nicht nur Texte, Bilder und Videos hinterlassen, sondern auch Relikte seiner programmierenden Tätigkeiten.“ (Enge/Kramski 2017). Er interessierte sich neben der Literatur, Musik und Philosophie vor allem für Technologien, Medien und das Programmieren (Winthrop-Young, 2017: 210). „[W]ie ungezählte Teenager in dieser Zeit“ habe er vor allem mit Codes gespielt und autodidaktisch gelernt (Pias 2014: 39–44). Das spiegelt sich in gewisser Weise auch in seinem Nachlass wider. So befinden sich im Arbeitskorpus beispielsweise auch mit Kommentaren versehene Dateien der Programmiersprache C/C++ (insbesondere *.c und *.h-Dateien).

                

            

        
