

 Dabei hat der Ereignistyp, der Zustandsveränderungen beschreibt, den höchsten Wert, gefolgt von weiteren weniger ereignishaften Kategorien. Vgl. zu den Kategorien und zur Bestimmung der Ereignistypen die Annotationsrichtlinien Vauth & Gius (2021) und zu den Narrativitätskurven Vauth et al. (2021), für den Classifier Hatzel (2022).

                        Zur Diskussion der beiden narratologischen Ereigniskonzepte und ihrem Verhältnis vgl. Hühn (2014).
                    
Trotz des mit diesen beiden Ansätzen erreichten hohen Handlungsbezugs haben 13% der Sätze keine entsprechend annotierte Spanne im Originaltext (18% bei den professionellen und 9% bei den semiprofessionellen Zusammenfassungen).

                        Die Daten wurden in Vauth & Gius (2022) publiziert.
                    
 Der Gesamtscore ist also jener, der bei der zufälligen Auswahl der Events aus dem Text im Durchschnitt zustande kommt. ‘1‘ bedeutet also, dass der Score zufällig ausgewählten Events entspricht, ‘2‘ heißt, dass der Score doppelt so hoch ist wie bei zufälligen Events.
 Für die Berechnung wurde SciPy verwendet: 
                        https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.peak_prominences.html#scipy.signal.peak_prominences 
                    
 Für die Berechnung wurde SciPy verwendet: 
                    https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.peak_prominences.html#scipy.signal.peak_prominences 
                
            
                
                    Bibliographie
                    Arnold, Frederik, und Benjamin Fiechter. 2022. „Lesen, was wirklich wichtig ist. Die Identifikation von Schlüsselstellen durch ein neues Instrument zur Zitatanalyse”. In DHd2022. Potsdam, Deutschland. https://doi.org/10.5281/zenodo.6327917
                    Arnold, Heinz Ludwig, Hrsg. 2020. Kindlers Literatur Lexikon (KLL). Stuttgart: J.B. Metzler. https://doi.org/10.1007/978-3-476-05728-0.
                    Baroni, Raphaël. 2012. „Tellability“. In the living handbook of narratology, herausgegeben von Peter Hühn, John Pier, Wolf Schmid, und Jörg Schönert. Hamburg: Hamburg University Press. http://hup.sub.uni-hamburg.de/lhn/index.php?title=Tellability&oldid=1577.
                    Gius, Evelyn, und Michael Vauth. 2022. „Inter Annotator Agreement und Intersubjektivität“. In DHd2022, 147-151. Potsdam, Deutschland.
                    Groeben, Norbert. 1977. Rezeptionsforschung als empirische Literaturwissenschaft: Paradigma- durch Methodendiskussion an Untersuchungsbeispielen. Empirische Literaturwissenschaft. Bd. 1. Königstein/Ts.: Athenäum.
                    Hatzel, Hans Ole. 2022. Event Narrativity Classifier. Zenodo. https://doi.org/10.5281/zenodo.6821142.
                    Hühn, Peter. 2014. „Event and Eventfulness“. In the living handbook of narratology, herausgegeben von Peter Hühn, John Pier, Wolf Schmid, und Jörg Schönert. Hamburg: Hamburg University Press. https://www.lhn.uni-hamburg.de/node/39.html.
                    Iser, Wolfgang. 1976. Der Akt des Lesens. Theorie ästhetischer Wirkung. München: Fink.
                    Lin, Chin-Yew. 2004. „ROUGE: A Package for Automatic Evaluation of Summaries“. In Text Summarization Branches Out, 74–81. Barcelona, ​​Spanien: Association for Computational Linguistics. https://aclanthology.org/W04-1013.
                    Miall, David, und Don Kuiken. 2001. “Shifting perspectives: Readers’ feelings and literary response”. In New Perspectives on Narrative Perspective, herausgegeben von Willi Van Peer und Seymour Chatman, 289-301. Albany: SUNY Press.
                    Nenkova, Ani, und Rebecca Passonneau. 2004. „Evaluating Content Selection in Summarization: The Pyramid Method“. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, 145–52. Boston, Massachusetts, USA: Association for Computational Linguistics. https://aclanthology.org/N04-1019.
                    Papineni, Kishore, Salim Roukos, Todd Ward, und Wei-Jing Zhu. 2002. „BLEU: a Method for Automatic Evaluation of Machine Translation“. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 311–18. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics. https://doi.org/10.3115/1073083.1073135.
                    Vauth, Michael, und Gius, Evelyn. 2021. „Richtlinien für die Annotation narratologischer Ereigniskonzepte“. Zenodo. https://doi.org/10.5281/zenodo.5078174.
                    Vauth, Michael, und Evelyn Gius. 2022. forTEXT/EvENT_Dataset: v.1.1 (Version v.1.1). Zenodo. https://doi.org/10.5281/ZENODO.6406568.
                    Vauth, Michael, Hans Ole Hatzel, Evelyn Gius, und Chris Biemann. 2021. „Automated Event Annotation in Literary Texts“. In CHR 2021: Computational Humanities Research Conference, 333–45. Amsterdam, Niederlande. http://ceur-ws.org/Vol-2989/short_paper18.pdf. 
                    Zhang, Tianyi, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, und Yoav Artzi. „BERTScore: Evaluating Text Generation with BERT.” In International Conference on Learning Representations. Online, 2020. https://openreview.net/forum?id=SkeHuCVFDr.
                
            
        