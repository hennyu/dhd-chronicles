

Zur Filiation von Musik um 1500
Die Filiation als Methode zur Rekonstruktion der Überlieferungslinien eines Textes wird in der Renaissancemusikforschung nicht nur im Rahmen der Edition zur Bewertung von Lesarten verwendet, sondern auch in repertoiregeschichtlichen Studien. So diente sie als methodischer Grundpfeiler sowohl in Atlas‘ Studie (Atlas 1975) zum Cappella Giulia Chansonnier als auch in Urchueguías Studie (Urchueguía 2003) zu Messvertonungen des Siglo de oro. In dem sich aus den spezifischen Herausforderungen dieses Repertoires resultierenden methodischen Diskurs zu den Voraussetzungen und dem Potential der Filiation in der Renaissancemusikforschung, tritt insbesondere die starke hermeneutische Prägung zutage. Bezogen auf ein Material, bei dem eine bloße Recensio zumeist nur zu Variantenträgern führt, wird der Examinatio ein besonderer Stellenwert zugesprochen. (Just 1983: 130) In dieser Konsequenz ist die Filiation von Musikquellen insbesondere vom Begriff der Signifikanz geprägt – auf den Punkt gebracht von Margarent Bent:
„It has often been said that manuscripts – and variants – should be weighed and not counted. Statistical counts of readings tell us nothing unless it is clear that the versions are stemmatically independent. However, although the strongest evidence for relating sources comes from variants that are not only shared but ‚significant‘[…]” (Bent 1981: 307)
Dass eben diese starke Fokussierung auf die Signifikanz von Lesarten ein tieferes inhaltliches Verständnis des zu untersuchenden Textes erfordert, stellt damit ein wesentliches Charakteristikum dar. So beruht bereits der Variantenbegriff in der Musikphilologie auf der Unterscheidung zwischen substantiellen – d.h. Tonhöhe und -dauer betreffenden – und akzidentiellen Varianten. (Feder 1987: 60f.) Werden über methodentheoretische Beiträge hinaus noch kommentierte Stemmata der New Josquin Edition konsultiert, lassen sich wiederkehrende Argumentationsmuster beobachten:
Die inhaltliche Gewichtung von Lesarten folgt zumeist einer klaren Hierarchie. Die Klassifikation einer Lesart als Fehler, Variante – im Sinne einer Abweichung von Tonhöhe und/oder Rhythmus – oder als minor variant hat einen erheblichen Einfluss auf die ihr zugesprochene Beweiskraft. So zeigt sich, dass üblicherweise ein komplexes Geflecht aus kleineren und größeren Fehlern wie auch Varianten gebildet wird, das argumentativ gegeneinander abgewogen wird. Wird hierbei einer Lesart Leitcharakter zugesprochen, übertrifft deren Beweiskraft immer die der, stellenweise zahlreichen, anderen Befunde. Steht infolgedessen die Direktionalität der Überlieferung zur Diskussion, basieren die Argumentationsmuster zumeist auf Konzepten wie der Lectio difficilior, der musikalischen Plausibilität von Überlieferungsrichtungen, oder Vorannahmen über Eigenschaften des Archetypus, zumeist begleitet von ästhetischen Werkansprüchen – so werden beispielsweise Hyparchetypen in ein Stemma eingefügt um falsche Lesarten des Archetypus zu vermeiden. In letzter Konsequenz kann beobachtet werden, dass zwei Stemmata derselben Überlieferung erheblich abweichen können, wenn unterschiedliche Vorannahmen zugrunde gelegt werden.


Mensuralnotation als mehrdeutiges Zeichensystem
Gerade in Bezug auf die Musiküberlieferung vor 1600 erscheinen einige dieser Befunde aus der Perspektive einer Digitalen Musikwissenschaft als erstaunlich. So sind nicht nur Implikationen bezüglich der Stabilität des Werkbegriffs zu hinterfragen. In dem Bewusstsein über die prinzipielle Zeichenhaftigkeit von Musiknotation und eine dieser inhärenten semantischen Mehrdimensionalität, rückt auch das semiologische Gefüge der Überlieferungsform an sich, die Mensuralnotation, in den Fokus.
So weist Selfridge-Field bereits in ihrem Sammelband zu Musikkodierungsformaten auf die verschiedenen Kontexte hin, die eine musikalische Information beschreiben kann. (Selfridge-Field 1997: 7) Im Kodierungsformat der Music Encoding Initiative (MEI) kommt diesen Kontexten, übernommen aus der Standard Music Description Language, in Form von semantischen Domänen eine zentrale Rolle zu. Damit sind Informationen über Höhe und Dauer eines Tones vornehmlich der logischen Domäne zuzuordnen, während die visuelle Ebene den graphischen Befund, die gesturale Domäne die praktischen Ausführung bzw. klanglichen Ebene und die analytische Domäne Annotationen und Analysen umfasst. Betrachtet man nun Mensuralnotation auf dieser Basis, tritt insbesondere ihre strukturelle Ambiguität hervor. So ist gerade das Verhältnis zwischen einem Zeichen und dessen Bedeutung in der logischen Domäne nicht klar einander überführbar. Vielmehr können sich kontextbedingt Bedeutungen ändern, wenn das mensurale Gesamtgefüge die Dauer eines einzelnen Tons beeinflussen kann, oder im Falle der Alternation von Tonstufen der notierte Befund Leser*innen voraussetzt, die auch implizite Alterationen als solche zu erkennen vermögen. Darüber hinaus spiegeln sich auch Entwicklungsprozesse der Musiktheorie in der Notation wider, insbesondere im Falle von Mensur- und Proportionszeichen. Hier ergeben im äußersten Fall nicht nur verschiedene Symbole dasselbe Resultat auf der logisch-konzeptionellen Ebene. Gleichzeitig ist es möglich, dass ein und dasselbe Zeichen unterschiedliche Bedeutungen haben kann und man lediglich anhand musikalischer Gesichtspunkte abzuwägen vermag, welche der Möglichkeiten zu den geringsten strukturellen Schwierigkeiten führt. Vor diesem Hintergrund erscheint es damit geradezu als konsequent, dass Dumitrescu und van Berchum mit Blick auf die Edition des Occo Codex im CMME-Projekt die Existenz von nicht-substantiellen Varianten grundsätzlich infrage stellen:
„The extent to which these can be considered ‘non-substantive’ is questionable: the positioning of line breaks, for instance, will have an effect on an editor’s interpretation of the duration of manuscript accidentals, or stem direction may actually have an effect on rhythm in certain notational styles (as in some brands of 14th-century notation).” (Dumitrescu / van Berchum 2009: 143)


Filiation als Distinktion von Differenz
Im hier umrissenen Ansatz soll die Perspektive eröffnet werden, Repertoirestudien zur Renaissancemusik durch den Einsatz computergestützter Analyseverfahren auf eine breitere Basis, jenseits von Fallstudien, zu stellen. Entsprechend der seit den 1990er Jahren erprobten Übernahme von Verfahren der bioinformatischen Phylogenie, wird auch hier ein solcher Ansatz verfolgt. Doch während beim Alignment von Musik bisher insbesondere Ansätze verfolgt wurden, die explizit repräsentationsbedingte Abweichungen zu minimieren suchten, oder auf Retrieval-Szenarien ausgelegt sind, ergibt sich bei der computergestützten Filiation eine andere Konstellation. Vielmehr lässt sich diese als Methode beschreiben, mit der eine Gruppe ähnlicher Texte entsprechend ihrer Differenz in Relation gebracht werden soll. Die Anforderung an einen Prozess lässt sich somit in der spezifischen Distinktionsfähigkeit verorten. Damit stehen die Kodierung, die Datenaufbereitung sowie die Analyse sowohl in Hinblick auf die Leitthemen der traditionellen Filiation als auch in der Auseinandersetzung mit dem Material vor grundsätzlichen Fragen: Welche Rolle spielt das Verhältnis von visuellem Quellentext und dessen logisch-konzeptioneller Ebene? Wie kann das dem Leseprozess inhärenten Interpretationsniveau methodisch so behandelt werden, dass ein schlüssiger Untersuchungsprozess möglich wird? In Bezug auf die Filiation als Verfahren, das sich per se Textzeugen widmet, stellt sich somit die Frage, wie die Integrität dieser auch bei der Analyse einer maschinenlesbaren Repräsentation gesichert werden kann.
Basierend auf einem Konzept von Kodierung als Datenerhebung, wurde an einem begrenzten Korpus von Quellen – Überlieferungen von Josquins 
                    Missa D’ung aulte amer und der damit verwandten Motette 
                    Tu solus qui facis mirabilia – ein Ansatz gewählt, der begonnen mit der Kodierung das Verhältnis von visuellem und logisch-konzeptionellem Befund in den Blick nimmt. Im Rahmen der Verfahrensentwicklung wurden diese beiden semantischen Domänen als Parametersets formalisiert, die als Grundlage für ein Alignment des gewählten Materials dienen. In diesem Zuge wurde, da sich der ausschließliche Vergleich der Ergebnisse mit traditionellen Stemmata aufgrund konkurrierender Ansichten über die rekonstruierte Überlieferungsgeschichte als nur wenig überzeugend erwies, eine datenbasierte Evaluationsmethode entwickelt.
                


Datenbasierte Evaluation von Substitutionsmodellen
Die wesentliche Herausforderung bestand hierbei in den noch äußerst unzureichenden Erfahrungen im Alignment von Renaissancemusik. Es wurden deshalb differenzbasierte Substitutionsmodelle und ein globales Sequenzalignment verwendet, obwohl statistische Theorien auf ähnlichkeitsbasierten Modellen und lokalen Alignments aufbauen. Ohne zumindest basales Vorwissen über Substitutionsprozesse in der Renaissancemusik, sind die qualitativen Anforderungen an ähnlichkeitsbasierte Substitutionsmatrizen allerdings nicht erfüllbar. Indem gefordert ist, dass der Erwartungswert negativ und gleichzeitig mindestens ein positiver Wert im Scoringmodell möglich sein soll (Altschul 1991: 556), verlangt dies letztendlich die Festlegung einer neutralen Ähnlichkeit: Die Stelle auf der Skala, an der die Ähnlichkeit nicht groß genug ist, um einen positiven Wert zuzulassen, aber die gleichzeitig nicht different genug ist, um mit einem negativen Wert quantifiziert zu werden. Doch auch wenn eine derartige Zuweisung nicht vorgenommen wird, kann noch immer ein globales Sequenzalignment auf der Basis eines distanzbasierten Modells verwendet werden. Da in diesem Fall aber keine Modelle angewendet werden können, die auf Maximal Segment Scores beruhen, wurde ein rein datenbasierter Ansatz zur Evaluation gewählt.
                
Der hier vorgestellte Ansatz baut auf dem Konzept des Vergleichs mit einem Zufallsmodell auf. Im Zentrum steht das Verfahren der Surrogatdatenanalyse, einem Verfahren aus der Zeitreihenanalyse (Theiler u.a. 1992: 77-78). Dabei werden basierend auf realen Reihen künstliche Daten erzeugt, bei denen gezielt ausgewählte Eigenschaften randomisiert und im Nachgang mit den Originaldaten in einem Hypothesentest verglichen werden können. Im konkreten Fall wird durch Shuffling realer Sequenzen ein Szenario erzeugt, in dem sich die vorliegenden Daten vollständig durch einen unabhängig und identisch verteilten Zufallsprozess beschreiben lassen. Hierbei wird im Wesentlichen der Grundannahme gefolgt, dass sich die Ähnlichkeit zweier Sequenzen in deren Binnenstrukturen manifestiert, also in der konkreten Reihenfolge, in der die Elemente innerhalb von Sequenzen angeordnet sind. Indem diese Binnenstrukturen in den geshuffelten Sequenzen zerstört werden, bleibt die Auftrittswahrscheinlichkeit der Elemente identisch, allerdings werden sämtliche Korrelationen mit der Reihenfolge, in der diese Elemente auftreten, zerstört:

  
   Abbildung 1: Zerstörung der Binnenstrukturen durch Shuffling.


Die Abweichung zwischen den Alignments von realen Sequenzen und Alignments von aus diesen erzeugten randomisierten Sequenzen kann hierdurch quantifiziert und als Grundlage für eine Evaluierung unterschiedlicher Substitutionsmodelle genutzt werden. In dem Fall, in dem durch einen Binnenvergleich ähnlicher Sequenzen diese entsprechend ihrer Differenz in Relation zu setzen sind, kann die Distinktionsfähigkeit von verschiedenen Ähnlichkeits- bzw. Differenzniveaus als zentrale Anforderung an ein Substitutionsmodell formuliert werden.
Diese Distinktionsfähigkeit kann im Rahmen eines Versuchsaufbaus überprüft werden, in dem eine Zahl von Vergleichen anhand der erwarteten Ähnlichkeit in Gruppen eingeteilt werden, die die Differenzniveaus widerspiegeln. So bilden beinahe identische Sequenzen eine Gruppe, etwas weniger ähnliche eine weitere etc. Dazu kommen Gruppen von Vergleichen, die als gerade so noch ähnlich bewertet werden, und wiederum Vergleiche mit erwarteter Unähnlichkeit – zusätzlich nimmt eine Gruppe von Vergleichen mit einem völlig anderen Stück die Funktion einer Kontrollgruppe ein. Für all diese Gruppen kann nun der Abstand des Vergleichs zwischen den beiden Originalsequenzen und den Surrogaten ermittelt werden.
Dieser Aufbau ermöglicht es, Mindestkriterien für die Eignung eines Substitutionsmodells zu formulieren. So ist es beispielsweise naheliegend, dass der Abstand zwischen den Originaldaten und den Surrogaten in der Kontrollgruppe möglichst nicht signifikant sein sollte. Ebenso sollte definiert werden, zwischen welchen beiden Gruppen nach Möglichkeit die Signifikanzgrenze liegen sollte, um die so gewünschte Sensitivität festzulegen. Auch kann basierend auf der vorgenommenen Gruppierung eine Varianzanalyse durchgeführt werden, um den Zusammenhang zwischen der vorgenommenen Gruppierung und dem Abstand zwischen Originaldaten und Surrogaten zu überprüfen. Basierend auf den zuvor formulierten Mindestkriterien und der Entwicklung des Abstandes von Originalen und Surrogaten, können somit Modelle verglichen und eine informierte Entscheidung über die Eignung in einem gewählten Einsatzszenario getroffen werden.


Fazit
Letztendlich kann konstatiert werden, dass die Rekonstruktion von Überlieferungsprozessen immer auf Vorannahmen beruht, egal welches Verfahren angewendet wird. Im Rahmen der traditionellen Stemmatologie führen Annahmen über die Bedingungen des Überlieferungsprozesses oder die Gestalt des Archetypus zu mitunter deutlichen Abweichungen in Stemmata, die ohne eine begleitende Erläuterung nicht zu eruieren sind. Automatisierte Verfahren basierend auf dem Alignment von Sequenzen können hierzu eine fruchtbare Ergänzung darstellen. Bei diesen stellt die Wahl des Substitutionsmodells einen wesentlichen Einflussfaktor dar. Indem allerdings gezielt Analyseparameter in Substitutionsregeln überführt werden, kann deren Einfluss überprüfbar gemacht werden. Wenn so konkurrierende Modelle an identischen Daten verglichen werden, wird deren Einfluss auf die Rekonstruktion von Beziehungen zwischen Textzeugen überprüfbar. (vgl. Plaksin 2019). Die Methode, das Verhalten eines Substitutionsmodells anhand des Vergleichs von echten und künstlich erzeugten Daten sichtbar zu machen, dient hierbei als Evaluationsverfahren und stellt damit einen wesentlichen Bestandteil in der Modellentwicklung dar.

