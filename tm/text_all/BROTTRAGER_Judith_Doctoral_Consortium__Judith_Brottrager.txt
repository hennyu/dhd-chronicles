
            
                Von Beginn an war die Inklusion einer größeren Textmenge und damit auch nicht-kanonisierter Werke eines der Hauptargumente für digitale Ansätze in der Literaturwissenschaft: Durch Distant Reading, könne, so Moretti, eine alternative Literaturgeschichte erfasst werden, die auch das sogenannte „Great Unread“ mit einschließt (Moretti 2013: 48f). Forschungsprojekte der letzten Jahre haben jedoch gezeigt, dass der Mehrwert der digitalen Literaturwissenschaft nicht nur in der rein quantitativen Erweiterung der Untersuchungsgegenstände, sondern auch in der tiefgreifenden Kontextualisierung der untersuchten Texte in einer, wie es Bode nennt, „data-rich literary history“ (Bode 2018: 37–57) liegt. Diese kontextreichen Ansätze, wie beispielsweise Analysen zum Verhältnis von Prestige und Popularität (Underwood/Sellers 2016; Algee-Hewitt et al. 2016; Porter 2018; Underwood 2019: 68–110), zeigen, wie quantitative Methoden trotz nötiger Formalisierungen und Abstraktionen die Komplexität des literarischen Systems (cf. Bode 2017: 91) beschreiben können.
            
            Mein Dissertationsprojekt folgt dieser Tradition, indem kanonisierte und nicht-kanonisierte literarische Werke auf unterschiedlichen Ebenen mit Rückgriffen auf literaturhistorische Daten miteinander in Beziehung gesetzt werden. Die für die Untersuchungen erstellten Korpora und Datensätze umfassen etwa 1.200 englisch- und deutschsprachige Texte von 1688 bis 1914, wodurch diachrone und synchrone Vergleiche von Kanonisierungsprozessen und -mustern über Sprachgrenzen hinweg ermöglicht werden. Durch diese Datenvielfalt sollen einerseits etablierte literaturhistorische Narrative untersucht und quantitativ überprüft werden und andererseits literaturwissenschaftliche Kategorien wie Kanonisierung und Wertung so operationalisiert werden, dass sie mit computationellen Methoden zur Bestimmung von Textähnlichkeiten gewinnbringend kombiniert und diese Ähnlichkeit schließlich als Netzwerkmodelle dargestellt werden können. 
            
                Korpusaufbau
                Der Korpusaufbau folgt einem systematisch angepassten Ansatz von Algee-Hewitt und McGurl, der darauf abzielt, von einem vorgefundenen zu einem maßgeschneiderten Korpus zu gelangen, indem Bestenlisten, Bestsellerlisten und von Expert*innen kuratierte Literaturlisten kombiniert werden, um ein repräsentatives Korpus für die englischsprachige Literatur des 20. Jahrhunderts zu erstellen (Algee-Hewitt/McGurl 2015). Durch die Kombination dieser Listen decken Algee-Hewitt und McGurl drei Ebenen der literarischen Produktion ab: den normativ-exklusiven Kanon, populäre Texte und von Expert*innen für Postkoloniale und Feministische Literaturwissenschaft vorgeschlagene Werke. Für die Umsetzung für die Zeitspanne von 1688-1914 wurde dieser Ansatz systematisch adaptiert, indem narrative Literaturgeschichten, Anthologien und (spezialisierte) Sekundärtexte, die diese Ebenen abdecken, identifiziert und als bibliografische Quellen für die Korpuserstellung genutzt wurden. Der Workflow umfasst Webscraping, X-Technologien/Transformationen und Retro-Digitalisierungen.
            
            
                Metadatensätze
                Analog zur Korpuserstellung wurden Daten zur Kontextualisierung der jeweiligen Korpustexte, aber auch der gesamten literarischen Produktion gesammelt. Durch diese Daten können die Korpora mit den von Algee-Hewitt et al. als „the published“ und „the archive“ (Algee-Hewitt et al. 2016: 2) bezeichneten Ebenen der Literaturgeschichte verglichen werden, wodurch wiederum eine Einordnung der Untersuchungsergebnisse und eine Einschätzung der unvermeidlichen Selektionsprozesse bei der Korpuserstellung möglich sind (cf. Bode 2017: 85). Die Daten reichen von Publikationslisten und Leihbibliothekskatalogen bis hin zu Rezensionen und Daten zu Zweitauflagen; der Aufbau der Datensätze orientiert sich am Beispiel der von Garside zur Verfügung gestellten Datenbank 
                    British Fiction 1800-1829 (Garside 2011). In Anlehnung an erfolgreiche Metadatenanalysen (z.B. Jockers 2013: 35–62) sollen diese Daten zur Überprüfung von Hypothesen zum Zusammenhang zwischen dem Aufschwung des Romans als Gattung und gegenderten Kanonisierungsprozessen verwendet werden (cf. Watt 1957; Raven 1987; Raven/Forster 2000; Tuchman/Fortin 2012). 
                
            
            
                Operationalisierungen
                Die gesammelten Daten werden neben diesen Metadatenanalysen auch für die Operationalisierungen der literaturwissenschaftlichen Konzepte der Kanonisierung und Wertung eingesetzt. Aufbauend auf die theoretischen Grundlagen von Heydebrand und Winko werden Kanonisierung und Wertung als Scores implementiert, die ausdrücken, wie hoch die Wahrscheinlichkeit ist, dass ein bestimmter Text sehr kanonisiert ist beziehungsweise zur Entstehungszeit sehr gut rezipiert wurde (Heydebrand/Winko 1996). Ein besonderes Augenmerk liegt hierbei auf der Differenzierung der Konzepte und der Einbindung der Rezeptionsebene über Marker für Publikumsinteresse (wie Einträge in Leihbibliothekskatalogen und Zweitauflagen innerhalb einer Generation) und sprachliche Werturteile, die durch Sentiment Analysen vergleichbar werden. 
            
            
                Ausblick
                Unter Verwendung der generierten Scores soll schließlich untersucht werden, ob Kanonisierung und Wertung mit textintrinsischen Merkmalen in Verbindung gebracht werden können. Stilometrische Berechnungen, Topic Modeling und Word Embeddings sowie wortartenbasierte Ansätze sollen dabei als alleinstehende Analysen der Textähnlichkeiten durchgeführt werden. Als zusätzliche Analysen- und Visualisierungsmethode dienen Netzwerkmodelle, die anhand der Ergebnisse der Textähnlichkeitsberechnungen erstellt werden, zur Exploration von Ähnlichkeitsstrukturen. Besonders auf dieser Ebene soll der Bezug zur literaturwissenschaftlichen Forschung durch die Identifikation von dichten stilistischen Ähnlichkeitsgruppen und durch aus den Modellen abgeleitete Einzeltextanalysen hergestellt werden.
            
        