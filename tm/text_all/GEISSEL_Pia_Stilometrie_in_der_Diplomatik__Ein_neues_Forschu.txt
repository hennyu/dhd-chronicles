
            Seit rund 20 Jahren steigt die online verfügbare Menge an digitalen Daten in unterschiedlichster Form und Qualität. Dies erleichtert den Geisteswissenschaftler|innen den Zugang zu neuen Materialien und senkt die Hemmschwelle sich mit Big Data zu beschäftigen und der „methodological monoculture of close reading“(Karsdorp et al. 2021, 
                preface) zu entkommen. Dadurch müssen sie sich aber auch vermehrt mit den technischen, sprich, computergestützten Anwendungen und Programmiersprachen beschäftigen. Zudem müssen Daten meist noch gesammelt, von überflüssiger Information bereinigt und Ergebnisse visuell aufbereitet werden. Alle diese neuen Schritte sind je nach Datengrundlage aufwändig, da Techniken erst erlernt und korrekt angewendet werden müssen. Als zusätzlicher Faktor werden für Berechnungen großer Datenmengen fachfremde Methoden aus der Mathematik oder Informatik entlehnt. Die Anwendung dieser arithmetischen Methoden werden jedoch oftmals nicht ausreichend hinterfragt. Zusätzlich fehlt zur Überprüfung der Methoden auch die empirische Evidenz vor allem dann, wenn das Untersuchungsmaterial keine faktischen Hinweise liefern kann. 
            
            Auch in der Geschichtswissenschaft wenden sich, angeregt durch niedrigschwelligen Zugang zu online verfügbaren Texten, Geisteswissenschaftler:innen neuen Forschungsfragen zu. So eröffnet beispielsweise die Stilometrie neue Möglichkeiten, die Anonymität eines mittelalterlichen Textzeugens aufzuheben und neue Thesen bezüglich Überlieferung und Organisation in Schreibstuben und Kanzleien aufzustellen. Dabei bewegen sich jedoch erstens die Forschungsfragen häufig um eine konkrete Identifizierung eines Stiles, einer Kanzlei oder einer Schreibschule und weniger über Makrosignale wie übergeordnete geographische, kulturelle oder sprachliche Dimensionen. Zweitens vernachlässigt die Arbeit am konkreten Korpus auch die Auseinandersetzung mit der Auswahl der geeigneten mathematischen Verfahren, die hinter den computergestützten Berechnungen stehen. Eine Auseinandersetzung auf der Makroebene über die Effektivität von beispielsweise einem Distanzmaß-Verfahren wie Burrows’s Delta auf die konkrete Textgattung der Urkunden findet bis heute noch nicht statt. Zwar gibt es Messungen über den Wirkungsgrad des Verfahrens für Lyrik und Prosa in lateinischer Sprache und kurzer oder auch fehlerhafter Texte, dennoch finden darin die spezifischen Eigenschaften von Urkunden nicht ausreichend Berücksichtigung: Individuelle, orthographische Signale, formelhafte Sprache und lückenhafte Überlieferungen sind nur einige der spezifischen Faktoren, welche noch nicht ausreichend für Burrow‘s Delta untersucht wurden (vgl. Eder 2013, 2015). 
            
            Betrachtet man das weitere Forschungsfeld zum Thema Autorschaftsattributionen werden aktuell vermehrt andere statistische Ansätze in Erwägung gezogen, die nicht zwingend auf Textfeatures wie Wortlängen oder inhaltlichen Merkmale wie Worthäufigkeiten basieren. Eine Fokussierung auf syntaktische oder semantische Zusammenhänge beispielsweise könnte bei der Untersuchung der Signale in Urkunden ebenso Distinktionen herausheben. Eine Loslösung vom Vector-Space-Model und Burrows Delta hin zu Topics und Neuronalen Netzen wurde zwar bisher auf dem Typus Urkunden noch nicht im großen Rahmen angewandt, dennoch könnte man dadurch potentiell das Manko der geringen Textlängen und der formelhaften Sprache umgehen. Neuere Ansätze haben sich zudem das Ziel gesetzt, durch eine Kombination mehrerer methodischer und textimmanenter Ansätze Merkmale herauszuarbeiten. Diese umschließen dann folglich nicht mehr nur die klassische Stilometrie, sondern auch die oben genannten syntaktischen und semantischen Features. Ob diese neuen Ansätze bei Urkunden Wirkung zeigen, soll ein Ziel dieser Dissertation werden.
            Ein grundsätzliches Problem bezüglich der Urkundentexte ist allerdings, dass diese häufig nicht von ihren originalen Textzeugen, sondern nur aus digitalisierten Editionen entnommen sind, die nicht dem eigentlichen Überlieferungstext entsprechen müssen. Wie sehr vertraut man Texten aus älteren Editionen vor 1945, in denen die Lachmannsche Editionstechnik angewendet wurde, mit der die 
                emendatio angeblicher Fremdeingriffe unbedarft angewendet und zudem schlecht oder gänzlich undokumentiert in den Druck gegeben wurde (vgl. Plachta 2006, S. 29)?
            
            Aus diesen Überlegungen heraus ist es naheliegend, die stilometrischen Verfahren einmal aus der Makroperspektive zu untersuchen: Anstatt sich mit einer These zu beschäftigen, die aus dem konkreten Material, vielleicht sogar aus dem close reading-Prozess selbst, entstanden ist, sollten die mathematischen Methoden an einer großen und diversen Menge an Urkundenmaterial untersucht werden. Eine mannigfaltige Auswahl bieten dazu mehrere Urkundenportale, wie monasterium.net, Cartae Europae Medii Aevi, Codice diplomatico della Lombardia medievale oder Telma. Dabei spielen zunächst weder die Urkundentypen, noch die zeitliche Dimension, in der die Urkunden entstanden sind, eine Rolle. Die Annahme ist eher, dass verschiedene Verfahren unterschiedlich starke Distinktionen der Urkunden unterstreichen. Ihre jeweilige Sensibilität für bestimmte Eigenschaften des Textmaterials gilt es herauszuarbeiten und methodisch zu begründen. So lassen sich Stärken und Schwächen der Methoden analysieren und gegenüberstellen, nicht im Sinne eines Rankings (‚bessere vs schlechtere Methode‘), sondern im Hinblick auf ihre Eignung für spezifische Korpora und Fragestellungen. Ein solcher Ansatz erlaubt es Forschenden nicht nur, eine fundiertere Entscheidung über die anzuwendende Methode zu treffen, sondern im Idealfall sogar, diese gezielt auf den eigenen Anwendungsfall abzustimmen und zu verfeinern.
            
        